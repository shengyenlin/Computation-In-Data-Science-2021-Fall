{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_observation = 200, num_features = 9):\n",
    "    beta_true = np.array(\n",
    "        [-1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n",
    "    ) #(10, )\n",
    "    X_0 = np.ones(num_observation).reshape(-1, 1)\n",
    "    X = np.random.normal(\n",
    "        size = (num_observation, num_features)\n",
    "    )\n",
    "    X = np.concatenate(\n",
    "        [X_0, X], axis = 1\n",
    "    ) #(200, 10)\n",
    "    z = np.dot(X, beta_true)\n",
    "    logit = np.exp(z) / (1 + np.exp(z))\n",
    "    y = np.random.binomial(1, logit)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandCraftLogisticRegression:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.copy()\n",
    "        self.y = y.copy()\n",
    "        self.grad_record = []\n",
    "        self.loss_record = []\n",
    "\n",
    "    def solve(self, max_iter, step, tol):\n",
    "        w = self.initialize_params()\n",
    "        loss = 10 ** 5\n",
    "        num_iter = 0\n",
    "        batch_size = 10\n",
    "\n",
    "        #adagrad\n",
    "        eps = 1e-12\n",
    "        g_w = np.ones(self.X.shape[1])\n",
    "\n",
    "        while num_iter < max_iter and loss > tol:\n",
    "            #Shuffle when each epoch begin\n",
    "            index = np.arange(self.X.shape[0])\n",
    "            np.random.shuffle(index)\n",
    "            X = self.X[index]\n",
    "            y = self.y[index]\n",
    "            for num_batch in range(int(self.X.shape[0] / batch_size)):\n",
    "                x_batch = X[num_batch * batch_size:(num_batch + 1) * batch_size]\n",
    "                y_batch = y[num_batch * batch_size:(num_batch + 1) * batch_size]\n",
    "\n",
    "                #implement adagrad\n",
    "                w_grad = self.compute_gradient(x_batch, y_batch, w)\n",
    "                g_w += w_grad ** 2\n",
    "\n",
    "                w = w - step * w_grad / np.sqrt(g_w + eps)\n",
    "\n",
    "\n",
    "            w_grad_norm = np.linalg.norm(self.compute_gradient(X, y, w), ord = 2)\n",
    "            self.grad_record.append(w_grad_norm)\n",
    "\n",
    "            #compute loss\n",
    "            y_pred = self.compute_logistic_value(self.X, w)\n",
    "            loss = self.compute_cross_entropy_loss(y_pred, self.y)\n",
    "            self.loss_record.append(loss)\n",
    "            acc = self.compute_accuracy(y_pred, self.y)\n",
    "            print(f\"Iteration {num_iter + 1}, BCE loss: {loss}, Acc: {round(acc, 4)}, Grad norm: {w_grad_norm}\")\n",
    "            num_iter += 1\n",
    "\n",
    "    def initialize_params(self):\n",
    "        w = np.random.rand(self.X.shape[1])\n",
    "        return w\n",
    "\n",
    "    def compute_gradient(self, X, y_true, w):\n",
    "        #print(w.shape)\n",
    "        y_pred = self.compute_logistic_value(X, w).flatten() #dim = (batch_size, )\n",
    "        pred_error = y_true - y_pred\n",
    "        w_grad = -np.dot(X.T, pred_error) #dim = (feature_size, )\n",
    "        return w_grad\n",
    "\n",
    "    def compute_logistic_value(self, X, w):\n",
    "        return self.sigmoid(np.matmul(X, w))\n",
    "\n",
    "    def compute_cross_entropy_loss(self, y_pred, y_true):\n",
    "        eps = 1e-12\n",
    "        y_pred = np.clip(y_pred, eps, 1-eps)\n",
    "        cross_entropy = -np.dot(y_true, np.log(y_pred)) - np.dot((1-y_true), np.log(1 - y_pred))\n",
    "        return cross_entropy\n",
    "\n",
    "    def compute_accuracy(self, y_pred, y_true):\n",
    "        accuracy = 1 - np.mean(np.abs(y_pred - y_true))\n",
    "        return accuracy\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        res = 1 / (1.0 + np.exp(-z))\n",
    "        return np.clip(res, 1e-6, 1 - (1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, BCE loss: 67.84860269568543, Acc: 0.8206, Grad norm: 17.402861392425912\n",
      "Iteration 2, BCE loss: 62.23237492037791, Acc: 0.8168, Grad norm: 12.188373384906829\n",
      "Iteration 3, BCE loss: 61.265057325292396, Acc: 0.8158, Grad norm: 10.793162914176648\n",
      "Iteration 4, BCE loss: 59.774445112799214, Acc: 0.8226, Grad norm: 8.50267705685398\n",
      "Iteration 5, BCE loss: 60.64346812027409, Acc: 0.8257, Grad norm: 9.81128318327609\n",
      "Iteration 6, BCE loss: 59.10033731668683, Acc: 0.8195, Grad norm: 7.065834962859622\n",
      "Iteration 7, BCE loss: 59.13340525775293, Acc: 0.8168, Grad norm: 6.900311387191687\n",
      "Iteration 8, BCE loss: 60.076900472084496, Acc: 0.828, Grad norm: 8.856589092116712\n",
      "Iteration 9, BCE loss: 59.879564352696335, Acc: 0.8208, Grad norm: 8.811372546134075\n",
      "Iteration 10, BCE loss: 59.10852090245559, Acc: 0.8233, Grad norm: 6.798787207808598\n",
      "Iteration 11, BCE loss: 58.699365272063986, Acc: 0.8239, Grad norm: 5.645900905115701\n",
      "Iteration 12, BCE loss: 59.06795011717581, Acc: 0.822, Grad norm: 6.7776390083953135\n",
      "Iteration 13, BCE loss: 58.14111281009224, Acc: 0.8249, Grad norm: 3.8491394143159914\n",
      "Iteration 14, BCE loss: 58.43272755299792, Acc: 0.8246, Grad norm: 4.689620659214812\n",
      "Iteration 15, BCE loss: 58.992835305734104, Acc: 0.8236, Grad norm: 6.954760024107491\n",
      "Iteration 16, BCE loss: 58.815682895374394, Acc: 0.8263, Grad norm: 6.27096397453365\n",
      "Iteration 17, BCE loss: 59.22200975462992, Acc: 0.82, Grad norm: 7.848700351834601\n",
      "Iteration 18, BCE loss: 58.38560070946497, Acc: 0.8223, Grad norm: 4.572668580427545\n",
      "Iteration 19, BCE loss: 58.501500483617704, Acc: 0.8187, Grad norm: 5.402650149160054\n",
      "Iteration 20, BCE loss: 58.060466243927266, Acc: 0.8229, Grad norm: 3.1781430181119985\n",
      "Iteration 21, BCE loss: 58.070729266284054, Acc: 0.8225, Grad norm: 3.442044895293273\n",
      "Iteration 22, BCE loss: 58.082095384123434, Acc: 0.8247, Grad norm: 3.5892745246585487\n",
      "Iteration 23, BCE loss: 57.90742878105135, Acc: 0.8194, Grad norm: 2.244547999855949\n",
      "Iteration 24, BCE loss: 57.93529813360712, Acc: 0.8213, Grad norm: 2.350390684525133\n",
      "Iteration 25, BCE loss: 58.41156741252199, Acc: 0.8172, Grad norm: 5.463212731753343\n",
      "Iteration 26, BCE loss: 58.25127685137456, Acc: 0.8202, Grad norm: 4.599923535865766\n",
      "Iteration 27, BCE loss: 58.00698155512973, Acc: 0.8201, Grad norm: 3.0451755564673175\n",
      "Iteration 28, BCE loss: 58.02996702745409, Acc: 0.8181, Grad norm: 3.20215283918966\n",
      "Iteration 29, BCE loss: 58.22325899558244, Acc: 0.8202, Grad norm: 4.357343818208356\n",
      "Iteration 30, BCE loss: 58.00533465992709, Acc: 0.8176, Grad norm: 3.192201124812089\n",
      "Iteration 31, BCE loss: 58.02572292443922, Acc: 0.8217, Grad norm: 3.4131266077990787\n",
      "Iteration 32, BCE loss: 57.97532725000814, Acc: 0.8203, Grad norm: 3.0050445302753346\n",
      "Iteration 33, BCE loss: 58.00606289661168, Acc: 0.8185, Grad norm: 3.1756029382504356\n",
      "Iteration 34, BCE loss: 58.240522118726425, Acc: 0.8188, Grad norm: 4.332112103222745\n",
      "Iteration 35, BCE loss: 57.91707594447705, Acc: 0.8218, Grad norm: 2.2909929496675856\n",
      "Iteration 36, BCE loss: 57.986498359391874, Acc: 0.8216, Grad norm: 2.6984649982839533\n",
      "Iteration 37, BCE loss: 58.1625416681258, Acc: 0.822, Grad norm: 3.9986654399946664\n",
      "Iteration 38, BCE loss: 57.875241718779435, Acc: 0.8205, Grad norm: 1.8835870127025252\n",
      "Iteration 39, BCE loss: 57.976258047163995, Acc: 0.8202, Grad norm: 2.8777637930089646\n",
      "Iteration 40, BCE loss: 57.97199760119152, Acc: 0.8207, Grad norm: 3.009466388056983\n",
      "Iteration 41, BCE loss: 58.03322756636658, Acc: 0.8211, Grad norm: 3.5419855139690957\n",
      "Iteration 42, BCE loss: 57.84168752931953, Acc: 0.8206, Grad norm: 1.8615511794803785\n",
      "Iteration 43, BCE loss: 57.81619439167646, Acc: 0.8187, Grad norm: 1.5090509214210583\n",
      "Iteration 44, BCE loss: 58.13866874353266, Acc: 0.8191, Grad norm: 4.020257056505148\n",
      "Iteration 45, BCE loss: 57.82820870281654, Acc: 0.8196, Grad norm: 1.6445246320479499\n",
      "Iteration 46, BCE loss: 57.89821535199775, Acc: 0.8185, Grad norm: 2.276919570331309\n",
      "Iteration 47, BCE loss: 57.83178250230471, Acc: 0.8193, Grad norm: 1.6737810708549925\n",
      "Iteration 48, BCE loss: 57.892211556903895, Acc: 0.8194, Grad norm: 2.1043399154004434\n",
      "Iteration 49, BCE loss: 57.875473409561494, Acc: 0.8192, Grad norm: 2.003926177483089\n",
      "Iteration 50, BCE loss: 57.90826027794941, Acc: 0.8187, Grad norm: 2.4230854319052657\n",
      "Iteration 51, BCE loss: 57.85877703023465, Acc: 0.8215, Grad norm: 1.9669584153356332\n",
      "Iteration 52, BCE loss: 57.892552431838595, Acc: 0.8192, Grad norm: 2.260991897971872\n",
      "Iteration 53, BCE loss: 57.831232370872264, Acc: 0.8208, Grad norm: 1.703312146216447\n",
      "Iteration 54, BCE loss: 57.92770266928659, Acc: 0.8184, Grad norm: 2.536371791979362\n",
      "Iteration 55, BCE loss: 58.03726856085077, Acc: 0.8172, Grad norm: 3.551631078860827\n",
      "Iteration 56, BCE loss: 57.83996611450105, Acc: 0.8188, Grad norm: 1.727456515931772\n",
      "Iteration 57, BCE loss: 57.79093450727049, Acc: 0.8194, Grad norm: 1.0247978183019573\n",
      "Iteration 58, BCE loss: 57.8969914964443, Acc: 0.8212, Grad norm: 2.4836954391893693\n",
      "Iteration 59, BCE loss: 57.877656004292646, Acc: 0.8196, Grad norm: 2.278943944205089\n",
      "Iteration 60, BCE loss: 57.832588355825976, Acc: 0.8197, Grad norm: 1.6979558890964406\n",
      "Iteration 61, BCE loss: 57.81203086838994, Acc: 0.8197, Grad norm: 1.4062562818978912\n",
      "Iteration 62, BCE loss: 57.864892121109875, Acc: 0.8194, Grad norm: 2.0559530930543346\n",
      "Iteration 63, BCE loss: 57.838027199014945, Acc: 0.8197, Grad norm: 1.8627934093881076\n",
      "Iteration 64, BCE loss: 57.99829678098241, Acc: 0.8212, Grad norm: 3.1494744580420453\n",
      "Iteration 65, BCE loss: 57.81414475429564, Acc: 0.819, Grad norm: 1.4440491668947566\n",
      "Iteration 66, BCE loss: 57.93965131353523, Acc: 0.8177, Grad norm: 2.794769995264354\n",
      "Iteration 67, BCE loss: 57.8515223344897, Acc: 0.8198, Grad norm: 1.8884354355858006\n",
      "Iteration 68, BCE loss: 57.827360528209624, Acc: 0.8197, Grad norm: 1.637112904403852\n",
      "Iteration 69, BCE loss: 57.81531552456863, Acc: 0.8207, Grad norm: 1.4150751043775303\n",
      "Iteration 70, BCE loss: 57.80259928138018, Acc: 0.8198, Grad norm: 1.2812398666535223\n",
      "Iteration 71, BCE loss: 57.823360287300815, Acc: 0.82, Grad norm: 1.6785273201318223\n",
      "Iteration 72, BCE loss: 57.797818662875535, Acc: 0.8202, Grad norm: 1.2149179902637466\n",
      "Iteration 73, BCE loss: 57.81187166173278, Acc: 0.8194, Grad norm: 1.4565797013337518\n",
      "Iteration 74, BCE loss: 57.78321656608654, Acc: 0.8198, Grad norm: 0.9663982471795799\n",
      "Iteration 75, BCE loss: 57.943415644933935, Acc: 0.8212, Grad norm: 2.7291852587039354\n",
      "Iteration 76, BCE loss: 57.84132799397694, Acc: 0.8202, Grad norm: 1.8656047307112227\n",
      "Iteration 77, BCE loss: 57.92903119666854, Acc: 0.8204, Grad norm: 2.432206067825166\n",
      "Iteration 78, BCE loss: 57.85903348977833, Acc: 0.8208, Grad norm: 2.016251755743251\n",
      "Iteration 79, BCE loss: 57.82987132307359, Acc: 0.8203, Grad norm: 1.738679885354635\n",
      "Iteration 80, BCE loss: 57.79323269999908, Acc: 0.8206, Grad norm: 1.0541642709406196\n",
      "Iteration 81, BCE loss: 57.798811842157484, Acc: 0.8194, Grad norm: 1.2582594940289658\n",
      "Iteration 82, BCE loss: 57.77880090633735, Acc: 0.8197, Grad norm: 0.9198960906363403\n",
      "Iteration 83, BCE loss: 57.82918342952664, Acc: 0.8203, Grad norm: 1.665849242563208\n",
      "Iteration 84, BCE loss: 57.824130343152916, Acc: 0.8211, Grad norm: 1.6182958507214498\n",
      "Iteration 85, BCE loss: 57.82598107938499, Acc: 0.82, Grad norm: 1.6264958125382778\n",
      "Iteration 86, BCE loss: 57.8759858074874, Acc: 0.8192, Grad norm: 2.2155314313461556\n",
      "Iteration 87, BCE loss: 57.846903042032984, Acc: 0.8194, Grad norm: 1.9970246734852146\n",
      "Iteration 88, BCE loss: 57.859021378565814, Acc: 0.8187, Grad norm: 2.1434569816249742\n",
      "Iteration 89, BCE loss: 57.85927089051948, Acc: 0.8197, Grad norm: 2.1398475212054806\n",
      "Iteration 90, BCE loss: 57.78916493513033, Acc: 0.8201, Grad norm: 1.0579527574080902\n",
      "Iteration 91, BCE loss: 57.81191726199252, Acc: 0.8195, Grad norm: 1.44266109072668\n",
      "Iteration 92, BCE loss: 57.80429803687632, Acc: 0.8204, Grad norm: 1.4171791801521847\n",
      "Iteration 93, BCE loss: 57.838756751134895, Acc: 0.819, Grad norm: 1.871991975805384\n",
      "Iteration 94, BCE loss: 57.838562416771296, Acc: 0.8197, Grad norm: 1.890340703391233\n",
      "Iteration 95, BCE loss: 57.839467253398986, Acc: 0.821, Grad norm: 1.8431064655111242\n",
      "Iteration 96, BCE loss: 57.809192418593625, Acc: 0.8207, Grad norm: 1.3636145115223277\n",
      "Iteration 97, BCE loss: 57.82602114323876, Acc: 0.8196, Grad norm: 1.6923034923836768\n",
      "Iteration 98, BCE loss: 57.796739717676886, Acc: 0.8194, Grad norm: 1.3218053630366993\n",
      "Iteration 99, BCE loss: 57.799841912862796, Acc: 0.8194, Grad norm: 1.2963529877475704\n",
      "Iteration 100, BCE loss: 57.820801549845946, Acc: 0.8207, Grad norm: 1.6593834378361354\n",
      "Iteration 101, BCE loss: 57.820396166890475, Acc: 0.8206, Grad norm: 1.5230669692769103\n",
      "Iteration 102, BCE loss: 57.83101220196845, Acc: 0.8206, Grad norm: 1.6937966046531936\n",
      "Iteration 103, BCE loss: 57.82393763149486, Acc: 0.8206, Grad norm: 1.6152092066805424\n",
      "Iteration 104, BCE loss: 57.80071431265222, Acc: 0.8213, Grad norm: 1.202646118563869\n",
      "Iteration 105, BCE loss: 57.77719929853191, Acc: 0.8209, Grad norm: 0.8078983785455924\n",
      "Iteration 106, BCE loss: 57.7945398767482, Acc: 0.8194, Grad norm: 1.230971548326516\n",
      "Iteration 107, BCE loss: 57.854476093280546, Acc: 0.8186, Grad norm: 2.0427853149221544\n",
      "Iteration 108, BCE loss: 57.7883970295168, Acc: 0.8204, Grad norm: 1.0748449766657873\n",
      "Iteration 109, BCE loss: 57.77390620551775, Acc: 0.8204, Grad norm: 0.7720561843761174\n",
      "Iteration 110, BCE loss: 57.79656345789391, Acc: 0.8201, Grad norm: 1.1871512650112326\n",
      "Iteration 111, BCE loss: 57.7941227132948, Acc: 0.8205, Grad norm: 1.1565228233373457\n",
      "Iteration 112, BCE loss: 57.81110812993413, Acc: 0.8203, Grad norm: 1.3776499120866141\n",
      "Iteration 113, BCE loss: 57.781882004842544, Acc: 0.8195, Grad norm: 0.945064148183834\n",
      "Iteration 114, BCE loss: 57.813461366821215, Acc: 0.8202, Grad norm: 1.505412432057933\n",
      "Iteration 115, BCE loss: 57.80050590731521, Acc: 0.8202, Grad norm: 1.2985076158326567\n",
      "Iteration 116, BCE loss: 57.812522912103944, Acc: 0.8203, Grad norm: 1.3875821122244836\n",
      "Iteration 117, BCE loss: 57.769223687316156, Acc: 0.8199, Grad norm: 0.613916576582703\n",
      "Iteration 118, BCE loss: 57.78964595239185, Acc: 0.8199, Grad norm: 1.0579747713155747\n",
      "Iteration 119, BCE loss: 57.81210253900567, Acc: 0.8195, Grad norm: 1.4280178605941225\n",
      "Iteration 120, BCE loss: 57.79353049296876, Acc: 0.8204, Grad norm: 1.2499312250904298\n",
      "Iteration 121, BCE loss: 57.82532935253769, Acc: 0.8196, Grad norm: 1.7075081822519504\n",
      "Iteration 122, BCE loss: 57.783042906572405, Acc: 0.8201, Grad norm: 0.925802447192622\n",
      "Iteration 123, BCE loss: 57.790720125062315, Acc: 0.8197, Grad norm: 1.0890914709188446\n",
      "Iteration 124, BCE loss: 57.80288407837773, Acc: 0.821, Grad norm: 1.2366050382427067\n",
      "Iteration 125, BCE loss: 57.76575893754125, Acc: 0.8202, Grad norm: 0.5156189902105618\n",
      "Iteration 126, BCE loss: 57.79826666130644, Acc: 0.8191, Grad norm: 1.2581686995095471\n",
      "Iteration 127, BCE loss: 57.788788196134014, Acc: 0.8199, Grad norm: 1.1304329493872138\n",
      "Iteration 128, BCE loss: 57.810851849632144, Acc: 0.8198, Grad norm: 1.4510153667765058\n",
      "Iteration 129, BCE loss: 57.78434076472172, Acc: 0.82, Grad norm: 1.0045747277814234\n",
      "Iteration 130, BCE loss: 57.78236006117221, Acc: 0.8193, Grad norm: 0.9152442347677222\n",
      "Iteration 131, BCE loss: 57.80600516412309, Acc: 0.8203, Grad norm: 1.437254344224344\n",
      "Iteration 132, BCE loss: 57.78296735047097, Acc: 0.8201, Grad norm: 0.9963567996461176\n",
      "Iteration 133, BCE loss: 57.779505810649624, Acc: 0.8193, Grad norm: 0.8540141340193594\n",
      "Iteration 134, BCE loss: 57.79212433376317, Acc: 0.8201, Grad norm: 1.1546289930627796\n",
      "Iteration 135, BCE loss: 57.78824414931711, Acc: 0.8202, Grad norm: 1.0989053821299706\n",
      "Iteration 136, BCE loss: 57.77264017296381, Acc: 0.8196, Grad norm: 0.728179881776574\n",
      "Iteration 137, BCE loss: 57.81294308976483, Acc: 0.8206, Grad norm: 1.533192300146221\n",
      "Iteration 138, BCE loss: 57.811868994296276, Acc: 0.82, Grad norm: 1.4846426381062183\n",
      "Iteration 139, BCE loss: 57.76537825343295, Acc: 0.8203, Grad norm: 0.5250144813606701\n",
      "Iteration 140, BCE loss: 57.7737840519224, Acc: 0.8201, Grad norm: 0.8406782287720064\n",
      "Iteration 141, BCE loss: 57.82154413791548, Acc: 0.8202, Grad norm: 1.6943187382384763\n",
      "Iteration 142, BCE loss: 57.82772253682607, Acc: 0.8198, Grad norm: 1.7900060304837415\n",
      "Iteration 143, BCE loss: 57.79949121791495, Acc: 0.8193, Grad norm: 1.349638637829697\n",
      "Iteration 144, BCE loss: 57.784020927462024, Acc: 0.8198, Grad norm: 1.0890920693678423\n",
      "Iteration 145, BCE loss: 57.78137300556, Acc: 0.8197, Grad norm: 0.9581891242186467\n",
      "Iteration 146, BCE loss: 57.7717815292778, Acc: 0.82, Grad norm: 0.7083435326589667\n",
      "Iteration 147, BCE loss: 57.79259481459918, Acc: 0.8193, Grad norm: 1.2726490270867057\n",
      "Iteration 148, BCE loss: 57.76796706091132, Acc: 0.8196, Grad norm: 0.6535167015651824\n",
      "Iteration 149, BCE loss: 57.78898947303193, Acc: 0.8193, Grad norm: 1.1079543973639199\n",
      "Iteration 150, BCE loss: 57.79487983856139, Acc: 0.819, Grad norm: 1.2016940294913967\n",
      "Iteration 151, BCE loss: 57.80327146031262, Acc: 0.8191, Grad norm: 1.2943984068066485\n",
      "Iteration 152, BCE loss: 57.79505429888914, Acc: 0.8195, Grad norm: 1.1873250384893135\n",
      "Iteration 153, BCE loss: 57.81253389456181, Acc: 0.82, Grad norm: 1.5135904529695128\n",
      "Iteration 154, BCE loss: 57.792983736984894, Acc: 0.8192, Grad norm: 1.1143347107273218\n",
      "Iteration 155, BCE loss: 57.76944617398222, Acc: 0.8198, Grad norm: 0.66912694850216\n",
      "Iteration 156, BCE loss: 57.78448364879118, Acc: 0.8194, Grad norm: 0.9984388657757958\n",
      "Iteration 157, BCE loss: 57.79585989978319, Acc: 0.8195, Grad norm: 1.2774895930583665\n",
      "Iteration 158, BCE loss: 57.78283250800594, Acc: 0.8193, Grad norm: 0.9615964942681603\n",
      "Iteration 159, BCE loss: 57.77559645073786, Acc: 0.8195, Grad norm: 0.7945588503568553\n",
      "Iteration 160, BCE loss: 57.806104724376176, Acc: 0.8198, Grad norm: 1.397933710692745\n",
      "Iteration 161, BCE loss: 57.79533313638209, Acc: 0.8193, Grad norm: 1.22476690029196\n",
      "Iteration 162, BCE loss: 57.770260434305634, Acc: 0.819, Grad norm: 0.6712099726057651\n",
      "Iteration 163, BCE loss: 57.772147452543386, Acc: 0.8193, Grad norm: 0.7435359226668088\n",
      "Iteration 164, BCE loss: 57.77510402167876, Acc: 0.8193, Grad norm: 0.8208720352746934\n",
      "Iteration 165, BCE loss: 57.782809943795954, Acc: 0.8199, Grad norm: 0.9977615732824162\n",
      "Iteration 166, BCE loss: 57.76530480237818, Acc: 0.8198, Grad norm: 0.5725672676220446\n",
      "Iteration 167, BCE loss: 57.76521352407555, Acc: 0.8199, Grad norm: 0.5851676897167942\n",
      "Iteration 168, BCE loss: 57.7681288578233, Acc: 0.8201, Grad norm: 0.6045742965002364\n",
      "Iteration 169, BCE loss: 57.80641723558038, Acc: 0.8208, Grad norm: 1.3713278322431337\n",
      "Iteration 170, BCE loss: 57.80704859339072, Acc: 0.8208, Grad norm: 1.4080334691616518\n",
      "Iteration 171, BCE loss: 57.77530307603972, Acc: 0.82, Grad norm: 0.8140885690444223\n",
      "Iteration 172, BCE loss: 57.769144826548235, Acc: 0.8203, Grad norm: 0.6560339695007075\n",
      "Iteration 173, BCE loss: 57.77531394076916, Acc: 0.8203, Grad norm: 0.8536974169863276\n",
      "Iteration 174, BCE loss: 57.789533449532186, Acc: 0.8196, Grad norm: 1.192455229606991\n",
      "Iteration 175, BCE loss: 57.76584073908701, Acc: 0.8202, Grad norm: 0.5598838492611754\n",
      "Iteration 176, BCE loss: 57.765045179187375, Acc: 0.82, Grad norm: 0.5505350728919762\n",
      "Iteration 177, BCE loss: 57.771898040267224, Acc: 0.8198, Grad norm: 0.7579798710758933\n",
      "Iteration 178, BCE loss: 57.76825518549253, Acc: 0.8196, Grad norm: 0.5656722541295623\n",
      "Iteration 179, BCE loss: 57.77683793321538, Acc: 0.8197, Grad norm: 0.8062734484781472\n",
      "Iteration 180, BCE loss: 57.77246012432224, Acc: 0.8193, Grad norm: 0.7557231652160543\n",
      "Iteration 181, BCE loss: 57.780404379330534, Acc: 0.8189, Grad norm: 1.014011016368752\n",
      "Iteration 182, BCE loss: 57.77476610199261, Acc: 0.8192, Grad norm: 0.8701716222556442\n",
      "Iteration 183, BCE loss: 57.77062952182601, Acc: 0.8196, Grad norm: 0.7366822552805593\n",
      "Iteration 184, BCE loss: 57.77279358642144, Acc: 0.8191, Grad norm: 0.7848038750215551\n",
      "Iteration 185, BCE loss: 57.777529120900844, Acc: 0.8198, Grad norm: 0.9275170513152761\n",
      "Iteration 186, BCE loss: 57.76190536284617, Acc: 0.8195, Grad norm: 0.4260164821662413\n",
      "Iteration 187, BCE loss: 57.77245840220516, Acc: 0.8198, Grad norm: 0.7134680215029016\n",
      "Iteration 188, BCE loss: 57.7653557692005, Acc: 0.8201, Grad norm: 0.5585900567628502\n",
      "Iteration 189, BCE loss: 57.767795889802784, Acc: 0.8201, Grad norm: 0.6798660340941457\n",
      "Iteration 190, BCE loss: 57.769802097968935, Acc: 0.8203, Grad norm: 0.6532549347652046\n",
      "Iteration 191, BCE loss: 57.780222829304, Acc: 0.8206, Grad norm: 0.8749432208465806\n",
      "Iteration 192, BCE loss: 57.7703961273933, Acc: 0.8203, Grad norm: 0.6784465700501215\n",
      "Iteration 193, BCE loss: 57.76867385495335, Acc: 0.8199, Grad norm: 0.65895632080197\n",
      "Iteration 194, BCE loss: 57.77134506156092, Acc: 0.8198, Grad norm: 0.7179093870294904\n",
      "Iteration 195, BCE loss: 57.77579577070294, Acc: 0.82, Grad norm: 0.826573272036241\n",
      "Iteration 196, BCE loss: 57.79985713202719, Acc: 0.8201, Grad norm: 1.231619865242\n",
      "Iteration 197, BCE loss: 57.77707025760432, Acc: 0.8198, Grad norm: 0.842294443571898\n",
      "Iteration 198, BCE loss: 57.778796993273346, Acc: 0.8194, Grad norm: 0.9732565012721149\n",
      "Iteration 199, BCE loss: 57.783077203524606, Acc: 0.8195, Grad norm: 1.0256576487631015\n",
      "Iteration 200, BCE loss: 57.7790115021171, Acc: 0.8194, Grad norm: 0.936692692422152\n",
      "Iteration 201, BCE loss: 57.79031385105406, Acc: 0.8187, Grad norm: 1.174172298137568\n",
      "Iteration 202, BCE loss: 57.77956076055413, Acc: 0.8195, Grad norm: 0.9434716877050578\n",
      "Iteration 203, BCE loss: 57.79350397501985, Acc: 0.8193, Grad norm: 1.1926870915320507\n",
      "Iteration 204, BCE loss: 57.78870701152819, Acc: 0.8191, Grad norm: 1.1580351175270591\n",
      "Iteration 205, BCE loss: 57.79099114349631, Acc: 0.8197, Grad norm: 1.2041981579347714\n",
      "Iteration 206, BCE loss: 57.770937286849, Acc: 0.8195, Grad norm: 0.7292600206823509\n",
      "Iteration 207, BCE loss: 57.76759504922411, Acc: 0.8198, Grad norm: 0.6320841705347633\n",
      "Iteration 208, BCE loss: 57.78062486050156, Acc: 0.8202, Grad norm: 1.0099821941798426\n",
      "Iteration 209, BCE loss: 57.7793316385623, Acc: 0.8203, Grad norm: 0.9844006426691655\n",
      "Iteration 210, BCE loss: 57.76435004565829, Acc: 0.8198, Grad norm: 0.5236309879721677\n",
      "Iteration 211, BCE loss: 57.76500014764637, Acc: 0.8194, Grad norm: 0.5483102480976995\n",
      "Iteration 212, BCE loss: 57.76499935249643, Acc: 0.8197, Grad norm: 0.4907233211066152\n",
      "Iteration 213, BCE loss: 57.763926542652904, Acc: 0.8198, Grad norm: 0.4837970672627076\n",
      "Iteration 214, BCE loss: 57.77176439538394, Acc: 0.82, Grad norm: 0.7639237880928639\n",
      "Iteration 215, BCE loss: 57.80391677087196, Acc: 0.8199, Grad norm: 1.4229570716766264\n",
      "Iteration 216, BCE loss: 57.77726549188161, Acc: 0.8196, Grad norm: 0.8578216525129454\n",
      "Iteration 217, BCE loss: 57.77572358611575, Acc: 0.8196, Grad norm: 0.768425879824926\n",
      "Iteration 218, BCE loss: 57.78275307367741, Acc: 0.8204, Grad norm: 1.0336876041089258\n",
      "Iteration 219, BCE loss: 57.77595383556626, Acc: 0.82, Grad norm: 0.834482014573678\n",
      "Iteration 220, BCE loss: 57.7647749654915, Acc: 0.8196, Grad norm: 0.49784204504987783\n",
      "Iteration 221, BCE loss: 57.76505681122153, Acc: 0.8197, Grad norm: 0.5183796225231943\n",
      "Iteration 222, BCE loss: 57.75983493527713, Acc: 0.8194, Grad norm: 0.28219122077333547\n",
      "Iteration 223, BCE loss: 57.76326612374908, Acc: 0.8199, Grad norm: 0.501933349341198\n",
      "Iteration 224, BCE loss: 57.768259972940825, Acc: 0.8197, Grad norm: 0.6904869487243683\n",
      "Iteration 225, BCE loss: 57.76616977937056, Acc: 0.8196, Grad norm: 0.589229914639245\n",
      "Iteration 226, BCE loss: 57.76933345598193, Acc: 0.8193, Grad norm: 0.6772572540619026\n",
      "Iteration 227, BCE loss: 57.770323680645035, Acc: 0.8194, Grad norm: 0.6841767582467697\n",
      "Iteration 228, BCE loss: 57.77688205457365, Acc: 0.8187, Grad norm: 0.879708202494878\n",
      "Iteration 229, BCE loss: 57.7736459681064, Acc: 0.8191, Grad norm: 0.7914851012021774\n",
      "Iteration 230, BCE loss: 57.76721466670976, Acc: 0.8195, Grad norm: 0.6037748992955195\n",
      "Iteration 231, BCE loss: 57.766954705872124, Acc: 0.8199, Grad norm: 0.6251991950408113\n",
      "Iteration 232, BCE loss: 57.771777284768554, Acc: 0.8202, Grad norm: 0.763264349646088\n",
      "Iteration 233, BCE loss: 57.77884571977535, Acc: 0.8205, Grad norm: 0.9313593595310663\n",
      "Iteration 234, BCE loss: 57.76944699896572, Acc: 0.8203, Grad norm: 0.6757847463978757\n",
      "Iteration 235, BCE loss: 57.76145537518363, Acc: 0.8199, Grad norm: 0.3864667786293365\n",
      "Iteration 236, BCE loss: 57.762156020323125, Acc: 0.8202, Grad norm: 0.39053389686483037\n",
      "Iteration 237, BCE loss: 57.761192348299005, Acc: 0.8198, Grad norm: 0.3433082091272129\n",
      "Iteration 238, BCE loss: 57.77213085816604, Acc: 0.8206, Grad norm: 0.7703499683885111\n",
      "Iteration 239, BCE loss: 57.764587203398875, Acc: 0.8201, Grad norm: 0.5122878689632676\n",
      "Iteration 240, BCE loss: 57.76767746938563, Acc: 0.8202, Grad norm: 0.6418118758308435\n",
      "Iteration 241, BCE loss: 57.76684143202431, Acc: 0.8204, Grad norm: 0.5974496928167596\n",
      "Iteration 242, BCE loss: 57.761887574807936, Acc: 0.8199, Grad norm: 0.4110288627182821\n",
      "Iteration 243, BCE loss: 57.772559919552236, Acc: 0.8199, Grad norm: 0.7853173083105325\n",
      "Iteration 244, BCE loss: 57.77612742492371, Acc: 0.8202, Grad norm: 0.9050466506864817\n",
      "Iteration 245, BCE loss: 57.766301129409655, Acc: 0.8202, Grad norm: 0.6031333192454172\n",
      "Iteration 246, BCE loss: 57.767274373004284, Acc: 0.8199, Grad norm: 0.6166890966412024\n",
      "Iteration 247, BCE loss: 57.76546803301573, Acc: 0.8201, Grad norm: 0.569556220339176\n",
      "Iteration 248, BCE loss: 57.767069202181645, Acc: 0.8196, Grad norm: 0.627135027185799\n",
      "Iteration 249, BCE loss: 57.766752508755445, Acc: 0.8195, Grad norm: 0.6099671288831009\n",
      "Iteration 250, BCE loss: 57.76464918224215, Acc: 0.8199, Grad norm: 0.558085023394404\n",
      "Iteration 251, BCE loss: 57.76349711035079, Acc: 0.8197, Grad norm: 0.46299575325599723\n",
      "Iteration 252, BCE loss: 57.782125289971624, Acc: 0.8202, Grad norm: 1.0065480310302677\n",
      "Iteration 253, BCE loss: 57.79064243397302, Acc: 0.8201, Grad norm: 1.215528816884391\n",
      "Iteration 254, BCE loss: 57.77036730030999, Acc: 0.8198, Grad norm: 0.7428452166044798\n",
      "Iteration 255, BCE loss: 57.76405770011997, Acc: 0.8198, Grad norm: 0.49875941808211527\n",
      "Iteration 256, BCE loss: 57.76402854728656, Acc: 0.8197, Grad norm: 0.5220029627275374\n",
      "Iteration 257, BCE loss: 57.76517227755719, Acc: 0.8196, Grad norm: 0.5599959916004943\n",
      "Iteration 258, BCE loss: 57.763081914246214, Acc: 0.8196, Grad norm: 0.4755261668117247\n",
      "Iteration 259, BCE loss: 57.76755070297392, Acc: 0.8197, Grad norm: 0.6548809884758292\n",
      "Iteration 260, BCE loss: 57.772051380186745, Acc: 0.8202, Grad norm: 0.8266391933933238\n",
      "Iteration 261, BCE loss: 57.76871817033597, Acc: 0.8199, Grad norm: 0.6828102523515523\n",
      "Iteration 262, BCE loss: 57.77932072038945, Acc: 0.8195, Grad norm: 0.9425977158360748\n",
      "Iteration 263, BCE loss: 57.76346432050255, Acc: 0.8196, Grad norm: 0.432202307179571\n",
      "Iteration 264, BCE loss: 57.77950677055509, Acc: 0.8192, Grad norm: 0.9225944577882216\n",
      "Iteration 265, BCE loss: 57.788250851156306, Acc: 0.8195, Grad norm: 1.189135720414901\n",
      "Iteration 266, BCE loss: 57.7853670570658, Acc: 0.82, Grad norm: 1.1159151257891669\n",
      "Iteration 267, BCE loss: 57.76678009340398, Acc: 0.8198, Grad norm: 0.6184021775982902\n",
      "Iteration 268, BCE loss: 57.763542685225275, Acc: 0.8196, Grad norm: 0.5096718431085832\n",
      "Iteration 269, BCE loss: 57.763922965870776, Acc: 0.8197, Grad norm: 0.5441111031872576\n",
      "Iteration 270, BCE loss: 57.763335461441926, Acc: 0.8196, Grad norm: 0.4676037220667524\n",
      "Iteration 271, BCE loss: 57.77048087509031, Acc: 0.8197, Grad norm: 0.7288183440056142\n",
      "Iteration 272, BCE loss: 57.77257874836734, Acc: 0.8195, Grad norm: 0.7903114775779813\n",
      "Iteration 273, BCE loss: 57.7681175692596, Acc: 0.8191, Grad norm: 0.6411522613170705\n",
      "Iteration 274, BCE loss: 57.763211600609246, Acc: 0.8193, Grad norm: 0.46066581987248784\n",
      "Iteration 275, BCE loss: 57.765261910952546, Acc: 0.8195, Grad norm: 0.5136744970296829\n",
      "Iteration 276, BCE loss: 57.761822430419926, Acc: 0.8194, Grad norm: 0.392930121494228\n",
      "Iteration 277, BCE loss: 57.76685571983384, Acc: 0.8191, Grad norm: 0.6438893550702858\n",
      "Iteration 278, BCE loss: 57.764450532429194, Acc: 0.8192, Grad norm: 0.5141077472976859\n",
      "Iteration 279, BCE loss: 57.768592592347346, Acc: 0.8193, Grad norm: 0.6776609970432975\n",
      "Iteration 280, BCE loss: 57.77213903023036, Acc: 0.8192, Grad norm: 0.8028503434707969\n",
      "Iteration 281, BCE loss: 57.769970571857584, Acc: 0.8198, Grad norm: 0.6930016007590017\n",
      "Iteration 282, BCE loss: 57.76448372232866, Acc: 0.8197, Grad norm: 0.5334211433299546\n",
      "Iteration 283, BCE loss: 57.76642064998435, Acc: 0.8196, Grad norm: 0.56025721300547\n",
      "Iteration 284, BCE loss: 57.770629005478554, Acc: 0.8198, Grad norm: 0.7424657544406259\n",
      "Iteration 285, BCE loss: 57.764492191360446, Acc: 0.8199, Grad norm: 0.5452383278999371\n",
      "Iteration 286, BCE loss: 57.766462696168645, Acc: 0.8201, Grad norm: 0.6214205702067911\n",
      "Iteration 287, BCE loss: 57.762643108241, Acc: 0.8197, Grad norm: 0.44058951149219877\n",
      "Iteration 288, BCE loss: 57.76873128464871, Acc: 0.8197, Grad norm: 0.6949364076489557\n",
      "Iteration 289, BCE loss: 57.77454944788093, Acc: 0.8198, Grad norm: 0.8755220389142718\n",
      "Iteration 290, BCE loss: 57.77076759705122, Acc: 0.82, Grad norm: 0.7711096784983653\n",
      "Iteration 291, BCE loss: 57.784020797112085, Acc: 0.8206, Grad norm: 1.0725256379726489\n",
      "Iteration 292, BCE loss: 57.77100777073183, Acc: 0.8203, Grad norm: 0.7658125476995538\n",
      "Iteration 293, BCE loss: 57.77639181911401, Acc: 0.8202, Grad norm: 0.9253466253782555\n",
      "Iteration 294, BCE loss: 57.780353522677935, Acc: 0.8203, Grad norm: 1.0302265841617866\n",
      "Iteration 295, BCE loss: 57.76790911574059, Acc: 0.8199, Grad norm: 0.613649529652996\n",
      "Iteration 296, BCE loss: 57.77254499744174, Acc: 0.8196, Grad norm: 0.7686161924993933\n",
      "Iteration 297, BCE loss: 57.76515499477408, Acc: 0.8197, Grad norm: 0.5413705775505243\n",
      "Iteration 298, BCE loss: 57.76466614078056, Acc: 0.8198, Grad norm: 0.4895613491955131\n",
      "Iteration 299, BCE loss: 57.765660302072504, Acc: 0.8197, Grad norm: 0.5152572268996111\n",
      "Iteration 300, BCE loss: 57.77255173730008, Acc: 0.8197, Grad norm: 0.6986092483550848\n",
      "Iteration 301, BCE loss: 57.76817433284556, Acc: 0.8194, Grad norm: 0.6185535140328725\n",
      "Iteration 302, BCE loss: 57.76542080440897, Acc: 0.8196, Grad norm: 0.513850646743237\n",
      "Iteration 303, BCE loss: 57.78054509701836, Acc: 0.8199, Grad norm: 1.005427633665257\n",
      "Iteration 304, BCE loss: 57.775729542807916, Acc: 0.82, Grad norm: 0.9271683835733676\n",
      "Iteration 305, BCE loss: 57.772113547130274, Acc: 0.8196, Grad norm: 0.8312558824731459\n",
      "Iteration 306, BCE loss: 57.78184292560992, Acc: 0.8199, Grad norm: 1.0722646856464257\n",
      "Iteration 307, BCE loss: 57.78466687456563, Acc: 0.8201, Grad norm: 1.10377116577717\n",
      "Iteration 308, BCE loss: 57.780233118020895, Acc: 0.8196, Grad norm: 1.006867838371103\n",
      "Iteration 309, BCE loss: 57.76431899767189, Acc: 0.8198, Grad norm: 0.5409769469440474\n",
      "Iteration 310, BCE loss: 57.76550375929483, Acc: 0.8193, Grad norm: 0.5825451548110419\n",
      "Iteration 311, BCE loss: 57.75915020759636, Acc: 0.8195, Grad norm: 0.2581054818374597\n",
      "Iteration 312, BCE loss: 57.76137005539364, Acc: 0.82, Grad norm: 0.40518150573168193\n",
      "Iteration 313, BCE loss: 57.762034346613405, Acc: 0.8201, Grad norm: 0.4017502197650262\n",
      "Iteration 314, BCE loss: 57.76459122994285, Acc: 0.8201, Grad norm: 0.5243574390894069\n",
      "Iteration 315, BCE loss: 57.77394001352613, Acc: 0.8199, Grad norm: 0.8170271126912257\n",
      "Iteration 316, BCE loss: 57.77543315138789, Acc: 0.82, Grad norm: 0.9065213782579374\n",
      "Iteration 317, BCE loss: 57.773728105076316, Acc: 0.8197, Grad norm: 0.8378333514113719\n",
      "Iteration 318, BCE loss: 57.77338467075188, Acc: 0.8201, Grad norm: 0.8128677918278444\n",
      "Iteration 319, BCE loss: 57.76961577785019, Acc: 0.8193, Grad norm: 0.7237475189646878\n",
      "Iteration 320, BCE loss: 57.764596358155984, Acc: 0.8196, Grad norm: 0.5370863639512513\n",
      "Iteration 321, BCE loss: 57.7656077943464, Acc: 0.8196, Grad norm: 0.59266982403325\n",
      "Iteration 322, BCE loss: 57.776462256245146, Acc: 0.8192, Grad norm: 0.9322028893815574\n",
      "Iteration 323, BCE loss: 57.770654853643066, Acc: 0.8193, Grad norm: 0.7659507284634375\n",
      "Iteration 324, BCE loss: 57.76456528573223, Acc: 0.8193, Grad norm: 0.518300448543281\n",
      "Iteration 325, BCE loss: 57.764936889068736, Acc: 0.8195, Grad norm: 0.5184923107653328\n",
      "Iteration 326, BCE loss: 57.77175779316381, Acc: 0.8193, Grad norm: 0.7024038862957396\n",
      "Iteration 327, BCE loss: 57.76874744118139, Acc: 0.8194, Grad norm: 0.6127993207631216\n",
      "Iteration 328, BCE loss: 57.76419754876005, Acc: 0.8195, Grad norm: 0.47495460014094504\n",
      "Iteration 329, BCE loss: 57.7646814799768, Acc: 0.82, Grad norm: 0.5003710089132031\n",
      "Iteration 330, BCE loss: 57.7636630050209, Acc: 0.8195, Grad norm: 0.47542625835367175\n",
      "Iteration 331, BCE loss: 57.76260298283932, Acc: 0.8198, Grad norm: 0.42554028220273266\n",
      "Iteration 332, BCE loss: 57.76748709765469, Acc: 0.8195, Grad norm: 0.6101540241894403\n",
      "Iteration 333, BCE loss: 57.765657842856754, Acc: 0.8196, Grad norm: 0.5421287538664404\n",
      "Iteration 334, BCE loss: 57.763746187069984, Acc: 0.8198, Grad norm: 0.4948821335924085\n",
      "Iteration 335, BCE loss: 57.760522509829926, Acc: 0.8197, Grad norm: 0.35996125693541364\n",
      "Iteration 336, BCE loss: 57.76517198161031, Acc: 0.8199, Grad norm: 0.558223542464731\n",
      "Iteration 337, BCE loss: 57.7622677465328, Acc: 0.8198, Grad norm: 0.4302583879017172\n",
      "Iteration 338, BCE loss: 57.77514495944706, Acc: 0.8197, Grad norm: 0.8708676545201398\n",
      "Iteration 339, BCE loss: 57.77548680153486, Acc: 0.8201, Grad norm: 0.8882816672935904\n",
      "Iteration 340, BCE loss: 57.76364748304357, Acc: 0.8201, Grad norm: 0.5255987537601088\n",
      "Iteration 341, BCE loss: 57.763824666407615, Acc: 0.8198, Grad norm: 0.5353536690950983\n",
      "Iteration 342, BCE loss: 57.76396376491087, Acc: 0.8197, Grad norm: 0.4912499806519048\n",
      "Iteration 343, BCE loss: 57.76663365460994, Acc: 0.8195, Grad norm: 0.5960951122786897\n",
      "Iteration 344, BCE loss: 57.774849773330985, Acc: 0.8196, Grad norm: 0.8541189531096103\n",
      "Iteration 345, BCE loss: 57.769205771892615, Acc: 0.8195, Grad norm: 0.6717098500558196\n",
      "Iteration 346, BCE loss: 57.7676304411988, Acc: 0.8199, Grad norm: 0.6373664598716043\n",
      "Iteration 347, BCE loss: 57.76882002835765, Acc: 0.8197, Grad norm: 0.6884928021913789\n",
      "Iteration 348, BCE loss: 57.762240358455315, Acc: 0.8193, Grad norm: 0.39732047756176064\n",
      "Iteration 349, BCE loss: 57.76225857347276, Acc: 0.8192, Grad norm: 0.4116661695327035\n",
      "Iteration 350, BCE loss: 57.76321272362064, Acc: 0.8192, Grad norm: 0.476900167085793\n",
      "Iteration 351, BCE loss: 57.77044136748397, Acc: 0.8193, Grad norm: 0.746049309413502\n",
      "Iteration 352, BCE loss: 57.771085248070385, Acc: 0.8191, Grad norm: 0.723068058666607\n",
      "Iteration 353, BCE loss: 57.77276416867137, Acc: 0.8188, Grad norm: 0.7830758286925326\n",
      "Iteration 354, BCE loss: 57.7768399649602, Acc: 0.8187, Grad norm: 0.9110638577631295\n",
      "Iteration 355, BCE loss: 57.7714466024445, Acc: 0.8189, Grad norm: 0.7840859713076734\n",
      "Iteration 356, BCE loss: 57.761412388593314, Acc: 0.8192, Grad norm: 0.4061518773289128\n",
      "Iteration 357, BCE loss: 57.77030571830411, Acc: 0.8191, Grad norm: 0.7263506803443873\n",
      "Iteration 358, BCE loss: 57.76754970314293, Acc: 0.8192, Grad norm: 0.637841323446987\n",
      "Iteration 359, BCE loss: 57.76074140674751, Acc: 0.8197, Grad norm: 0.32344805066422777\n",
      "Iteration 360, BCE loss: 57.76567269876559, Acc: 0.8196, Grad norm: 0.5737728035800426\n",
      "Iteration 361, BCE loss: 57.768043081714104, Acc: 0.8198, Grad norm: 0.6525238448485895\n",
      "Iteration 362, BCE loss: 57.76880710143603, Acc: 0.8193, Grad norm: 0.6723337135111994\n",
      "Iteration 363, BCE loss: 57.76781774087597, Acc: 0.8195, Grad norm: 0.6363865167227258\n",
      "Iteration 364, BCE loss: 57.771605124221225, Acc: 0.8197, Grad norm: 0.7886149869224998\n",
      "Iteration 365, BCE loss: 57.76164788248632, Acc: 0.8198, Grad norm: 0.43708339322445433\n",
      "Iteration 366, BCE loss: 57.76215990140255, Acc: 0.8199, Grad norm: 0.4394384826736502\n",
      "Iteration 367, BCE loss: 57.76130725502536, Acc: 0.8198, Grad norm: 0.38704033180129116\n",
      "Iteration 368, BCE loss: 57.76197394653512, Acc: 0.82, Grad norm: 0.4317944079723352\n",
      "Iteration 369, BCE loss: 57.77342461839611, Acc: 0.8201, Grad norm: 0.8349639193512601\n",
      "Iteration 370, BCE loss: 57.76605179989869, Acc: 0.82, Grad norm: 0.6002379522572215\n",
      "Iteration 371, BCE loss: 57.75943988065691, Acc: 0.8199, Grad norm: 0.2874468210645776\n",
      "Iteration 372, BCE loss: 57.76229806658713, Acc: 0.82, Grad norm: 0.46689603873934205\n",
      "Iteration 373, BCE loss: 57.76406343166795, Acc: 0.8201, Grad norm: 0.5423373091940596\n",
      "Iteration 374, BCE loss: 57.760929610542746, Acc: 0.8201, Grad norm: 0.34551179028319\n",
      "Iteration 375, BCE loss: 57.76166084412089, Acc: 0.8196, Grad norm: 0.3901177532299649\n",
      "Iteration 376, BCE loss: 57.76528035038366, Acc: 0.8195, Grad norm: 0.5398307302416168\n",
      "Iteration 377, BCE loss: 57.76530246112748, Acc: 0.8197, Grad norm: 0.5530029518274892\n",
      "Iteration 378, BCE loss: 57.76500967020232, Acc: 0.8196, Grad norm: 0.586704353452391\n",
      "Iteration 379, BCE loss: 57.76460372575162, Acc: 0.8196, Grad norm: 0.535320098670096\n",
      "Iteration 380, BCE loss: 57.76502986887837, Acc: 0.8195, Grad norm: 0.5309445493433371\n",
      "Iteration 381, BCE loss: 57.76462167042767, Acc: 0.8194, Grad norm: 0.5149214275957428\n",
      "Iteration 382, BCE loss: 57.76457801889116, Acc: 0.8198, Grad norm: 0.5044249970926249\n",
      "Iteration 383, BCE loss: 57.76001976417962, Acc: 0.8196, Grad norm: 0.3378935043584322\n",
      "Iteration 384, BCE loss: 57.76322522220291, Acc: 0.8196, Grad norm: 0.5091946558874536\n",
      "Iteration 385, BCE loss: 57.761237454846345, Acc: 0.8198, Grad norm: 0.3989793641014861\n",
      "Iteration 386, BCE loss: 57.762299841972464, Acc: 0.8197, Grad norm: 0.44665764491697296\n",
      "Iteration 387, BCE loss: 57.76025419745986, Acc: 0.8194, Grad norm: 0.33059719368755247\n",
      "Iteration 388, BCE loss: 57.76277356922223, Acc: 0.8197, Grad norm: 0.4797448323712244\n",
      "Iteration 389, BCE loss: 57.7620070131846, Acc: 0.8195, Grad norm: 0.42195812945777444\n",
      "Iteration 390, BCE loss: 57.75972524913828, Acc: 0.8197, Grad norm: 0.27476048459489116\n",
      "Iteration 391, BCE loss: 57.75963586519124, Acc: 0.8195, Grad norm: 0.2792088535605831\n",
      "Iteration 392, BCE loss: 57.76287708038063, Acc: 0.82, Grad norm: 0.491361803637454\n",
      "Iteration 393, BCE loss: 57.76631589196322, Acc: 0.8197, Grad norm: 0.622801228192641\n",
      "Iteration 394, BCE loss: 57.77189573071111, Acc: 0.8198, Grad norm: 0.7975778786142557\n",
      "Iteration 395, BCE loss: 57.767292495844, Acc: 0.8197, Grad norm: 0.632466877907142\n",
      "Iteration 396, BCE loss: 57.76307569653926, Acc: 0.82, Grad norm: 0.48393416311756776\n",
      "Iteration 397, BCE loss: 57.76216271796778, Acc: 0.8198, Grad norm: 0.4219636256443988\n",
      "Iteration 398, BCE loss: 57.7626666624435, Acc: 0.8198, Grad norm: 0.44015443842381263\n",
      "Iteration 399, BCE loss: 57.764458617077665, Acc: 0.8195, Grad norm: 0.5435354993992595\n",
      "Iteration 400, BCE loss: 57.76766244637311, Acc: 0.8198, Grad norm: 0.6767084539270429\n",
      "Iteration 401, BCE loss: 57.76705717832671, Acc: 0.8194, Grad norm: 0.6625825556124649\n",
      "Iteration 402, BCE loss: 57.76399173127082, Acc: 0.8194, Grad norm: 0.5257358538261342\n",
      "Iteration 403, BCE loss: 57.76838833049011, Acc: 0.8195, Grad norm: 0.685097005479913\n",
      "Iteration 404, BCE loss: 57.76404675127497, Acc: 0.8197, Grad norm: 0.5365700940727302\n",
      "Iteration 405, BCE loss: 57.76924463709494, Acc: 0.8194, Grad norm: 0.736048733567713\n",
      "Iteration 406, BCE loss: 57.76575849747297, Acc: 0.8195, Grad norm: 0.607722927110132\n",
      "Iteration 407, BCE loss: 57.76186969230267, Acc: 0.8198, Grad norm: 0.4229070813847629\n",
      "Iteration 408, BCE loss: 57.76858788962235, Acc: 0.82, Grad norm: 0.680290770995178\n",
      "Iteration 409, BCE loss: 57.76703019938046, Acc: 0.8203, Grad norm: 0.6170091891045366\n",
      "Iteration 410, BCE loss: 57.76205804576209, Acc: 0.8199, Grad norm: 0.4088139943660042\n",
      "Iteration 411, BCE loss: 57.764486382712505, Acc: 0.8201, Grad norm: 0.5392801804641011\n",
      "Iteration 412, BCE loss: 57.76951351118768, Acc: 0.8202, Grad norm: 0.7301197117637876\n",
      "Iteration 413, BCE loss: 57.76212481480903, Acc: 0.8197, Grad norm: 0.4568624518442629\n",
      "Iteration 414, BCE loss: 57.76116403233934, Acc: 0.8198, Grad norm: 0.39071140750112\n",
      "Iteration 415, BCE loss: 57.76663399135481, Acc: 0.8203, Grad norm: 0.6261959815597729\n",
      "Iteration 416, BCE loss: 57.76209979976272, Acc: 0.8201, Grad norm: 0.41137650147738364\n",
      "Iteration 417, BCE loss: 57.75871608714607, Acc: 0.8198, Grad norm: 0.22212398941140513\n",
      "Iteration 418, BCE loss: 57.75977012941622, Acc: 0.8198, Grad norm: 0.29835575683584686\n",
      "Iteration 419, BCE loss: 57.76321384938836, Acc: 0.8199, Grad norm: 0.4830734704308596\n",
      "Iteration 420, BCE loss: 57.7630636941058, Acc: 0.8197, Grad norm: 0.4695640140214055\n",
      "Iteration 421, BCE loss: 57.761390220083555, Acc: 0.8196, Grad norm: 0.3919260107477921\n",
      "Iteration 422, BCE loss: 57.76190757815644, Acc: 0.8194, Grad norm: 0.4301044441731076\n",
      "Iteration 423, BCE loss: 57.763772161379435, Acc: 0.8198, Grad norm: 0.5211526787666483\n",
      "Iteration 424, BCE loss: 57.763810426341735, Acc: 0.8195, Grad norm: 0.5158630830594718\n",
      "Iteration 425, BCE loss: 57.76475181165584, Acc: 0.8196, Grad norm: 0.5431745208483115\n",
      "Iteration 426, BCE loss: 57.766832777205174, Acc: 0.8196, Grad norm: 0.6059555001873697\n",
      "Iteration 427, BCE loss: 57.769323292964884, Acc: 0.8196, Grad norm: 0.6901628348108283\n",
      "Iteration 428, BCE loss: 57.77000071668472, Acc: 0.8197, Grad norm: 0.7288110159614353\n",
      "Iteration 429, BCE loss: 57.760874909337716, Acc: 0.8197, Grad norm: 0.3865724345677087\n",
      "Iteration 430, BCE loss: 57.761804473857694, Acc: 0.8198, Grad norm: 0.44490495798019436\n",
      "Iteration 431, BCE loss: 57.76296756449861, Acc: 0.8196, Grad norm: 0.46794992135265084\n",
      "Iteration 432, BCE loss: 57.764782762365044, Acc: 0.8198, Grad norm: 0.5370631420768794\n",
      "Iteration 433, BCE loss: 57.7622456770782, Acc: 0.8199, Grad norm: 0.45102418700132635\n",
      "Iteration 434, BCE loss: 57.759951923992105, Acc: 0.8197, Grad norm: 0.3000116149886383\n",
      "Iteration 435, BCE loss: 57.758514933790764, Acc: 0.8196, Grad norm: 0.22081358779187765\n",
      "Iteration 436, BCE loss: 57.76007997620994, Acc: 0.8196, Grad norm: 0.3342012095873903\n",
      "Iteration 437, BCE loss: 57.76566390607252, Acc: 0.8196, Grad norm: 0.574279127672984\n",
      "Iteration 438, BCE loss: 57.76488679988482, Acc: 0.8197, Grad norm: 0.5260392674910694\n",
      "Iteration 439, BCE loss: 57.76295900384119, Acc: 0.8196, Grad norm: 0.45691897955026983\n",
      "Iteration 440, BCE loss: 57.76017858208295, Acc: 0.8198, Grad norm: 0.32804192523883297\n",
      "Iteration 441, BCE loss: 57.76449911198499, Acc: 0.8199, Grad norm: 0.5401458402057489\n",
      "Iteration 442, BCE loss: 57.762134195245494, Acc: 0.8197, Grad norm: 0.4384063814863273\n",
      "Iteration 443, BCE loss: 57.764513288867136, Acc: 0.8198, Grad norm: 0.5403535469046421\n",
      "Iteration 444, BCE loss: 57.76978983528522, Acc: 0.8198, Grad norm: 0.7302602745515111\n",
      "Iteration 445, BCE loss: 57.76894500167821, Acc: 0.8197, Grad norm: 0.7091639769404202\n",
      "Iteration 446, BCE loss: 57.766572420329354, Acc: 0.8195, Grad norm: 0.6382952557960861\n",
      "Iteration 447, BCE loss: 57.7679582688708, Acc: 0.8194, Grad norm: 0.6847438654073961\n",
      "Iteration 448, BCE loss: 57.7675967546581, Acc: 0.8192, Grad norm: 0.6809310081463132\n",
      "Iteration 449, BCE loss: 57.76462699794556, Acc: 0.8193, Grad norm: 0.5618554436729781\n",
      "Iteration 450, BCE loss: 57.76739273305867, Acc: 0.8193, Grad norm: 0.6858012600579452\n",
      "Iteration 451, BCE loss: 57.76725716063137, Acc: 0.8193, Grad norm: 0.658191489659068\n",
      "Iteration 452, BCE loss: 57.76333920453827, Acc: 0.8197, Grad norm: 0.48752667547841094\n",
      "Iteration 453, BCE loss: 57.76111863055359, Acc: 0.8194, Grad norm: 0.3814083857302664\n",
      "Iteration 454, BCE loss: 57.76474192099403, Acc: 0.8193, Grad norm: 0.5877212444284288\n",
      "Iteration 455, BCE loss: 57.76188136429306, Acc: 0.8195, Grad norm: 0.4519174146155614\n",
      "Iteration 456, BCE loss: 57.76179020216148, Acc: 0.8197, Grad norm: 0.39820863670171375\n",
      "Iteration 457, BCE loss: 57.76282878115663, Acc: 0.8195, Grad norm: 0.42762832477025\n",
      "Iteration 458, BCE loss: 57.76389296609761, Acc: 0.8195, Grad norm: 0.5019100757656988\n",
      "Iteration 459, BCE loss: 57.76344158053122, Acc: 0.8197, Grad norm: 0.48641020334508345\n",
      "Iteration 460, BCE loss: 57.76014868561448, Acc: 0.8196, Grad norm: 0.32497652680071604\n",
      "Iteration 461, BCE loss: 57.76124249747075, Acc: 0.8194, Grad norm: 0.41937434280244507\n",
      "Iteration 462, BCE loss: 57.762654597123245, Acc: 0.8193, Grad norm: 0.42992808187098197\n",
      "Iteration 463, BCE loss: 57.76215711402516, Acc: 0.8195, Grad norm: 0.4044323833027139\n",
      "Iteration 464, BCE loss: 57.76294016369859, Acc: 0.82, Grad norm: 0.4461046972151106\n",
      "Iteration 465, BCE loss: 57.761326118526256, Acc: 0.8199, Grad norm: 0.3751224726178165\n",
      "Iteration 466, BCE loss: 57.76300077855315, Acc: 0.82, Grad norm: 0.43771787400039425\n",
      "Iteration 467, BCE loss: 57.76434904584829, Acc: 0.8199, Grad norm: 0.49561065690674827\n",
      "Iteration 468, BCE loss: 57.76832813334536, Acc: 0.8196, Grad norm: 0.6660503798928649\n",
      "Iteration 469, BCE loss: 57.76357432848101, Acc: 0.8195, Grad norm: 0.4817512069559229\n",
      "Iteration 470, BCE loss: 57.761402287256615, Acc: 0.8195, Grad norm: 0.3804433775684411\n",
      "Iteration 471, BCE loss: 57.76218228743611, Acc: 0.8196, Grad norm: 0.4201672542555162\n",
      "Iteration 472, BCE loss: 57.762192706367, Acc: 0.8197, Grad norm: 0.4428859582632608\n",
      "Iteration 473, BCE loss: 57.76079764201275, Acc: 0.8197, Grad norm: 0.3738808598078493\n",
      "Iteration 474, BCE loss: 57.76132608693197, Acc: 0.8196, Grad norm: 0.4150389899863737\n",
      "Iteration 475, BCE loss: 57.75945607537874, Acc: 0.8194, Grad norm: 0.2745236013659742\n",
      "Iteration 476, BCE loss: 57.76080480940589, Acc: 0.8194, Grad norm: 0.3553758660154474\n",
      "Iteration 477, BCE loss: 57.76095387640678, Acc: 0.8194, Grad norm: 0.3724196660985344\n",
      "Iteration 478, BCE loss: 57.76448720677244, Acc: 0.8193, Grad norm: 0.5650438142328811\n",
      "Iteration 479, BCE loss: 57.75993520815611, Acc: 0.8195, Grad norm: 0.30481790180650853\n",
      "Iteration 480, BCE loss: 57.76050132696845, Acc: 0.8199, Grad norm: 0.34006752397754575\n",
      "Iteration 481, BCE loss: 57.76071355729371, Acc: 0.8198, Grad norm: 0.36352421343726976\n",
      "Iteration 482, BCE loss: 57.761369440144705, Acc: 0.8199, Grad norm: 0.43242686935554203\n",
      "Iteration 483, BCE loss: 57.76338495699113, Acc: 0.8199, Grad norm: 0.5092876819547022\n",
      "Iteration 484, BCE loss: 57.75929382211814, Acc: 0.8197, Grad norm: 0.26777832108944927\n",
      "Iteration 485, BCE loss: 57.75911840585965, Acc: 0.8197, Grad norm: 0.2742726801367924\n",
      "Iteration 486, BCE loss: 57.75985828305912, Acc: 0.8196, Grad norm: 0.3208516872903047\n",
      "Iteration 487, BCE loss: 57.76448678585314, Acc: 0.8196, Grad norm: 0.5303093662153117\n",
      "Iteration 488, BCE loss: 57.76563159277842, Acc: 0.8195, Grad norm: 0.5987578387230428\n",
      "Iteration 489, BCE loss: 57.76063012325638, Acc: 0.8198, Grad norm: 0.35322029703012814\n",
      "Iteration 490, BCE loss: 57.76016585579329, Acc: 0.8199, Grad norm: 0.3128954465511169\n",
      "Iteration 491, BCE loss: 57.75933483311671, Acc: 0.8199, Grad norm: 0.29087306584399897\n",
      "Iteration 492, BCE loss: 57.761509943512465, Acc: 0.8199, Grad norm: 0.42064130496282187\n",
      "Iteration 493, BCE loss: 57.75916601170185, Acc: 0.8198, Grad norm: 0.24899776424202658\n",
      "Iteration 494, BCE loss: 57.76185068258291, Acc: 0.8197, Grad norm: 0.3916021190119224\n",
      "Iteration 495, BCE loss: 57.76490448154841, Acc: 0.8196, Grad norm: 0.5314844467823945\n",
      "Iteration 496, BCE loss: 57.76352390750153, Acc: 0.8196, Grad norm: 0.47706883213951706\n",
      "Iteration 497, BCE loss: 57.76573796525396, Acc: 0.8201, Grad norm: 0.5652937644759675\n",
      "Iteration 498, BCE loss: 57.76426482829655, Acc: 0.8202, Grad norm: 0.5505931963956416\n",
      "Iteration 499, BCE loss: 57.761084446591795, Acc: 0.8199, Grad norm: 0.41440834439206153\n",
      "Iteration 500, BCE loss: 57.76169387528297, Acc: 0.8197, Grad norm: 0.4310528211312588\n",
      "Iteration 501, BCE loss: 57.75965697208404, Acc: 0.8197, Grad norm: 0.30112713564454385\n",
      "Iteration 502, BCE loss: 57.760932613563554, Acc: 0.8197, Grad norm: 0.3939216651496004\n",
      "Iteration 503, BCE loss: 57.75812466571738, Acc: 0.8197, Grad norm: 0.1758067477473681\n",
      "Iteration 504, BCE loss: 57.75875617335648, Acc: 0.8197, Grad norm: 0.21643224719563328\n",
      "Iteration 505, BCE loss: 57.762779671496176, Acc: 0.8196, Grad norm: 0.4669480135705109\n",
      "Iteration 506, BCE loss: 57.76227016171278, Acc: 0.8197, Grad norm: 0.4417279867837607\n",
      "Iteration 507, BCE loss: 57.76648922911025, Acc: 0.8196, Grad norm: 0.6085628388460811\n",
      "Iteration 508, BCE loss: 57.76195981925242, Acc: 0.8199, Grad norm: 0.4325932161193337\n",
      "Iteration 509, BCE loss: 57.76169015032904, Acc: 0.8199, Grad norm: 0.44597077988338196\n",
      "Iteration 510, BCE loss: 57.766371736135184, Acc: 0.8198, Grad norm: 0.6338126763877329\n",
      "Iteration 511, BCE loss: 57.76197931750879, Acc: 0.8197, Grad norm: 0.43283000745202344\n",
      "Iteration 512, BCE loss: 57.758947464080975, Acc: 0.8195, Grad norm: 0.25599356863943645\n",
      "Iteration 513, BCE loss: 57.75820407093083, Acc: 0.8197, Grad norm: 0.1889110749695166\n",
      "Iteration 514, BCE loss: 57.75950382070375, Acc: 0.8198, Grad norm: 0.3040061407700521\n",
      "Iteration 515, BCE loss: 57.76185106773796, Acc: 0.8198, Grad norm: 0.4346617683693972\n",
      "Iteration 516, BCE loss: 57.76032502064078, Acc: 0.8196, Grad norm: 0.3522047144461713\n",
      "Iteration 517, BCE loss: 57.76065963399396, Acc: 0.8196, Grad norm: 0.3715093615100599\n",
      "Iteration 518, BCE loss: 57.76332063463313, Acc: 0.8199, Grad norm: 0.5197008701208183\n",
      "Iteration 519, BCE loss: 57.76404176477406, Acc: 0.82, Grad norm: 0.5412910398986801\n",
      "Iteration 520, BCE loss: 57.76145059792916, Acc: 0.8198, Grad norm: 0.433131101606971\n",
      "Iteration 521, BCE loss: 57.75994563705422, Acc: 0.8198, Grad norm: 0.3418991310139056\n",
      "Iteration 522, BCE loss: 57.763350772868975, Acc: 0.8199, Grad norm: 0.5354274092452211\n",
      "Iteration 523, BCE loss: 57.75921105329249, Acc: 0.8197, Grad norm: 0.29940540598083965\n",
      "Iteration 524, BCE loss: 57.75909387790773, Acc: 0.8197, Grad norm: 0.27121171610335454\n",
      "Iteration 525, BCE loss: 57.76257964782333, Acc: 0.8196, Grad norm: 0.43831801914349994\n",
      "Iteration 526, BCE loss: 57.75967188423654, Acc: 0.8196, Grad norm: 0.2856784284052375\n",
      "Iteration 527, BCE loss: 57.76314430143415, Acc: 0.8197, Grad norm: 0.46779277469811\n",
      "Iteration 528, BCE loss: 57.760891571638965, Acc: 0.8196, Grad norm: 0.35963104013524283\n",
      "Iteration 529, BCE loss: 57.76003762341283, Acc: 0.8196, Grad norm: 0.3123330365867576\n",
      "Iteration 530, BCE loss: 57.76044075874847, Acc: 0.8198, Grad norm: 0.3582315147388264\n",
      "Iteration 531, BCE loss: 57.76281522495302, Acc: 0.8199, Grad norm: 0.5015693416503361\n",
      "Iteration 532, BCE loss: 57.767137570458175, Acc: 0.8202, Grad norm: 0.668432415268534\n",
      "Iteration 533, BCE loss: 57.76538876572252, Acc: 0.82, Grad norm: 0.594429905085432\n",
      "Iteration 534, BCE loss: 57.76717625727909, Acc: 0.8199, Grad norm: 0.6387635545409789\n",
      "Iteration 535, BCE loss: 57.762088187375, Acc: 0.8198, Grad norm: 0.4318948431659083\n",
      "Iteration 536, BCE loss: 57.76304670363163, Acc: 0.8198, Grad norm: 0.4705891812733351\n",
      "Iteration 537, BCE loss: 57.76408248240079, Acc: 0.8198, Grad norm: 0.5291316075918452\n",
      "Iteration 538, BCE loss: 57.7675936037589, Acc: 0.8196, Grad norm: 0.6575207639187131\n",
      "Iteration 539, BCE loss: 57.76498880238405, Acc: 0.8197, Grad norm: 0.5754222273489438\n",
      "Iteration 540, BCE loss: 57.76628816971879, Acc: 0.8198, Grad norm: 0.6239189663005923\n",
      "Iteration 541, BCE loss: 57.762078509102004, Acc: 0.8199, Grad norm: 0.4378822339541303\n",
      "Iteration 542, BCE loss: 57.76284180624463, Acc: 0.8197, Grad norm: 0.46222939867316004\n",
      "Iteration 543, BCE loss: 57.761354099769605, Acc: 0.8194, Grad norm: 0.3806044177558007\n",
      "Iteration 544, BCE loss: 57.762199814946655, Acc: 0.8195, Grad norm: 0.44443366202987844\n",
      "Iteration 545, BCE loss: 57.75859864393506, Acc: 0.8195, Grad norm: 0.21512616871615445\n",
      "Iteration 546, BCE loss: 57.76003840286032, Acc: 0.8195, Grad norm: 0.3315000511775213\n",
      "Iteration 547, BCE loss: 57.76380585980233, Acc: 0.8196, Grad norm: 0.5348143070218091\n",
      "Iteration 548, BCE loss: 57.760176862249, Acc: 0.8198, Grad norm: 0.32720232613302896\n",
      "Iteration 549, BCE loss: 57.76083430523087, Acc: 0.8198, Grad norm: 0.33283253013471564\n",
      "Iteration 550, BCE loss: 57.7606855822136, Acc: 0.8198, Grad norm: 0.33267354605131866\n",
      "Iteration 551, BCE loss: 57.7622354729328, Acc: 0.82, Grad norm: 0.3950521095450107\n",
      "Iteration 552, BCE loss: 57.763460942404144, Acc: 0.8201, Grad norm: 0.45628209820039267\n",
      "Iteration 553, BCE loss: 57.766519436654804, Acc: 0.82, Grad norm: 0.5935727360695766\n",
      "Iteration 554, BCE loss: 57.7681098256286, Acc: 0.8201, Grad norm: 0.6255910142848647\n",
      "Iteration 555, BCE loss: 57.76511861461316, Acc: 0.8201, Grad norm: 0.5122217643388847\n",
      "Iteration 556, BCE loss: 57.761699475678164, Acc: 0.82, Grad norm: 0.40961542481507096\n",
      "Iteration 557, BCE loss: 57.761679656423226, Acc: 0.8198, Grad norm: 0.4291728021044347\n",
      "Iteration 558, BCE loss: 57.76109608639274, Acc: 0.8198, Grad norm: 0.40213919790490754\n",
      "Iteration 559, BCE loss: 57.76061098064848, Acc: 0.8196, Grad norm: 0.3590218125565876\n",
      "Iteration 560, BCE loss: 57.760453632691494, Acc: 0.8194, Grad norm: 0.34935057635507416\n",
      "Iteration 561, BCE loss: 57.7606011128849, Acc: 0.8198, Grad norm: 0.36774762773051284\n",
      "Iteration 562, BCE loss: 57.76178145281379, Acc: 0.8195, Grad norm: 0.415198959925467\n",
      "Iteration 563, BCE loss: 57.764653846949656, Acc: 0.8195, Grad norm: 0.5615733697304527\n",
      "Iteration 564, BCE loss: 57.764091173380905, Acc: 0.8196, Grad norm: 0.544187511107633\n",
      "Iteration 565, BCE loss: 57.765643992367075, Acc: 0.8195, Grad norm: 0.5926560774967319\n",
      "Iteration 566, BCE loss: 57.761363318966005, Acc: 0.8195, Grad norm: 0.3844367817739627\n",
      "Iteration 567, BCE loss: 57.76013082832941, Acc: 0.8196, Grad norm: 0.3431721622123213\n",
      "Iteration 568, BCE loss: 57.76107162071126, Acc: 0.8196, Grad norm: 0.38973588767772105\n",
      "Iteration 569, BCE loss: 57.76119235917548, Acc: 0.8198, Grad norm: 0.39334282752007166\n",
      "Iteration 570, BCE loss: 57.75994534276559, Acc: 0.8197, Grad norm: 0.314646897917186\n",
      "Iteration 571, BCE loss: 57.76056677547035, Acc: 0.8198, Grad norm: 0.3566739757671036\n",
      "Iteration 572, BCE loss: 57.76304241261041, Acc: 0.8197, Grad norm: 0.49443489141569397\n",
      "Iteration 573, BCE loss: 57.76134799166389, Acc: 0.8197, Grad norm: 0.403869651901921\n",
      "Iteration 574, BCE loss: 57.7638907534446, Acc: 0.8198, Grad norm: 0.5500381567508712\n",
      "Iteration 575, BCE loss: 57.75958164612051, Acc: 0.8196, Grad norm: 0.2990555891449518\n",
      "Iteration 576, BCE loss: 57.75893782086052, Acc: 0.8197, Grad norm: 0.23916352992336873\n",
      "Iteration 577, BCE loss: 57.75955195551743, Acc: 0.8195, Grad norm: 0.29330384593694103\n",
      "Iteration 578, BCE loss: 57.75898043648261, Acc: 0.8196, Grad norm: 0.2423711557154075\n",
      "Iteration 579, BCE loss: 57.75888341171039, Acc: 0.8198, Grad norm: 0.2242968779491988\n",
      "Iteration 580, BCE loss: 57.75938743135275, Acc: 0.8196, Grad norm: 0.2504873107424682\n",
      "Iteration 581, BCE loss: 57.760472552386105, Acc: 0.8196, Grad norm: 0.335317171119854\n",
      "Iteration 582, BCE loss: 57.762679121307144, Acc: 0.8196, Grad norm: 0.47523760524674585\n",
      "Iteration 583, BCE loss: 57.76379997858764, Acc: 0.8196, Grad norm: 0.5304940137011307\n",
      "Iteration 584, BCE loss: 57.76260735206165, Acc: 0.8199, Grad norm: 0.464651603376231\n",
      "Iteration 585, BCE loss: 57.76356788057037, Acc: 0.8197, Grad norm: 0.48988007138966055\n",
      "Iteration 586, BCE loss: 57.75949266926505, Acc: 0.8199, Grad norm: 0.26703317473964444\n",
      "Iteration 587, BCE loss: 57.76029157853841, Acc: 0.8198, Grad norm: 0.3416394986331336\n",
      "Iteration 588, BCE loss: 57.764409395243156, Acc: 0.8198, Grad norm: 0.5382817039223514\n",
      "Iteration 589, BCE loss: 57.765196136073875, Acc: 0.8196, Grad norm: 0.5753375854928036\n",
      "Iteration 590, BCE loss: 57.76196051200095, Acc: 0.8196, Grad norm: 0.4045100602451488\n",
      "Iteration 591, BCE loss: 57.76449816601134, Acc: 0.8197, Grad norm: 0.5765493014443309\n",
      "Iteration 592, BCE loss: 57.762207256892204, Acc: 0.82, Grad norm: 0.4666059825987203\n",
      "Iteration 593, BCE loss: 57.761270302846086, Acc: 0.8201, Grad norm: 0.39346255319810264\n",
      "Iteration 594, BCE loss: 57.760261581686365, Acc: 0.82, Grad norm: 0.3379744072223697\n",
      "Iteration 595, BCE loss: 57.76151344444044, Acc: 0.82, Grad norm: 0.42055999666099764\n",
      "Iteration 596, BCE loss: 57.76164124227606, Acc: 0.8198, Grad norm: 0.41854048675776695\n",
      "Iteration 597, BCE loss: 57.75936884543802, Acc: 0.8199, Grad norm: 0.2704249522764406\n",
      "Iteration 598, BCE loss: 57.76011756692391, Acc: 0.8198, Grad norm: 0.32816884381257255\n",
      "Iteration 599, BCE loss: 57.7607463025425, Acc: 0.8199, Grad norm: 0.35184754201376445\n",
      "Iteration 600, BCE loss: 57.76374297107704, Acc: 0.82, Grad norm: 0.500156973255753\n",
      "Iteration 601, BCE loss: 57.76121683988857, Acc: 0.8197, Grad norm: 0.3588374031475392\n",
      "Iteration 602, BCE loss: 57.760527044443265, Acc: 0.8194, Grad norm: 0.32707832669351256\n",
      "Iteration 603, BCE loss: 57.76011930020437, Acc: 0.8193, Grad norm: 0.32013973731388917\n",
      "Iteration 604, BCE loss: 57.7617965609912, Acc: 0.8194, Grad norm: 0.42350097863275776\n",
      "Iteration 605, BCE loss: 57.76059886352522, Acc: 0.8194, Grad norm: 0.36319376478824605\n",
      "Iteration 606, BCE loss: 57.76327357812263, Acc: 0.8195, Grad norm: 0.5094521583935062\n",
      "Iteration 607, BCE loss: 57.75906963944068, Acc: 0.8197, Grad norm: 0.26080763972429877\n",
      "Iteration 608, BCE loss: 57.760780634652704, Acc: 0.8198, Grad norm: 0.381764640336877\n",
      "Iteration 609, BCE loss: 57.763120769848776, Acc: 0.8198, Grad norm: 0.5097080403876537\n",
      "Iteration 610, BCE loss: 57.76132018995453, Acc: 0.8197, Grad norm: 0.4154216667509909\n",
      "Iteration 611, BCE loss: 57.76124462673645, Acc: 0.8197, Grad norm: 0.41110925053805325\n",
      "Iteration 612, BCE loss: 57.75967724586188, Acc: 0.8197, Grad norm: 0.3186999706083655\n",
      "Iteration 613, BCE loss: 57.766581856123416, Acc: 0.8198, Grad norm: 0.6484168053038363\n",
      "Iteration 614, BCE loss: 57.758359592168745, Acc: 0.8196, Grad norm: 0.20313630166036845\n",
      "Iteration 615, BCE loss: 57.758606913649466, Acc: 0.8197, Grad norm: 0.21702702131522805\n",
      "Iteration 616, BCE loss: 57.75904979781172, Acc: 0.8196, Grad norm: 0.251370718223367\n",
      "Iteration 617, BCE loss: 57.75887477990668, Acc: 0.8197, Grad norm: 0.2542280147292505\n",
      "Iteration 618, BCE loss: 57.762315640220464, Acc: 0.8197, Grad norm: 0.4616830946967373\n",
      "Iteration 619, BCE loss: 57.76396561607563, Acc: 0.8196, Grad norm: 0.5358663096105248\n",
      "Iteration 620, BCE loss: 57.76194741865547, Acc: 0.8196, Grad norm: 0.43295141304363083\n",
      "Iteration 621, BCE loss: 57.76217502362332, Acc: 0.8197, Grad norm: 0.4533817330469017\n",
      "Iteration 622, BCE loss: 57.76120641782737, Acc: 0.8199, Grad norm: 0.4169696564800775\n",
      "Iteration 623, BCE loss: 57.75792040848278, Acc: 0.8196, Grad norm: 0.1474983886263536\n",
      "Iteration 624, BCE loss: 57.758242460706896, Acc: 0.8197, Grad norm: 0.19356341252945675\n",
      "Iteration 625, BCE loss: 57.75880967512603, Acc: 0.8196, Grad norm: 0.23611149191193195\n",
      "Iteration 626, BCE loss: 57.759951160719645, Acc: 0.8196, Grad norm: 0.31686097664867\n",
      "Iteration 627, BCE loss: 57.75998382206406, Acc: 0.8197, Grad norm: 0.31864333678153844\n",
      "Iteration 628, BCE loss: 57.75872299142192, Acc: 0.8197, Grad norm: 0.2274131305232162\n",
      "Iteration 629, BCE loss: 57.76007697653491, Acc: 0.8197, Grad norm: 0.3296668067401887\n",
      "Iteration 630, BCE loss: 57.759230643797004, Acc: 0.8196, Grad norm: 0.2677790475364559\n",
      "Iteration 631, BCE loss: 57.76036353607046, Acc: 0.8197, Grad norm: 0.3414172340341442\n",
      "Iteration 632, BCE loss: 57.75906853904701, Acc: 0.8196, Grad norm: 0.24492750092933413\n",
      "Iteration 633, BCE loss: 57.758855641363375, Acc: 0.8194, Grad norm: 0.22892513532173864\n",
      "Iteration 634, BCE loss: 57.76007650477035, Acc: 0.8194, Grad norm: 0.3132887889203653\n",
      "Iteration 635, BCE loss: 57.7610729542767, Acc: 0.8196, Grad norm: 0.3573933998146836\n",
      "Iteration 636, BCE loss: 57.76010890543651, Acc: 0.8198, Grad norm: 0.3491989833036182\n",
      "Iteration 637, BCE loss: 57.75950616868556, Acc: 0.8198, Grad norm: 0.293029407438899\n",
      "Iteration 638, BCE loss: 57.76025008343256, Acc: 0.8197, Grad norm: 0.34388112137935056\n",
      "Iteration 639, BCE loss: 57.759768042952906, Acc: 0.8197, Grad norm: 0.3135074289848349\n",
      "Iteration 640, BCE loss: 57.75884800909893, Acc: 0.8196, Grad norm: 0.240203093966481\n",
      "Iteration 641, BCE loss: 57.76064631665222, Acc: 0.8199, Grad norm: 0.37977699742789656\n",
      "Iteration 642, BCE loss: 57.759921912317054, Acc: 0.8199, Grad norm: 0.3383966446795096\n",
      "Iteration 643, BCE loss: 57.76045197877713, Acc: 0.8198, Grad norm: 0.35424761416242057\n",
      "Iteration 644, BCE loss: 57.760697068673906, Acc: 0.8197, Grad norm: 0.3794236681033774\n",
      "Iteration 645, BCE loss: 57.76030796232955, Acc: 0.8197, Grad norm: 0.3574052884844787\n",
      "Iteration 646, BCE loss: 57.75977903625856, Acc: 0.8196, Grad norm: 0.3281172064836538\n",
      "Iteration 647, BCE loss: 57.75949708195432, Acc: 0.8195, Grad norm: 0.30364844469059976\n",
      "Iteration 648, BCE loss: 57.762689361190475, Acc: 0.8195, Grad norm: 0.4602774703090814\n",
      "Iteration 649, BCE loss: 57.762254471661684, Acc: 0.8197, Grad norm: 0.43043846356862964\n",
      "Iteration 650, BCE loss: 57.76339430902455, Acc: 0.8198, Grad norm: 0.4668629097032346\n",
      "Iteration 651, BCE loss: 57.76284341864329, Acc: 0.8198, Grad norm: 0.4573873308083729\n",
      "Iteration 652, BCE loss: 57.76315357817662, Acc: 0.8199, Grad norm: 0.46816756184057096\n",
      "Iteration 653, BCE loss: 57.76378975983545, Acc: 0.8198, Grad norm: 0.5170163412402216\n",
      "Iteration 654, BCE loss: 57.76246379804364, Acc: 0.8199, Grad norm: 0.44782515002542506\n",
      "Iteration 655, BCE loss: 57.76238242026564, Acc: 0.8201, Grad norm: 0.4164172688202331\n",
      "Iteration 656, BCE loss: 57.7594501357581, Acc: 0.8199, Grad norm: 0.2664426996741814\n",
      "Iteration 657, BCE loss: 57.758711491257266, Acc: 0.8198, Grad norm: 0.22475317581115145\n",
      "Iteration 658, BCE loss: 57.75972030480017, Acc: 0.8198, Grad norm: 0.2975643496762999\n",
      "Iteration 659, BCE loss: 57.762184082299214, Acc: 0.8197, Grad norm: 0.44725707303351053\n",
      "Iteration 660, BCE loss: 57.76111847390602, Acc: 0.8196, Grad norm: 0.4054357090253353\n",
      "Iteration 661, BCE loss: 57.75960497515695, Acc: 0.8194, Grad norm: 0.32150893344319054\n",
      "Iteration 662, BCE loss: 57.75842625292526, Acc: 0.8195, Grad norm: 0.21985843069772104\n",
      "Iteration 663, BCE loss: 57.7582342906267, Acc: 0.8196, Grad norm: 0.19243966558656356\n",
      "Iteration 664, BCE loss: 57.76008948066558, Acc: 0.8198, Grad norm: 0.3281951104800525\n",
      "Iteration 665, BCE loss: 57.75940718659413, Acc: 0.8197, Grad norm: 0.28152282685215396\n",
      "Iteration 666, BCE loss: 57.76005638328713, Acc: 0.8196, Grad norm: 0.3355055741542264\n",
      "Iteration 667, BCE loss: 57.76085397818771, Acc: 0.8195, Grad norm: 0.4012700804686676\n",
      "Iteration 668, BCE loss: 57.75894048839881, Acc: 0.8194, Grad norm: 0.2566940931617085\n",
      "Iteration 669, BCE loss: 57.75938384834454, Acc: 0.8197, Grad norm: 0.29775728724993483\n",
      "Iteration 670, BCE loss: 57.75810819196713, Acc: 0.8197, Grad norm: 0.17550426748790254\n",
      "Iteration 671, BCE loss: 57.759759393630205, Acc: 0.8197, Grad norm: 0.32164168732991866\n",
      "Iteration 672, BCE loss: 57.75871070006024, Acc: 0.8197, Grad norm: 0.23647343214138783\n",
      "Iteration 673, BCE loss: 57.75857072425028, Acc: 0.8196, Grad norm: 0.2315267278427681\n",
      "Iteration 674, BCE loss: 57.75865569954735, Acc: 0.8198, Grad norm: 0.23743521223041755\n",
      "Iteration 675, BCE loss: 57.7595133967822, Acc: 0.8198, Grad norm: 0.31430406625988067\n",
      "Iteration 676, BCE loss: 57.759604239445096, Acc: 0.8197, Grad norm: 0.32328715412834663\n",
      "Iteration 677, BCE loss: 57.7588467252761, Acc: 0.8198, Grad norm: 0.24782808861669992\n",
      "Iteration 678, BCE loss: 57.75885357423108, Acc: 0.8196, Grad norm: 0.23857413237443628\n",
      "Iteration 679, BCE loss: 57.75986930196704, Acc: 0.8197, Grad norm: 0.31415694386084986\n",
      "Iteration 680, BCE loss: 57.7591380566775, Acc: 0.8198, Grad norm: 0.2549390976064469\n",
      "Iteration 681, BCE loss: 57.758950752772336, Acc: 0.8197, Grad norm: 0.2597426459932356\n",
      "Iteration 682, BCE loss: 57.758511803727, Acc: 0.8196, Grad norm: 0.2291427766451376\n",
      "Iteration 683, BCE loss: 57.75801617719509, Acc: 0.8197, Grad norm: 0.1768055529614496\n",
      "Iteration 684, BCE loss: 57.760096988666106, Acc: 0.8197, Grad norm: 0.33832692067466325\n",
      "Iteration 685, BCE loss: 57.760736548024326, Acc: 0.8199, Grad norm: 0.3821779302404859\n",
      "Iteration 686, BCE loss: 57.760848439307445, Acc: 0.8198, Grad norm: 0.3895163066239813\n",
      "Iteration 687, BCE loss: 57.76237655917201, Acc: 0.8197, Grad norm: 0.4623218173547005\n",
      "Iteration 688, BCE loss: 57.760283871627514, Acc: 0.8196, Grad norm: 0.3505644759751364\n",
      "Iteration 689, BCE loss: 57.75982546142855, Acc: 0.8197, Grad norm: 0.3229095059334526\n",
      "Iteration 690, BCE loss: 57.76211101996137, Acc: 0.8195, Grad norm: 0.46124454435363454\n",
      "Iteration 691, BCE loss: 57.760341269596296, Acc: 0.8196, Grad norm: 0.3692502143143749\n",
      "Iteration 692, BCE loss: 57.76187155228467, Acc: 0.8198, Grad norm: 0.4425679958126491\n",
      "Iteration 693, BCE loss: 57.75998219329131, Acc: 0.8196, Grad norm: 0.3160009537509818\n",
      "Iteration 694, BCE loss: 57.75869190009006, Acc: 0.8196, Grad norm: 0.23179919893042567\n",
      "Iteration 695, BCE loss: 57.75974612100425, Acc: 0.8197, Grad norm: 0.31522948430151626\n",
      "Iteration 696, BCE loss: 57.758662480728255, Acc: 0.8196, Grad norm: 0.21673856946691966\n",
      "Iteration 697, BCE loss: 57.7592555480257, Acc: 0.8195, Grad norm: 0.2617547797705333\n",
      "Iteration 698, BCE loss: 57.75934650729468, Acc: 0.8196, Grad norm: 0.28082663685973736\n",
      "Iteration 699, BCE loss: 57.75985412272327, Acc: 0.8196, Grad norm: 0.30913070976098234\n",
      "Iteration 700, BCE loss: 57.759413010129705, Acc: 0.8197, Grad norm: 0.30487234080035686\n",
      "Iteration 701, BCE loss: 57.75950920125893, Acc: 0.8198, Grad norm: 0.298277112618619\n",
      "Iteration 702, BCE loss: 57.76113510884448, Acc: 0.8199, Grad norm: 0.4124164087452947\n",
      "Iteration 703, BCE loss: 57.758821397768976, Acc: 0.8199, Grad norm: 0.2552804033127238\n",
      "Iteration 704, BCE loss: 57.76100757871776, Acc: 0.82, Grad norm: 0.3990398109906342\n",
      "Iteration 705, BCE loss: 57.759783758056386, Acc: 0.82, Grad norm: 0.31039395730722946\n",
      "Iteration 706, BCE loss: 57.758903761978715, Acc: 0.8199, Grad norm: 0.24643107789753715\n",
      "Iteration 707, BCE loss: 57.75840956870918, Acc: 0.8198, Grad norm: 0.19407834497796628\n",
      "Iteration 708, BCE loss: 57.75954584744558, Acc: 0.8198, Grad norm: 0.2944701669418637\n",
      "Iteration 709, BCE loss: 57.758618092097876, Acc: 0.8197, Grad norm: 0.2251853136399296\n",
      "Iteration 710, BCE loss: 57.76100560543472, Acc: 0.8196, Grad norm: 0.38727142938762094\n",
      "Iteration 711, BCE loss: 57.76150884168689, Acc: 0.8198, Grad norm: 0.4071877789905155\n",
      "Iteration 712, BCE loss: 57.7593353142831, Acc: 0.8197, Grad norm: 0.2918030003943523\n",
      "Iteration 713, BCE loss: 57.75953092803829, Acc: 0.8198, Grad norm: 0.28281518709330106\n",
      "Iteration 714, BCE loss: 57.75874024729983, Acc: 0.8198, Grad norm: 0.22598542066427008\n",
      "Iteration 715, BCE loss: 57.75894897516902, Acc: 0.8198, Grad norm: 0.2423892242374086\n",
      "Iteration 716, BCE loss: 57.7584111977717, Acc: 0.8198, Grad norm: 0.20368576047897785\n",
      "Iteration 717, BCE loss: 57.759284902267765, Acc: 0.8197, Grad norm: 0.2870249161893708\n",
      "Iteration 718, BCE loss: 57.75949828815038, Acc: 0.8198, Grad norm: 0.2849713192134291\n",
      "Iteration 719, BCE loss: 57.75901660494506, Acc: 0.8196, Grad norm: 0.24558950129445276\n",
      "Iteration 720, BCE loss: 57.75880539910038, Acc: 0.8197, Grad norm: 0.22940030011649098\n",
      "Iteration 721, BCE loss: 57.75858784446792, Acc: 0.8196, Grad norm: 0.2067044084151248\n",
      "Iteration 722, BCE loss: 57.75902765568304, Acc: 0.8197, Grad norm: 0.246256078026445\n",
      "Iteration 723, BCE loss: 57.75894058796656, Acc: 0.8196, Grad norm: 0.23431902808076607\n",
      "Iteration 724, BCE loss: 57.760585967096716, Acc: 0.8197, Grad norm: 0.3621388982510088\n",
      "Iteration 725, BCE loss: 57.762978563965554, Acc: 0.8197, Grad norm: 0.5093690337292601\n",
      "Iteration 726, BCE loss: 57.762140350990926, Acc: 0.8197, Grad norm: 0.4475633048303433\n",
      "Iteration 727, BCE loss: 57.76196459539056, Acc: 0.8197, Grad norm: 0.4410207277777422\n",
      "Iteration 728, BCE loss: 57.76296931316995, Acc: 0.8197, Grad norm: 0.5034515532476463\n",
      "Iteration 729, BCE loss: 57.75997863607513, Acc: 0.8197, Grad norm: 0.32730471736022954\n",
      "Iteration 730, BCE loss: 57.75997178871418, Acc: 0.8197, Grad norm: 0.32625611379057073\n",
      "Iteration 731, BCE loss: 57.759911435518895, Acc: 0.8198, Grad norm: 0.322596722157896\n",
      "Iteration 732, BCE loss: 57.760986261820946, Acc: 0.8199, Grad norm: 0.3942151530092083\n",
      "Iteration 733, BCE loss: 57.76331754002666, Acc: 0.82, Grad norm: 0.518568756584879\n",
      "Iteration 734, BCE loss: 57.76184556508588, Acc: 0.82, Grad norm: 0.44301827976776115\n",
      "Iteration 735, BCE loss: 57.763399638233025, Acc: 0.82, Grad norm: 0.5246648949806078\n",
      "Iteration 736, BCE loss: 57.76319474857166, Acc: 0.8199, Grad norm: 0.5180194374974537\n",
      "Iteration 737, BCE loss: 57.763164906597424, Acc: 0.8198, Grad norm: 0.5061302305773214\n",
      "Iteration 738, BCE loss: 57.76632211903288, Acc: 0.8198, Grad norm: 0.643484734027971\n",
      "Iteration 739, BCE loss: 57.76174190101776, Acc: 0.8196, Grad norm: 0.44327658917290874\n",
      "Iteration 740, BCE loss: 57.7593289979947, Acc: 0.8197, Grad norm: 0.29592924315612223\n",
      "Iteration 741, BCE loss: 57.7594515690065, Acc: 0.8195, Grad norm: 0.2969992064617007\n",
      "Iteration 742, BCE loss: 57.759285768483295, Acc: 0.8198, Grad norm: 0.2903702500211778\n",
      "Iteration 743, BCE loss: 57.76261638619488, Acc: 0.8197, Grad norm: 0.49633944667174756\n",
      "Iteration 744, BCE loss: 57.76526486834459, Acc: 0.8197, Grad norm: 0.6110813464935586\n",
      "Iteration 745, BCE loss: 57.76354453685858, Acc: 0.8197, Grad norm: 0.5381984417329004\n",
      "Iteration 746, BCE loss: 57.764694825896434, Acc: 0.8198, Grad norm: 0.5887353743666356\n",
      "Iteration 747, BCE loss: 57.7647816815509, Acc: 0.8198, Grad norm: 0.5890521469999619\n",
      "Iteration 748, BCE loss: 57.75925551214419, Acc: 0.8196, Grad norm: 0.2878915844212152\n",
      "Iteration 749, BCE loss: 57.760458045945136, Acc: 0.8197, Grad norm: 0.3608681023366517\n",
      "Iteration 750, BCE loss: 57.75841485717142, Acc: 0.8196, Grad norm: 0.21776172431250346\n",
      "Iteration 751, BCE loss: 57.75842985579919, Acc: 0.8196, Grad norm: 0.19754186802515566\n",
      "Iteration 752, BCE loss: 57.75877084054953, Acc: 0.8197, Grad norm: 0.2472791623579413\n",
      "Iteration 753, BCE loss: 57.76005815220844, Acc: 0.8198, Grad norm: 0.3462492507447329\n",
      "Iteration 754, BCE loss: 57.75899470274734, Acc: 0.8198, Grad norm: 0.2693910809687353\n",
      "Iteration 755, BCE loss: 57.75854191303674, Acc: 0.8197, Grad norm: 0.22052602675245148\n",
      "Iteration 756, BCE loss: 57.75793392183911, Acc: 0.8197, Grad norm: 0.15086278727817287\n",
      "Iteration 757, BCE loss: 57.75822880003199, Acc: 0.8197, Grad norm: 0.183967356877134\n",
      "Iteration 758, BCE loss: 57.75894055304493, Acc: 0.8196, Grad norm: 0.24144702217022063\n",
      "Iteration 759, BCE loss: 57.75911266626761, Acc: 0.8196, Grad norm: 0.25336249239400677\n",
      "Iteration 760, BCE loss: 57.758189000206684, Acc: 0.8195, Grad norm: 0.18421600139220967\n",
      "Iteration 761, BCE loss: 57.75874123013428, Acc: 0.8195, Grad norm: 0.25640947283031074\n",
      "Iteration 762, BCE loss: 57.76026170467742, Acc: 0.8195, Grad norm: 0.35980978661603996\n",
      "Iteration 763, BCE loss: 57.75913955843201, Acc: 0.8196, Grad norm: 0.25763508425071197\n",
      "Iteration 764, BCE loss: 57.75887573665361, Acc: 0.8195, Grad norm: 0.2615661885037761\n",
      "Iteration 765, BCE loss: 57.759165681184626, Acc: 0.8195, Grad norm: 0.276054520353339\n",
      "Iteration 766, BCE loss: 57.75858375214742, Acc: 0.8196, Grad norm: 0.2344303571692016\n",
      "Iteration 767, BCE loss: 57.75918523701836, Acc: 0.8197, Grad norm: 0.25787548595115345\n",
      "Iteration 768, BCE loss: 57.75830502732495, Acc: 0.8198, Grad norm: 0.18939335977640917\n",
      "Iteration 769, BCE loss: 57.75849032505913, Acc: 0.8197, Grad norm: 0.20799902820401925\n",
      "Iteration 770, BCE loss: 57.76042363160404, Acc: 0.8197, Grad norm: 0.3458732622906078\n",
      "Iteration 771, BCE loss: 57.76010014788618, Acc: 0.8197, Grad norm: 0.3261271367636735\n",
      "Iteration 772, BCE loss: 57.761563636863045, Acc: 0.8199, Grad norm: 0.41315740430762876\n",
      "Iteration 773, BCE loss: 57.760296789378515, Acc: 0.8197, Grad norm: 0.3474215690811163\n",
      "Iteration 774, BCE loss: 57.76013447647449, Acc: 0.8197, Grad norm: 0.3298610760756363\n",
      "Iteration 775, BCE loss: 57.760787019775485, Acc: 0.8198, Grad norm: 0.3845591875190328\n",
      "Iteration 776, BCE loss: 57.75993801661929, Acc: 0.8198, Grad norm: 0.33089525031024436\n",
      "Iteration 777, BCE loss: 57.760396004676174, Acc: 0.8198, Grad norm: 0.33919351582813956\n",
      "Iteration 778, BCE loss: 57.75888176010054, Acc: 0.8197, Grad norm: 0.23490790883869142\n",
      "Iteration 779, BCE loss: 57.76024312086596, Acc: 0.8198, Grad norm: 0.34341621172968845\n",
      "Iteration 780, BCE loss: 57.76009353578169, Acc: 0.8197, Grad norm: 0.3310179602286494\n",
      "Iteration 781, BCE loss: 57.76201511685139, Acc: 0.8197, Grad norm: 0.42998449564970337\n",
      "Iteration 782, BCE loss: 57.76070272433917, Acc: 0.8197, Grad norm: 0.3696068713738215\n",
      "Iteration 783, BCE loss: 57.75982818813829, Acc: 0.8196, Grad norm: 0.31992906475577626\n",
      "Iteration 784, BCE loss: 57.760244908696166, Acc: 0.8195, Grad norm: 0.3446742105160258\n",
      "Iteration 785, BCE loss: 57.75960508245733, Acc: 0.8197, Grad norm: 0.3188129675203558\n",
      "Iteration 786, BCE loss: 57.761018643844764, Acc: 0.8195, Grad norm: 0.3967178681706821\n",
      "Iteration 787, BCE loss: 57.76076066370592, Acc: 0.8196, Grad norm: 0.38508840814166867\n",
      "Iteration 788, BCE loss: 57.76192317003426, Acc: 0.8197, Grad norm: 0.4433459543025209\n",
      "Iteration 789, BCE loss: 57.76849456798092, Acc: 0.8196, Grad norm: 0.7112613682765758\n",
      "Iteration 790, BCE loss: 57.76213558670035, Acc: 0.8196, Grad norm: 0.46636608916947947\n",
      "Iteration 791, BCE loss: 57.76075218485822, Acc: 0.8197, Grad norm: 0.37387619890081425\n",
      "Iteration 792, BCE loss: 57.75916298947237, Acc: 0.8197, Grad norm: 0.26176372156608924\n",
      "Iteration 793, BCE loss: 57.760777610731694, Acc: 0.8197, Grad norm: 0.3717550644248672\n",
      "Iteration 794, BCE loss: 57.76215878328854, Acc: 0.8197, Grad norm: 0.4511260568141808\n",
      "Iteration 795, BCE loss: 57.76316089758374, Acc: 0.8198, Grad norm: 0.49554475497701805\n",
      "Iteration 796, BCE loss: 57.75932023119718, Acc: 0.8198, Grad norm: 0.27181650626559184\n",
      "Iteration 797, BCE loss: 57.759031485440076, Acc: 0.8197, Grad norm: 0.24746677616752108\n",
      "Iteration 798, BCE loss: 57.759500285297015, Acc: 0.8199, Grad norm: 0.26875448221917214\n",
      "Iteration 799, BCE loss: 57.75975063495525, Acc: 0.8198, Grad norm: 0.28610609632717915\n",
      "Iteration 800, BCE loss: 57.76032756378312, Acc: 0.8198, Grad norm: 0.34657063550554307\n",
      "Iteration 801, BCE loss: 57.76065672904091, Acc: 0.8199, Grad norm: 0.37209345444779623\n",
      "Iteration 802, BCE loss: 57.758015401836104, Acc: 0.8197, Grad norm: 0.16358289668367915\n",
      "Iteration 803, BCE loss: 57.75843089689769, Acc: 0.8198, Grad norm: 0.20632376019318188\n",
      "Iteration 804, BCE loss: 57.75889857065664, Acc: 0.8198, Grad norm: 0.24412323687372905\n",
      "Iteration 805, BCE loss: 57.758192751311356, Acc: 0.8197, Grad norm: 0.16926413936353207\n",
      "Iteration 806, BCE loss: 57.75842094022451, Acc: 0.8195, Grad norm: 0.21147670855453948\n",
      "Iteration 807, BCE loss: 57.75802823593519, Acc: 0.8196, Grad norm: 0.16241161182039263\n",
      "Iteration 808, BCE loss: 57.75901901203373, Acc: 0.8197, Grad norm: 0.2522145842325527\n",
      "Iteration 809, BCE loss: 57.759903990156296, Acc: 0.8196, Grad norm: 0.32093439284946096\n",
      "Iteration 810, BCE loss: 57.76326621988591, Acc: 0.8195, Grad norm: 0.5006995711277602\n",
      "Iteration 811, BCE loss: 57.76276420016325, Acc: 0.8195, Grad norm: 0.4851921774970798\n",
      "Iteration 812, BCE loss: 57.75818588399675, Acc: 0.8195, Grad norm: 0.16764404532773197\n",
      "Iteration 813, BCE loss: 57.758955002899455, Acc: 0.8197, Grad norm: 0.23400201201326784\n",
      "Iteration 814, BCE loss: 57.75903780601297, Acc: 0.8197, Grad norm: 0.2473865532323733\n",
      "Iteration 815, BCE loss: 57.76056222351665, Acc: 0.8195, Grad norm: 0.373568258615653\n",
      "Iteration 816, BCE loss: 57.76163296945879, Acc: 0.8195, Grad norm: 0.43996715037965817\n",
      "Iteration 817, BCE loss: 57.76176603112974, Acc: 0.8194, Grad norm: 0.45345345437362417\n",
      "Iteration 818, BCE loss: 57.76039777757713, Acc: 0.8194, Grad norm: 0.36143155283109896\n",
      "Iteration 819, BCE loss: 57.759946426072645, Acc: 0.8197, Grad norm: 0.3168189027048267\n",
      "Iteration 820, BCE loss: 57.75914703204206, Acc: 0.8197, Grad norm: 0.2538993882594685\n",
      "Iteration 821, BCE loss: 57.76088371385357, Acc: 0.8198, Grad norm: 0.3834740238070729\n",
      "Iteration 822, BCE loss: 57.760805377114465, Acc: 0.8198, Grad norm: 0.3796146265819104\n",
      "Iteration 823, BCE loss: 57.76232486008613, Acc: 0.82, Grad norm: 0.4713012869818858\n",
      "Iteration 824, BCE loss: 57.759348448123745, Acc: 0.8199, Grad norm: 0.27890578447751035\n",
      "Iteration 825, BCE loss: 57.7603879494679, Acc: 0.8198, Grad norm: 0.3273226376438659\n",
      "Iteration 826, BCE loss: 57.76045846885458, Acc: 0.8197, Grad norm: 0.33037343646290895\n",
      "Iteration 827, BCE loss: 57.760303946152746, Acc: 0.8198, Grad norm: 0.3246828676943539\n",
      "Iteration 828, BCE loss: 57.75928061156391, Acc: 0.8197, Grad norm: 0.2595303672574733\n",
      "Iteration 829, BCE loss: 57.75936198512125, Acc: 0.8197, Grad norm: 0.29257557348088653\n",
      "Iteration 830, BCE loss: 57.75878762776766, Acc: 0.8197, Grad norm: 0.2376907719779476\n",
      "Iteration 831, BCE loss: 57.759576869984045, Acc: 0.8198, Grad norm: 0.30681929981327904\n",
      "Iteration 832, BCE loss: 57.75897407089529, Acc: 0.8198, Grad norm: 0.2537690027150467\n",
      "Iteration 833, BCE loss: 57.76041504390352, Acc: 0.8199, Grad norm: 0.35901665908387276\n",
      "Iteration 834, BCE loss: 57.760606868746784, Acc: 0.8197, Grad norm: 0.3781434790428687\n",
      "Iteration 835, BCE loss: 57.760605782092334, Acc: 0.8197, Grad norm: 0.38796274639325956\n",
      "Iteration 836, BCE loss: 57.75927778537648, Acc: 0.8196, Grad norm: 0.29421151341048973\n",
      "Iteration 837, BCE loss: 57.758165829163715, Acc: 0.8197, Grad norm: 0.18939533609539147\n",
      "Iteration 838, BCE loss: 57.760772159592875, Acc: 0.8198, Grad norm: 0.38697082060958105\n",
      "Iteration 839, BCE loss: 57.75980934949201, Acc: 0.8199, Grad norm: 0.3007595726992318\n",
      "Iteration 840, BCE loss: 57.7601248081843, Acc: 0.8199, Grad norm: 0.3387384497916202\n",
      "Iteration 841, BCE loss: 57.7603313185503, Acc: 0.8197, Grad norm: 0.3539726350270739\n",
      "Iteration 842, BCE loss: 57.76022579679972, Acc: 0.8198, Grad norm: 0.32361762829991486\n",
      "Iteration 843, BCE loss: 57.759378051241384, Acc: 0.8198, Grad norm: 0.26962746194836273\n",
      "Iteration 844, BCE loss: 57.75919298175825, Acc: 0.8199, Grad norm: 0.2705476790393377\n",
      "Iteration 845, BCE loss: 57.76207168854151, Acc: 0.8198, Grad norm: 0.4283891134520565\n",
      "Iteration 846, BCE loss: 57.762752642000365, Acc: 0.8198, Grad norm: 0.46085417062383605\n",
      "Iteration 847, BCE loss: 57.761120090368145, Acc: 0.8197, Grad norm: 0.3880569814659979\n",
      "Iteration 848, BCE loss: 57.75956169854992, Acc: 0.8197, Grad norm: 0.29776704966230605\n",
      "Iteration 849, BCE loss: 57.758312390600736, Acc: 0.8198, Grad norm: 0.17924740102986406\n",
      "Iteration 850, BCE loss: 57.75837581037341, Acc: 0.8198, Grad norm: 0.19816668621958955\n",
      "Iteration 851, BCE loss: 57.75839178291734, Acc: 0.8197, Grad norm: 0.20128699014739956\n",
      "Iteration 852, BCE loss: 57.75976236883392, Acc: 0.8196, Grad norm: 0.3111859642930558\n",
      "Iteration 853, BCE loss: 57.75983790844103, Acc: 0.8196, Grad norm: 0.3206564250386286\n",
      "Iteration 854, BCE loss: 57.761421492473644, Acc: 0.8197, Grad norm: 0.39492368936986855\n",
      "Iteration 855, BCE loss: 57.76002081311006, Acc: 0.8197, Grad norm: 0.3247361670635504\n",
      "Iteration 856, BCE loss: 57.75925626649726, Acc: 0.8198, Grad norm: 0.26606725407088117\n",
      "Iteration 857, BCE loss: 57.75976813915487, Acc: 0.8197, Grad norm: 0.308488934443322\n",
      "Iteration 858, BCE loss: 57.75931416679167, Acc: 0.8199, Grad norm: 0.27411483991825136\n",
      "Iteration 859, BCE loss: 57.75819446173226, Acc: 0.8197, Grad norm: 0.1798046657016554\n",
      "Iteration 860, BCE loss: 57.75814650284718, Acc: 0.8197, Grad norm: 0.18111288056974212\n",
      "Iteration 861, BCE loss: 57.76016347665825, Acc: 0.8196, Grad norm: 0.3518241105291052\n",
      "Iteration 862, BCE loss: 57.76114675585978, Acc: 0.8195, Grad norm: 0.4183103521152593\n",
      "Iteration 863, BCE loss: 57.76019981802591, Acc: 0.8195, Grad norm: 0.36521984177483535\n",
      "Iteration 864, BCE loss: 57.75933079216117, Acc: 0.8195, Grad norm: 0.29914021403254076\n",
      "Iteration 865, BCE loss: 57.75941762439513, Acc: 0.8196, Grad norm: 0.286659418479276\n",
      "Iteration 866, BCE loss: 57.759982814578464, Acc: 0.8196, Grad norm: 0.3400010395508686\n",
      "Iteration 867, BCE loss: 57.759602289166395, Acc: 0.8197, Grad norm: 0.3079660865709944\n",
      "Iteration 868, BCE loss: 57.76052521323711, Acc: 0.8198, Grad norm: 0.3591028603843173\n",
      "Iteration 869, BCE loss: 57.76047612825516, Acc: 0.8199, Grad norm: 0.35049374739518707\n",
      "Iteration 870, BCE loss: 57.760606352145416, Acc: 0.8197, Grad norm: 0.38171100315943235\n",
      "Iteration 871, BCE loss: 57.76146844608331, Acc: 0.8195, Grad norm: 0.4403578990883554\n",
      "Iteration 872, BCE loss: 57.75987891208848, Acc: 0.8195, Grad norm: 0.3342476319579202\n",
      "Iteration 873, BCE loss: 57.758993564410744, Acc: 0.8196, Grad norm: 0.266132415042505\n",
      "Iteration 874, BCE loss: 57.758616469340396, Acc: 0.8196, Grad norm: 0.22104017360422543\n",
      "Iteration 875, BCE loss: 57.75775929614219, Acc: 0.8197, Grad norm: 0.12351088843384009\n",
      "Iteration 876, BCE loss: 57.75839350158811, Acc: 0.8198, Grad norm: 0.20043088432848324\n",
      "Iteration 877, BCE loss: 57.760210398282915, Acc: 0.8199, Grad norm: 0.340734699171414\n",
      "Iteration 878, BCE loss: 57.7586860962647, Acc: 0.8196, Grad norm: 0.22880366663234078\n",
      "Iteration 879, BCE loss: 57.759195155774, Acc: 0.8196, Grad norm: 0.2653387407167868\n",
      "Iteration 880, BCE loss: 57.758493925040725, Acc: 0.8197, Grad norm: 0.2134402487517122\n",
      "Iteration 881, BCE loss: 57.759524162021734, Acc: 0.8197, Grad norm: 0.29217324748038037\n",
      "Iteration 882, BCE loss: 57.75959686955059, Acc: 0.8197, Grad norm: 0.30605503293470127\n",
      "Iteration 883, BCE loss: 57.76146148915248, Acc: 0.8197, Grad norm: 0.4197819826480574\n",
      "Iteration 884, BCE loss: 57.76205725236123, Acc: 0.8196, Grad norm: 0.4524802417390812\n",
      "Iteration 885, BCE loss: 57.76222333076776, Acc: 0.8195, Grad norm: 0.46155109319298343\n",
      "Iteration 886, BCE loss: 57.76489550006515, Acc: 0.8196, Grad norm: 0.5916948095382323\n",
      "Iteration 887, BCE loss: 57.76329079644779, Acc: 0.8198, Grad norm: 0.5166497781725254\n",
      "Iteration 888, BCE loss: 57.761125716182015, Acc: 0.8199, Grad norm: 0.4131561099537756\n",
      "Iteration 889, BCE loss: 57.75964048821735, Acc: 0.8199, Grad norm: 0.3141701951562203\n",
      "Iteration 890, BCE loss: 57.75985738083308, Acc: 0.8198, Grad norm: 0.3096300397486327\n",
      "Iteration 891, BCE loss: 57.759023157758534, Acc: 0.8197, Grad norm: 0.25088602874438076\n",
      "Iteration 892, BCE loss: 57.759616620046785, Acc: 0.8197, Grad norm: 0.2939079330824528\n",
      "Iteration 893, BCE loss: 57.75934925059344, Acc: 0.8196, Grad norm: 0.29054160244488525\n",
      "Iteration 894, BCE loss: 57.75982152796302, Acc: 0.8196, Grad norm: 0.3237374597548889\n",
      "Iteration 895, BCE loss: 57.75922573833342, Acc: 0.8196, Grad norm: 0.2869608131633843\n",
      "Iteration 896, BCE loss: 57.758641058704185, Acc: 0.8196, Grad norm: 0.2348317441085328\n",
      "Iteration 897, BCE loss: 57.75778437460532, Acc: 0.8197, Grad norm: 0.12503844953603294\n",
      "Iteration 898, BCE loss: 57.75850945780748, Acc: 0.8198, Grad norm: 0.22475657965238852\n",
      "Iteration 899, BCE loss: 57.75833595236539, Acc: 0.8197, Grad norm: 0.1948637579366217\n",
      "Iteration 900, BCE loss: 57.759000228630626, Acc: 0.8197, Grad norm: 0.260541567920643\n",
      "Iteration 901, BCE loss: 57.759875294150866, Acc: 0.8197, Grad norm: 0.3186869597728346\n",
      "Iteration 902, BCE loss: 57.76200067285673, Acc: 0.8198, Grad norm: 0.43687877718788415\n",
      "Iteration 903, BCE loss: 57.76007620569888, Acc: 0.8198, Grad norm: 0.3325292379538142\n",
      "Iteration 904, BCE loss: 57.758957372937886, Acc: 0.8197, Grad norm: 0.2502851937055443\n",
      "Iteration 905, BCE loss: 57.758802522474454, Acc: 0.8197, Grad norm: 0.2404070108348535\n",
      "Iteration 906, BCE loss: 57.76043922459476, Acc: 0.8199, Grad norm: 0.36572207900783027\n",
      "Iteration 907, BCE loss: 57.76009702108874, Acc: 0.82, Grad norm: 0.33332120958928374\n",
      "Iteration 908, BCE loss: 57.76328032550843, Acc: 0.82, Grad norm: 0.5082969122886843\n",
      "Iteration 909, BCE loss: 57.76075348451869, Acc: 0.8199, Grad norm: 0.37581214761869536\n",
      "Iteration 910, BCE loss: 57.759522158144115, Acc: 0.8197, Grad norm: 0.3084305673635846\n",
      "Iteration 911, BCE loss: 57.75837217670023, Acc: 0.8196, Grad norm: 0.20159769998378174\n",
      "Iteration 912, BCE loss: 57.7588901787084, Acc: 0.8195, Grad norm: 0.23045712984483838\n",
      "Iteration 913, BCE loss: 57.75820957881017, Acc: 0.8196, Grad norm: 0.18992861880987538\n",
      "Iteration 914, BCE loss: 57.75910372494493, Acc: 0.8196, Grad norm: 0.2808733848298131\n",
      "Iteration 915, BCE loss: 57.75985545982406, Acc: 0.8194, Grad norm: 0.32133212211157736\n",
      "Iteration 916, BCE loss: 57.75911640665233, Acc: 0.8195, Grad norm: 0.2780565463948652\n",
      "Iteration 917, BCE loss: 57.759024931459166, Acc: 0.8196, Grad norm: 0.2665560766195499\n",
      "Iteration 918, BCE loss: 57.76074032826965, Acc: 0.8196, Grad norm: 0.3884251339794352\n",
      "Iteration 919, BCE loss: 57.758926071922524, Acc: 0.8196, Grad norm: 0.2636108862746843\n",
      "Iteration 920, BCE loss: 57.759925801979, Acc: 0.8198, Grad norm: 0.31923336430530297\n",
      "Iteration 921, BCE loss: 57.759362647327386, Acc: 0.8197, Grad norm: 0.2906978091630978\n",
      "Iteration 922, BCE loss: 57.758354950252844, Acc: 0.8197, Grad norm: 0.2008680575665818\n",
      "Iteration 923, BCE loss: 57.75891694927867, Acc: 0.8196, Grad norm: 0.2589105390094109\n",
      "Iteration 924, BCE loss: 57.759103366660185, Acc: 0.8197, Grad norm: 0.27862476153624827\n",
      "Iteration 925, BCE loss: 57.760223465549174, Acc: 0.8196, Grad norm: 0.3569416034961264\n",
      "Iteration 926, BCE loss: 57.75877233952902, Acc: 0.8197, Grad norm: 0.2323515975161701\n",
      "Iteration 927, BCE loss: 57.758715858478794, Acc: 0.8198, Grad norm: 0.2322574137320214\n",
      "Iteration 928, BCE loss: 57.758424826087946, Acc: 0.8197, Grad norm: 0.20686353009372735\n",
      "Iteration 929, BCE loss: 57.75896270489979, Acc: 0.8196, Grad norm: 0.250311348076289\n",
      "Iteration 930, BCE loss: 57.75943413376813, Acc: 0.8196, Grad norm: 0.2883351599911387\n",
      "Iteration 931, BCE loss: 57.75870720187181, Acc: 0.8196, Grad norm: 0.23384438800004573\n",
      "Iteration 932, BCE loss: 57.75867340189826, Acc: 0.8196, Grad norm: 0.2307923380061604\n",
      "Iteration 933, BCE loss: 57.758770035706064, Acc: 0.8195, Grad norm: 0.22695855989214908\n",
      "Iteration 934, BCE loss: 57.758355343817456, Acc: 0.8196, Grad norm: 0.17912744710020603\n",
      "Iteration 935, BCE loss: 57.75916241711482, Acc: 0.8195, Grad norm: 0.2593582929674212\n",
      "Iteration 936, BCE loss: 57.761072306211354, Acc: 0.8193, Grad norm: 0.3795650836317545\n",
      "Iteration 937, BCE loss: 57.760244510180016, Acc: 0.8195, Grad norm: 0.3285760671040999\n",
      "Iteration 938, BCE loss: 57.76047047554512, Acc: 0.8195, Grad norm: 0.31762320151364587\n",
      "Iteration 939, BCE loss: 57.75967185596193, Acc: 0.8196, Grad norm: 0.2882167744323984\n",
      "Iteration 940, BCE loss: 57.760025191617686, Acc: 0.8196, Grad norm: 0.3185471990160166\n",
      "Iteration 941, BCE loss: 57.75928468328623, Acc: 0.8196, Grad norm: 0.27458653686754037\n",
      "Iteration 942, BCE loss: 57.759453808809994, Acc: 0.8196, Grad norm: 0.2761142358179035\n",
      "Iteration 943, BCE loss: 57.7589889885185, Acc: 0.8197, Grad norm: 0.2333666177667215\n",
      "Iteration 944, BCE loss: 57.758730434127145, Acc: 0.8197, Grad norm: 0.23116073054382924\n",
      "Iteration 945, BCE loss: 57.75920423802151, Acc: 0.8196, Grad norm: 0.27589349972516064\n",
      "Iteration 946, BCE loss: 57.7586441170068, Acc: 0.8197, Grad norm: 0.2270711431833649\n",
      "Iteration 947, BCE loss: 57.75786108303433, Acc: 0.8196, Grad norm: 0.14042118918740165\n",
      "Iteration 948, BCE loss: 57.75887487913599, Acc: 0.8196, Grad norm: 0.24613487560804154\n",
      "Iteration 949, BCE loss: 57.75856544970236, Acc: 0.8197, Grad norm: 0.22985601501633188\n",
      "Iteration 950, BCE loss: 57.75855606123801, Acc: 0.8196, Grad norm: 0.22755746365051088\n",
      "Iteration 951, BCE loss: 57.76003663677266, Acc: 0.8197, Grad norm: 0.34565158003852076\n",
      "Iteration 952, BCE loss: 57.75891257444499, Acc: 0.8195, Grad norm: 0.2428442058057562\n",
      "Iteration 953, BCE loss: 57.75852904954212, Acc: 0.8196, Grad norm: 0.21921567552441992\n",
      "Iteration 954, BCE loss: 57.75792268894337, Acc: 0.8196, Grad norm: 0.15360336125350948\n",
      "Iteration 955, BCE loss: 57.75774908856005, Acc: 0.8196, Grad norm: 0.12601618241848725\n",
      "Iteration 956, BCE loss: 57.75770409046771, Acc: 0.8197, Grad norm: 0.13194294307499171\n",
      "Iteration 957, BCE loss: 57.76065720415171, Acc: 0.8198, Grad norm: 0.3918700023498546\n",
      "Iteration 958, BCE loss: 57.759558499818056, Acc: 0.8197, Grad norm: 0.31281062407522187\n",
      "Iteration 959, BCE loss: 57.757752325261684, Acc: 0.8196, Grad norm: 0.1287180092773294\n",
      "Iteration 960, BCE loss: 57.75895526338422, Acc: 0.8198, Grad norm: 0.2583232812746611\n",
      "Iteration 961, BCE loss: 57.75879335345244, Acc: 0.8196, Grad norm: 0.24109146103817206\n",
      "Iteration 962, BCE loss: 57.758133042487444, Acc: 0.8196, Grad norm: 0.1692845973372482\n",
      "Iteration 963, BCE loss: 57.75812825845429, Acc: 0.8196, Grad norm: 0.1727444551971193\n",
      "Iteration 964, BCE loss: 57.75850308566132, Acc: 0.8195, Grad norm: 0.2231064082187089\n",
      "Iteration 965, BCE loss: 57.760518756919936, Acc: 0.8197, Grad norm: 0.3842292506111091\n",
      "Iteration 966, BCE loss: 57.759716302088364, Acc: 0.8195, Grad norm: 0.3252248033184201\n",
      "Iteration 967, BCE loss: 57.75971845892301, Acc: 0.8193, Grad norm: 0.31638628182375045\n",
      "Iteration 968, BCE loss: 57.75924388034778, Acc: 0.8193, Grad norm: 0.2744119311811673\n",
      "Iteration 969, BCE loss: 57.7575981830481, Acc: 0.8195, Grad norm: 0.10105280247074802\n",
      "Iteration 970, BCE loss: 57.758325511532824, Acc: 0.8194, Grad norm: 0.19328834924826083\n",
      "Iteration 971, BCE loss: 57.75918707666385, Acc: 0.8195, Grad norm: 0.260582302641227\n",
      "Iteration 972, BCE loss: 57.760702583177405, Acc: 0.8195, Grad norm: 0.35678806463449897\n",
      "Iteration 973, BCE loss: 57.75977886455045, Acc: 0.8195, Grad norm: 0.2991280514145218\n",
      "Iteration 974, BCE loss: 57.75948022274162, Acc: 0.8195, Grad norm: 0.28452837466341124\n",
      "Iteration 975, BCE loss: 57.75868698279254, Acc: 0.8195, Grad norm: 0.22994589381594854\n",
      "Iteration 976, BCE loss: 57.759582436390815, Acc: 0.8195, Grad norm: 0.30232197347710416\n",
      "Iteration 977, BCE loss: 57.75909274196671, Acc: 0.8196, Grad norm: 0.2524106604728986\n",
      "Iteration 978, BCE loss: 57.75911477606486, Acc: 0.8196, Grad norm: 0.2544327016678659\n",
      "Iteration 979, BCE loss: 57.75913356099112, Acc: 0.8194, Grad norm: 0.2451774545671999\n",
      "Iteration 980, BCE loss: 57.75983669450813, Acc: 0.8194, Grad norm: 0.3054435035428324\n",
      "Iteration 981, BCE loss: 57.758840499434356, Acc: 0.8196, Grad norm: 0.23402836623812384\n",
      "Iteration 982, BCE loss: 57.75932333592671, Acc: 0.8196, Grad norm: 0.2708941490140999\n",
      "Iteration 983, BCE loss: 57.75924755620849, Acc: 0.8195, Grad norm: 0.28178651375712144\n",
      "Iteration 984, BCE loss: 57.758907893876426, Acc: 0.8197, Grad norm: 0.26069618901631925\n",
      "Iteration 985, BCE loss: 57.75856884305658, Acc: 0.8197, Grad norm: 0.2334092235079367\n",
      "Iteration 986, BCE loss: 57.75858198592103, Acc: 0.8196, Grad norm: 0.2272982177301299\n",
      "Iteration 987, BCE loss: 57.75929556331094, Acc: 0.8197, Grad norm: 0.28702655673027105\n",
      "Iteration 988, BCE loss: 57.75920193143913, Acc: 0.8196, Grad norm: 0.26803469333745344\n",
      "Iteration 989, BCE loss: 57.75818050520973, Acc: 0.8196, Grad norm: 0.19004272813437975\n",
      "Iteration 990, BCE loss: 57.75815539187765, Acc: 0.8196, Grad norm: 0.18246885359537518\n",
      "Iteration 991, BCE loss: 57.7582730356515, Acc: 0.8196, Grad norm: 0.20339396223410824\n",
      "Iteration 992, BCE loss: 57.75846998092466, Acc: 0.8195, Grad norm: 0.21536597371005575\n",
      "Iteration 993, BCE loss: 57.75950147152802, Acc: 0.8196, Grad norm: 0.2986020241041785\n",
      "Iteration 994, BCE loss: 57.75906656552885, Acc: 0.8195, Grad norm: 0.2628809820030356\n",
      "Iteration 995, BCE loss: 57.758636277480555, Acc: 0.8194, Grad norm: 0.21697582536553275\n",
      "Iteration 996, BCE loss: 57.75890413001429, Acc: 0.8196, Grad norm: 0.23118066898112533\n",
      "Iteration 997, BCE loss: 57.7585078763802, Acc: 0.8196, Grad norm: 0.20477753236169238\n",
      "Iteration 998, BCE loss: 57.7583464954612, Acc: 0.8196, Grad norm: 0.1886824228136195\n",
      "Iteration 999, BCE loss: 57.75927622296375, Acc: 0.8196, Grad norm: 0.27839565635873625\n",
      "Iteration 1000, BCE loss: 57.759019086156556, Acc: 0.8196, Grad norm: 0.26805277847487136\n",
      "Iteration 1001, BCE loss: 57.76115671545262, Acc: 0.8196, Grad norm: 0.39843149503755254\n",
      "Iteration 1002, BCE loss: 57.75986048448149, Acc: 0.8196, Grad norm: 0.3150416747469645\n",
      "Iteration 1003, BCE loss: 57.75938194229308, Acc: 0.8197, Grad norm: 0.27598381999169763\n",
      "Iteration 1004, BCE loss: 57.76016716391424, Acc: 0.8197, Grad norm: 0.34429725488475504\n",
      "Iteration 1005, BCE loss: 57.760083672287706, Acc: 0.8196, Grad norm: 0.33941473091508595\n",
      "Iteration 1006, BCE loss: 57.75941081754269, Acc: 0.8197, Grad norm: 0.30470259421332013\n",
      "Iteration 1007, BCE loss: 57.75969802656428, Acc: 0.8196, Grad norm: 0.32662052351929544\n",
      "Iteration 1008, BCE loss: 57.759887138177376, Acc: 0.8195, Grad norm: 0.3394725310957668\n",
      "Iteration 1009, BCE loss: 57.7614305576257, Acc: 0.8195, Grad norm: 0.43016218909109305\n",
      "Iteration 1010, BCE loss: 57.76071518529765, Acc: 0.8195, Grad norm: 0.3912614100926486\n",
      "Iteration 1011, BCE loss: 57.75962799355819, Acc: 0.8195, Grad norm: 0.3112856578322224\n",
      "Iteration 1012, BCE loss: 57.759196260816054, Acc: 0.8195, Grad norm: 0.26968565371839176\n",
      "Iteration 1013, BCE loss: 57.75901736947422, Acc: 0.8196, Grad norm: 0.25886396420750385\n",
      "Iteration 1014, BCE loss: 57.7588188561748, Acc: 0.8195, Grad norm: 0.24362196243733336\n",
      "Iteration 1015, BCE loss: 57.75856324075295, Acc: 0.8195, Grad norm: 0.2292162752321517\n",
      "Iteration 1016, BCE loss: 57.75834684839426, Acc: 0.8195, Grad norm: 0.19158179126788943\n",
      "Iteration 1017, BCE loss: 57.7582475297813, Acc: 0.8195, Grad norm: 0.20505027205631088\n",
      "Iteration 1018, BCE loss: 57.75960231348546, Acc: 0.8195, Grad norm: 0.3140916954489675\n",
      "Iteration 1019, BCE loss: 57.7596099035932, Acc: 0.8196, Grad norm: 0.3088943107696735\n",
      "Iteration 1020, BCE loss: 57.761196067667655, Acc: 0.8197, Grad norm: 0.4116585502547519\n",
      "Iteration 1021, BCE loss: 57.758985167754865, Acc: 0.8197, Grad norm: 0.26599870185784774\n",
      "Iteration 1022, BCE loss: 57.75854570260009, Acc: 0.8197, Grad norm: 0.22924636822140135\n",
      "Iteration 1023, BCE loss: 57.759486576977366, Acc: 0.8195, Grad norm: 0.3105134506077136\n",
      "Iteration 1024, BCE loss: 57.75848198355854, Acc: 0.8196, Grad norm: 0.2193292002071308\n",
      "Iteration 1025, BCE loss: 57.758579175305854, Acc: 0.8197, Grad norm: 0.2370385415639136\n",
      "Iteration 1026, BCE loss: 57.75825454476576, Acc: 0.8197, Grad norm: 0.19972991447934418\n",
      "Iteration 1027, BCE loss: 57.75905897434262, Acc: 0.8196, Grad norm: 0.26753395166238303\n",
      "Iteration 1028, BCE loss: 57.75946985837696, Acc: 0.8197, Grad norm: 0.29087702758935235\n",
      "Iteration 1029, BCE loss: 57.76118523625344, Acc: 0.8198, Grad norm: 0.40012927443412505\n",
      "Iteration 1030, BCE loss: 57.76179191144038, Acc: 0.8198, Grad norm: 0.43347227691241963\n",
      "Iteration 1031, BCE loss: 57.759040962365546, Acc: 0.8197, Grad norm: 0.264660730351058\n",
      "Iteration 1032, BCE loss: 57.75988504778711, Acc: 0.8197, Grad norm: 0.33023909913737026\n",
      "Iteration 1033, BCE loss: 57.75944925595016, Acc: 0.8196, Grad norm: 0.29202512585089124\n",
      "Iteration 1034, BCE loss: 57.75813116740083, Acc: 0.8195, Grad norm: 0.17748260874676627\n",
      "Iteration 1035, BCE loss: 57.759831462403625, Acc: 0.8195, Grad norm: 0.31905098004311827\n",
      "Iteration 1036, BCE loss: 57.76005975493682, Acc: 0.8194, Grad norm: 0.3387001627381531\n",
      "Iteration 1037, BCE loss: 57.75942154121021, Acc: 0.8195, Grad norm: 0.30319658634922436\n",
      "Iteration 1038, BCE loss: 57.761435045264236, Acc: 0.8193, Grad norm: 0.4226168988913288\n",
      "Iteration 1039, BCE loss: 57.75890598850938, Acc: 0.8195, Grad norm: 0.244978832677887\n",
      "Iteration 1040, BCE loss: 57.759018967795974, Acc: 0.8195, Grad norm: 0.25344332791747815\n",
      "Iteration 1041, BCE loss: 57.75861658525595, Acc: 0.8195, Grad norm: 0.23351898705604693\n",
      "Iteration 1042, BCE loss: 57.75985734132161, Acc: 0.8194, Grad norm: 0.3232301270696964\n",
      "Iteration 1043, BCE loss: 57.75989499925858, Acc: 0.8194, Grad norm: 0.3210968732367852\n",
      "Iteration 1044, BCE loss: 57.76031439377853, Acc: 0.8193, Grad norm: 0.3417888113889237\n",
      "Iteration 1045, BCE loss: 57.759439723693646, Acc: 0.8193, Grad norm: 0.28756818581519367\n",
      "Iteration 1046, BCE loss: 57.75899218087103, Acc: 0.8194, Grad norm: 0.25280590723537744\n",
      "Iteration 1047, BCE loss: 57.75872006784901, Acc: 0.8194, Grad norm: 0.24330598929005717\n",
      "Iteration 1048, BCE loss: 57.75864285019594, Acc: 0.8194, Grad norm: 0.2374163995271017\n",
      "Iteration 1049, BCE loss: 57.758729801480285, Acc: 0.8195, Grad norm: 0.23420072594336358\n",
      "Iteration 1050, BCE loss: 57.759006473343376, Acc: 0.8195, Grad norm: 0.2543995097357223\n",
      "Iteration 1051, BCE loss: 57.758242442232614, Acc: 0.8196, Grad norm: 0.19510955544254496\n",
      "Iteration 1052, BCE loss: 57.75810115514945, Acc: 0.8196, Grad norm: 0.18453142709892617\n",
      "Iteration 1053, BCE loss: 57.75794343440124, Acc: 0.8196, Grad norm: 0.16261827125502984\n",
      "Iteration 1054, BCE loss: 57.75810906608784, Acc: 0.8198, Grad norm: 0.1803038836155769\n",
      "Iteration 1055, BCE loss: 57.75936114906856, Acc: 0.8198, Grad norm: 0.28406039025241087\n",
      "Iteration 1056, BCE loss: 57.75968330344031, Acc: 0.8199, Grad norm: 0.3100553466213776\n",
      "Iteration 1057, BCE loss: 57.75913263003229, Acc: 0.8199, Grad norm: 0.2798750250488917\n",
      "Iteration 1058, BCE loss: 57.758725865897866, Acc: 0.8198, Grad norm: 0.2377954329777402\n",
      "Iteration 1059, BCE loss: 57.7589935924196, Acc: 0.8197, Grad norm: 0.2585756926609732\n",
      "Iteration 1060, BCE loss: 57.75902163910674, Acc: 0.8198, Grad norm: 0.26733183861591636\n",
      "Iteration 1061, BCE loss: 57.7596732640756, Acc: 0.8198, Grad norm: 0.30992034557651754\n",
      "Iteration 1062, BCE loss: 57.76150836819, Acc: 0.8199, Grad norm: 0.4155255666494022\n",
      "Iteration 1063, BCE loss: 57.76051496897266, Acc: 0.8199, Grad norm: 0.3440343246590383\n",
      "Iteration 1064, BCE loss: 57.76006686495663, Acc: 0.8198, Grad norm: 0.30640627396394604\n",
      "Iteration 1065, BCE loss: 57.75994064979024, Acc: 0.8198, Grad norm: 0.3120051603930657\n",
      "Iteration 1066, BCE loss: 57.7607666054404, Acc: 0.8199, Grad norm: 0.3758373076467802\n",
      "Iteration 1067, BCE loss: 57.7620437166991, Acc: 0.8199, Grad norm: 0.45047951243689205\n",
      "Iteration 1068, BCE loss: 57.75881286827859, Acc: 0.8197, Grad norm: 0.24582410685354011\n",
      "Iteration 1069, BCE loss: 57.75891639801402, Acc: 0.8198, Grad norm: 0.25098026307564486\n",
      "Iteration 1070, BCE loss: 57.758596692636885, Acc: 0.8197, Grad norm: 0.23427379786400993\n",
      "Iteration 1071, BCE loss: 57.7583868132938, Acc: 0.8195, Grad norm: 0.20336161252192533\n",
      "Iteration 1072, BCE loss: 57.75853103713206, Acc: 0.8195, Grad norm: 0.23084402283321184\n",
      "Iteration 1073, BCE loss: 57.75806512233335, Acc: 0.8195, Grad norm: 0.17635530904184166\n",
      "Iteration 1074, BCE loss: 57.75838114677397, Acc: 0.8195, Grad norm: 0.2090610964495597\n",
      "Iteration 1075, BCE loss: 57.75803111978331, Acc: 0.8197, Grad norm: 0.17617636144720086\n",
      "Iteration 1076, BCE loss: 57.75950411732575, Acc: 0.8199, Grad norm: 0.319730075356615\n",
      "Iteration 1077, BCE loss: 57.76130514541103, Acc: 0.82, Grad norm: 0.4330428938705468\n",
      "Iteration 1078, BCE loss: 57.76095158296408, Acc: 0.8199, Grad norm: 0.40941058830880445\n",
      "Iteration 1079, BCE loss: 57.76256309974764, Acc: 0.8198, Grad norm: 0.4871587838559626\n",
      "Iteration 1080, BCE loss: 57.75877638934955, Acc: 0.8197, Grad norm: 0.2558774061890908\n",
      "Iteration 1081, BCE loss: 57.758173531672895, Acc: 0.8196, Grad norm: 0.1912134461544133\n",
      "Iteration 1082, BCE loss: 57.75778397663214, Acc: 0.8196, Grad norm: 0.12394303985873036\n",
      "Iteration 1083, BCE loss: 57.757749136606904, Acc: 0.8197, Grad norm: 0.12245585875156649\n",
      "Iteration 1084, BCE loss: 57.7582270257291, Acc: 0.8196, Grad norm: 0.19175065189290763\n",
      "Iteration 1085, BCE loss: 57.75778371836366, Acc: 0.8196, Grad norm: 0.14130357276066263\n",
      "Iteration 1086, BCE loss: 57.758394799896195, Acc: 0.8195, Grad norm: 0.2186856017915412\n",
      "Iteration 1087, BCE loss: 57.75887584792051, Acc: 0.8195, Grad norm: 0.2588943187231883\n",
      "Iteration 1088, BCE loss: 57.759405017401846, Acc: 0.8195, Grad norm: 0.2964161758856601\n",
      "Iteration 1089, BCE loss: 57.759143348607914, Acc: 0.8195, Grad norm: 0.2791231254366368\n",
      "Iteration 1090, BCE loss: 57.759179824082125, Acc: 0.8196, Grad norm: 0.27631016306228284\n",
      "Iteration 1091, BCE loss: 57.75922689965587, Acc: 0.8196, Grad norm: 0.2745406735869482\n",
      "Iteration 1092, BCE loss: 57.75836589958482, Acc: 0.8196, Grad norm: 0.19841495297304512\n",
      "Iteration 1093, BCE loss: 57.758777237758544, Acc: 0.8198, Grad norm: 0.25375866686194576\n",
      "Iteration 1094, BCE loss: 57.75830169031772, Acc: 0.8198, Grad norm: 0.19865008076563814\n",
      "Iteration 1095, BCE loss: 57.75827751293105, Acc: 0.8197, Grad norm: 0.18593238221612687\n",
      "Iteration 1096, BCE loss: 57.758892699404356, Acc: 0.8197, Grad norm: 0.25903161239419237\n",
      "Iteration 1097, BCE loss: 57.7582747818105, Acc: 0.8197, Grad norm: 0.20551343455291504\n",
      "Iteration 1098, BCE loss: 57.75938256412228, Acc: 0.8196, Grad norm: 0.30664947462518666\n",
      "Iteration 1099, BCE loss: 57.75814610564913, Acc: 0.8196, Grad norm: 0.18806948660356584\n",
      "Iteration 1100, BCE loss: 57.75820379818767, Acc: 0.8197, Grad norm: 0.18625745569646726\n",
      "Iteration 1101, BCE loss: 57.75811085595122, Acc: 0.8197, Grad norm: 0.17319072236938174\n",
      "Iteration 1102, BCE loss: 57.757885056772956, Acc: 0.8197, Grad norm: 0.15304694837554242\n",
      "Iteration 1103, BCE loss: 57.758076890678545, Acc: 0.8197, Grad norm: 0.17488688491434462\n",
      "Iteration 1104, BCE loss: 57.75908599414008, Acc: 0.8197, Grad norm: 0.2811784926222984\n",
      "Iteration 1105, BCE loss: 57.75911627465713, Acc: 0.8196, Grad norm: 0.27831201342734696\n",
      "Iteration 1106, BCE loss: 57.75926425082884, Acc: 0.8195, Grad norm: 0.29262570743660904\n",
      "Iteration 1107, BCE loss: 57.759399170517476, Acc: 0.8195, Grad norm: 0.2929197934735632\n",
      "Iteration 1108, BCE loss: 57.759300093934, Acc: 0.8196, Grad norm: 0.2902294254625347\n",
      "Iteration 1109, BCE loss: 57.75895901137183, Acc: 0.8195, Grad norm: 0.2714675876711853\n",
      "Iteration 1110, BCE loss: 57.75903847207511, Acc: 0.8194, Grad norm: 0.2772037400628539\n",
      "Iteration 1111, BCE loss: 57.75801592323093, Acc: 0.8196, Grad norm: 0.1717265414222861\n",
      "Iteration 1112, BCE loss: 57.75774115400762, Acc: 0.8195, Grad norm: 0.13140871795705683\n",
      "Iteration 1113, BCE loss: 57.75803312833813, Acc: 0.8196, Grad norm: 0.15650561389468154\n",
      "Iteration 1114, BCE loss: 57.75789036811729, Acc: 0.8196, Grad norm: 0.14913361975992293\n",
      "Iteration 1115, BCE loss: 57.75816978532323, Acc: 0.8197, Grad norm: 0.1805261270327327\n",
      "Iteration 1116, BCE loss: 57.75860243755176, Acc: 0.8196, Grad norm: 0.2099111862317158\n",
      "Iteration 1117, BCE loss: 57.75902638676901, Acc: 0.8195, Grad norm: 0.24108834004802332\n",
      "Iteration 1118, BCE loss: 57.758171040554245, Acc: 0.8196, Grad norm: 0.1742944918844673\n",
      "Iteration 1119, BCE loss: 57.75912040529201, Acc: 0.8195, Grad norm: 0.2581482389390657\n",
      "Iteration 1120, BCE loss: 57.758471349788806, Acc: 0.8195, Grad norm: 0.22028372630540996\n",
      "Iteration 1121, BCE loss: 57.758779799954, Acc: 0.8196, Grad norm: 0.2492718801799084\n",
      "Iteration 1122, BCE loss: 57.7579371271883, Acc: 0.8196, Grad norm: 0.15472104573195666\n",
      "Iteration 1123, BCE loss: 57.75881220162333, Acc: 0.8195, Grad norm: 0.2543816714723891\n",
      "Iteration 1124, BCE loss: 57.75881126124233, Acc: 0.8196, Grad norm: 0.23554839330121224\n",
      "Iteration 1125, BCE loss: 57.75913249887371, Acc: 0.8196, Grad norm: 0.26745028896891593\n",
      "Iteration 1126, BCE loss: 57.759022732111376, Acc: 0.8197, Grad norm: 0.2656250749702027\n",
      "Iteration 1127, BCE loss: 57.75840839450234, Acc: 0.8198, Grad norm: 0.22288918795601184\n",
      "Iteration 1128, BCE loss: 57.758824850871264, Acc: 0.8198, Grad norm: 0.25265763216283654\n",
      "Iteration 1129, BCE loss: 57.75880114030029, Acc: 0.8197, Grad norm: 0.24161965142890207\n",
      "Iteration 1130, BCE loss: 57.75877378677311, Acc: 0.8197, Grad norm: 0.2354993091106182\n",
      "Iteration 1131, BCE loss: 57.75970634627292, Acc: 0.8199, Grad norm: 0.31758420202399396\n",
      "Iteration 1132, BCE loss: 57.760143658763916, Acc: 0.8198, Grad norm: 0.35214972347428575\n",
      "Iteration 1133, BCE loss: 57.75964171620244, Acc: 0.8199, Grad norm: 0.3229611275478749\n",
      "Iteration 1134, BCE loss: 57.76215659268672, Acc: 0.82, Grad norm: 0.4687516837666563\n",
      "Iteration 1135, BCE loss: 57.760899546166385, Acc: 0.82, Grad norm: 0.39636634767587536\n",
      "Iteration 1136, BCE loss: 57.75950213803584, Acc: 0.8198, Grad norm: 0.3096143367913998\n",
      "Iteration 1137, BCE loss: 57.759399849218426, Acc: 0.8198, Grad norm: 0.2960790073460116\n",
      "Iteration 1138, BCE loss: 57.758692309134915, Acc: 0.8197, Grad norm: 0.24085940027527714\n",
      "Iteration 1139, BCE loss: 57.75873106479266, Acc: 0.8197, Grad norm: 0.2485079985088134\n",
      "Iteration 1140, BCE loss: 57.75779394482707, Acc: 0.8196, Grad norm: 0.14655785769225105\n",
      "Iteration 1141, BCE loss: 57.75822636617089, Acc: 0.8195, Grad norm: 0.19867602705276047\n",
      "Iteration 1142, BCE loss: 57.757929875688454, Acc: 0.8196, Grad norm: 0.15792035318998557\n",
      "Iteration 1143, BCE loss: 57.75888668045921, Acc: 0.8196, Grad norm: 0.2448562770315392\n",
      "Iteration 1144, BCE loss: 57.758210569564596, Acc: 0.8196, Grad norm: 0.19736139034328204\n",
      "Iteration 1145, BCE loss: 57.75786574347566, Acc: 0.8196, Grad norm: 0.14582550273143124\n",
      "Iteration 1146, BCE loss: 57.758020904613545, Acc: 0.8196, Grad norm: 0.16902004490518424\n",
      "Iteration 1147, BCE loss: 57.75770628786549, Acc: 0.8197, Grad norm: 0.13487558018802814\n",
      "Iteration 1148, BCE loss: 57.758470754976116, Acc: 0.8198, Grad norm: 0.21964128292139024\n",
      "Iteration 1149, BCE loss: 57.7595858912662, Acc: 0.8197, Grad norm: 0.3153356829288127\n",
      "Iteration 1150, BCE loss: 57.759207773102375, Acc: 0.8197, Grad norm: 0.28903647474496286\n",
      "Iteration 1151, BCE loss: 57.75812701364973, Acc: 0.8196, Grad norm: 0.168412519359454\n",
      "Iteration 1152, BCE loss: 57.758284281610216, Acc: 0.8196, Grad norm: 0.1852747099199348\n",
      "Iteration 1153, BCE loss: 57.758392766515996, Acc: 0.8197, Grad norm: 0.20840902276846274\n",
      "Iteration 1154, BCE loss: 57.75927869637155, Acc: 0.8198, Grad norm: 0.2850312920830865\n",
      "Iteration 1155, BCE loss: 57.760125047845264, Acc: 0.8198, Grad norm: 0.34543411096591564\n",
      "Iteration 1156, BCE loss: 57.75872487653925, Acc: 0.8197, Grad norm: 0.2321090156542389\n",
      "Iteration 1157, BCE loss: 57.75918677247658, Acc: 0.8197, Grad norm: 0.27793430739387026\n",
      "Iteration 1158, BCE loss: 57.76004144869144, Acc: 0.8197, Grad norm: 0.34206655118030516\n",
      "Iteration 1159, BCE loss: 57.759717363906574, Acc: 0.8197, Grad norm: 0.32422679159114\n",
      "Iteration 1160, BCE loss: 57.75856150368617, Acc: 0.8197, Grad norm: 0.22550998495181707\n",
      "Iteration 1161, BCE loss: 57.75792488308183, Acc: 0.8197, Grad norm: 0.1495048487326918\n",
      "Iteration 1162, BCE loss: 57.758531848015004, Acc: 0.8197, Grad norm: 0.20580371085163243\n",
      "Iteration 1163, BCE loss: 57.758693817939246, Acc: 0.8197, Grad norm: 0.2142220267559554\n",
      "Iteration 1164, BCE loss: 57.758730531911894, Acc: 0.8197, Grad norm: 0.23596712812482742\n",
      "Iteration 1165, BCE loss: 57.759305291629154, Acc: 0.8197, Grad norm: 0.27114030372333364\n",
      "Iteration 1166, BCE loss: 57.75964089052252, Acc: 0.8197, Grad norm: 0.2972255595093274\n",
      "Iteration 1167, BCE loss: 57.75831815038457, Acc: 0.8196, Grad norm: 0.191371427547574\n",
      "Iteration 1168, BCE loss: 57.75977002394588, Acc: 0.8196, Grad norm: 0.31573568413448927\n",
      "Iteration 1169, BCE loss: 57.75906559470953, Acc: 0.8197, Grad norm: 0.2665759115893246\n",
      "Iteration 1170, BCE loss: 57.759790205588985, Acc: 0.8198, Grad norm: 0.3264569774767524\n",
      "Iteration 1171, BCE loss: 57.759294049907496, Acc: 0.8197, Grad norm: 0.29178216282324015\n",
      "Iteration 1172, BCE loss: 57.7583615110301, Acc: 0.8197, Grad norm: 0.20245422144566044\n",
      "Iteration 1173, BCE loss: 57.75777060114447, Acc: 0.8197, Grad norm: 0.1265944570340046\n",
      "Iteration 1174, BCE loss: 57.75810371976436, Acc: 0.8196, Grad norm: 0.16596486248096531\n",
      "Iteration 1175, BCE loss: 57.75943462227832, Acc: 0.8195, Grad norm: 0.2959445781120633\n",
      "Iteration 1176, BCE loss: 57.75858300856378, Acc: 0.8196, Grad norm: 0.22509739142079035\n",
      "Iteration 1177, BCE loss: 57.75875174953128, Acc: 0.8194, Grad norm: 0.251877811386801\n",
      "Iteration 1178, BCE loss: 57.75826977819299, Acc: 0.8195, Grad norm: 0.20419294024939566\n",
      "Iteration 1179, BCE loss: 57.75885686473807, Acc: 0.8194, Grad norm: 0.24338879196831498\n",
      "Iteration 1180, BCE loss: 57.75951064921813, Acc: 0.8195, Grad norm: 0.2986956193162241\n",
      "Iteration 1181, BCE loss: 57.75948209821494, Acc: 0.8195, Grad norm: 0.301920573660329\n",
      "Iteration 1182, BCE loss: 57.758758521156125, Acc: 0.8197, Grad norm: 0.22094171612182756\n",
      "Iteration 1183, BCE loss: 57.75828442993502, Acc: 0.8197, Grad norm: 0.18540818934055153\n",
      "Iteration 1184, BCE loss: 57.75789223035831, Acc: 0.8196, Grad norm: 0.1490992491879105\n",
      "Iteration 1185, BCE loss: 57.75844667485684, Acc: 0.8195, Grad norm: 0.213883970608995\n",
      "Iteration 1186, BCE loss: 57.75861311759553, Acc: 0.8197, Grad norm: 0.23199837925146716\n",
      "Iteration 1187, BCE loss: 57.75861192111155, Acc: 0.8197, Grad norm: 0.2250947803223914\n",
      "Iteration 1188, BCE loss: 57.75842093279482, Acc: 0.8195, Grad norm: 0.20408826138817293\n",
      "Iteration 1189, BCE loss: 57.758876131562964, Acc: 0.8197, Grad norm: 0.24780568988943052\n",
      "Iteration 1190, BCE loss: 57.75851814540009, Acc: 0.8197, Grad norm: 0.21370193916642588\n",
      "Iteration 1191, BCE loss: 57.758457002586105, Acc: 0.8196, Grad norm: 0.2091473092635186\n",
      "Iteration 1192, BCE loss: 57.758540636005336, Acc: 0.8196, Grad norm: 0.21676143999937872\n",
      "Iteration 1193, BCE loss: 57.758694851352715, Acc: 0.8197, Grad norm: 0.23405431955621447\n",
      "Iteration 1194, BCE loss: 57.758830183963624, Acc: 0.8198, Grad norm: 0.23775143957609127\n",
      "Iteration 1195, BCE loss: 57.75926377772336, Acc: 0.8196, Grad norm: 0.283033378200443\n",
      "Iteration 1196, BCE loss: 57.75795420724991, Acc: 0.8197, Grad norm: 0.14989940932459675\n",
      "Iteration 1197, BCE loss: 57.75807400409982, Acc: 0.8197, Grad norm: 0.17732330820650535\n",
      "Iteration 1198, BCE loss: 57.758584407791965, Acc: 0.8197, Grad norm: 0.22260813524409923\n",
      "Iteration 1199, BCE loss: 57.75810599283572, Acc: 0.8198, Grad norm: 0.17272057878756022\n",
      "Iteration 1200, BCE loss: 57.75890081961224, Acc: 0.8196, Grad norm: 0.27107026988774224\n",
      "Iteration 1201, BCE loss: 57.75903341597329, Acc: 0.8198, Grad norm: 0.2719993678980394\n",
      "Iteration 1202, BCE loss: 57.75932437727006, Acc: 0.8197, Grad norm: 0.29476254755354275\n",
      "Iteration 1203, BCE loss: 57.76000368213903, Acc: 0.8197, Grad norm: 0.343121902841953\n",
      "Iteration 1204, BCE loss: 57.759135929976026, Acc: 0.8197, Grad norm: 0.27412047514734916\n",
      "Iteration 1205, BCE loss: 57.7591205243875, Acc: 0.8196, Grad norm: 0.2692932818635078\n",
      "Iteration 1206, BCE loss: 57.75922056555558, Acc: 0.8197, Grad norm: 0.2782425506079511\n",
      "Iteration 1207, BCE loss: 57.75878471247944, Acc: 0.8196, Grad norm: 0.2383621799957053\n",
      "Iteration 1208, BCE loss: 57.75838969239476, Acc: 0.8195, Grad norm: 0.20583808527942263\n",
      "Iteration 1209, BCE loss: 57.75885852517439, Acc: 0.8197, Grad norm: 0.2507031782781356\n",
      "Iteration 1210, BCE loss: 57.758966923092956, Acc: 0.8196, Grad norm: 0.26464214507251566\n",
      "Iteration 1211, BCE loss: 57.7579488268317, Acc: 0.8196, Grad norm: 0.16559341091649274\n",
      "Iteration 1212, BCE loss: 57.75789191905555, Acc: 0.8197, Grad norm: 0.16923593964617348\n",
      "Iteration 1213, BCE loss: 57.75795715957902, Acc: 0.8197, Grad norm: 0.17367881391628048\n",
      "Iteration 1214, BCE loss: 57.75771909811052, Acc: 0.8196, Grad norm: 0.1260567074744818\n",
      "Iteration 1215, BCE loss: 57.758146014631215, Acc: 0.8196, Grad norm: 0.16514635438603306\n",
      "Iteration 1216, BCE loss: 57.75782764842809, Acc: 0.8197, Grad norm: 0.14325921969440897\n",
      "Iteration 1217, BCE loss: 57.75835704754704, Acc: 0.8198, Grad norm: 0.19872127425745717\n",
      "Iteration 1218, BCE loss: 57.76012644275036, Acc: 0.8198, Grad norm: 0.3442404510586155\n",
      "Iteration 1219, BCE loss: 57.760524923941006, Acc: 0.82, Grad norm: 0.3652006725900871\n",
      "Iteration 1220, BCE loss: 57.76156349388873, Acc: 0.8199, Grad norm: 0.4311388389699256\n",
      "Iteration 1221, BCE loss: 57.76004261422867, Acc: 0.8198, Grad norm: 0.33763827842173966\n",
      "Iteration 1222, BCE loss: 57.75978355420865, Acc: 0.8198, Grad norm: 0.3209522427215971\n",
      "Iteration 1223, BCE loss: 57.76064530904178, Acc: 0.8196, Grad norm: 0.38266497760613266\n",
      "Iteration 1224, BCE loss: 57.75971389676724, Acc: 0.8197, Grad norm: 0.3070345273514355\n",
      "Iteration 1225, BCE loss: 57.75993436567596, Acc: 0.8198, Grad norm: 0.3179194441891722\n",
      "Iteration 1226, BCE loss: 57.76003233054068, Acc: 0.8197, Grad norm: 0.32653638793150974\n",
      "Iteration 1227, BCE loss: 57.758856414763386, Acc: 0.8196, Grad norm: 0.2543084691990533\n",
      "Iteration 1228, BCE loss: 57.75870067234747, Acc: 0.8196, Grad norm: 0.24674718104477944\n",
      "Iteration 1229, BCE loss: 57.759198180848344, Acc: 0.8195, Grad norm: 0.29523206892457426\n",
      "Iteration 1230, BCE loss: 57.75851137287272, Acc: 0.8196, Grad norm: 0.23222136204965432\n",
      "Iteration 1231, BCE loss: 57.757506361511645, Acc: 0.8196, Grad norm: 0.08400194322405487\n",
      "Iteration 1232, BCE loss: 57.757814545866495, Acc: 0.8196, Grad norm: 0.14383053221695838\n",
      "Iteration 1233, BCE loss: 57.75792260887364, Acc: 0.8196, Grad norm: 0.15256580739099995\n",
      "Iteration 1234, BCE loss: 57.75817985199183, Acc: 0.8196, Grad norm: 0.18375067028315595\n",
      "Iteration 1235, BCE loss: 57.7581170389938, Acc: 0.8196, Grad norm: 0.17405972749101556\n",
      "Iteration 1236, BCE loss: 57.758579720399354, Acc: 0.8196, Grad norm: 0.20134631743129094\n",
      "Iteration 1237, BCE loss: 57.75932219329458, Acc: 0.8195, Grad norm: 0.2680101039634946\n",
      "Iteration 1238, BCE loss: 57.75860577710522, Acc: 0.8195, Grad norm: 0.225405324145943\n",
      "Iteration 1239, BCE loss: 57.75937109485696, Acc: 0.8194, Grad norm: 0.2849030780393693\n",
      "Iteration 1240, BCE loss: 57.7587743624629, Acc: 0.8196, Grad norm: 0.22723560606057414\n",
      "Iteration 1241, BCE loss: 57.75940835647441, Acc: 0.8196, Grad norm: 0.2873142500353823\n",
      "Iteration 1242, BCE loss: 57.75861032010889, Acc: 0.8196, Grad norm: 0.21147568975201278\n",
      "Iteration 1243, BCE loss: 57.759201226749575, Acc: 0.8198, Grad norm: 0.26706326352541887\n",
      "Iteration 1244, BCE loss: 57.758482811269175, Acc: 0.8196, Grad norm: 0.21736304044281485\n",
      "Iteration 1245, BCE loss: 57.760223642095156, Acc: 0.8195, Grad norm: 0.35288852600803433\n",
      "Iteration 1246, BCE loss: 57.75924326100326, Acc: 0.8195, Grad norm: 0.2704553916085886\n",
      "Iteration 1247, BCE loss: 57.75908798620368, Acc: 0.8194, Grad norm: 0.25146760493520054\n",
      "Iteration 1248, BCE loss: 57.75913693578858, Acc: 0.8195, Grad norm: 0.2643048465928975\n",
      "Iteration 1249, BCE loss: 57.75833267848688, Acc: 0.8194, Grad norm: 0.20117332822124312\n",
      "Iteration 1250, BCE loss: 57.75789136525117, Acc: 0.8194, Grad norm: 0.15275357702718792\n",
      "Iteration 1251, BCE loss: 57.75801759026701, Acc: 0.8194, Grad norm: 0.17684325281499533\n",
      "Iteration 1252, BCE loss: 57.75767392380517, Acc: 0.8194, Grad norm: 0.1321024645778624\n",
      "Iteration 1253, BCE loss: 57.758211312688864, Acc: 0.8194, Grad norm: 0.21235023263618724\n",
      "Iteration 1254, BCE loss: 57.75898522390534, Acc: 0.8194, Grad norm: 0.28656419556038076\n",
      "Iteration 1255, BCE loss: 57.75831283744818, Acc: 0.8196, Grad norm: 0.20159055389437727\n",
      "Iteration 1256, BCE loss: 57.75847760814628, Acc: 0.8196, Grad norm: 0.22381958140967642\n",
      "Iteration 1257, BCE loss: 57.75863713327228, Acc: 0.8196, Grad norm: 0.2434660125347275\n",
      "Iteration 1258, BCE loss: 57.7581328360136, Acc: 0.8196, Grad norm: 0.1858219024857227\n",
      "Iteration 1259, BCE loss: 57.75799789947594, Acc: 0.8196, Grad norm: 0.15810734691764033\n",
      "Iteration 1260, BCE loss: 57.758096198372, Acc: 0.8196, Grad norm: 0.17479790673720477\n",
      "Iteration 1261, BCE loss: 57.75861755213546, Acc: 0.8196, Grad norm: 0.24726070329258135\n",
      "Iteration 1262, BCE loss: 57.757780687213895, Acc: 0.8196, Grad norm: 0.14711801060493224\n",
      "Iteration 1263, BCE loss: 57.75747735434788, Acc: 0.8196, Grad norm: 0.08929834494328155\n",
      "Iteration 1264, BCE loss: 57.75760571608534, Acc: 0.8196, Grad norm: 0.12082992930916496\n",
      "Iteration 1265, BCE loss: 57.75918348493849, Acc: 0.8196, Grad norm: 0.2863938931808652\n",
      "Iteration 1266, BCE loss: 57.75997328839141, Acc: 0.8197, Grad norm: 0.3357009920927878\n",
      "Iteration 1267, BCE loss: 57.75889324754681, Acc: 0.8197, Grad norm: 0.25534914644884404\n",
      "Iteration 1268, BCE loss: 57.75817263615052, Acc: 0.8197, Grad norm: 0.1840781554965127\n",
      "Iteration 1269, BCE loss: 57.757969398274476, Acc: 0.8196, Grad norm: 0.15925401880436446\n",
      "Iteration 1270, BCE loss: 57.75886267665285, Acc: 0.8196, Grad norm: 0.25895459952571687\n",
      "Iteration 1271, BCE loss: 57.758653501878655, Acc: 0.8196, Grad norm: 0.24271096402981132\n",
      "Iteration 1272, BCE loss: 57.75829732019909, Acc: 0.8198, Grad norm: 0.1936059728570539\n",
      "Iteration 1273, BCE loss: 57.75854315683781, Acc: 0.8197, Grad norm: 0.20478818076763086\n",
      "Iteration 1274, BCE loss: 57.758467037072414, Acc: 0.8197, Grad norm: 0.204297208629501\n",
      "Iteration 1275, BCE loss: 57.75918848830356, Acc: 0.8197, Grad norm: 0.2753104854166058\n",
      "Iteration 1276, BCE loss: 57.75907164294043, Acc: 0.8197, Grad norm: 0.2625428735369741\n",
      "Iteration 1277, BCE loss: 57.75889177795501, Acc: 0.8197, Grad norm: 0.23081830597224687\n",
      "Iteration 1278, BCE loss: 57.75829581301949, Acc: 0.8197, Grad norm: 0.1928794870389893\n",
      "Iteration 1279, BCE loss: 57.75819494303305, Acc: 0.8197, Grad norm: 0.17080262651275954\n",
      "Iteration 1280, BCE loss: 57.75928033926155, Acc: 0.8197, Grad norm: 0.2739583264326024\n",
      "Iteration 1281, BCE loss: 57.75883007796595, Acc: 0.8197, Grad norm: 0.24909444231809075\n",
      "Iteration 1282, BCE loss: 57.759164150606104, Acc: 0.8198, Grad norm: 0.280451971351582\n",
      "Iteration 1283, BCE loss: 57.75881726678668, Acc: 0.8198, Grad norm: 0.2407676919084445\n",
      "Iteration 1284, BCE loss: 57.757841520587675, Acc: 0.8197, Grad norm: 0.1431950947090636\n",
      "Iteration 1285, BCE loss: 57.758476761594686, Acc: 0.8196, Grad norm: 0.22347202996127843\n",
      "Iteration 1286, BCE loss: 57.75757405325034, Acc: 0.8196, Grad norm: 0.10133136366238059\n",
      "Iteration 1287, BCE loss: 57.757654393263834, Acc: 0.8196, Grad norm: 0.11978218110592077\n",
      "Iteration 1288, BCE loss: 57.75762060048005, Acc: 0.8197, Grad norm: 0.1093940389940361\n",
      "Iteration 1289, BCE loss: 57.75790931066565, Acc: 0.8196, Grad norm: 0.16252640770338583\n",
      "Iteration 1290, BCE loss: 57.757948693334285, Acc: 0.8195, Grad norm: 0.16148183141206077\n",
      "Iteration 1291, BCE loss: 57.75819617046677, Acc: 0.8196, Grad norm: 0.1954091352433829\n",
      "Iteration 1292, BCE loss: 57.75798255289168, Acc: 0.8196, Grad norm: 0.16137194766717516\n",
      "Iteration 1293, BCE loss: 57.7582942465403, Acc: 0.8196, Grad norm: 0.20827111946101814\n",
      "Iteration 1294, BCE loss: 57.758266001769826, Acc: 0.8197, Grad norm: 0.19496092487015193\n",
      "Iteration 1295, BCE loss: 57.7591560636751, Acc: 0.8196, Grad norm: 0.28510995701695824\n",
      "Iteration 1296, BCE loss: 57.75934003481943, Acc: 0.8196, Grad norm: 0.2982319656130663\n",
      "Iteration 1297, BCE loss: 57.758905322385644, Acc: 0.8196, Grad norm: 0.2648783845837491\n",
      "Iteration 1298, BCE loss: 57.75916624278126, Acc: 0.8195, Grad norm: 0.28131005125457975\n",
      "Iteration 1299, BCE loss: 57.759913011615225, Acc: 0.8195, Grad norm: 0.33504601835883363\n",
      "Iteration 1300, BCE loss: 57.75926698629064, Acc: 0.8196, Grad norm: 0.29169973263195004\n",
      "Iteration 1301, BCE loss: 57.75919198054438, Acc: 0.8195, Grad norm: 0.28420508838988706\n",
      "Iteration 1302, BCE loss: 57.75988840583385, Acc: 0.8196, Grad norm: 0.34082696993134315\n",
      "Iteration 1303, BCE loss: 57.759332362615865, Acc: 0.8195, Grad norm: 0.2899742387992695\n",
      "Iteration 1304, BCE loss: 57.759465497326325, Acc: 0.8196, Grad norm: 0.30869229612351634\n",
      "Iteration 1305, BCE loss: 57.760095380916994, Acc: 0.8197, Grad norm: 0.36046594691514894\n",
      "Iteration 1306, BCE loss: 57.75990877441711, Acc: 0.8196, Grad norm: 0.34174626264917674\n",
      "Iteration 1307, BCE loss: 57.76021037838042, Acc: 0.8196, Grad norm: 0.36113176408931696\n",
      "Iteration 1308, BCE loss: 57.76075721118755, Acc: 0.8197, Grad norm: 0.40420592495037944\n",
      "Iteration 1309, BCE loss: 57.762230220220005, Acc: 0.8197, Grad norm: 0.4810251214301197\n",
      "Iteration 1310, BCE loss: 57.75966803099156, Acc: 0.8196, Grad norm: 0.32955058151718736\n",
      "Iteration 1311, BCE loss: 57.75893816410604, Acc: 0.8196, Grad norm: 0.2706088080296875\n",
      "Iteration 1312, BCE loss: 57.7582897828632, Acc: 0.8196, Grad norm: 0.19882083384942298\n",
      "Iteration 1313, BCE loss: 57.75806929455333, Acc: 0.8197, Grad norm: 0.17547103312384424\n",
      "Iteration 1314, BCE loss: 57.75782248257317, Acc: 0.8196, Grad norm: 0.14745889012843494\n",
      "Iteration 1315, BCE loss: 57.758870132557554, Acc: 0.8197, Grad norm: 0.26478194433288743\n",
      "Iteration 1316, BCE loss: 57.75877572260152, Acc: 0.8197, Grad norm: 0.2592240396247796\n",
      "Iteration 1317, BCE loss: 57.75824611647411, Acc: 0.8197, Grad norm: 0.2037422125659204\n",
      "Iteration 1318, BCE loss: 57.75843718041598, Acc: 0.8197, Grad norm: 0.22395840345218077\n",
      "Iteration 1319, BCE loss: 57.758500880655234, Acc: 0.8196, Grad norm: 0.21237946087600446\n",
      "Iteration 1320, BCE loss: 57.75801525250048, Acc: 0.8196, Grad norm: 0.16155821726521785\n",
      "Iteration 1321, BCE loss: 57.757744454610204, Acc: 0.8196, Grad norm: 0.13688591103523945\n",
      "Iteration 1322, BCE loss: 57.75772716388778, Acc: 0.8196, Grad norm: 0.13408491934820965\n",
      "Iteration 1323, BCE loss: 57.757684795231214, Acc: 0.8196, Grad norm: 0.11798606069381426\n",
      "Iteration 1324, BCE loss: 57.7580948379065, Acc: 0.8196, Grad norm: 0.18331113644775876\n",
      "Iteration 1325, BCE loss: 57.758899263338876, Acc: 0.8195, Grad norm: 0.26590825347148206\n",
      "Iteration 1326, BCE loss: 57.758535790269036, Acc: 0.8195, Grad norm: 0.23778796535210756\n",
      "Iteration 1327, BCE loss: 57.75849294438491, Acc: 0.8196, Grad norm: 0.2333505458933674\n",
      "Iteration 1328, BCE loss: 57.75894927097155, Acc: 0.8195, Grad norm: 0.27049490168637447\n",
      "Iteration 1329, BCE loss: 57.7589322997523, Acc: 0.8196, Grad norm: 0.26718852860124104\n",
      "Iteration 1330, BCE loss: 57.759793864789145, Acc: 0.8196, Grad norm: 0.3256222205332156\n",
      "Iteration 1331, BCE loss: 57.76088166907037, Acc: 0.8194, Grad norm: 0.3980828040122103\n",
      "Iteration 1332, BCE loss: 57.760013715477974, Acc: 0.8194, Grad norm: 0.3498679588618155\n",
      "Iteration 1333, BCE loss: 57.7608280119541, Acc: 0.8193, Grad norm: 0.3944037213899076\n",
      "Iteration 1334, BCE loss: 57.758551885586954, Acc: 0.8194, Grad norm: 0.21143459033019465\n",
      "Iteration 1335, BCE loss: 57.758856563702125, Acc: 0.8195, Grad norm: 0.24243221368370502\n",
      "Iteration 1336, BCE loss: 57.7584005530187, Acc: 0.8196, Grad norm: 0.2032935397209663\n",
      "Iteration 1337, BCE loss: 57.75776778210614, Acc: 0.8195, Grad norm: 0.13468071966506565\n",
      "Iteration 1338, BCE loss: 57.75854793676304, Acc: 0.8194, Grad norm: 0.22569987336718314\n",
      "Iteration 1339, BCE loss: 57.758836327902884, Acc: 0.8194, Grad norm: 0.26105773001127414\n",
      "Iteration 1340, BCE loss: 57.758255954705746, Acc: 0.8195, Grad norm: 0.19838927167287815\n",
      "Iteration 1341, BCE loss: 57.757667930918664, Acc: 0.8195, Grad norm: 0.12246167460635375\n",
      "Iteration 1342, BCE loss: 57.75808584949069, Acc: 0.8195, Grad norm: 0.17282156959978912\n",
      "Iteration 1343, BCE loss: 57.75805422532855, Acc: 0.8195, Grad norm: 0.17792276854024247\n",
      "Iteration 1344, BCE loss: 57.75801708499972, Acc: 0.8195, Grad norm: 0.1761478444683846\n",
      "Iteration 1345, BCE loss: 57.75847334804701, Acc: 0.8195, Grad norm: 0.22847683502873448\n",
      "Iteration 1346, BCE loss: 57.75934628538581, Acc: 0.8195, Grad norm: 0.3033161830736895\n",
      "Iteration 1347, BCE loss: 57.75933801511669, Acc: 0.8196, Grad norm: 0.30306749714058584\n",
      "Iteration 1348, BCE loss: 57.75838956751423, Acc: 0.8197, Grad norm: 0.2181279847251234\n",
      "Iteration 1349, BCE loss: 57.75774930116382, Acc: 0.8197, Grad norm: 0.13315189407786757\n",
      "Iteration 1350, BCE loss: 57.758477872134776, Acc: 0.8197, Grad norm: 0.22519830723078224\n",
      "Iteration 1351, BCE loss: 57.757856474476256, Acc: 0.8197, Grad norm: 0.1582329706880252\n",
      "Iteration 1352, BCE loss: 57.7576655927251, Acc: 0.8196, Grad norm: 0.12890402265911705\n",
      "Iteration 1353, BCE loss: 57.757605871120546, Acc: 0.8197, Grad norm: 0.11408332049259134\n",
      "Iteration 1354, BCE loss: 57.75825294071113, Acc: 0.8197, Grad norm: 0.20732595884884483\n",
      "Iteration 1355, BCE loss: 57.758054572368295, Acc: 0.8197, Grad norm: 0.18508434170255803\n",
      "Iteration 1356, BCE loss: 57.758566678712015, Acc: 0.8197, Grad norm: 0.23914226485934947\n",
      "Iteration 1357, BCE loss: 57.75897589859753, Acc: 0.8197, Grad norm: 0.2746087156321518\n",
      "Iteration 1358, BCE loss: 57.759453819203856, Acc: 0.8196, Grad norm: 0.31163163360922613\n",
      "Iteration 1359, BCE loss: 57.758531424127945, Acc: 0.8196, Grad norm: 0.22742552564025376\n",
      "Iteration 1360, BCE loss: 57.758262229988105, Acc: 0.8195, Grad norm: 0.19494696782950485\n",
      "Iteration 1361, BCE loss: 57.758293237420986, Acc: 0.8195, Grad norm: 0.19434177021226579\n",
      "Iteration 1362, BCE loss: 57.758266376422135, Acc: 0.8196, Grad norm: 0.1975886326728699\n",
      "Iteration 1363, BCE loss: 57.75848482648665, Acc: 0.8197, Grad norm: 0.22386927620952318\n",
      "Iteration 1364, BCE loss: 57.758329783470934, Acc: 0.8196, Grad norm: 0.20097505376404456\n",
      "Iteration 1365, BCE loss: 57.75901365102223, Acc: 0.8197, Grad norm: 0.268523266044029\n",
      "Iteration 1366, BCE loss: 57.75829084012219, Acc: 0.8197, Grad norm: 0.1916520821876541\n",
      "Iteration 1367, BCE loss: 57.7586691323447, Acc: 0.8197, Grad norm: 0.22579082674140802\n",
      "Iteration 1368, BCE loss: 57.7580795486825, Acc: 0.8196, Grad norm: 0.1714076314752564\n",
      "Iteration 1369, BCE loss: 57.75787218974121, Acc: 0.8196, Grad norm: 0.145134492944501\n",
      "Iteration 1370, BCE loss: 57.75953507818492, Acc: 0.8198, Grad norm: 0.30585072406494124\n",
      "Iteration 1371, BCE loss: 57.75920756498685, Acc: 0.8198, Grad norm: 0.28369419163494497\n",
      "Iteration 1372, BCE loss: 57.759763458105574, Acc: 0.8198, Grad norm: 0.3207656986891334\n",
      "Iteration 1373, BCE loss: 57.75928683147073, Acc: 0.8198, Grad norm: 0.280767460974492\n",
      "Iteration 1374, BCE loss: 57.75858103852839, Acc: 0.8197, Grad norm: 0.22451817374989472\n",
      "Iteration 1375, BCE loss: 57.75853373299795, Acc: 0.8197, Grad norm: 0.21309877236899014\n",
      "Iteration 1376, BCE loss: 57.75911565776235, Acc: 0.8197, Grad norm: 0.2656792538113062\n",
      "Iteration 1377, BCE loss: 57.75881957517925, Acc: 0.8197, Grad norm: 0.2505542436901427\n",
      "Iteration 1378, BCE loss: 57.75916905113954, Acc: 0.8198, Grad norm: 0.28476461853836366\n",
      "Iteration 1379, BCE loss: 57.758213390258824, Acc: 0.8198, Grad norm: 0.18163706708681773\n",
      "Iteration 1380, BCE loss: 57.75858824235779, Acc: 0.8198, Grad norm: 0.22365347435297242\n",
      "Iteration 1381, BCE loss: 57.758043263947286, Acc: 0.8198, Grad norm: 0.1629269818365963\n",
      "Iteration 1382, BCE loss: 57.758643685740246, Acc: 0.8198, Grad norm: 0.2239502868693712\n",
      "Iteration 1383, BCE loss: 57.75877763461601, Acc: 0.8199, Grad norm: 0.2288963893505089\n",
      "Iteration 1384, BCE loss: 57.75871264991871, Acc: 0.8198, Grad norm: 0.22943062200691433\n",
      "Iteration 1385, BCE loss: 57.75853721886462, Acc: 0.8198, Grad norm: 0.21420267089732944\n",
      "Iteration 1386, BCE loss: 57.75815893570351, Acc: 0.8196, Grad norm: 0.1883396059417246\n",
      "Iteration 1387, BCE loss: 57.75812445245614, Acc: 0.8196, Grad norm: 0.18350842625884153\n",
      "Iteration 1388, BCE loss: 57.7583263880295, Acc: 0.8197, Grad norm: 0.19375101794239297\n",
      "Iteration 1389, BCE loss: 57.75864454393646, Acc: 0.8197, Grad norm: 0.23899795894097398\n",
      "Iteration 1390, BCE loss: 57.7588581252155, Acc: 0.8197, Grad norm: 0.2669543184749181\n",
      "Iteration 1391, BCE loss: 57.759669209704924, Acc: 0.8197, Grad norm: 0.3174190891682782\n",
      "Iteration 1392, BCE loss: 57.76002289873766, Acc: 0.8197, Grad norm: 0.3396305269608694\n",
      "Iteration 1393, BCE loss: 57.75958487134012, Acc: 0.8196, Grad norm: 0.3125500733142332\n",
      "Iteration 1394, BCE loss: 57.75913767269776, Acc: 0.8197, Grad norm: 0.26898893221395564\n",
      "Iteration 1395, BCE loss: 57.759436750767485, Acc: 0.8198, Grad norm: 0.2932971156962352\n",
      "Iteration 1396, BCE loss: 57.759169693613636, Acc: 0.8197, Grad norm: 0.2640483842919514\n",
      "Iteration 1397, BCE loss: 57.758740581552246, Acc: 0.8197, Grad norm: 0.2163664440608786\n",
      "Iteration 1398, BCE loss: 57.75907212691415, Acc: 0.8197, Grad norm: 0.25179166667560876\n",
      "Iteration 1399, BCE loss: 57.75957443914523, Acc: 0.8198, Grad norm: 0.30895078157827716\n",
      "Iteration 1400, BCE loss: 57.758825836898666, Acc: 0.8198, Grad norm: 0.2534004373221223\n",
      "Iteration 1401, BCE loss: 57.75902621522944, Acc: 0.8198, Grad norm: 0.2532140711573362\n",
      "Iteration 1402, BCE loss: 57.758208780659515, Acc: 0.8197, Grad norm: 0.17152007127309066\n",
      "Iteration 1403, BCE loss: 57.758162675119195, Acc: 0.8197, Grad norm: 0.17786053930542972\n",
      "Iteration 1404, BCE loss: 57.758250692449565, Acc: 0.8198, Grad norm: 0.19687161512587156\n",
      "Iteration 1405, BCE loss: 57.75913446814115, Acc: 0.8197, Grad norm: 0.2665672681051877\n",
      "Iteration 1406, BCE loss: 57.759672554085, Acc: 0.8197, Grad norm: 0.31440049442190027\n",
      "Iteration 1407, BCE loss: 57.758992683899606, Acc: 0.8198, Grad norm: 0.2711059295635544\n",
      "Iteration 1408, BCE loss: 57.75874384385799, Acc: 0.8197, Grad norm: 0.2501425064642395\n",
      "Iteration 1409, BCE loss: 57.75881147230139, Acc: 0.8198, Grad norm: 0.26229744706534047\n",
      "Iteration 1410, BCE loss: 57.758072907086245, Acc: 0.8198, Grad norm: 0.1800867084533955\n",
      "Iteration 1411, BCE loss: 57.7577209701547, Acc: 0.8196, Grad norm: 0.12614116697883218\n",
      "Iteration 1412, BCE loss: 57.75845660896736, Acc: 0.8196, Grad norm: 0.22516920547112762\n",
      "Iteration 1413, BCE loss: 57.759638941935336, Acc: 0.8196, Grad norm: 0.3187958983027526\n",
      "Iteration 1414, BCE loss: 57.757845923477745, Acc: 0.8196, Grad norm: 0.1517572854190878\n",
      "Iteration 1415, BCE loss: 57.75774460290478, Acc: 0.8197, Grad norm: 0.1284998336000564\n",
      "Iteration 1416, BCE loss: 57.75822080895303, Acc: 0.8198, Grad norm: 0.19299060958976544\n",
      "Iteration 1417, BCE loss: 57.75805506199582, Acc: 0.8198, Grad norm: 0.1823147621718628\n",
      "Iteration 1418, BCE loss: 57.758367840040194, Acc: 0.8198, Grad norm: 0.21697326223227648\n",
      "Iteration 1419, BCE loss: 57.758123946349016, Acc: 0.8197, Grad norm: 0.19251613575239065\n",
      "Iteration 1420, BCE loss: 57.75779599335689, Acc: 0.8197, Grad norm: 0.14793000675453175\n",
      "Iteration 1421, BCE loss: 57.75765295408854, Acc: 0.8196, Grad norm: 0.12014086900815496\n",
      "Iteration 1422, BCE loss: 57.757569997082314, Acc: 0.8196, Grad norm: 0.11031805821935328\n",
      "Iteration 1423, BCE loss: 57.75828495307138, Acc: 0.8196, Grad norm: 0.2035835344476842\n",
      "Iteration 1424, BCE loss: 57.75862659665795, Acc: 0.8195, Grad norm: 0.22202278631637642\n",
      "Iteration 1425, BCE loss: 57.75960862848488, Acc: 0.8195, Grad norm: 0.3077188539959937\n",
      "Iteration 1426, BCE loss: 57.75887837475428, Acc: 0.8194, Grad norm: 0.24094497584356991\n",
      "Iteration 1427, BCE loss: 57.758704436389166, Acc: 0.8194, Grad norm: 0.23206295886099118\n",
      "Iteration 1428, BCE loss: 57.75773081258244, Acc: 0.8195, Grad norm: 0.1327379841210918\n",
      "Iteration 1429, BCE loss: 57.758340119934346, Acc: 0.8196, Grad norm: 0.21763660105057306\n",
      "Iteration 1430, BCE loss: 57.75878334189338, Acc: 0.8195, Grad norm: 0.2550862428421029\n",
      "Iteration 1431, BCE loss: 57.759014957070335, Acc: 0.8195, Grad norm: 0.2743154745746576\n",
      "Iteration 1432, BCE loss: 57.75887347719988, Acc: 0.8194, Grad norm: 0.24939699488780823\n",
      "Iteration 1433, BCE loss: 57.75917045640692, Acc: 0.8194, Grad norm: 0.27225155360136144\n",
      "Iteration 1434, BCE loss: 57.758902867194294, Acc: 0.8195, Grad norm: 0.26088508209189226\n",
      "Iteration 1435, BCE loss: 57.7591860007458, Acc: 0.8194, Grad norm: 0.28083701602194955\n",
      "Iteration 1436, BCE loss: 57.75849345036557, Acc: 0.8195, Grad norm: 0.2234599552162053\n",
      "Iteration 1437, BCE loss: 57.75780196586642, Acc: 0.8196, Grad norm: 0.15072408882844002\n",
      "Iteration 1438, BCE loss: 57.757612483354784, Acc: 0.8197, Grad norm: 0.12600500005407408\n",
      "Iteration 1439, BCE loss: 57.75775612434052, Acc: 0.8196, Grad norm: 0.14124035008346414\n",
      "Iteration 1440, BCE loss: 57.75828349725556, Acc: 0.8197, Grad norm: 0.21086755297738655\n",
      "Iteration 1441, BCE loss: 57.757987728355815, Acc: 0.8197, Grad norm: 0.17652008232855676\n",
      "Iteration 1442, BCE loss: 57.757686137418645, Acc: 0.8197, Grad norm: 0.12758976211761291\n",
      "Iteration 1443, BCE loss: 57.75789299492598, Acc: 0.8196, Grad norm: 0.14872401748291098\n",
      "Iteration 1444, BCE loss: 57.758189328596885, Acc: 0.8196, Grad norm: 0.18683304530983166\n",
      "Iteration 1445, BCE loss: 57.75805034483896, Acc: 0.8196, Grad norm: 0.17298923248348563\n",
      "Iteration 1446, BCE loss: 57.75809667545766, Acc: 0.8196, Grad norm: 0.17616230852568038\n",
      "Iteration 1447, BCE loss: 57.758564868945086, Acc: 0.8196, Grad norm: 0.2242252684896124\n",
      "Iteration 1448, BCE loss: 57.758468860183214, Acc: 0.8196, Grad norm: 0.22123956237785847\n",
      "Iteration 1449, BCE loss: 57.75904441627715, Acc: 0.8196, Grad norm: 0.2640577808168474\n",
      "Iteration 1450, BCE loss: 57.758885782035584, Acc: 0.8196, Grad norm: 0.25048597326778516\n",
      "Iteration 1451, BCE loss: 57.758370976476684, Acc: 0.8195, Grad norm: 0.20367989297119618\n",
      "Iteration 1452, BCE loss: 57.757908914310676, Acc: 0.8195, Grad norm: 0.15679593005458461\n",
      "Iteration 1453, BCE loss: 57.75818461748129, Acc: 0.8196, Grad norm: 0.1917615377175713\n",
      "Iteration 1454, BCE loss: 57.75810591851446, Acc: 0.8195, Grad norm: 0.18128592324478204\n",
      "Iteration 1455, BCE loss: 57.75842205250642, Acc: 0.8196, Grad norm: 0.2105979678443594\n",
      "Iteration 1456, BCE loss: 57.75823519121553, Acc: 0.8196, Grad norm: 0.18948368514452302\n",
      "Iteration 1457, BCE loss: 57.7584543375865, Acc: 0.8195, Grad norm: 0.21946784691090282\n",
      "Iteration 1458, BCE loss: 57.75785563564439, Acc: 0.8196, Grad norm: 0.15982015473060399\n",
      "Iteration 1459, BCE loss: 57.75798625714609, Acc: 0.8196, Grad norm: 0.18185298948937706\n",
      "Iteration 1460, BCE loss: 57.757576986960714, Acc: 0.8197, Grad norm: 0.10708040092361565\n",
      "Iteration 1461, BCE loss: 57.75747601662583, Acc: 0.8196, Grad norm: 0.08373343413410572\n",
      "Iteration 1462, BCE loss: 57.75781447221206, Acc: 0.8197, Grad norm: 0.14177757520968817\n",
      "Iteration 1463, BCE loss: 57.758016361840596, Acc: 0.8197, Grad norm: 0.1697110433879033\n",
      "Iteration 1464, BCE loss: 57.75758323872856, Acc: 0.8196, Grad norm: 0.09718750840177426\n",
      "Iteration 1465, BCE loss: 57.7576482070462, Acc: 0.8196, Grad norm: 0.11084807830607066\n",
      "Iteration 1466, BCE loss: 57.75796084399333, Acc: 0.8196, Grad norm: 0.15531481942476008\n",
      "Iteration 1467, BCE loss: 57.75793840040579, Acc: 0.8196, Grad norm: 0.1478097532922788\n",
      "Iteration 1468, BCE loss: 57.75798856900175, Acc: 0.8196, Grad norm: 0.15280467850811263\n",
      "Iteration 1469, BCE loss: 57.757915931108556, Acc: 0.8196, Grad norm: 0.15207058159730472\n",
      "Iteration 1470, BCE loss: 57.757787290649986, Acc: 0.8196, Grad norm: 0.1349564465107212\n",
      "Iteration 1471, BCE loss: 57.7575404034088, Acc: 0.8195, Grad norm: 0.0979201090032043\n",
      "Iteration 1472, BCE loss: 57.75773619571818, Acc: 0.8195, Grad norm: 0.1341918910543344\n",
      "Iteration 1473, BCE loss: 57.75769045568884, Acc: 0.8196, Grad norm: 0.12041068824492632\n",
      "Iteration 1474, BCE loss: 57.75809223817903, Acc: 0.8195, Grad norm: 0.1791053391064431\n",
      "Iteration 1475, BCE loss: 57.7579766505305, Acc: 0.8195, Grad norm: 0.16663250282849976\n",
      "Iteration 1476, BCE loss: 57.75770767893803, Acc: 0.8196, Grad norm: 0.12012662937629097\n",
      "Iteration 1477, BCE loss: 57.757883991055024, Acc: 0.8197, Grad norm: 0.1492625307473313\n",
      "Iteration 1478, BCE loss: 57.75803753638148, Acc: 0.8197, Grad norm: 0.15694832922487592\n",
      "Iteration 1479, BCE loss: 57.758037769447014, Acc: 0.8197, Grad norm: 0.16086853091927422\n",
      "Iteration 1480, BCE loss: 57.75813540011056, Acc: 0.8195, Grad norm: 0.18138816189427281\n",
      "Iteration 1481, BCE loss: 57.75822640324502, Acc: 0.8195, Grad norm: 0.2013876351030979\n",
      "Iteration 1482, BCE loss: 57.75787159368204, Acc: 0.8195, Grad norm: 0.14263349274841158\n",
      "Iteration 1483, BCE loss: 57.75779092337534, Acc: 0.8195, Grad norm: 0.13337223282923683\n",
      "Iteration 1484, BCE loss: 57.7576856084393, Acc: 0.8196, Grad norm: 0.11974852842191497\n",
      "Iteration 1485, BCE loss: 57.758217868427366, Acc: 0.8196, Grad norm: 0.19598731966258612\n",
      "Iteration 1486, BCE loss: 57.75850445768194, Acc: 0.8196, Grad norm: 0.21517093885077054\n",
      "Iteration 1487, BCE loss: 57.758076177549626, Acc: 0.8195, Grad norm: 0.16233799672447927\n",
      "Iteration 1488, BCE loss: 57.758004578904234, Acc: 0.8195, Grad norm: 0.15197346821621494\n",
      "Iteration 1489, BCE loss: 57.757832388978, Acc: 0.8196, Grad norm: 0.135654081538244\n",
      "Iteration 1490, BCE loss: 57.75808901957327, Acc: 0.8196, Grad norm: 0.16845495257571666\n",
      "Iteration 1491, BCE loss: 57.757590651181, Acc: 0.8196, Grad norm: 0.10602810172591458\n",
      "Iteration 1492, BCE loss: 57.757752025804734, Acc: 0.8196, Grad norm: 0.13557685630924324\n",
      "Iteration 1493, BCE loss: 57.75820956505359, Acc: 0.8197, Grad norm: 0.1961856414180611\n",
      "Iteration 1494, BCE loss: 57.75871125109238, Acc: 0.8198, Grad norm: 0.25096011990830214\n",
      "Iteration 1495, BCE loss: 57.7593918753748, Acc: 0.8197, Grad norm: 0.30604007182592147\n",
      "Iteration 1496, BCE loss: 57.758747786322516, Acc: 0.8197, Grad norm: 0.25712866736950396\n",
      "Iteration 1497, BCE loss: 57.758202978481975, Acc: 0.8196, Grad norm: 0.19626190616455683\n",
      "Iteration 1498, BCE loss: 57.758383967824926, Acc: 0.8196, Grad norm: 0.20942480834077706\n",
      "Iteration 1499, BCE loss: 57.7577683376739, Acc: 0.8196, Grad norm: 0.12694445031530396\n",
      "Iteration 1500, BCE loss: 57.758440995301626, Acc: 0.8195, Grad norm: 0.20863750151118704\n",
      "Iteration 1501, BCE loss: 57.75937410105848, Acc: 0.8194, Grad norm: 0.28764285121582334\n",
      "Iteration 1502, BCE loss: 57.75924372293646, Acc: 0.8195, Grad norm: 0.2766073151344426\n",
      "Iteration 1503, BCE loss: 57.75962484231677, Acc: 0.8196, Grad norm: 0.31049583798735336\n",
      "Iteration 1504, BCE loss: 57.75967505566963, Acc: 0.8197, Grad norm: 0.30874076178888377\n",
      "Iteration 1505, BCE loss: 57.759028354703986, Acc: 0.8197, Grad norm: 0.2639478581601342\n",
      "Iteration 1506, BCE loss: 57.75859536984838, Acc: 0.8196, Grad norm: 0.22945504286966809\n",
      "Iteration 1507, BCE loss: 57.75847042405254, Acc: 0.8196, Grad norm: 0.21870734727050437\n",
      "Iteration 1508, BCE loss: 57.75883579006927, Acc: 0.8196, Grad norm: 0.24520430629130807\n",
      "Iteration 1509, BCE loss: 57.75916180682097, Acc: 0.8196, Grad norm: 0.2767345888469445\n",
      "Iteration 1510, BCE loss: 57.75816257022959, Acc: 0.8196, Grad norm: 0.19595400167272012\n",
      "Iteration 1511, BCE loss: 57.75811369981311, Acc: 0.8196, Grad norm: 0.18714136053308333\n",
      "Iteration 1512, BCE loss: 57.75802604912026, Acc: 0.8196, Grad norm: 0.1722105816313202\n",
      "Iteration 1513, BCE loss: 57.7579045432577, Acc: 0.8195, Grad norm: 0.1504631345443168\n",
      "Iteration 1514, BCE loss: 57.757809306334366, Acc: 0.8194, Grad norm: 0.14643407382900103\n",
      "Iteration 1515, BCE loss: 57.75836957112432, Acc: 0.8194, Grad norm: 0.22480483756667885\n",
      "Iteration 1516, BCE loss: 57.75820441565841, Acc: 0.8194, Grad norm: 0.20807185202028053\n",
      "Iteration 1517, BCE loss: 57.75816350885319, Acc: 0.8195, Grad norm: 0.2024643000746237\n",
      "Iteration 1518, BCE loss: 57.757441305155, Acc: 0.8196, Grad norm: 0.07889838875367737\n",
      "Iteration 1519, BCE loss: 57.757519579306546, Acc: 0.8196, Grad norm: 0.10390650153291084\n",
      "Iteration 1520, BCE loss: 57.75781716048125, Acc: 0.8195, Grad norm: 0.14516195153248798\n",
      "Iteration 1521, BCE loss: 57.757899137053755, Acc: 0.8195, Grad norm: 0.15345632958815517\n",
      "Iteration 1522, BCE loss: 57.758147597390476, Acc: 0.8196, Grad norm: 0.19541990757304342\n",
      "Iteration 1523, BCE loss: 57.758121128640056, Acc: 0.8196, Grad norm: 0.18645591023556\n",
      "Iteration 1524, BCE loss: 57.75805031032327, Acc: 0.8196, Grad norm: 0.1825432466163572\n",
      "Iteration 1525, BCE loss: 57.75765987734141, Acc: 0.8195, Grad norm: 0.12197233488108725\n",
      "Iteration 1526, BCE loss: 57.75796382277083, Acc: 0.8195, Grad norm: 0.16586115472403315\n",
      "Iteration 1527, BCE loss: 57.75819322673172, Acc: 0.8194, Grad norm: 0.18276403939641944\n",
      "Iteration 1528, BCE loss: 57.75776900321681, Acc: 0.8195, Grad norm: 0.1332030999234044\n",
      "Iteration 1529, BCE loss: 57.757854712386894, Acc: 0.8194, Grad norm: 0.14414773253515895\n",
      "Iteration 1530, BCE loss: 57.7576912756756, Acc: 0.8194, Grad norm: 0.12881396685965393\n",
      "Iteration 1531, BCE loss: 57.757584847734634, Acc: 0.8195, Grad norm: 0.11000450822671504\n",
      "Iteration 1532, BCE loss: 57.757577035878505, Acc: 0.8195, Grad norm: 0.10946277629926358\n",
      "Iteration 1533, BCE loss: 57.75748107562448, Acc: 0.8195, Grad norm: 0.08866509432024923\n",
      "Iteration 1534, BCE loss: 57.758217362685414, Acc: 0.8196, Grad norm: 0.1945624649239507\n",
      "Iteration 1535, BCE loss: 57.757756832448294, Acc: 0.8196, Grad norm: 0.13093246319213622\n",
      "Iteration 1536, BCE loss: 57.7580139875336, Acc: 0.8196, Grad norm: 0.1742250333858839\n",
      "Iteration 1537, BCE loss: 57.75922058311347, Acc: 0.8197, Grad norm: 0.285797061644606\n",
      "Iteration 1538, BCE loss: 57.7592834126705, Acc: 0.8197, Grad norm: 0.29070730423826785\n",
      "Iteration 1539, BCE loss: 57.75828358528689, Acc: 0.8196, Grad norm: 0.19781710747221654\n",
      "Iteration 1540, BCE loss: 57.758226962539446, Acc: 0.8196, Grad norm: 0.1942994813337956\n",
      "Iteration 1541, BCE loss: 57.75788288987327, Acc: 0.8195, Grad norm: 0.15156081856267736\n",
      "Iteration 1542, BCE loss: 57.75799657399959, Acc: 0.8195, Grad norm: 0.16431898233457923\n",
      "Iteration 1543, BCE loss: 57.75813972884062, Acc: 0.8196, Grad norm: 0.18517866917542555\n",
      "Iteration 1544, BCE loss: 57.758980516532496, Acc: 0.8196, Grad norm: 0.27381972906101093\n",
      "Iteration 1545, BCE loss: 57.758234473948775, Acc: 0.8195, Grad norm: 0.1961690149975637\n",
      "Iteration 1546, BCE loss: 57.75912582754259, Acc: 0.8196, Grad norm: 0.27637590971522363\n",
      "Iteration 1547, BCE loss: 57.75873416618819, Acc: 0.8196, Grad norm: 0.2477632957011347\n",
      "Iteration 1548, BCE loss: 57.758416257648435, Acc: 0.8197, Grad norm: 0.21599614761708016\n",
      "Iteration 1549, BCE loss: 57.75804567752805, Acc: 0.8196, Grad norm: 0.17590685830885866\n",
      "Iteration 1550, BCE loss: 57.7575392575483, Acc: 0.8196, Grad norm: 0.09854736027459784\n",
      "Iteration 1551, BCE loss: 57.75751494021937, Acc: 0.8196, Grad norm: 0.09507804834988824\n",
      "Iteration 1552, BCE loss: 57.75787416202783, Acc: 0.8196, Grad norm: 0.15967534332125158\n",
      "Iteration 1553, BCE loss: 57.757800773504044, Acc: 0.8196, Grad norm: 0.1412926023877129\n",
      "Iteration 1554, BCE loss: 57.75766093680733, Acc: 0.8197, Grad norm: 0.10961171998485811\n",
      "Iteration 1555, BCE loss: 57.757987906923105, Acc: 0.8196, Grad norm: 0.16167080572582176\n",
      "Iteration 1556, BCE loss: 57.75765461795667, Acc: 0.8196, Grad norm: 0.11137074317467092\n",
      "Iteration 1557, BCE loss: 57.757883449084275, Acc: 0.8197, Grad norm: 0.14897356103283332\n",
      "Iteration 1558, BCE loss: 57.75792578652211, Acc: 0.8197, Grad norm: 0.14854495396621278\n",
      "Iteration 1559, BCE loss: 57.75778209544341, Acc: 0.8196, Grad norm: 0.13527697152743937\n",
      "Iteration 1560, BCE loss: 57.75878674519906, Acc: 0.8196, Grad norm: 0.24656792819621462\n",
      "Iteration 1561, BCE loss: 57.75813269171455, Acc: 0.8196, Grad norm: 0.18237499744576183\n",
      "Iteration 1562, BCE loss: 57.75805351627257, Acc: 0.8196, Grad norm: 0.16887273917929208\n",
      "Iteration 1563, BCE loss: 57.75791377443016, Acc: 0.8196, Grad norm: 0.15131495848410864\n",
      "Iteration 1564, BCE loss: 57.75789208444802, Acc: 0.8195, Grad norm: 0.15023748998918532\n",
      "Iteration 1565, BCE loss: 57.75804938907231, Acc: 0.8196, Grad norm: 0.16201121985630346\n",
      "Iteration 1566, BCE loss: 57.75839398365038, Acc: 0.8195, Grad norm: 0.20237086011416577\n",
      "Iteration 1567, BCE loss: 57.758113164228746, Acc: 0.8195, Grad norm: 0.1711743827396176\n",
      "Iteration 1568, BCE loss: 57.759171515079174, Acc: 0.8195, Grad norm: 0.2663496487669405\n",
      "Iteration 1569, BCE loss: 57.7596556328578, Acc: 0.8196, Grad norm: 0.3101931379625282\n",
      "Iteration 1570, BCE loss: 57.758888645947025, Acc: 0.8195, Grad norm: 0.24942986249365284\n",
      "Iteration 1571, BCE loss: 57.75831192400091, Acc: 0.8195, Grad norm: 0.19614064919696614\n",
      "Iteration 1572, BCE loss: 57.758911549886776, Acc: 0.8196, Grad norm: 0.2514031293855889\n",
      "Iteration 1573, BCE loss: 57.75843580310423, Acc: 0.8196, Grad norm: 0.20329498095744084\n",
      "Iteration 1574, BCE loss: 57.75814062026363, Acc: 0.8196, Grad norm: 0.17990166066665866\n",
      "Iteration 1575, BCE loss: 57.75873690809254, Acc: 0.8196, Grad norm: 0.23612520079879187\n",
      "Iteration 1576, BCE loss: 57.75886922279223, Acc: 0.8197, Grad norm: 0.23773835485594902\n",
      "Iteration 1577, BCE loss: 57.758933802799845, Acc: 0.8197, Grad norm: 0.24947842965536576\n",
      "Iteration 1578, BCE loss: 57.757889789143846, Acc: 0.8197, Grad norm: 0.13743653288282623\n",
      "Iteration 1579, BCE loss: 57.758482110951896, Acc: 0.8198, Grad norm: 0.2225293170016792\n",
      "Iteration 1580, BCE loss: 57.75765834676321, Acc: 0.8196, Grad norm: 0.11232004770591954\n",
      "Iteration 1581, BCE loss: 57.75770214847347, Acc: 0.8195, Grad norm: 0.1237387192621504\n",
      "Iteration 1582, BCE loss: 57.75762717661725, Acc: 0.8196, Grad norm: 0.11855643463292281\n",
      "Iteration 1583, BCE loss: 57.75763588218216, Acc: 0.8196, Grad norm: 0.1128515683475973\n",
      "Iteration 1584, BCE loss: 57.757708021649734, Acc: 0.8195, Grad norm: 0.12619894065814974\n",
      "Iteration 1585, BCE loss: 57.757741561670215, Acc: 0.8195, Grad norm: 0.12421165355072092\n",
      "Iteration 1586, BCE loss: 57.75824836341874, Acc: 0.8196, Grad norm: 0.1937015764545501\n",
      "Iteration 1587, BCE loss: 57.758164540744566, Acc: 0.8195, Grad norm: 0.18436585625517016\n",
      "Iteration 1588, BCE loss: 57.75798646120623, Acc: 0.8195, Grad norm: 0.15900831093000328\n",
      "Iteration 1589, BCE loss: 57.758374213852875, Acc: 0.8196, Grad norm: 0.19678966644736076\n",
      "Iteration 1590, BCE loss: 57.7584517984634, Acc: 0.8196, Grad norm: 0.1996210234065788\n",
      "Iteration 1591, BCE loss: 57.758053855068184, Acc: 0.8196, Grad norm: 0.16518318743867702\n",
      "Iteration 1592, BCE loss: 57.758378188363494, Acc: 0.8195, Grad norm: 0.20519684946520625\n",
      "Iteration 1593, BCE loss: 57.75832952686635, Acc: 0.8195, Grad norm: 0.17822036098361\n",
      "Iteration 1594, BCE loss: 57.75858703772269, Acc: 0.8196, Grad norm: 0.206722579940728\n",
      "Iteration 1595, BCE loss: 57.75844528783149, Acc: 0.8197, Grad norm: 0.2043993399984516\n",
      "Iteration 1596, BCE loss: 57.75886573170368, Acc: 0.8196, Grad norm: 0.25096078805065763\n",
      "Iteration 1597, BCE loss: 57.75919347432048, Acc: 0.8197, Grad norm: 0.2930894671080638\n",
      "Iteration 1598, BCE loss: 57.759226784858484, Acc: 0.8197, Grad norm: 0.2830772833043947\n",
      "Iteration 1599, BCE loss: 57.759042519334585, Acc: 0.8197, Grad norm: 0.26374793143030373\n",
      "Iteration 1600, BCE loss: 57.759058945226876, Acc: 0.8197, Grad norm: 0.2697054801269997\n",
      "Iteration 1601, BCE loss: 57.75857448091483, Acc: 0.8196, Grad norm: 0.22076692830029843\n",
      "Iteration 1602, BCE loss: 57.75863754932507, Acc: 0.8196, Grad norm: 0.24006081517471584\n",
      "Iteration 1603, BCE loss: 57.758323206611344, Acc: 0.8196, Grad norm: 0.20279463215862753\n",
      "Iteration 1604, BCE loss: 57.75853376928255, Acc: 0.8196, Grad norm: 0.2240776567998773\n",
      "Iteration 1605, BCE loss: 57.75815184284993, Acc: 0.8195, Grad norm: 0.18059389802929265\n",
      "Iteration 1606, BCE loss: 57.75816225075011, Acc: 0.8195, Grad norm: 0.187763172278551\n",
      "Iteration 1607, BCE loss: 57.75827885703531, Acc: 0.8195, Grad norm: 0.20275497147483995\n",
      "Iteration 1608, BCE loss: 57.75893836444205, Acc: 0.8196, Grad norm: 0.2563779400827467\n",
      "Iteration 1609, BCE loss: 57.75886623463644, Acc: 0.8195, Grad norm: 0.25457592109505295\n",
      "Iteration 1610, BCE loss: 57.758524797604494, Acc: 0.8196, Grad norm: 0.2254472757607385\n",
      "Iteration 1611, BCE loss: 57.75869690870164, Acc: 0.8195, Grad norm: 0.24243668035443067\n",
      "Iteration 1612, BCE loss: 57.758839315231654, Acc: 0.8195, Grad norm: 0.25771085873789384\n",
      "Iteration 1613, BCE loss: 57.75829622153066, Acc: 0.8195, Grad norm: 0.20036846825258658\n",
      "Iteration 1614, BCE loss: 57.757938709739236, Acc: 0.8195, Grad norm: 0.16482825353764138\n",
      "Iteration 1615, BCE loss: 57.75838606682957, Acc: 0.8195, Grad norm: 0.21899260382366165\n",
      "Iteration 1616, BCE loss: 57.75841159875743, Acc: 0.8195, Grad norm: 0.21633034993018455\n",
      "Iteration 1617, BCE loss: 57.75833101557309, Acc: 0.8196, Grad norm: 0.21305655686194983\n",
      "Iteration 1618, BCE loss: 57.758345804349354, Acc: 0.8196, Grad norm: 0.20878644912443528\n",
      "Iteration 1619, BCE loss: 57.75859758379592, Acc: 0.8196, Grad norm: 0.2279391826179252\n",
      "Iteration 1620, BCE loss: 57.75868885725613, Acc: 0.8195, Grad norm: 0.2299464285665766\n",
      "Iteration 1621, BCE loss: 57.758556047216494, Acc: 0.8195, Grad norm: 0.21082068278185942\n",
      "Iteration 1622, BCE loss: 57.75830684214978, Acc: 0.8196, Grad norm: 0.18881995611943667\n",
      "Iteration 1623, BCE loss: 57.757972199203266, Acc: 0.8196, Grad norm: 0.1670750663862753\n",
      "Iteration 1624, BCE loss: 57.75832681290965, Acc: 0.8196, Grad norm: 0.20548091174863797\n",
      "Iteration 1625, BCE loss: 57.75752796368161, Acc: 0.8196, Grad norm: 0.09537186555080616\n",
      "Iteration 1626, BCE loss: 57.75768894978121, Acc: 0.8196, Grad norm: 0.12197632970743828\n",
      "Iteration 1627, BCE loss: 57.75799024165853, Acc: 0.8197, Grad norm: 0.17022826588788068\n",
      "Iteration 1628, BCE loss: 57.75782726765319, Acc: 0.8196, Grad norm: 0.1458160931479365\n",
      "Iteration 1629, BCE loss: 57.75771735021878, Acc: 0.8196, Grad norm: 0.12528064792680602\n",
      "Iteration 1630, BCE loss: 57.7580275954995, Acc: 0.8196, Grad norm: 0.17218341324647576\n",
      "Iteration 1631, BCE loss: 57.758707948988096, Acc: 0.8197, Grad norm: 0.2471127307887902\n",
      "Iteration 1632, BCE loss: 57.75881976065183, Acc: 0.8198, Grad norm: 0.2646816612024688\n",
      "Iteration 1633, BCE loss: 57.75798038448827, Acc: 0.8197, Grad norm: 0.17367305684932197\n",
      "Iteration 1634, BCE loss: 57.75814508929915, Acc: 0.8196, Grad norm: 0.19322935553220613\n",
      "Iteration 1635, BCE loss: 57.75836755658243, Acc: 0.8197, Grad norm: 0.2120659826981481\n",
      "Iteration 1636, BCE loss: 57.75838396064436, Acc: 0.8196, Grad norm: 0.21431342050621255\n",
      "Iteration 1637, BCE loss: 57.75901820885484, Acc: 0.8197, Grad norm: 0.2809003652115051\n",
      "Iteration 1638, BCE loss: 57.75823598108563, Acc: 0.8196, Grad norm: 0.2011704870140688\n",
      "Iteration 1639, BCE loss: 57.758226823671485, Acc: 0.8196, Grad norm: 0.2001370910554345\n",
      "Iteration 1640, BCE loss: 57.758449100870315, Acc: 0.8196, Grad norm: 0.2239389538989498\n",
      "Iteration 1641, BCE loss: 57.757903379514886, Acc: 0.8197, Grad norm: 0.1526831254042732\n",
      "Iteration 1642, BCE loss: 57.75765858105066, Acc: 0.8197, Grad norm: 0.11964299002777022\n",
      "Iteration 1643, BCE loss: 57.75779470687408, Acc: 0.8196, Grad norm: 0.1356856329319047\n",
      "Iteration 1644, BCE loss: 57.757803437288814, Acc: 0.8196, Grad norm: 0.13449379120329052\n",
      "Iteration 1645, BCE loss: 57.757896432366465, Acc: 0.8196, Grad norm: 0.13863465997171512\n",
      "Iteration 1646, BCE loss: 57.75809096532434, Acc: 0.8197, Grad norm: 0.17430199781558156\n",
      "Iteration 1647, BCE loss: 57.75795798914683, Acc: 0.8197, Grad norm: 0.14699079752789648\n",
      "Iteration 1648, BCE loss: 57.75804168716013, Acc: 0.8197, Grad norm: 0.17129858262314807\n",
      "Iteration 1649, BCE loss: 57.75917073214599, Acc: 0.8197, Grad norm: 0.2839006355656256\n",
      "Iteration 1650, BCE loss: 57.7596775022759, Acc: 0.8197, Grad norm: 0.3302102833005097\n",
      "Iteration 1651, BCE loss: 57.760061865692066, Acc: 0.8197, Grad norm: 0.35422123609401923\n",
      "Iteration 1652, BCE loss: 57.760191128630325, Acc: 0.8198, Grad norm: 0.36258905408046993\n",
      "Iteration 1653, BCE loss: 57.760296323581684, Acc: 0.8198, Grad norm: 0.3651881483032452\n",
      "Iteration 1654, BCE loss: 57.75983153781657, Acc: 0.8198, Grad norm: 0.3384446392132077\n",
      "Iteration 1655, BCE loss: 57.75942455904108, Acc: 0.8198, Grad norm: 0.30825229129154874\n",
      "Iteration 1656, BCE loss: 57.760139475961154, Acc: 0.8197, Grad norm: 0.3605958699554896\n",
      "Iteration 1657, BCE loss: 57.759949083002724, Acc: 0.8197, Grad norm: 0.34653531093460754\n",
      "Iteration 1658, BCE loss: 57.76082116551498, Acc: 0.8196, Grad norm: 0.4073392051369749\n",
      "Iteration 1659, BCE loss: 57.759662660936556, Acc: 0.8196, Grad norm: 0.3327698301443275\n",
      "Iteration 1660, BCE loss: 57.75844787788802, Acc: 0.8195, Grad norm: 0.22844064625585958\n",
      "Iteration 1661, BCE loss: 57.75769422001499, Acc: 0.8196, Grad norm: 0.13565508399952608\n",
      "Iteration 1662, BCE loss: 57.75773739077933, Acc: 0.8195, Grad norm: 0.13870010506026004\n",
      "Iteration 1663, BCE loss: 57.75790353965186, Acc: 0.8195, Grad norm: 0.15826868249488674\n",
      "Iteration 1664, BCE loss: 57.75779815469723, Acc: 0.8195, Grad norm: 0.14850391770271104\n",
      "Iteration 1665, BCE loss: 57.758025887875604, Acc: 0.8196, Grad norm: 0.18102496349901046\n",
      "Iteration 1666, BCE loss: 57.758207246835354, Acc: 0.8196, Grad norm: 0.1956095952693504\n",
      "Iteration 1667, BCE loss: 57.758443799765324, Acc: 0.8197, Grad norm: 0.22670330793304372\n",
      "Iteration 1668, BCE loss: 57.75806508335192, Acc: 0.8197, Grad norm: 0.18329710803024235\n",
      "Iteration 1669, BCE loss: 57.75790343062486, Acc: 0.8196, Grad norm: 0.15329923724411373\n",
      "Iteration 1670, BCE loss: 57.75794773643941, Acc: 0.8196, Grad norm: 0.16098346783068954\n",
      "Iteration 1671, BCE loss: 57.75811885475291, Acc: 0.8196, Grad norm: 0.1809131986158481\n",
      "Iteration 1672, BCE loss: 57.758450534170024, Acc: 0.8195, Grad norm: 0.21874343422619008\n",
      "Iteration 1673, BCE loss: 57.75822651683551, Acc: 0.8195, Grad norm: 0.19039712748456947\n",
      "Iteration 1674, BCE loss: 57.758947912669626, Acc: 0.8196, Grad norm: 0.27387688529815263\n",
      "Iteration 1675, BCE loss: 57.75839516639, Acc: 0.8195, Grad norm: 0.22003480207210885\n",
      "Iteration 1676, BCE loss: 57.7584564698469, Acc: 0.8196, Grad norm: 0.22439433321957555\n",
      "Iteration 1677, BCE loss: 57.759312000525505, Acc: 0.8196, Grad norm: 0.29484528852259206\n",
      "Iteration 1678, BCE loss: 57.75925715594441, Acc: 0.8196, Grad norm: 0.28716023303239807\n",
      "Iteration 1679, BCE loss: 57.75893709744324, Acc: 0.8196, Grad norm: 0.26513255753874\n",
      "Iteration 1680, BCE loss: 57.757945848052266, Acc: 0.8196, Grad norm: 0.16490258708699035\n",
      "Iteration 1681, BCE loss: 57.75784052291475, Acc: 0.8196, Grad norm: 0.1520822444236989\n",
      "Iteration 1682, BCE loss: 57.75823274704806, Acc: 0.8197, Grad norm: 0.19860730355314843\n",
      "Iteration 1683, BCE loss: 57.758589729031534, Acc: 0.8197, Grad norm: 0.23705424182743848\n",
      "Iteration 1684, BCE loss: 57.75820911926491, Acc: 0.8196, Grad norm: 0.19605236526697103\n",
      "Iteration 1685, BCE loss: 57.75878483556599, Acc: 0.8197, Grad norm: 0.25344206544282977\n",
      "Iteration 1686, BCE loss: 57.759162203230616, Acc: 0.8197, Grad norm: 0.28333782005184877\n",
      "Iteration 1687, BCE loss: 57.759075570018105, Acc: 0.8198, Grad norm: 0.2724968279386353\n",
      "Iteration 1688, BCE loss: 57.75890308528891, Acc: 0.8197, Grad norm: 0.25709570383124536\n",
      "Iteration 1689, BCE loss: 57.75942822808046, Acc: 0.8198, Grad norm: 0.2976102882451611\n",
      "Iteration 1690, BCE loss: 57.75909762578828, Acc: 0.8197, Grad norm: 0.2710908481091795\n",
      "Iteration 1691, BCE loss: 57.758805849961476, Acc: 0.8197, Grad norm: 0.2508461320757535\n",
      "Iteration 1692, BCE loss: 57.75914437167917, Acc: 0.8197, Grad norm: 0.2757823081246242\n",
      "Iteration 1693, BCE loss: 57.75935395440033, Acc: 0.8196, Grad norm: 0.29111192083600473\n",
      "Iteration 1694, BCE loss: 57.759622027031355, Acc: 0.8197, Grad norm: 0.31484772793247057\n",
      "Iteration 1695, BCE loss: 57.75927970811429, Acc: 0.8196, Grad norm: 0.2808051622845367\n",
      "Iteration 1696, BCE loss: 57.75891858743904, Acc: 0.8198, Grad norm: 0.26141803804330904\n",
      "Iteration 1697, BCE loss: 57.75913590545689, Acc: 0.8198, Grad norm: 0.27607819643898635\n",
      "Iteration 1698, BCE loss: 57.75833476076082, Acc: 0.8197, Grad norm: 0.20693543304857986\n",
      "Iteration 1699, BCE loss: 57.75813085509611, Acc: 0.8196, Grad norm: 0.18006046505211953\n",
      "Iteration 1700, BCE loss: 57.758237076776155, Acc: 0.8197, Grad norm: 0.1893220359475392\n",
      "Iteration 1701, BCE loss: 57.758554813022855, Acc: 0.8197, Grad norm: 0.22095442407255494\n",
      "Iteration 1702, BCE loss: 57.75867099093355, Acc: 0.8197, Grad norm: 0.22817850916173862\n",
      "Iteration 1703, BCE loss: 57.758515476189274, Acc: 0.8197, Grad norm: 0.2201981473603727\n",
      "Iteration 1704, BCE loss: 57.758103340062235, Acc: 0.8196, Grad norm: 0.17532584814174365\n",
      "Iteration 1705, BCE loss: 57.758129982882636, Acc: 0.8196, Grad norm: 0.17539095036627286\n",
      "Iteration 1706, BCE loss: 57.75843709601949, Acc: 0.8196, Grad norm: 0.20690114858484981\n",
      "Iteration 1707, BCE loss: 57.75826893675237, Acc: 0.8196, Grad norm: 0.19558702442213102\n",
      "Iteration 1708, BCE loss: 57.7584087390599, Acc: 0.8196, Grad norm: 0.2143957098537051\n",
      "Iteration 1709, BCE loss: 57.758440932443065, Acc: 0.8196, Grad norm: 0.21188499683492792\n",
      "Iteration 1710, BCE loss: 57.75870872278659, Acc: 0.8196, Grad norm: 0.2485463561647409\n",
      "Iteration 1711, BCE loss: 57.75863829414722, Acc: 0.8195, Grad norm: 0.24724986004229732\n",
      "Iteration 1712, BCE loss: 57.7585453193152, Acc: 0.8195, Grad norm: 0.2321779874379682\n",
      "Iteration 1713, BCE loss: 57.758217825778516, Acc: 0.8195, Grad norm: 0.19083427521457333\n",
      "Iteration 1714, BCE loss: 57.757942666437046, Acc: 0.8196, Grad norm: 0.14653560137095314\n",
      "Iteration 1715, BCE loss: 57.757678753632234, Acc: 0.8196, Grad norm: 0.11538835914243199\n",
      "Iteration 1716, BCE loss: 57.75818430618055, Acc: 0.8196, Grad norm: 0.19217464847173835\n",
      "Iteration 1717, BCE loss: 57.75815892427502, Acc: 0.8197, Grad norm: 0.17969482851492863\n",
      "Iteration 1718, BCE loss: 57.75819641550005, Acc: 0.8197, Grad norm: 0.18716040196068187\n",
      "Iteration 1719, BCE loss: 57.75864175677892, Acc: 0.8198, Grad norm: 0.2381337043677473\n",
      "Iteration 1720, BCE loss: 57.75846519128488, Acc: 0.8198, Grad norm: 0.2240552338783081\n",
      "Iteration 1721, BCE loss: 57.7586180147029, Acc: 0.8197, Grad norm: 0.23657665939045353\n",
      "Iteration 1722, BCE loss: 57.75883354612844, Acc: 0.8197, Grad norm: 0.2554247521845006\n",
      "Iteration 1723, BCE loss: 57.75804627184868, Acc: 0.8197, Grad norm: 0.18187058479995216\n",
      "Iteration 1724, BCE loss: 57.758007242705304, Acc: 0.8197, Grad norm: 0.17467458198390812\n",
      "Iteration 1725, BCE loss: 57.75764822275825, Acc: 0.8196, Grad norm: 0.12737580257669853\n",
      "Iteration 1726, BCE loss: 57.757743152097916, Acc: 0.8196, Grad norm: 0.13683883865064028\n",
      "Iteration 1727, BCE loss: 57.758118283897396, Acc: 0.8196, Grad norm: 0.18907719485133204\n",
      "Iteration 1728, BCE loss: 57.75799294692156, Acc: 0.8196, Grad norm: 0.15877755758400622\n",
      "Iteration 1729, BCE loss: 57.75787125647591, Acc: 0.8196, Grad norm: 0.13568061193111586\n",
      "Iteration 1730, BCE loss: 57.75783256279668, Acc: 0.8196, Grad norm: 0.12984259827283376\n",
      "Iteration 1731, BCE loss: 57.75783399212034, Acc: 0.8195, Grad norm: 0.143085828430233\n",
      "Iteration 1732, BCE loss: 57.757676127054395, Acc: 0.8196, Grad norm: 0.11755911957048418\n",
      "Iteration 1733, BCE loss: 57.757773409241885, Acc: 0.8196, Grad norm: 0.1338094479597791\n",
      "Iteration 1734, BCE loss: 57.757467101588816, Acc: 0.8196, Grad norm: 0.07727230016049245\n",
      "Iteration 1735, BCE loss: 57.757710009610065, Acc: 0.8196, Grad norm: 0.13481973191786414\n",
      "Iteration 1736, BCE loss: 57.758117491349495, Acc: 0.8197, Grad norm: 0.1860778016341245\n",
      "Iteration 1737, BCE loss: 57.75799351283649, Acc: 0.8197, Grad norm: 0.17157635752032968\n",
      "Iteration 1738, BCE loss: 57.75811257985671, Acc: 0.8197, Grad norm: 0.1846836692786508\n",
      "Iteration 1739, BCE loss: 57.75806291461801, Acc: 0.8197, Grad norm: 0.1779361102928143\n",
      "Iteration 1740, BCE loss: 57.758172220126475, Acc: 0.8197, Grad norm: 0.18321702017892139\n",
      "Iteration 1741, BCE loss: 57.75797642375207, Acc: 0.8196, Grad norm: 0.1595251440624378\n",
      "Iteration 1742, BCE loss: 57.7581398783194, Acc: 0.8196, Grad norm: 0.17767983422261516\n",
      "Iteration 1743, BCE loss: 57.75772743556669, Acc: 0.8195, Grad norm: 0.12799700366986758\n",
      "Iteration 1744, BCE loss: 57.75798933660907, Acc: 0.8196, Grad norm: 0.16882033659934154\n",
      "Iteration 1745, BCE loss: 57.757698854684214, Acc: 0.8195, Grad norm: 0.12809505195137844\n",
      "Iteration 1746, BCE loss: 57.75853245715052, Acc: 0.8196, Grad norm: 0.23254390669568517\n",
      "Iteration 1747, BCE loss: 57.75842109016146, Acc: 0.8196, Grad norm: 0.21980814589283257\n",
      "Iteration 1748, BCE loss: 57.75798195867307, Acc: 0.8197, Grad norm: 0.1694260772759588\n",
      "Iteration 1749, BCE loss: 57.75786971289896, Acc: 0.8196, Grad norm: 0.14837299609873242\n",
      "Iteration 1750, BCE loss: 57.75837300983956, Acc: 0.8195, Grad norm: 0.20769043336064508\n",
      "Iteration 1751, BCE loss: 57.758360385487606, Acc: 0.8194, Grad norm: 0.20746021585102375\n",
      "Iteration 1752, BCE loss: 57.75803921131765, Acc: 0.8194, Grad norm: 0.16308229388322362\n",
      "Iteration 1753, BCE loss: 57.75782086679771, Acc: 0.8195, Grad norm: 0.14181849674371325\n",
      "Iteration 1754, BCE loss: 57.75804779028368, Acc: 0.8195, Grad norm: 0.17929571085883533\n",
      "Iteration 1755, BCE loss: 57.75758575054621, Acc: 0.8196, Grad norm: 0.10753365485535434\n",
      "Iteration 1756, BCE loss: 57.757655521527056, Acc: 0.8195, Grad norm: 0.12023708564104778\n",
      "Iteration 1757, BCE loss: 57.75754439965929, Acc: 0.8196, Grad norm: 0.093654794990407\n",
      "Iteration 1758, BCE loss: 57.75781446070971, Acc: 0.8195, Grad norm: 0.1466428115390771\n",
      "Iteration 1759, BCE loss: 57.75783529727322, Acc: 0.8196, Grad norm: 0.15430756626050782\n",
      "Iteration 1760, BCE loss: 57.75769158008811, Acc: 0.8195, Grad norm: 0.13267702440225285\n",
      "Iteration 1761, BCE loss: 57.75770217881245, Acc: 0.8195, Grad norm: 0.1382673715782522\n",
      "Iteration 1762, BCE loss: 57.75782741396407, Acc: 0.8195, Grad norm: 0.15003342378861115\n",
      "Iteration 1763, BCE loss: 57.758140460893216, Acc: 0.8194, Grad norm: 0.18812527245779387\n",
      "Iteration 1764, BCE loss: 57.75781607567962, Acc: 0.8195, Grad norm: 0.14665287209821207\n",
      "Iteration 1765, BCE loss: 57.75756085951315, Acc: 0.8194, Grad norm: 0.10622190461183628\n",
      "Iteration 1766, BCE loss: 57.757566978596145, Acc: 0.8195, Grad norm: 0.11163700287361043\n",
      "Iteration 1767, BCE loss: 57.75786884037112, Acc: 0.8194, Grad norm: 0.16185932962066243\n",
      "Iteration 1768, BCE loss: 57.75767754327464, Acc: 0.8195, Grad norm: 0.13268972035929497\n",
      "Iteration 1769, BCE loss: 57.75777117375027, Acc: 0.8195, Grad norm: 0.13178537701271328\n",
      "Iteration 1770, BCE loss: 57.75851726297317, Acc: 0.8196, Grad norm: 0.219241099976148\n",
      "Iteration 1771, BCE loss: 57.75848753427428, Acc: 0.8195, Grad norm: 0.21155707631876391\n",
      "Iteration 1772, BCE loss: 57.75839461891263, Acc: 0.8195, Grad norm: 0.20691234408907191\n",
      "Iteration 1773, BCE loss: 57.758326102807935, Acc: 0.8195, Grad norm: 0.20563618574718046\n",
      "Iteration 1774, BCE loss: 57.758490589012794, Acc: 0.8195, Grad norm: 0.22591339507474714\n",
      "Iteration 1775, BCE loss: 57.75854239372223, Acc: 0.8195, Grad norm: 0.22603017564910827\n",
      "Iteration 1776, BCE loss: 57.757907936479725, Acc: 0.8196, Grad norm: 0.15113983956959035\n",
      "Iteration 1777, BCE loss: 57.75799770598381, Acc: 0.8196, Grad norm: 0.1699147178479783\n",
      "Iteration 1778, BCE loss: 57.75820121352481, Acc: 0.8196, Grad norm: 0.1917673198467304\n",
      "Iteration 1779, BCE loss: 57.758489498839694, Acc: 0.8195, Grad norm: 0.22220293860729098\n",
      "Iteration 1780, BCE loss: 57.75829186107752, Acc: 0.8195, Grad norm: 0.20980278200309269\n",
      "Iteration 1781, BCE loss: 57.75852635918483, Acc: 0.8195, Grad norm: 0.23393022465242722\n",
      "Iteration 1782, BCE loss: 57.75815265236144, Acc: 0.8196, Grad norm: 0.1863619846729008\n",
      "Iteration 1783, BCE loss: 57.758175488737116, Acc: 0.8196, Grad norm: 0.18160350599106703\n",
      "Iteration 1784, BCE loss: 57.757717310213366, Acc: 0.8196, Grad norm: 0.12153338050366348\n",
      "Iteration 1785, BCE loss: 57.75752848751266, Acc: 0.8196, Grad norm: 0.08703260677815244\n",
      "Iteration 1786, BCE loss: 57.75805805959581, Acc: 0.8195, Grad norm: 0.1672916024875107\n",
      "Iteration 1787, BCE loss: 57.758073872815416, Acc: 0.8195, Grad norm: 0.1770133013348787\n",
      "Iteration 1788, BCE loss: 57.75828692006677, Acc: 0.8194, Grad norm: 0.1978885735037068\n",
      "Iteration 1789, BCE loss: 57.75792881344353, Acc: 0.8194, Grad norm: 0.15134560761359936\n",
      "Iteration 1790, BCE loss: 57.758037825504125, Acc: 0.8194, Grad norm: 0.1701673439490404\n",
      "Iteration 1791, BCE loss: 57.75783439811953, Acc: 0.8194, Grad norm: 0.1452414610188031\n",
      "Iteration 1792, BCE loss: 57.7579370665888, Acc: 0.8195, Grad norm: 0.1656100787279585\n",
      "Iteration 1793, BCE loss: 57.7576312142329, Acc: 0.8196, Grad norm: 0.12014801697051652\n",
      "Iteration 1794, BCE loss: 57.75818072252896, Acc: 0.8197, Grad norm: 0.197962451050653\n",
      "Iteration 1795, BCE loss: 57.757924456904604, Acc: 0.8196, Grad norm: 0.1614165586705268\n",
      "Iteration 1796, BCE loss: 57.7579739108277, Acc: 0.8197, Grad norm: 0.17125794631260458\n",
      "Iteration 1797, BCE loss: 57.75795667718518, Acc: 0.8197, Grad norm: 0.1668291235649354\n",
      "Iteration 1798, BCE loss: 57.75792991208658, Acc: 0.8196, Grad norm: 0.1636983561833361\n",
      "Iteration 1799, BCE loss: 57.757953284136875, Acc: 0.8196, Grad norm: 0.1620288971755698\n",
      "Iteration 1800, BCE loss: 57.75816821446564, Acc: 0.8196, Grad norm: 0.18296967573988532\n",
      "Iteration 1801, BCE loss: 57.75814670655636, Acc: 0.8195, Grad norm: 0.1859467697092816\n",
      "Iteration 1802, BCE loss: 57.7579975364821, Acc: 0.8196, Grad norm: 0.16595105242929184\n",
      "Iteration 1803, BCE loss: 57.75798041466492, Acc: 0.8196, Grad norm: 0.1557465978685774\n",
      "Iteration 1804, BCE loss: 57.75804039123324, Acc: 0.8196, Grad norm: 0.173987575663849\n",
      "Iteration 1805, BCE loss: 57.75859532399653, Acc: 0.8196, Grad norm: 0.2406050674324961\n",
      "Iteration 1806, BCE loss: 57.758441971384, Acc: 0.8195, Grad norm: 0.22364311117050362\n",
      "Iteration 1807, BCE loss: 57.75878660028674, Acc: 0.8195, Grad norm: 0.2584211217458462\n",
      "Iteration 1808, BCE loss: 57.758502148579026, Acc: 0.8195, Grad norm: 0.23537664249096718\n",
      "Iteration 1809, BCE loss: 57.75800997025917, Acc: 0.8195, Grad norm: 0.1729136493296614\n",
      "Iteration 1810, BCE loss: 57.758007733268265, Acc: 0.8195, Grad norm: 0.17372479653467052\n",
      "Iteration 1811, BCE loss: 57.758630786591056, Acc: 0.8195, Grad norm: 0.23538649237122755\n",
      "Iteration 1812, BCE loss: 57.75832469327672, Acc: 0.8195, Grad norm: 0.206271573422832\n",
      "Iteration 1813, BCE loss: 57.7579680226105, Acc: 0.8195, Grad norm: 0.15951404361574942\n",
      "Iteration 1814, BCE loss: 57.75784985833688, Acc: 0.8197, Grad norm: 0.14596826108651517\n",
      "Iteration 1815, BCE loss: 57.75815842511602, Acc: 0.8197, Grad norm: 0.1931104657784081\n",
      "Iteration 1816, BCE loss: 57.757901057213644, Acc: 0.8196, Grad norm: 0.16066006436397542\n",
      "Iteration 1817, BCE loss: 57.75842512019375, Acc: 0.8198, Grad norm: 0.21900206957140883\n",
      "Iteration 1818, BCE loss: 57.75823816099087, Acc: 0.8197, Grad norm: 0.19643900357556818\n",
      "Iteration 1819, BCE loss: 57.758450379876116, Acc: 0.8196, Grad norm: 0.22384652407827743\n",
      "Iteration 1820, BCE loss: 57.758035482700095, Acc: 0.8197, Grad norm: 0.18125651303408305\n",
      "Iteration 1821, BCE loss: 57.75811102672478, Acc: 0.8196, Grad norm: 0.1869804180070368\n",
      "Iteration 1822, BCE loss: 57.75796093681444, Acc: 0.8195, Grad norm: 0.16698624112083954\n",
      "Iteration 1823, BCE loss: 57.75856002672094, Acc: 0.8196, Grad norm: 0.2326533379350837\n",
      "Iteration 1824, BCE loss: 57.758124100372825, Acc: 0.8196, Grad norm: 0.1871105591933288\n",
      "Iteration 1825, BCE loss: 57.75813669814653, Acc: 0.8196, Grad norm: 0.19163262587054303\n",
      "Iteration 1826, BCE loss: 57.75779678099285, Acc: 0.8196, Grad norm: 0.14518464014047078\n",
      "Iteration 1827, BCE loss: 57.75801398191172, Acc: 0.8195, Grad norm: 0.16851818257072174\n",
      "Iteration 1828, BCE loss: 57.75773040892346, Acc: 0.8196, Grad norm: 0.13895445377915025\n",
      "Iteration 1829, BCE loss: 57.75765760475584, Acc: 0.8196, Grad norm: 0.12757868419963375\n",
      "Iteration 1830, BCE loss: 57.75769163008037, Acc: 0.8195, Grad norm: 0.12005013846822639\n",
      "Iteration 1831, BCE loss: 57.757973464874, Acc: 0.8195, Grad norm: 0.16231596042812665\n",
      "Iteration 1832, BCE loss: 57.75874365598797, Acc: 0.8194, Grad norm: 0.2508652993454209\n",
      "Iteration 1833, BCE loss: 57.75983866828426, Acc: 0.8194, Grad norm: 0.3384249343939673\n",
      "Iteration 1834, BCE loss: 57.75885538035075, Acc: 0.8194, Grad norm: 0.26192453848399155\n",
      "Iteration 1835, BCE loss: 57.75866752782554, Acc: 0.8194, Grad norm: 0.24199638915315522\n",
      "Iteration 1836, BCE loss: 57.758114469912016, Acc: 0.8194, Grad norm: 0.18128009843815585\n",
      "Iteration 1837, BCE loss: 57.75788055538663, Acc: 0.8195, Grad norm: 0.1502582960174949\n",
      "Iteration 1838, BCE loss: 57.7584265682287, Acc: 0.8195, Grad norm: 0.21297207955825298\n",
      "Iteration 1839, BCE loss: 57.75805790853579, Acc: 0.8195, Grad norm: 0.17972481783973215\n",
      "Iteration 1840, BCE loss: 57.75770087933987, Acc: 0.8195, Grad norm: 0.13268401581665218\n",
      "Iteration 1841, BCE loss: 57.75802887990663, Acc: 0.8195, Grad norm: 0.18027162211871614\n",
      "Iteration 1842, BCE loss: 57.758515060204104, Acc: 0.8195, Grad norm: 0.23213822231002582\n",
      "Iteration 1843, BCE loss: 57.75828177879555, Acc: 0.8195, Grad norm: 0.21148156078962205\n",
      "Iteration 1844, BCE loss: 57.75815748150855, Acc: 0.8196, Grad norm: 0.2018160449251482\n",
      "Iteration 1845, BCE loss: 57.75779631906133, Acc: 0.8196, Grad norm: 0.15096055313172616\n",
      "Iteration 1846, BCE loss: 57.758029964467646, Acc: 0.8195, Grad norm: 0.18219818229241122\n",
      "Iteration 1847, BCE loss: 57.75883313765674, Acc: 0.8196, Grad norm: 0.2716864185048002\n",
      "Iteration 1848, BCE loss: 57.75819848731015, Acc: 0.8196, Grad norm: 0.20894240119026156\n",
      "Iteration 1849, BCE loss: 57.75778009237962, Acc: 0.8196, Grad norm: 0.15668483257728008\n",
      "Iteration 1850, BCE loss: 57.757509826100055, Acc: 0.8196, Grad norm: 0.10646689211078617\n",
      "Iteration 1851, BCE loss: 57.757818521770915, Acc: 0.8197, Grad norm: 0.1549876828129982\n",
      "Iteration 1852, BCE loss: 57.758204000991874, Acc: 0.8197, Grad norm: 0.20980598364768305\n",
      "Iteration 1853, BCE loss: 57.75836426948112, Acc: 0.8197, Grad norm: 0.22721806431219044\n",
      "Iteration 1854, BCE loss: 57.75815040860297, Acc: 0.8197, Grad norm: 0.203241821192762\n",
      "Iteration 1855, BCE loss: 57.759089402054165, Acc: 0.8197, Grad norm: 0.2910835534200913\n",
      "Iteration 1856, BCE loss: 57.75904160171318, Acc: 0.8198, Grad norm: 0.28611589867034604\n",
      "Iteration 1857, BCE loss: 57.7584763257321, Acc: 0.8197, Grad norm: 0.23341895713043528\n",
      "Iteration 1858, BCE loss: 57.759500604206494, Acc: 0.8198, Grad norm: 0.3192881238653347\n",
      "Iteration 1859, BCE loss: 57.759099426242486, Acc: 0.8198, Grad norm: 0.2861154225898597\n",
      "Iteration 1860, BCE loss: 57.75848427818434, Acc: 0.8197, Grad norm: 0.23145987405120383\n",
      "Iteration 1861, BCE loss: 57.75791207688171, Acc: 0.8197, Grad norm: 0.1658436197047943\n",
      "Iteration 1862, BCE loss: 57.75785148803854, Acc: 0.8197, Grad norm: 0.1622866233363871\n",
      "Iteration 1863, BCE loss: 57.7577000433911, Acc: 0.8197, Grad norm: 0.1365739129741908\n",
      "Iteration 1864, BCE loss: 57.757681099549735, Acc: 0.8197, Grad norm: 0.1260205384991432\n",
      "Iteration 1865, BCE loss: 57.75792854360739, Acc: 0.8197, Grad norm: 0.16923913925565612\n",
      "Iteration 1866, BCE loss: 57.757902093679945, Acc: 0.8197, Grad norm: 0.16593320635709816\n",
      "Iteration 1867, BCE loss: 57.7578826791062, Acc: 0.8196, Grad norm: 0.15667411022723385\n",
      "Iteration 1868, BCE loss: 57.758346471317324, Acc: 0.8196, Grad norm: 0.20731156801208683\n",
      "Iteration 1869, BCE loss: 57.757973756518915, Acc: 0.8196, Grad norm: 0.17411566944319004\n",
      "Iteration 1870, BCE loss: 57.75831967279077, Acc: 0.8196, Grad norm: 0.21256898442573757\n",
      "Iteration 1871, BCE loss: 57.757805255998434, Acc: 0.8196, Grad norm: 0.13816174692584185\n",
      "Iteration 1872, BCE loss: 57.7579002820781, Acc: 0.8196, Grad norm: 0.148088714342771\n",
      "Iteration 1873, BCE loss: 57.75779730831888, Acc: 0.8195, Grad norm: 0.13762074250156783\n",
      "Iteration 1874, BCE loss: 57.75835200253164, Acc: 0.8196, Grad norm: 0.2165535302130629\n",
      "Iteration 1875, BCE loss: 57.75803116945043, Acc: 0.8195, Grad norm: 0.1763580814711799\n",
      "Iteration 1876, BCE loss: 57.758474518376005, Acc: 0.8196, Grad norm: 0.2309801106523368\n",
      "Iteration 1877, BCE loss: 57.758558236493926, Acc: 0.8196, Grad norm: 0.2424450446574848\n",
      "Iteration 1878, BCE loss: 57.75857636374708, Acc: 0.8196, Grad norm: 0.24150380562803822\n",
      "Iteration 1879, BCE loss: 57.759054333272026, Acc: 0.8196, Grad norm: 0.2815655780742965\n",
      "Iteration 1880, BCE loss: 57.75993567030355, Acc: 0.8197, Grad norm: 0.3473099210107004\n",
      "Iteration 1881, BCE loss: 57.75976835551725, Acc: 0.8196, Grad norm: 0.33241796370577403\n",
      "Iteration 1882, BCE loss: 57.75828257553398, Acc: 0.8196, Grad norm: 0.20738431456634784\n",
      "Iteration 1883, BCE loss: 57.75821995406937, Acc: 0.8196, Grad norm: 0.19976173109679085\n",
      "Iteration 1884, BCE loss: 57.75775478345669, Acc: 0.8195, Grad norm: 0.1344196088344619\n",
      "Iteration 1885, BCE loss: 57.75830957702751, Acc: 0.8196, Grad norm: 0.2065944469437577\n",
      "Iteration 1886, BCE loss: 57.757998875135144, Acc: 0.8196, Grad norm: 0.17646002284436021\n",
      "Iteration 1887, BCE loss: 57.75797045738708, Acc: 0.8195, Grad norm: 0.16486322051933788\n",
      "Iteration 1888, BCE loss: 57.75779226050417, Acc: 0.8195, Grad norm: 0.14502136890177833\n",
      "Iteration 1889, BCE loss: 57.75752957040372, Acc: 0.8195, Grad norm: 0.09768369502894908\n",
      "Iteration 1890, BCE loss: 57.75746028590319, Acc: 0.8196, Grad norm: 0.08076560173250938\n",
      "Iteration 1891, BCE loss: 57.75769895012398, Acc: 0.8196, Grad norm: 0.13599123810815222\n",
      "Iteration 1892, BCE loss: 57.757810731455876, Acc: 0.8196, Grad norm: 0.1580849700950881\n",
      "Iteration 1893, BCE loss: 57.75773248911376, Acc: 0.8196, Grad norm: 0.14448261540712043\n",
      "Iteration 1894, BCE loss: 57.757581574667924, Acc: 0.8196, Grad norm: 0.11800842864198935\n",
      "Iteration 1895, BCE loss: 57.75736886232496, Acc: 0.8195, Grad norm: 0.061391058415232985\n",
      "Iteration 1896, BCE loss: 57.757449988668554, Acc: 0.8196, Grad norm: 0.08443711673095397\n",
      "Iteration 1897, BCE loss: 57.7576890615902, Acc: 0.8196, Grad norm: 0.13280681383740892\n",
      "Iteration 1898, BCE loss: 57.75766615665893, Acc: 0.8195, Grad norm: 0.12943094808618047\n",
      "Iteration 1899, BCE loss: 57.757516419875444, Acc: 0.8195, Grad norm: 0.0945472580685644\n",
      "Iteration 1900, BCE loss: 57.75766098353536, Acc: 0.8195, Grad norm: 0.12023868370842154\n",
      "Iteration 1901, BCE loss: 57.757799212750385, Acc: 0.8195, Grad norm: 0.14773681101296746\n",
      "Iteration 1902, BCE loss: 57.75759868280322, Acc: 0.8195, Grad norm: 0.11907226771198896\n",
      "Iteration 1903, BCE loss: 57.757679540713966, Acc: 0.8195, Grad norm: 0.13642690360732182\n",
      "Iteration 1904, BCE loss: 57.75772739895758, Acc: 0.8195, Grad norm: 0.14264555084255146\n",
      "Iteration 1905, BCE loss: 57.75803687689369, Acc: 0.8195, Grad norm: 0.18886041261827155\n",
      "Iteration 1906, BCE loss: 57.75798284193353, Acc: 0.8195, Grad norm: 0.17610734553405996\n",
      "Iteration 1907, BCE loss: 57.7575703219534, Acc: 0.8196, Grad norm: 0.1081771251689511\n",
      "Iteration 1908, BCE loss: 57.75762369774655, Acc: 0.8196, Grad norm: 0.11833997658929113\n",
      "Iteration 1909, BCE loss: 57.757789399102094, Acc: 0.8196, Grad norm: 0.14785580291220024\n",
      "Iteration 1910, BCE loss: 57.75789482873407, Acc: 0.8196, Grad norm: 0.16760521087489683\n",
      "Iteration 1911, BCE loss: 57.75753919740592, Acc: 0.8196, Grad norm: 0.11071767520352624\n",
      "Iteration 1912, BCE loss: 57.75755034217049, Acc: 0.8195, Grad norm: 0.11025094570058278\n",
      "Iteration 1913, BCE loss: 57.75783405134139, Acc: 0.8194, Grad norm: 0.15011949305163988\n",
      "Iteration 1914, BCE loss: 57.75799510388132, Acc: 0.8194, Grad norm: 0.1809304637231388\n",
      "Iteration 1915, BCE loss: 57.75773431295544, Acc: 0.8195, Grad norm: 0.14104105113650087\n",
      "Iteration 1916, BCE loss: 57.75758054905519, Acc: 0.8195, Grad norm: 0.11203134296622397\n",
      "Iteration 1917, BCE loss: 57.75753857396125, Acc: 0.8195, Grad norm: 0.10471437642332743\n",
      "Iteration 1918, BCE loss: 57.75749798337232, Acc: 0.8195, Grad norm: 0.09114320045598921\n",
      "Iteration 1919, BCE loss: 57.757732214275336, Acc: 0.8195, Grad norm: 0.13546138267513216\n",
      "Iteration 1920, BCE loss: 57.757637932173225, Acc: 0.8194, Grad norm: 0.11788400855709374\n",
      "Iteration 1921, BCE loss: 57.7579403639037, Acc: 0.8194, Grad norm: 0.16654023235978482\n",
      "Iteration 1922, BCE loss: 57.75777980589977, Acc: 0.8194, Grad norm: 0.14446695893875486\n",
      "Iteration 1923, BCE loss: 57.757839365303326, Acc: 0.8194, Grad norm: 0.14972581743446595\n",
      "Iteration 1924, BCE loss: 57.75757737921376, Acc: 0.8195, Grad norm: 0.10204085404226436\n",
      "Iteration 1925, BCE loss: 57.757689905073235, Acc: 0.8195, Grad norm: 0.12289031592484409\n",
      "Iteration 1926, BCE loss: 57.75792563053472, Acc: 0.8195, Grad norm: 0.16331187006691053\n",
      "Iteration 1927, BCE loss: 57.75806348509167, Acc: 0.8195, Grad norm: 0.18107863907663732\n",
      "Iteration 1928, BCE loss: 57.757848550281956, Acc: 0.8195, Grad norm: 0.1431590365500122\n",
      "Iteration 1929, BCE loss: 57.75790391404949, Acc: 0.8195, Grad norm: 0.1468129556693526\n",
      "Iteration 1930, BCE loss: 57.75806678185312, Acc: 0.8196, Grad norm: 0.1792821480664029\n",
      "Iteration 1931, BCE loss: 57.7580113082625, Acc: 0.8196, Grad norm: 0.17561430125036842\n",
      "Iteration 1932, BCE loss: 57.75847586966185, Acc: 0.8195, Grad norm: 0.21981780938130815\n",
      "Iteration 1933, BCE loss: 57.75836041181456, Acc: 0.8195, Grad norm: 0.2156501205804398\n",
      "Iteration 1934, BCE loss: 57.75806064086345, Acc: 0.8195, Grad norm: 0.1807397352835178\n",
      "Iteration 1935, BCE loss: 57.75783031417858, Acc: 0.8195, Grad norm: 0.15139153839566563\n",
      "Iteration 1936, BCE loss: 57.757604980442146, Acc: 0.8195, Grad norm: 0.11069036494730876\n",
      "Iteration 1937, BCE loss: 57.75753471703732, Acc: 0.8195, Grad norm: 0.09927028533354836\n",
      "Iteration 1938, BCE loss: 57.75764632006219, Acc: 0.8195, Grad norm: 0.12950350155218057\n",
      "Iteration 1939, BCE loss: 57.75789158357173, Acc: 0.8194, Grad norm: 0.16888692151757972\n",
      "Iteration 1940, BCE loss: 57.758241777420096, Acc: 0.8194, Grad norm: 0.21157946161152077\n",
      "Iteration 1941, BCE loss: 57.758273460382654, Acc: 0.8194, Grad norm: 0.21704449082200458\n",
      "Iteration 1942, BCE loss: 57.75884358237903, Acc: 0.8195, Grad norm: 0.26784442705731326\n",
      "Iteration 1943, BCE loss: 57.75858371276725, Acc: 0.8195, Grad norm: 0.24484386467052838\n",
      "Iteration 1944, BCE loss: 57.75819060416882, Acc: 0.8195, Grad norm: 0.2049726969750666\n",
      "Iteration 1945, BCE loss: 57.75890523910647, Acc: 0.8194, Grad norm: 0.2761907710780529\n",
      "Iteration 1946, BCE loss: 57.75950889307765, Acc: 0.8194, Grad norm: 0.3260317811720429\n",
      "Iteration 1947, BCE loss: 57.75834147017491, Acc: 0.8194, Grad norm: 0.22653019258606708\n",
      "Iteration 1948, BCE loss: 57.75805615171467, Acc: 0.8195, Grad norm: 0.19106928203258247\n",
      "Iteration 1949, BCE loss: 57.75789542871743, Acc: 0.8195, Grad norm: 0.1592302419244194\n",
      "Iteration 1950, BCE loss: 57.75766174598495, Acc: 0.8196, Grad norm: 0.11777939478656994\n",
      "Iteration 1951, BCE loss: 57.75745965518392, Acc: 0.8196, Grad norm: 0.0834011167169254\n",
      "Iteration 1952, BCE loss: 57.757644670926936, Acc: 0.8195, Grad norm: 0.12480423209772241\n",
      "Iteration 1953, BCE loss: 57.7584201045933, Acc: 0.8195, Grad norm: 0.22365924323360176\n",
      "Iteration 1954, BCE loss: 57.75865819763125, Acc: 0.8195, Grad norm: 0.24499734122173658\n",
      "Iteration 1955, BCE loss: 57.758165089056256, Acc: 0.8195, Grad norm: 0.19492341962351012\n",
      "Iteration 1956, BCE loss: 57.75810984247457, Acc: 0.8195, Grad norm: 0.19177263197826938\n",
      "Iteration 1957, BCE loss: 57.757712470597454, Acc: 0.8195, Grad norm: 0.1361593034030121\n",
      "Iteration 1958, BCE loss: 57.75769272948191, Acc: 0.8196, Grad norm: 0.13346652353519753\n",
      "Iteration 1959, BCE loss: 57.75798305930985, Acc: 0.8197, Grad norm: 0.1738905219898227\n",
      "Iteration 1960, BCE loss: 57.75807205402205, Acc: 0.8196, Grad norm: 0.1808305415212237\n",
      "Iteration 1961, BCE loss: 57.75760785623318, Acc: 0.8196, Grad norm: 0.11501930394650078\n",
      "Iteration 1962, BCE loss: 57.75769212135644, Acc: 0.8196, Grad norm: 0.1314781506895172\n",
      "Iteration 1963, BCE loss: 57.7575545840549, Acc: 0.8196, Grad norm: 0.10576070909631972\n",
      "Iteration 1964, BCE loss: 57.75786867361887, Acc: 0.8196, Grad norm: 0.1555523544666438\n",
      "Iteration 1965, BCE loss: 57.75817037407903, Acc: 0.8196, Grad norm: 0.18588800461542035\n",
      "Iteration 1966, BCE loss: 57.75804255717994, Acc: 0.8195, Grad norm: 0.17421560537200673\n",
      "Iteration 1967, BCE loss: 57.7578821141782, Acc: 0.8195, Grad norm: 0.15617770371064937\n",
      "Iteration 1968, BCE loss: 57.757855622722374, Acc: 0.8194, Grad norm: 0.14850245977256477\n",
      "Iteration 1969, BCE loss: 57.75811919540464, Acc: 0.8195, Grad norm: 0.18305308459190175\n",
      "Iteration 1970, BCE loss: 57.75782394032347, Acc: 0.8195, Grad norm: 0.14881773573249515\n",
      "Iteration 1971, BCE loss: 57.75799125606444, Acc: 0.8195, Grad norm: 0.17472885528944754\n",
      "Iteration 1972, BCE loss: 57.75862942399796, Acc: 0.8196, Grad norm: 0.24689059237939828\n",
      "Iteration 1973, BCE loss: 57.758732800778105, Acc: 0.8196, Grad norm: 0.2540867954186499\n",
      "Iteration 1974, BCE loss: 57.757854634617054, Acc: 0.8196, Grad norm: 0.1622792946073224\n",
      "Iteration 1975, BCE loss: 57.75753068562199, Acc: 0.8195, Grad norm: 0.09719898808147971\n",
      "Iteration 1976, BCE loss: 57.75781455507486, Acc: 0.8195, Grad norm: 0.14771105175201224\n",
      "Iteration 1977, BCE loss: 57.75803827918919, Acc: 0.8195, Grad norm: 0.17870085558387108\n",
      "Iteration 1978, BCE loss: 57.75825960012088, Acc: 0.8194, Grad norm: 0.20327544802123954\n",
      "Iteration 1979, BCE loss: 57.75818254276842, Acc: 0.8194, Grad norm: 0.19603832961112594\n",
      "Iteration 1980, BCE loss: 57.758308650016986, Acc: 0.8194, Grad norm: 0.2106640908884531\n",
      "Iteration 1981, BCE loss: 57.758395097626945, Acc: 0.8194, Grad norm: 0.21729899838550093\n",
      "Iteration 1982, BCE loss: 57.75824835282749, Acc: 0.8194, Grad norm: 0.19572576272057302\n",
      "Iteration 1983, BCE loss: 57.75783676315482, Acc: 0.8195, Grad norm: 0.14447432077006486\n",
      "Iteration 1984, BCE loss: 57.75779398069468, Acc: 0.8195, Grad norm: 0.13867647785282247\n",
      "Iteration 1985, BCE loss: 57.75790638070447, Acc: 0.8195, Grad norm: 0.15378526734884165\n",
      "Iteration 1986, BCE loss: 57.75764438426953, Acc: 0.8196, Grad norm: 0.10950099873137052\n",
      "Iteration 1987, BCE loss: 57.75775142982634, Acc: 0.8196, Grad norm: 0.12768337665790366\n",
      "Iteration 1988, BCE loss: 57.75756174249864, Acc: 0.8195, Grad norm: 0.09676498325710857\n",
      "Iteration 1989, BCE loss: 57.75769274860748, Acc: 0.8195, Grad norm: 0.12063421454979584\n",
      "Iteration 1990, BCE loss: 57.75767581432382, Acc: 0.8195, Grad norm: 0.11740745443733411\n",
      "Iteration 1991, BCE loss: 57.7576796273615, Acc: 0.8195, Grad norm: 0.11595716865827259\n",
      "Iteration 1992, BCE loss: 57.75779311494277, Acc: 0.8195, Grad norm: 0.13322269097596864\n",
      "Iteration 1993, BCE loss: 57.75805073568658, Acc: 0.8195, Grad norm: 0.16230918418852872\n",
      "Iteration 1994, BCE loss: 57.75799081097541, Acc: 0.8196, Grad norm: 0.15216525658417676\n",
      "Iteration 1995, BCE loss: 57.7578879179953, Acc: 0.8196, Grad norm: 0.15395981290773714\n",
      "Iteration 1996, BCE loss: 57.75821698134378, Acc: 0.8197, Grad norm: 0.20551000495794564\n",
      "Iteration 1997, BCE loss: 57.75855741250825, Acc: 0.8197, Grad norm: 0.2389212832524197\n",
      "Iteration 1998, BCE loss: 57.75805457762213, Acc: 0.8196, Grad norm: 0.18019960602772897\n",
      "Iteration 1999, BCE loss: 57.757940387480026, Acc: 0.8196, Grad norm: 0.17068479121569863\n",
      "Iteration 2000, BCE loss: 57.757581109954245, Acc: 0.8196, Grad norm: 0.1089725501609374\n",
      "Iteration 2001, BCE loss: 57.75759728933637, Acc: 0.8196, Grad norm: 0.10315565300522034\n",
      "Iteration 2002, BCE loss: 57.757839385222255, Acc: 0.8196, Grad norm: 0.15155930464834758\n",
      "Iteration 2003, BCE loss: 57.75749154900886, Acc: 0.8196, Grad norm: 0.09621738731501095\n",
      "Iteration 2004, BCE loss: 57.757918648353815, Acc: 0.8195, Grad norm: 0.17513791915765123\n",
      "Iteration 2005, BCE loss: 57.7576542729075, Acc: 0.8196, Grad norm: 0.12648521277930527\n",
      "Iteration 2006, BCE loss: 57.75771312098005, Acc: 0.8195, Grad norm: 0.13160365627186393\n",
      "Iteration 2007, BCE loss: 57.75778940253579, Acc: 0.8195, Grad norm: 0.14118247437796083\n",
      "Iteration 2008, BCE loss: 57.7577771729007, Acc: 0.8195, Grad norm: 0.1403710193445873\n",
      "Iteration 2009, BCE loss: 57.75753891299291, Acc: 0.8196, Grad norm: 0.09688474030932737\n",
      "Iteration 2010, BCE loss: 57.757474021300396, Acc: 0.8196, Grad norm: 0.08603217643998258\n",
      "Iteration 2011, BCE loss: 57.757555326909326, Acc: 0.8196, Grad norm: 0.10339592880710434\n",
      "Iteration 2012, BCE loss: 57.75759711006454, Acc: 0.8197, Grad norm: 0.1148264449102251\n",
      "Iteration 2013, BCE loss: 57.75784715223877, Acc: 0.8197, Grad norm: 0.15879995179484416\n",
      "Iteration 2014, BCE loss: 57.75767250435973, Acc: 0.8197, Grad norm: 0.12536734395935217\n",
      "Iteration 2015, BCE loss: 57.757550406814104, Acc: 0.8196, Grad norm: 0.09974300846047982\n",
      "Iteration 2016, BCE loss: 57.75769243075272, Acc: 0.8196, Grad norm: 0.11871879896892552\n",
      "Iteration 2017, BCE loss: 57.75807586679167, Acc: 0.8197, Grad norm: 0.17984842547847357\n",
      "Iteration 2018, BCE loss: 57.75843307942566, Acc: 0.8197, Grad norm: 0.22119306264668373\n",
      "Iteration 2019, BCE loss: 57.75773764662095, Acc: 0.8196, Grad norm: 0.12734198283752657\n",
      "Iteration 2020, BCE loss: 57.757497127883994, Acc: 0.8196, Grad norm: 0.08315955257230578\n",
      "Iteration 2021, BCE loss: 57.757616298500196, Acc: 0.8197, Grad norm: 0.11438272634888488\n",
      "Iteration 2022, BCE loss: 57.75779471309126, Acc: 0.8197, Grad norm: 0.14557703899027788\n",
      "Iteration 2023, BCE loss: 57.758034923615384, Acc: 0.8198, Grad norm: 0.1771513972869457\n",
      "Iteration 2024, BCE loss: 57.75767991822973, Acc: 0.8197, Grad norm: 0.13397486530064587\n",
      "Iteration 2025, BCE loss: 57.75850086124851, Acc: 0.8197, Grad norm: 0.23513009090704284\n",
      "Iteration 2026, BCE loss: 57.75842031287098, Acc: 0.8197, Grad norm: 0.22577862020318534\n",
      "Iteration 2027, BCE loss: 57.75855019306877, Acc: 0.8197, Grad norm: 0.24193146499707063\n",
      "Iteration 2028, BCE loss: 57.75838136235615, Acc: 0.8196, Grad norm: 0.21764130743608687\n",
      "Iteration 2029, BCE loss: 57.75775696703761, Acc: 0.8197, Grad norm: 0.1395494795077432\n",
      "Iteration 2030, BCE loss: 57.75772119477119, Acc: 0.8196, Grad norm: 0.13436540460076662\n",
      "Iteration 2031, BCE loss: 57.75773885890595, Acc: 0.8196, Grad norm: 0.13224472153240152\n",
      "Iteration 2032, BCE loss: 57.75767820819037, Acc: 0.8196, Grad norm: 0.12176972703829331\n",
      "Iteration 2033, BCE loss: 57.75761957690828, Acc: 0.8197, Grad norm: 0.11141605699189842\n",
      "Iteration 2034, BCE loss: 57.75754006464258, Acc: 0.8197, Grad norm: 0.09890431095712257\n",
      "Iteration 2035, BCE loss: 57.75756068100986, Acc: 0.8197, Grad norm: 0.11247131125458262\n",
      "Iteration 2036, BCE loss: 57.75749174084241, Acc: 0.8196, Grad norm: 0.09594972836681101\n",
      "Iteration 2037, BCE loss: 57.757711633311644, Acc: 0.8196, Grad norm: 0.13789725398435895\n",
      "Iteration 2038, BCE loss: 57.75754230978559, Acc: 0.8197, Grad norm: 0.10692543422301604\n",
      "Iteration 2039, BCE loss: 57.75741982431371, Acc: 0.8196, Grad norm: 0.06993205254171783\n",
      "Iteration 2040, BCE loss: 57.75794391883346, Acc: 0.8197, Grad norm: 0.16858188106458707\n",
      "Iteration 2041, BCE loss: 57.75842614593815, Acc: 0.8198, Grad norm: 0.22888992051229695\n",
      "Iteration 2042, BCE loss: 57.75928016430631, Acc: 0.8198, Grad norm: 0.3086482214220015\n",
      "Iteration 2043, BCE loss: 57.75806054087667, Acc: 0.8197, Grad norm: 0.1903382718732127\n",
      "Iteration 2044, BCE loss: 57.75775534262595, Acc: 0.8197, Grad norm: 0.1466094416133064\n",
      "Iteration 2045, BCE loss: 57.75774226527052, Acc: 0.8197, Grad norm: 0.14418816920314065\n",
      "Iteration 2046, BCE loss: 57.757556602079234, Acc: 0.8196, Grad norm: 0.10824808139038786\n",
      "Iteration 2047, BCE loss: 57.75760989553286, Acc: 0.8196, Grad norm: 0.12025983542644093\n",
      "Iteration 2048, BCE loss: 57.75760961969301, Acc: 0.8196, Grad norm: 0.12080919810148916\n",
      "Iteration 2049, BCE loss: 57.75797600924585, Acc: 0.8196, Grad norm: 0.17537078356262964\n",
      "Iteration 2050, BCE loss: 57.75819242118587, Acc: 0.8197, Grad norm: 0.20331802021570738\n",
      "Iteration 2051, BCE loss: 57.75801785206991, Acc: 0.8197, Grad norm: 0.17794679834947263\n",
      "Iteration 2052, BCE loss: 57.758214000927495, Acc: 0.8197, Grad norm: 0.19954004670441639\n",
      "Iteration 2053, BCE loss: 57.75793019164156, Acc: 0.8197, Grad norm: 0.16149391499820437\n",
      "Iteration 2054, BCE loss: 57.75792983573608, Acc: 0.8196, Grad norm: 0.15933479689637509\n",
      "Iteration 2055, BCE loss: 57.75780065130437, Acc: 0.8197, Grad norm: 0.13746183719343658\n",
      "Iteration 2056, BCE loss: 57.75780506437974, Acc: 0.8197, Grad norm: 0.14453792226919912\n",
      "Iteration 2057, BCE loss: 57.757700187189386, Acc: 0.8196, Grad norm: 0.12830320034993664\n",
      "Iteration 2058, BCE loss: 57.75773958296536, Acc: 0.8196, Grad norm: 0.13879780999329874\n",
      "Iteration 2059, BCE loss: 57.75736422720202, Acc: 0.8196, Grad norm: 0.05988314556943044\n",
      "Iteration 2060, BCE loss: 57.75747985947699, Acc: 0.8196, Grad norm: 0.09257642365240507\n",
      "Iteration 2061, BCE loss: 57.75775244558364, Acc: 0.8196, Grad norm: 0.14680889482426804\n",
      "Iteration 2062, BCE loss: 57.757651246531594, Acc: 0.8196, Grad norm: 0.1268980570203914\n",
      "Iteration 2063, BCE loss: 57.75747720795282, Acc: 0.8196, Grad norm: 0.08581646305756412\n",
      "Iteration 2064, BCE loss: 57.75749030818957, Acc: 0.8196, Grad norm: 0.08665452857575277\n",
      "Iteration 2065, BCE loss: 57.75748184272334, Acc: 0.8196, Grad norm: 0.09005268778565963\n",
      "Iteration 2066, BCE loss: 57.757580172985975, Acc: 0.8195, Grad norm: 0.10696814831533369\n",
      "Iteration 2067, BCE loss: 57.757856048268216, Acc: 0.8195, Grad norm: 0.14256209715371\n",
      "Iteration 2068, BCE loss: 57.75768504084675, Acc: 0.8196, Grad norm: 0.1254479905416765\n",
      "Iteration 2069, BCE loss: 57.75769432640102, Acc: 0.8196, Grad norm: 0.12828835657812152\n",
      "Iteration 2070, BCE loss: 57.75775109030319, Acc: 0.8196, Grad norm: 0.12914289190690853\n",
      "Iteration 2071, BCE loss: 57.75770709387051, Acc: 0.8196, Grad norm: 0.124229834904943\n",
      "Iteration 2072, BCE loss: 57.757800238854514, Acc: 0.8196, Grad norm: 0.13798938447077277\n",
      "Iteration 2073, BCE loss: 57.75767340917393, Acc: 0.8195, Grad norm: 0.11621488004422043\n",
      "Iteration 2074, BCE loss: 57.757725725923514, Acc: 0.8196, Grad norm: 0.12571477376766488\n",
      "Iteration 2075, BCE loss: 57.75754101785308, Acc: 0.8196, Grad norm: 0.0957973658819297\n",
      "Iteration 2076, BCE loss: 57.75764349494743, Acc: 0.8196, Grad norm: 0.11749043992680724\n",
      "Iteration 2077, BCE loss: 57.75783793560502, Acc: 0.8196, Grad norm: 0.14544927847172343\n",
      "Iteration 2078, BCE loss: 57.7575253720451, Acc: 0.8196, Grad norm: 0.09560255076383352\n",
      "Iteration 2079, BCE loss: 57.75767402411396, Acc: 0.8195, Grad norm: 0.1305908735110442\n",
      "Iteration 2080, BCE loss: 57.757726566396656, Acc: 0.8196, Grad norm: 0.14117995615246703\n",
      "Iteration 2081, BCE loss: 57.75762865962879, Acc: 0.8196, Grad norm: 0.12007961716837551\n",
      "Iteration 2082, BCE loss: 57.75766788049326, Acc: 0.8196, Grad norm: 0.11995675423790841\n",
      "Iteration 2083, BCE loss: 57.757593399526044, Acc: 0.8196, Grad norm: 0.11126173668526633\n",
      "Iteration 2084, BCE loss: 57.7574417996629, Acc: 0.8195, Grad norm: 0.0842986059556264\n",
      "Iteration 2085, BCE loss: 57.757482392741224, Acc: 0.8195, Grad norm: 0.09390247509258066\n",
      "Iteration 2086, BCE loss: 57.757475322776045, Acc: 0.8195, Grad norm: 0.08854554114177209\n",
      "Iteration 2087, BCE loss: 57.75746111623931, Acc: 0.8195, Grad norm: 0.08156712276002186\n",
      "Iteration 2088, BCE loss: 57.757955506653715, Acc: 0.8195, Grad norm: 0.16936917772560842\n",
      "Iteration 2089, BCE loss: 57.75781456243918, Acc: 0.8196, Grad norm: 0.15147089923592996\n",
      "Iteration 2090, BCE loss: 57.75757856934177, Acc: 0.8196, Grad norm: 0.11500930147627358\n",
      "Iteration 2091, BCE loss: 57.7576879106956, Acc: 0.8197, Grad norm: 0.13082180257434425\n",
      "Iteration 2092, BCE loss: 57.75803832279547, Acc: 0.8197, Grad norm: 0.17659649860829896\n",
      "Iteration 2093, BCE loss: 57.75781099671135, Acc: 0.8197, Grad norm: 0.13965973808907464\n",
      "Iteration 2094, BCE loss: 57.75799268254225, Acc: 0.8197, Grad norm: 0.17120947663948055\n",
      "Iteration 2095, BCE loss: 57.75803652268857, Acc: 0.8197, Grad norm: 0.17413190078814073\n",
      "Iteration 2096, BCE loss: 57.757907857663724, Acc: 0.8196, Grad norm: 0.1612951795706675\n",
      "Iteration 2097, BCE loss: 57.757599547762496, Acc: 0.8196, Grad norm: 0.10843289291205488\n",
      "Iteration 2098, BCE loss: 57.757875121094095, Acc: 0.8196, Grad norm: 0.1568403354084848\n",
      "Iteration 2099, BCE loss: 57.75789398636798, Acc: 0.8197, Grad norm: 0.15791345534577375\n",
      "Iteration 2100, BCE loss: 57.757615878360575, Acc: 0.8197, Grad norm: 0.11257293868519415\n",
      "Iteration 2101, BCE loss: 57.75751004844227, Acc: 0.8196, Grad norm: 0.08591720980900987\n",
      "Iteration 2102, BCE loss: 57.75792031059278, Acc: 0.8196, Grad norm: 0.15656006310476175\n",
      "Iteration 2103, BCE loss: 57.758186520426015, Acc: 0.8195, Grad norm: 0.19039488761256915\n",
      "Iteration 2104, BCE loss: 57.757956536071646, Acc: 0.8195, Grad norm: 0.15469711382949045\n",
      "Iteration 2105, BCE loss: 57.75780749191968, Acc: 0.8195, Grad norm: 0.1380912945017801\n",
      "Iteration 2106, BCE loss: 57.757748642899344, Acc: 0.8196, Grad norm: 0.1232339768835897\n",
      "Iteration 2107, BCE loss: 57.75769408567949, Acc: 0.8196, Grad norm: 0.11555484276890135\n",
      "Iteration 2108, BCE loss: 57.75799249210826, Acc: 0.8196, Grad norm: 0.15133920049050825\n",
      "Iteration 2109, BCE loss: 57.75787589380756, Acc: 0.8196, Grad norm: 0.14764614957555788\n",
      "Iteration 2110, BCE loss: 57.757906198643454, Acc: 0.8196, Grad norm: 0.14834823234977526\n",
      "Iteration 2111, BCE loss: 57.758075695491314, Acc: 0.8196, Grad norm: 0.17364057330848362\n",
      "Iteration 2112, BCE loss: 57.758110562889115, Acc: 0.8196, Grad norm: 0.1723341715543653\n",
      "Iteration 2113, BCE loss: 57.75813058816681, Acc: 0.8196, Grad norm: 0.1743381851817429\n",
      "Iteration 2114, BCE loss: 57.757972814041175, Acc: 0.8196, Grad norm: 0.15500625335084706\n",
      "Iteration 2115, BCE loss: 57.757904006394725, Acc: 0.8196, Grad norm: 0.1508875555883743\n",
      "Iteration 2116, BCE loss: 57.757768244777196, Acc: 0.8196, Grad norm: 0.1322120560736623\n",
      "Iteration 2117, BCE loss: 57.75766128765641, Acc: 0.8196, Grad norm: 0.1160847855584111\n",
      "Iteration 2118, BCE loss: 57.75754038583737, Acc: 0.8196, Grad norm: 0.09464391195906995\n",
      "Iteration 2119, BCE loss: 57.75747245104865, Acc: 0.8196, Grad norm: 0.08224121383811915\n",
      "Iteration 2120, BCE loss: 57.757511432936624, Acc: 0.8196, Grad norm: 0.0921601213168498\n",
      "Iteration 2121, BCE loss: 57.7574877881526, Acc: 0.8196, Grad norm: 0.08597211447482367\n",
      "Iteration 2122, BCE loss: 57.75761525961701, Acc: 0.8196, Grad norm: 0.1177005785461025\n",
      "Iteration 2123, BCE loss: 57.757662825336205, Acc: 0.8196, Grad norm: 0.1224943025706914\n",
      "Iteration 2124, BCE loss: 57.75757461320316, Acc: 0.8195, Grad norm: 0.11165246634165747\n",
      "Iteration 2125, BCE loss: 57.75751537286993, Acc: 0.8195, Grad norm: 0.09898956721433361\n",
      "Iteration 2126, BCE loss: 57.75763618441157, Acc: 0.8195, Grad norm: 0.11425307206575441\n",
      "Iteration 2127, BCE loss: 57.75803394904899, Acc: 0.8195, Grad norm: 0.18050257438214667\n",
      "Iteration 2128, BCE loss: 57.758222305762445, Acc: 0.8196, Grad norm: 0.20178979674067304\n",
      "Iteration 2129, BCE loss: 57.75826842625199, Acc: 0.8196, Grad norm: 0.20672314217129464\n",
      "Iteration 2130, BCE loss: 57.75786987943033, Acc: 0.8196, Grad norm: 0.15808439236295266\n",
      "Iteration 2131, BCE loss: 57.75818246292812, Acc: 0.8196, Grad norm: 0.20117897827841702\n",
      "Iteration 2132, BCE loss: 57.75804522925979, Acc: 0.8196, Grad norm: 0.18102089991519676\n",
      "Iteration 2133, BCE loss: 57.758679207275044, Acc: 0.8196, Grad norm: 0.24442196667626748\n",
      "Iteration 2134, BCE loss: 57.75785133243018, Acc: 0.8196, Grad norm: 0.15275394399401665\n",
      "Iteration 2135, BCE loss: 57.75807397072535, Acc: 0.8197, Grad norm: 0.18336606648163437\n",
      "Iteration 2136, BCE loss: 57.757629486381205, Acc: 0.8196, Grad norm: 0.11710930882055759\n",
      "Iteration 2137, BCE loss: 57.75763888705875, Acc: 0.8196, Grad norm: 0.11768657234664177\n",
      "Iteration 2138, BCE loss: 57.75752844566772, Acc: 0.8196, Grad norm: 0.10057885737442798\n",
      "Iteration 2139, BCE loss: 57.75756063452915, Acc: 0.8196, Grad norm: 0.1073923770194461\n",
      "Iteration 2140, BCE loss: 57.75763281879599, Acc: 0.8196, Grad norm: 0.1125785885507296\n",
      "Iteration 2141, BCE loss: 57.757662983132846, Acc: 0.8195, Grad norm: 0.12590445510204576\n",
      "Iteration 2142, BCE loss: 57.757824379474556, Acc: 0.8195, Grad norm: 0.15689773906417714\n",
      "Iteration 2143, BCE loss: 57.75744926072952, Acc: 0.8196, Grad norm: 0.09066165846247508\n",
      "Iteration 2144, BCE loss: 57.757536964996014, Acc: 0.8195, Grad norm: 0.10560316672489875\n",
      "Iteration 2145, BCE loss: 57.75749718932457, Acc: 0.8195, Grad norm: 0.09705842378396289\n",
      "Iteration 2146, BCE loss: 57.75761396768406, Acc: 0.8195, Grad norm: 0.11477318410081587\n",
      "Iteration 2147, BCE loss: 57.75766600795589, Acc: 0.8195, Grad norm: 0.12200202941016448\n",
      "Iteration 2148, BCE loss: 57.75761216486667, Acc: 0.8196, Grad norm: 0.1167735789354393\n",
      "Iteration 2149, BCE loss: 57.757795084805224, Acc: 0.8196, Grad norm: 0.15374868525236823\n",
      "Iteration 2150, BCE loss: 57.75806471572875, Acc: 0.8196, Grad norm: 0.19289269806895434\n",
      "Iteration 2151, BCE loss: 57.75822900850626, Acc: 0.8195, Grad norm: 0.210995276410033\n",
      "Iteration 2152, BCE loss: 57.757654930484705, Acc: 0.8195, Grad norm: 0.12727301648704475\n",
      "Iteration 2153, BCE loss: 57.75764309337559, Acc: 0.8195, Grad norm: 0.12380739976033019\n",
      "Iteration 2154, BCE loss: 57.75758927250397, Acc: 0.8195, Grad norm: 0.11092312442446836\n",
      "Iteration 2155, BCE loss: 57.7578000962113, Acc: 0.8195, Grad norm: 0.14538970363101195\n",
      "Iteration 2156, BCE loss: 57.757882712604186, Acc: 0.8196, Grad norm: 0.15752385111647388\n",
      "Iteration 2157, BCE loss: 57.75775204418142, Acc: 0.8196, Grad norm: 0.14281539728666878\n",
      "Iteration 2158, BCE loss: 57.757861504853054, Acc: 0.8197, Grad norm: 0.15937903047437427\n",
      "Iteration 2159, BCE loss: 57.75798046414717, Acc: 0.8197, Grad norm: 0.17044855772724338\n",
      "Iteration 2160, BCE loss: 57.75795021579292, Acc: 0.8196, Grad norm: 0.16263380942304215\n",
      "Iteration 2161, BCE loss: 57.75782496182787, Acc: 0.8196, Grad norm: 0.138240658484747\n",
      "Iteration 2162, BCE loss: 57.75780684285509, Acc: 0.8196, Grad norm: 0.13605670368637182\n",
      "Iteration 2163, BCE loss: 57.75824242787452, Acc: 0.8197, Grad norm: 0.2015311960764028\n",
      "Iteration 2164, BCE loss: 57.75802616260614, Acc: 0.8197, Grad norm: 0.17472420918065795\n",
      "Iteration 2165, BCE loss: 57.75765026240957, Acc: 0.8196, Grad norm: 0.12331812783630412\n",
      "Iteration 2166, BCE loss: 57.75779208217613, Acc: 0.8197, Grad norm: 0.1442325460129038\n",
      "Iteration 2167, BCE loss: 57.7578986908686, Acc: 0.8197, Grad norm: 0.15225363883871573\n",
      "Iteration 2168, BCE loss: 57.75806829930301, Acc: 0.8197, Grad norm: 0.1791964489587754\n",
      "Iteration 2169, BCE loss: 57.758025968895026, Acc: 0.8197, Grad norm: 0.16764836511026632\n",
      "Iteration 2170, BCE loss: 57.75760992324891, Acc: 0.8196, Grad norm: 0.11281654431560656\n",
      "Iteration 2171, BCE loss: 57.75771868563241, Acc: 0.8197, Grad norm: 0.1322907533748431\n",
      "Iteration 2172, BCE loss: 57.757994412540825, Acc: 0.8197, Grad norm: 0.17553151194954694\n",
      "Iteration 2173, BCE loss: 57.75831710551369, Acc: 0.8197, Grad norm: 0.21544534376993404\n",
      "Iteration 2174, BCE loss: 57.75822150602029, Acc: 0.8197, Grad norm: 0.20925907002264874\n",
      "Iteration 2175, BCE loss: 57.75870544271801, Acc: 0.8197, Grad norm: 0.2552654013229472\n",
      "Iteration 2176, BCE loss: 57.75922130282328, Acc: 0.8197, Grad norm: 0.2969338527545649\n",
      "Iteration 2177, BCE loss: 57.759139338800445, Acc: 0.8196, Grad norm: 0.2866256920481994\n",
      "Iteration 2178, BCE loss: 57.75947722426328, Acc: 0.8197, Grad norm: 0.3160742450746249\n",
      "Iteration 2179, BCE loss: 57.75950176867805, Acc: 0.8197, Grad norm: 0.3175313691137034\n",
      "Iteration 2180, BCE loss: 57.759383328006926, Acc: 0.8197, Grad norm: 0.30932694742874545\n",
      "Iteration 2181, BCE loss: 57.758954066913276, Acc: 0.8196, Grad norm: 0.2755390662291605\n",
      "Iteration 2182, BCE loss: 57.75945153089452, Acc: 0.8197, Grad norm: 0.31350282746927977\n",
      "Iteration 2183, BCE loss: 57.759484362372454, Acc: 0.8197, Grad norm: 0.31372837669407844\n",
      "Iteration 2184, BCE loss: 57.75982081505398, Acc: 0.8197, Grad norm: 0.337598071133078\n",
      "Iteration 2185, BCE loss: 57.75973669856716, Acc: 0.8196, Grad norm: 0.33575952301862283\n",
      "Iteration 2186, BCE loss: 57.75875884417361, Acc: 0.8196, Grad norm: 0.2522093361340377\n",
      "Iteration 2187, BCE loss: 57.758037090995614, Acc: 0.8196, Grad norm: 0.17707990257471276\n",
      "Iteration 2188, BCE loss: 57.75843965699652, Acc: 0.8197, Grad norm: 0.2266174395036196\n",
      "Iteration 2189, BCE loss: 57.75799180973399, Acc: 0.8197, Grad norm: 0.16605032214933538\n",
      "Iteration 2190, BCE loss: 57.7578206442319, Acc: 0.8197, Grad norm: 0.1451584454969082\n",
      "Iteration 2191, BCE loss: 57.75800548579801, Acc: 0.8197, Grad norm: 0.16390319232040085\n",
      "Iteration 2192, BCE loss: 57.757663825752594, Acc: 0.8196, Grad norm: 0.11808382711350705\n",
      "Iteration 2193, BCE loss: 57.757806756280615, Acc: 0.8196, Grad norm: 0.14112529360785836\n",
      "Iteration 2194, BCE loss: 57.757885533815184, Acc: 0.8196, Grad norm: 0.15983933388071983\n",
      "Iteration 2195, BCE loss: 57.75775702162278, Acc: 0.8197, Grad norm: 0.14115694428970252\n",
      "Iteration 2196, BCE loss: 57.75800000697629, Acc: 0.8197, Grad norm: 0.17025788679575904\n",
      "Iteration 2197, BCE loss: 57.75766663164711, Acc: 0.8196, Grad norm: 0.11604761578891848\n",
      "Iteration 2198, BCE loss: 57.7575944794805, Acc: 0.8196, Grad norm: 0.10573234287243095\n",
      "Iteration 2199, BCE loss: 57.75766051697346, Acc: 0.8195, Grad norm: 0.12051788815207351\n",
      "Iteration 2200, BCE loss: 57.75774021427296, Acc: 0.8195, Grad norm: 0.14108582741468717\n",
      "Iteration 2201, BCE loss: 57.75752773227961, Acc: 0.8195, Grad norm: 0.09664992018870898\n",
      "Iteration 2202, BCE loss: 57.75801271007137, Acc: 0.8195, Grad norm: 0.16931872043677987\n",
      "Iteration 2203, BCE loss: 57.75764923892109, Acc: 0.8195, Grad norm: 0.12239263517989803\n",
      "Iteration 2204, BCE loss: 57.75792692781576, Acc: 0.8194, Grad norm: 0.1596877450284859\n",
      "Iteration 2205, BCE loss: 57.757997763558194, Acc: 0.8195, Grad norm: 0.16951071925407085\n",
      "Iteration 2206, BCE loss: 57.758029039874685, Acc: 0.8195, Grad norm: 0.1631401556731827\n",
      "Iteration 2207, BCE loss: 57.75776899341906, Acc: 0.8195, Grad norm: 0.13739432578930633\n",
      "Iteration 2208, BCE loss: 57.75816320229231, Acc: 0.8195, Grad norm: 0.191116163999485\n",
      "Iteration 2209, BCE loss: 57.75846256059171, Acc: 0.8195, Grad norm: 0.222528438059485\n",
      "Iteration 2210, BCE loss: 57.758287771288195, Acc: 0.8196, Grad norm: 0.2089933057195426\n",
      "Iteration 2211, BCE loss: 57.7576609169557, Acc: 0.8195, Grad norm: 0.123275393293806\n",
      "Iteration 2212, BCE loss: 57.75765414462207, Acc: 0.8194, Grad norm: 0.1204941449919748\n",
      "Iteration 2213, BCE loss: 57.75756963782541, Acc: 0.8195, Grad norm: 0.10202665046539966\n",
      "Iteration 2214, BCE loss: 57.75763107414718, Acc: 0.8195, Grad norm: 0.11519762909274228\n",
      "Iteration 2215, BCE loss: 57.75769826818029, Acc: 0.8195, Grad norm: 0.1301352410687396\n",
      "Iteration 2216, BCE loss: 57.75783123799464, Acc: 0.8196, Grad norm: 0.15708287425031517\n",
      "Iteration 2217, BCE loss: 57.75783649138229, Acc: 0.8196, Grad norm: 0.15634772882542006\n",
      "Iteration 2218, BCE loss: 57.75826562153577, Acc: 0.8196, Grad norm: 0.20224664025848624\n",
      "Iteration 2219, BCE loss: 57.75773868632879, Acc: 0.8195, Grad norm: 0.14052049223025206\n",
      "Iteration 2220, BCE loss: 57.75813554509606, Acc: 0.8196, Grad norm: 0.19532541786139573\n",
      "Iteration 2221, BCE loss: 57.757962173234375, Acc: 0.8196, Grad norm: 0.17337195345117437\n",
      "Iteration 2222, BCE loss: 57.75800101659429, Acc: 0.8196, Grad norm: 0.18003567878488388\n",
      "Iteration 2223, BCE loss: 57.758461188204926, Acc: 0.8196, Grad norm: 0.22894262614963543\n",
      "Iteration 2224, BCE loss: 57.75853366218983, Acc: 0.8196, Grad norm: 0.2381032063033627\n",
      "Iteration 2225, BCE loss: 57.757940515117284, Acc: 0.8196, Grad norm: 0.17140153416464635\n",
      "Iteration 2226, BCE loss: 57.75796322820625, Acc: 0.8196, Grad norm: 0.17127215230585904\n",
      "Iteration 2227, BCE loss: 57.75774124951239, Acc: 0.8196, Grad norm: 0.13459777639159148\n",
      "Iteration 2228, BCE loss: 57.758029652197436, Acc: 0.8196, Grad norm: 0.1738261799480737\n",
      "Iteration 2229, BCE loss: 57.758163609113524, Acc: 0.8196, Grad norm: 0.19130592667511004\n",
      "Iteration 2230, BCE loss: 57.75828326849525, Acc: 0.8196, Grad norm: 0.20695579310024192\n",
      "Iteration 2231, BCE loss: 57.75806830294424, Acc: 0.8196, Grad norm: 0.18851941565704552\n",
      "Iteration 2232, BCE loss: 57.75839641155184, Acc: 0.8196, Grad norm: 0.21933371273579388\n",
      "Iteration 2233, BCE loss: 57.75803346933303, Acc: 0.8196, Grad norm: 0.17720980779169798\n",
      "Iteration 2234, BCE loss: 57.75783302890523, Acc: 0.8196, Grad norm: 0.1543893626271253\n",
      "Iteration 2235, BCE loss: 57.7578077409218, Acc: 0.8196, Grad norm: 0.14500129288932925\n",
      "Iteration 2236, BCE loss: 57.75774564863877, Acc: 0.8196, Grad norm: 0.1358175868548292\n",
      "Iteration 2237, BCE loss: 57.7574976535007, Acc: 0.8196, Grad norm: 0.09473630769562609\n",
      "Iteration 2238, BCE loss: 57.757847790507654, Acc: 0.8195, Grad norm: 0.15068488470176675\n",
      "Iteration 2239, BCE loss: 57.75771641203086, Acc: 0.8195, Grad norm: 0.13140175200370965\n",
      "Iteration 2240, BCE loss: 57.75754423811227, Acc: 0.8195, Grad norm: 0.1027770568883112\n",
      "Iteration 2241, BCE loss: 57.75790015507053, Acc: 0.8194, Grad norm: 0.16519997565478267\n",
      "Iteration 2242, BCE loss: 57.758136476121514, Acc: 0.8195, Grad norm: 0.1939133852369355\n",
      "Iteration 2243, BCE loss: 57.75845684893682, Acc: 0.8195, Grad norm: 0.229088653068532\n",
      "Iteration 2244, BCE loss: 57.7585582519998, Acc: 0.8194, Grad norm: 0.24022226064838229\n",
      "Iteration 2245, BCE loss: 57.75870389610142, Acc: 0.8194, Grad norm: 0.2608418655528095\n",
      "Iteration 2246, BCE loss: 57.75834868381422, Acc: 0.8195, Grad norm: 0.22036128844940542\n",
      "Iteration 2247, BCE loss: 57.758257940003475, Acc: 0.8195, Grad norm: 0.21290043015740984\n",
      "Iteration 2248, BCE loss: 57.758269447030415, Acc: 0.8195, Grad norm: 0.21584272504347746\n",
      "Iteration 2249, BCE loss: 57.757829409416516, Acc: 0.8196, Grad norm: 0.15067995205872\n",
      "Iteration 2250, BCE loss: 57.75761611305916, Acc: 0.8196, Grad norm: 0.11764568981681173\n",
      "Iteration 2251, BCE loss: 57.757692952930654, Acc: 0.8196, Grad norm: 0.1208795934834821\n",
      "Iteration 2252, BCE loss: 57.75758925554301, Acc: 0.8196, Grad norm: 0.10495797921785499\n",
      "Iteration 2253, BCE loss: 57.75744559792217, Acc: 0.8196, Grad norm: 0.07756445266242293\n",
      "Iteration 2254, BCE loss: 57.757459737092795, Acc: 0.8196, Grad norm: 0.08770302259705957\n",
      "Iteration 2255, BCE loss: 57.75746394618333, Acc: 0.8195, Grad norm: 0.08762843423932691\n",
      "Iteration 2256, BCE loss: 57.757468483942674, Acc: 0.8195, Grad norm: 0.09472592312503812\n",
      "Iteration 2257, BCE loss: 57.75746308415906, Acc: 0.8195, Grad norm: 0.08976375588218766\n",
      "Iteration 2258, BCE loss: 57.757469298681144, Acc: 0.8196, Grad norm: 0.09234109315735561\n",
      "Iteration 2259, BCE loss: 57.75755546730235, Acc: 0.8196, Grad norm: 0.10701740473994814\n",
      "Iteration 2260, BCE loss: 57.75778270774885, Acc: 0.8196, Grad norm: 0.14650285979040775\n",
      "Iteration 2261, BCE loss: 57.757631621931196, Acc: 0.8196, Grad norm: 0.11845016595308505\n",
      "Iteration 2262, BCE loss: 57.75774293428141, Acc: 0.8196, Grad norm: 0.1399164114448179\n",
      "Iteration 2263, BCE loss: 57.75748930380555, Acc: 0.8196, Grad norm: 0.09329717884353686\n",
      "Iteration 2264, BCE loss: 57.757450665586184, Acc: 0.8196, Grad norm: 0.08398272807314751\n",
      "Iteration 2265, BCE loss: 57.757671276941224, Acc: 0.8195, Grad norm: 0.13107060430733156\n",
      "Iteration 2266, BCE loss: 57.75814399864062, Acc: 0.8196, Grad norm: 0.18569052609336462\n",
      "Iteration 2267, BCE loss: 57.75826354982204, Acc: 0.8195, Grad norm: 0.19825089245879932\n",
      "Iteration 2268, BCE loss: 57.758105697792466, Acc: 0.8195, Grad norm: 0.17733576867680453\n",
      "Iteration 2269, BCE loss: 57.757864189691766, Acc: 0.8195, Grad norm: 0.14859483579668395\n",
      "Iteration 2270, BCE loss: 57.75805845291856, Acc: 0.8196, Grad norm: 0.17465761997306972\n",
      "Iteration 2271, BCE loss: 57.75775146317114, Acc: 0.8196, Grad norm: 0.13482645344292696\n",
      "Iteration 2272, BCE loss: 57.75801636504103, Acc: 0.8196, Grad norm: 0.17292987442798954\n",
      "Iteration 2273, BCE loss: 57.75808646176729, Acc: 0.8196, Grad norm: 0.18364638688273496\n",
      "Iteration 2274, BCE loss: 57.757975026441514, Acc: 0.8197, Grad norm: 0.17383056864978352\n",
      "Iteration 2275, BCE loss: 57.75812888185545, Acc: 0.8197, Grad norm: 0.19169542399318126\n",
      "Iteration 2276, BCE loss: 57.758786176348295, Acc: 0.8197, Grad norm: 0.2612286726414004\n",
      "Iteration 2277, BCE loss: 57.75841006103199, Acc: 0.8197, Grad norm: 0.22416185379419515\n",
      "Iteration 2278, BCE loss: 57.75832457777076, Acc: 0.8197, Grad norm: 0.21316251703644568\n",
      "Iteration 2279, BCE loss: 57.7578483582151, Acc: 0.8196, Grad norm: 0.15216043604065144\n",
      "Iteration 2280, BCE loss: 57.75792812337191, Acc: 0.8197, Grad norm: 0.16692439692050356\n",
      "Iteration 2281, BCE loss: 57.75814000847568, Acc: 0.8197, Grad norm: 0.19381407055744723\n",
      "Iteration 2282, BCE loss: 57.75911032750417, Acc: 0.8198, Grad norm: 0.29121666914708666\n",
      "Iteration 2283, BCE loss: 57.758961149203905, Acc: 0.8198, Grad norm: 0.2794762966815109\n",
      "Iteration 2284, BCE loss: 57.75923462608584, Acc: 0.8198, Grad norm: 0.3017808487845417\n",
      "Iteration 2285, BCE loss: 57.75862973355528, Acc: 0.8197, Grad norm: 0.24456616931096545\n",
      "Iteration 2286, BCE loss: 57.757994252936996, Acc: 0.8197, Grad norm: 0.17479310172963528\n",
      "Iteration 2287, BCE loss: 57.75853904113128, Acc: 0.8197, Grad norm: 0.23895486796491772\n",
      "Iteration 2288, BCE loss: 57.75881901537889, Acc: 0.8197, Grad norm: 0.26871788162914895\n",
      "Iteration 2289, BCE loss: 57.758703724178666, Acc: 0.8197, Grad norm: 0.25928011695094494\n",
      "Iteration 2290, BCE loss: 57.758831755620754, Acc: 0.8197, Grad norm: 0.26661122123575315\n",
      "Iteration 2291, BCE loss: 57.75881581790088, Acc: 0.8197, Grad norm: 0.26338028887734205\n",
      "Iteration 2292, BCE loss: 57.758107703638316, Acc: 0.8197, Grad norm: 0.19150767121014786\n",
      "Iteration 2293, BCE loss: 57.75783800100362, Acc: 0.8197, Grad norm: 0.15346509555779536\n",
      "Iteration 2294, BCE loss: 57.758165467473155, Acc: 0.8197, Grad norm: 0.19166887119845308\n",
      "Iteration 2295, BCE loss: 57.757741286760975, Acc: 0.8197, Grad norm: 0.13672656555657822\n",
      "Iteration 2296, BCE loss: 57.75781778599442, Acc: 0.8197, Grad norm: 0.1517296730439244\n",
      "Iteration 2297, BCE loss: 57.757785522433785, Acc: 0.8196, Grad norm: 0.1513620331344396\n",
      "Iteration 2298, BCE loss: 57.7575779094371, Acc: 0.8197, Grad norm: 0.1121468061341185\n",
      "Iteration 2299, BCE loss: 57.75754398763222, Acc: 0.8196, Grad norm: 0.11029088605710798\n",
      "Iteration 2300, BCE loss: 57.757580045376216, Acc: 0.8197, Grad norm: 0.11428988500531467\n",
      "Iteration 2301, BCE loss: 57.757732521215345, Acc: 0.8197, Grad norm: 0.1410414917781164\n",
      "Iteration 2302, BCE loss: 57.75771343867808, Acc: 0.8196, Grad norm: 0.13862632956399049\n",
      "Iteration 2303, BCE loss: 57.75769654110458, Acc: 0.8196, Grad norm: 0.13221516395553956\n",
      "Iteration 2304, BCE loss: 57.757824035945035, Acc: 0.8196, Grad norm: 0.15097814662510892\n",
      "Iteration 2305, BCE loss: 57.758051085625354, Acc: 0.8197, Grad norm: 0.17595984827037073\n",
      "Iteration 2306, BCE loss: 57.757657399372434, Acc: 0.8197, Grad norm: 0.1203948849801475\n",
      "Iteration 2307, BCE loss: 57.75774744492634, Acc: 0.8197, Grad norm: 0.13465200756428397\n",
      "Iteration 2308, BCE loss: 57.75771572808392, Acc: 0.8197, Grad norm: 0.12962596449517425\n",
      "Iteration 2309, BCE loss: 57.757846861175246, Acc: 0.8197, Grad norm: 0.14316702705125475\n",
      "Iteration 2310, BCE loss: 57.757773559186205, Acc: 0.8197, Grad norm: 0.1370585161556943\n",
      "Iteration 2311, BCE loss: 57.75781351224447, Acc: 0.8197, Grad norm: 0.14429686584304838\n",
      "Iteration 2312, BCE loss: 57.758031188046175, Acc: 0.8196, Grad norm: 0.17605307602036058\n",
      "Iteration 2313, BCE loss: 57.757772815504964, Acc: 0.8196, Grad norm: 0.14464006951181183\n",
      "Iteration 2314, BCE loss: 57.75798158692153, Acc: 0.8196, Grad norm: 0.17261276926920396\n",
      "Iteration 2315, BCE loss: 57.75831587032363, Acc: 0.8197, Grad norm: 0.20367498790121055\n",
      "Iteration 2316, BCE loss: 57.758169021480995, Acc: 0.8197, Grad norm: 0.19297960417682686\n",
      "Iteration 2317, BCE loss: 57.75811740195637, Acc: 0.8196, Grad norm: 0.1915268338876211\n",
      "Iteration 2318, BCE loss: 57.75880282360802, Acc: 0.8196, Grad norm: 0.2595131649352646\n",
      "Iteration 2319, BCE loss: 57.75884305414594, Acc: 0.8196, Grad norm: 0.26476287585178565\n",
      "Iteration 2320, BCE loss: 57.75810749499327, Acc: 0.8196, Grad norm: 0.19018238443512608\n",
      "Iteration 2321, BCE loss: 57.758281194363065, Acc: 0.8196, Grad norm: 0.20541391266655534\n",
      "Iteration 2322, BCE loss: 57.758425149065914, Acc: 0.8197, Grad norm: 0.22023194203258492\n",
      "Iteration 2323, BCE loss: 57.75816543612946, Acc: 0.8196, Grad norm: 0.18750537129222464\n",
      "Iteration 2324, BCE loss: 57.758348713983736, Acc: 0.8196, Grad norm: 0.20736840105159665\n",
      "Iteration 2325, BCE loss: 57.75855575333716, Acc: 0.8195, Grad norm: 0.2330125212738537\n",
      "Iteration 2326, BCE loss: 57.75791131973344, Acc: 0.8196, Grad norm: 0.16096370722591408\n",
      "Iteration 2327, BCE loss: 57.75768205542764, Acc: 0.8196, Grad norm: 0.12529651986770676\n",
      "Iteration 2328, BCE loss: 57.75768752171371, Acc: 0.8196, Grad norm: 0.1285376024372159\n",
      "Iteration 2329, BCE loss: 57.75776810085229, Acc: 0.8196, Grad norm: 0.1398022169990869\n",
      "Iteration 2330, BCE loss: 57.75779604457078, Acc: 0.8196, Grad norm: 0.14730702187584555\n",
      "Iteration 2331, BCE loss: 57.757744813123466, Acc: 0.8196, Grad norm: 0.14039909388155183\n",
      "Iteration 2332, BCE loss: 57.757709702720945, Acc: 0.8196, Grad norm: 0.128248422434375\n",
      "Iteration 2333, BCE loss: 57.757809823927545, Acc: 0.8197, Grad norm: 0.14928849552508436\n",
      "Iteration 2334, BCE loss: 57.75774987265777, Acc: 0.8197, Grad norm: 0.1438957341706795\n",
      "Iteration 2335, BCE loss: 57.7576684466759, Acc: 0.8197, Grad norm: 0.12537800580713257\n",
      "Iteration 2336, BCE loss: 57.75759976679558, Acc: 0.8196, Grad norm: 0.10844495885189304\n",
      "Iteration 2337, BCE loss: 57.75761201411522, Acc: 0.8197, Grad norm: 0.11534117759514935\n",
      "Iteration 2338, BCE loss: 57.75754355204505, Acc: 0.8197, Grad norm: 0.09855677798193646\n",
      "Iteration 2339, BCE loss: 57.75759269454994, Acc: 0.8196, Grad norm: 0.1120369117829642\n",
      "Iteration 2340, BCE loss: 57.75750896610748, Acc: 0.8196, Grad norm: 0.09601570579627022\n",
      "Iteration 2341, BCE loss: 57.758008459210686, Acc: 0.8196, Grad norm: 0.17034277506467266\n",
      "Iteration 2342, BCE loss: 57.75783931736088, Acc: 0.8196, Grad norm: 0.1479922311326271\n",
      "Iteration 2343, BCE loss: 57.758008678062076, Acc: 0.8197, Grad norm: 0.16801845418850214\n",
      "Iteration 2344, BCE loss: 57.75796206957237, Acc: 0.8196, Grad norm: 0.16458758590601846\n",
      "Iteration 2345, BCE loss: 57.7578644329665, Acc: 0.8196, Grad norm: 0.15450064812563818\n",
      "Iteration 2346, BCE loss: 57.758367158550534, Acc: 0.8196, Grad norm: 0.2166418585693973\n",
      "Iteration 2347, BCE loss: 57.75800574753437, Acc: 0.8196, Grad norm: 0.1755138529426847\n",
      "Iteration 2348, BCE loss: 57.75791411532119, Acc: 0.8196, Grad norm: 0.16381319389972998\n",
      "Iteration 2349, BCE loss: 57.75762916872027, Acc: 0.8196, Grad norm: 0.11447296609862206\n",
      "Iteration 2350, BCE loss: 57.757763047043255, Acc: 0.8196, Grad norm: 0.13194287902992827\n",
      "Iteration 2351, BCE loss: 57.75785702095207, Acc: 0.8195, Grad norm: 0.1491881827680154\n",
      "Iteration 2352, BCE loss: 57.75772761996771, Acc: 0.8196, Grad norm: 0.12142756205689823\n",
      "Iteration 2353, BCE loss: 57.75788146812824, Acc: 0.8196, Grad norm: 0.15483992118218798\n",
      "Iteration 2354, BCE loss: 57.7582064819572, Acc: 0.8196, Grad norm: 0.1933306950501275\n",
      "Iteration 2355, BCE loss: 57.75773680283536, Acc: 0.8196, Grad norm: 0.12601223214529428\n",
      "Iteration 2356, BCE loss: 57.757777707801424, Acc: 0.8196, Grad norm: 0.13075106642923456\n",
      "Iteration 2357, BCE loss: 57.75758841296272, Acc: 0.8196, Grad norm: 0.10313805219791443\n",
      "Iteration 2358, BCE loss: 57.75779926564669, Acc: 0.8195, Grad norm: 0.14184869344504691\n",
      "Iteration 2359, BCE loss: 57.757978609390406, Acc: 0.8195, Grad norm: 0.17276854892213037\n",
      "Iteration 2360, BCE loss: 57.75858338664921, Acc: 0.8194, Grad norm: 0.24339025805227985\n",
      "Iteration 2361, BCE loss: 57.75856267747582, Acc: 0.8195, Grad norm: 0.2368304757291355\n",
      "Iteration 2362, BCE loss: 57.7581437697636, Acc: 0.8195, Grad norm: 0.19440395750439135\n",
      "Iteration 2363, BCE loss: 57.75876360257608, Acc: 0.8194, Grad norm: 0.2526451171842166\n",
      "Iteration 2364, BCE loss: 57.7583203117962, Acc: 0.8195, Grad norm: 0.20993528475524764\n",
      "Iteration 2365, BCE loss: 57.75790822946521, Acc: 0.8195, Grad norm: 0.16256293988940262\n",
      "Iteration 2366, BCE loss: 57.75774612284998, Acc: 0.8195, Grad norm: 0.13735700931912126\n",
      "Iteration 2367, BCE loss: 57.75769147398938, Acc: 0.8195, Grad norm: 0.13441427773655762\n",
      "Iteration 2368, BCE loss: 57.757737788572186, Acc: 0.8195, Grad norm: 0.13446555328029217\n",
      "Iteration 2369, BCE loss: 57.75772276776214, Acc: 0.8196, Grad norm: 0.13031173298186488\n",
      "Iteration 2370, BCE loss: 57.75757955651942, Acc: 0.8196, Grad norm: 0.10098476192065917\n",
      "Iteration 2371, BCE loss: 57.757613977169626, Acc: 0.8196, Grad norm: 0.10878079816114622\n",
      "Iteration 2372, BCE loss: 57.75757331023202, Acc: 0.8196, Grad norm: 0.10196394185247512\n",
      "Iteration 2373, BCE loss: 57.75758491172731, Acc: 0.8196, Grad norm: 0.10220428555481169\n",
      "Iteration 2374, BCE loss: 57.757683164788475, Acc: 0.8196, Grad norm: 0.12654396061246015\n",
      "Iteration 2375, BCE loss: 57.757531632683296, Acc: 0.8196, Grad norm: 0.10134747831482777\n",
      "Iteration 2376, BCE loss: 57.757699116959856, Acc: 0.8196, Grad norm: 0.13675693391964916\n",
      "Iteration 2377, BCE loss: 57.75759778485441, Acc: 0.8195, Grad norm: 0.11450383115853986\n",
      "Iteration 2378, BCE loss: 57.75770236165397, Acc: 0.8195, Grad norm: 0.1280066899205937\n",
      "Iteration 2379, BCE loss: 57.757742491351266, Acc: 0.8195, Grad norm: 0.13641855051917695\n",
      "Iteration 2380, BCE loss: 57.757607498121104, Acc: 0.8195, Grad norm: 0.11371540876792453\n",
      "Iteration 2381, BCE loss: 57.7576152340041, Acc: 0.8196, Grad norm: 0.11032891347496791\n",
      "Iteration 2382, BCE loss: 57.75755974920392, Acc: 0.8196, Grad norm: 0.10220113924277051\n",
      "Iteration 2383, BCE loss: 57.75796389934848, Acc: 0.8196, Grad norm: 0.1627143480694615\n",
      "Iteration 2384, BCE loss: 57.757828031644465, Acc: 0.8196, Grad norm: 0.14315277866177403\n",
      "Iteration 2385, BCE loss: 57.758040808798, Acc: 0.8196, Grad norm: 0.17355290759836758\n",
      "Iteration 2386, BCE loss: 57.7585278557856, Acc: 0.8196, Grad norm: 0.2323960419848268\n",
      "Iteration 2387, BCE loss: 57.758661827035205, Acc: 0.8196, Grad norm: 0.2478273220784709\n",
      "Iteration 2388, BCE loss: 57.75825629240697, Acc: 0.8196, Grad norm: 0.20893139670835364\n",
      "Iteration 2389, BCE loss: 57.75790569555759, Acc: 0.8196, Grad norm: 0.16817317455899278\n",
      "Iteration 2390, BCE loss: 57.75757532716346, Acc: 0.8196, Grad norm: 0.10909673519356924\n",
      "Iteration 2391, BCE loss: 57.75758851672141, Acc: 0.8195, Grad norm: 0.11451733402541531\n",
      "Iteration 2392, BCE loss: 57.7574254171183, Acc: 0.8196, Grad norm: 0.07852291099496643\n",
      "Iteration 2393, BCE loss: 57.75748415151179, Acc: 0.8196, Grad norm: 0.09498688804345543\n",
      "Iteration 2394, BCE loss: 57.75758703943405, Acc: 0.8196, Grad norm: 0.11744111875263194\n",
      "Iteration 2395, BCE loss: 57.7577968397994, Acc: 0.8195, Grad norm: 0.15282119262924918\n",
      "Iteration 2396, BCE loss: 57.75779016564887, Acc: 0.8196, Grad norm: 0.15258874685635793\n",
      "Iteration 2397, BCE loss: 57.757513286571445, Acc: 0.8196, Grad norm: 0.09984515002391373\n",
      "Iteration 2398, BCE loss: 57.75750298409163, Acc: 0.8196, Grad norm: 0.09508993474758255\n",
      "Iteration 2399, BCE loss: 57.75744740194467, Acc: 0.8196, Grad norm: 0.0840099903427931\n",
      "Iteration 2400, BCE loss: 57.75734301598955, Acc: 0.8196, Grad norm: 0.05287071209518819\n",
      "Iteration 2401, BCE loss: 57.75753445651539, Acc: 0.8197, Grad norm: 0.1046410086493408\n",
      "Iteration 2402, BCE loss: 57.757438965190865, Acc: 0.8196, Grad norm: 0.08811321866087177\n",
      "Iteration 2403, BCE loss: 57.757420314016045, Acc: 0.8196, Grad norm: 0.08156818460669008\n",
      "Iteration 2404, BCE loss: 57.757751591263485, Acc: 0.8196, Grad norm: 0.14785046082284165\n",
      "Iteration 2405, BCE loss: 57.75769660953284, Acc: 0.8196, Grad norm: 0.13996574357944316\n",
      "Iteration 2406, BCE loss: 57.75760239118604, Acc: 0.8196, Grad norm: 0.11788252632809476\n",
      "Iteration 2407, BCE loss: 57.75741717208837, Acc: 0.8196, Grad norm: 0.07258415755440162\n",
      "Iteration 2408, BCE loss: 57.75762121465018, Acc: 0.8197, Grad norm: 0.11891872764585666\n",
      "Iteration 2409, BCE loss: 57.757692962372374, Acc: 0.8197, Grad norm: 0.13463172001328805\n",
      "Iteration 2410, BCE loss: 57.75774846960145, Acc: 0.8197, Grad norm: 0.14324970790174132\n",
      "Iteration 2411, BCE loss: 57.75786112244823, Acc: 0.8197, Grad norm: 0.15809787738039055\n",
      "Iteration 2412, BCE loss: 57.75785889592041, Acc: 0.8197, Grad norm: 0.16045251772712496\n",
      "Iteration 2413, BCE loss: 57.75821684470307, Acc: 0.8197, Grad norm: 0.2045072015207396\n",
      "Iteration 2414, BCE loss: 57.758084443138785, Acc: 0.8196, Grad norm: 0.18892471613258927\n",
      "Iteration 2415, BCE loss: 57.75788338234949, Acc: 0.8196, Grad norm: 0.1656386132401056\n",
      "Iteration 2416, BCE loss: 57.757785125373054, Acc: 0.8195, Grad norm: 0.14503225531028632\n",
      "Iteration 2417, BCE loss: 57.75781275273179, Acc: 0.8195, Grad norm: 0.150584115127151\n",
      "Iteration 2418, BCE loss: 57.757965721168276, Acc: 0.8196, Grad norm: 0.1686349098046837\n",
      "Iteration 2419, BCE loss: 57.75813477414215, Acc: 0.8195, Grad norm: 0.19216585205855366\n",
      "Iteration 2420, BCE loss: 57.75797804921507, Acc: 0.8195, Grad norm: 0.16889829429402642\n",
      "Iteration 2421, BCE loss: 57.75789065022974, Acc: 0.8196, Grad norm: 0.15765984230099658\n",
      "Iteration 2422, BCE loss: 57.75763395859611, Acc: 0.8196, Grad norm: 0.11456687438300048\n",
      "Iteration 2423, BCE loss: 57.75762275906996, Acc: 0.8196, Grad norm: 0.10503896117108774\n",
      "Iteration 2424, BCE loss: 57.75775110795233, Acc: 0.8195, Grad norm: 0.12527763061830383\n",
      "Iteration 2425, BCE loss: 57.75796812065359, Acc: 0.8195, Grad norm: 0.1502048810032858\n",
      "Iteration 2426, BCE loss: 57.7579697329498, Acc: 0.8195, Grad norm: 0.15004544817900806\n",
      "Iteration 2427, BCE loss: 57.75785239115635, Acc: 0.8195, Grad norm: 0.13448003524356908\n",
      "Iteration 2428, BCE loss: 57.75764902215356, Acc: 0.8195, Grad norm: 0.11148409671439652\n",
      "Iteration 2429, BCE loss: 57.757668692278855, Acc: 0.8196, Grad norm: 0.11706850247968571\n",
      "Iteration 2430, BCE loss: 57.75780261925763, Acc: 0.8196, Grad norm: 0.13173986592135342\n",
      "Iteration 2431, BCE loss: 57.75776009144762, Acc: 0.8195, Grad norm: 0.12162396988528483\n",
      "Iteration 2432, BCE loss: 57.75770951175438, Acc: 0.8195, Grad norm: 0.11972835299002388\n",
      "Iteration 2433, BCE loss: 57.757690812643155, Acc: 0.8195, Grad norm: 0.11792845233501759\n",
      "Iteration 2434, BCE loss: 57.75767686742038, Acc: 0.8195, Grad norm: 0.12754934318208985\n",
      "Iteration 2435, BCE loss: 57.757733243514004, Acc: 0.8196, Grad norm: 0.13418547754019916\n",
      "Iteration 2436, BCE loss: 57.75759576551174, Acc: 0.8196, Grad norm: 0.11160266500876366\n",
      "Iteration 2437, BCE loss: 57.75765141230556, Acc: 0.8196, Grad norm: 0.12685050442438595\n",
      "Iteration 2438, BCE loss: 57.757494021564725, Acc: 0.8196, Grad norm: 0.0964017377256522\n",
      "Iteration 2439, BCE loss: 57.75745990548407, Acc: 0.8196, Grad norm: 0.08419689412801662\n",
      "Iteration 2440, BCE loss: 57.75752107876785, Acc: 0.8196, Grad norm: 0.09866910317177757\n",
      "Iteration 2441, BCE loss: 57.75764901689049, Acc: 0.8196, Grad norm: 0.1251024036458345\n",
      "Iteration 2442, BCE loss: 57.757693069371825, Acc: 0.8196, Grad norm: 0.1301656037030263\n",
      "Iteration 2443, BCE loss: 57.75773235952085, Acc: 0.8196, Grad norm: 0.13543689975851267\n",
      "Iteration 2444, BCE loss: 57.75775546426871, Acc: 0.8196, Grad norm: 0.13727603414981757\n",
      "Iteration 2445, BCE loss: 57.75791139452593, Acc: 0.8196, Grad norm: 0.16473251206138323\n",
      "Iteration 2446, BCE loss: 57.75762383611284, Acc: 0.8196, Grad norm: 0.12258112726619144\n",
      "Iteration 2447, BCE loss: 57.757489307838725, Acc: 0.8196, Grad norm: 0.08470109680637197\n",
      "Iteration 2448, BCE loss: 57.757504533120624, Acc: 0.8195, Grad norm: 0.09154027695550611\n",
      "Iteration 2449, BCE loss: 57.75742900211708, Acc: 0.8195, Grad norm: 0.07445250073210309\n",
      "Iteration 2450, BCE loss: 57.75740511598289, Acc: 0.8196, Grad norm: 0.07388021968774047\n",
      "Iteration 2451, BCE loss: 57.75740588666398, Acc: 0.8196, Grad norm: 0.06826671400858333\n",
      "Iteration 2452, BCE loss: 57.75740170994376, Acc: 0.8196, Grad norm: 0.06702027525540574\n",
      "Iteration 2453, BCE loss: 57.75738007176652, Acc: 0.8196, Grad norm: 0.06235878865340824\n",
      "Iteration 2454, BCE loss: 57.757319822515804, Acc: 0.8196, Grad norm: 0.04257447492361303\n",
      "Iteration 2455, BCE loss: 57.75739859624453, Acc: 0.8196, Grad norm: 0.06811782757115996\n",
      "Iteration 2456, BCE loss: 57.757523106527884, Acc: 0.8196, Grad norm: 0.08860467187819791\n",
      "Iteration 2457, BCE loss: 57.75761472534684, Acc: 0.8195, Grad norm: 0.10636196923826595\n",
      "Iteration 2458, BCE loss: 57.757879963356245, Acc: 0.8195, Grad norm: 0.14569414158803265\n",
      "Iteration 2459, BCE loss: 57.758051415663545, Acc: 0.8194, Grad norm: 0.17476274066242908\n",
      "Iteration 2460, BCE loss: 57.757871844830206, Acc: 0.8194, Grad norm: 0.15352672482476282\n",
      "Iteration 2461, BCE loss: 57.75774991613888, Acc: 0.8195, Grad norm: 0.1344809733352087\n",
      "Iteration 2462, BCE loss: 57.75768085147547, Acc: 0.8195, Grad norm: 0.12382724938559585\n",
      "Iteration 2463, BCE loss: 57.757678471817414, Acc: 0.8196, Grad norm: 0.12001269443584354\n",
      "Iteration 2464, BCE loss: 57.75789465267333, Acc: 0.8196, Grad norm: 0.15916802465735438\n",
      "Iteration 2465, BCE loss: 57.75774978208759, Acc: 0.8196, Grad norm: 0.14253019149253\n",
      "Iteration 2466, BCE loss: 57.75776846948993, Acc: 0.8197, Grad norm: 0.14414314882832588\n",
      "Iteration 2467, BCE loss: 57.7579460152671, Acc: 0.8197, Grad norm: 0.17084897660641238\n",
      "Iteration 2468, BCE loss: 57.757704752685974, Acc: 0.8197, Grad norm: 0.13180583651902447\n",
      "Iteration 2469, BCE loss: 57.75775554470525, Acc: 0.8197, Grad norm: 0.1392733879060361\n",
      "Iteration 2470, BCE loss: 57.75775834368922, Acc: 0.8197, Grad norm: 0.1386297430228506\n",
      "Iteration 2471, BCE loss: 57.75783840780342, Acc: 0.8197, Grad norm: 0.1509192672909579\n",
      "Iteration 2472, BCE loss: 57.75766394441784, Acc: 0.8197, Grad norm: 0.12361613476514866\n",
      "Iteration 2473, BCE loss: 57.7580074973456, Acc: 0.8198, Grad norm: 0.17345465703730034\n",
      "Iteration 2474, BCE loss: 57.758040909651285, Acc: 0.8198, Grad norm: 0.17993787749429263\n",
      "Iteration 2475, BCE loss: 57.75770939933602, Acc: 0.8197, Grad norm: 0.13557285899797392\n",
      "Iteration 2476, BCE loss: 57.75749211285566, Acc: 0.8196, Grad norm: 0.09701345405614667\n",
      "Iteration 2477, BCE loss: 57.757640750117424, Acc: 0.8197, Grad norm: 0.12403132186752684\n",
      "Iteration 2478, BCE loss: 57.75745053001734, Acc: 0.8197, Grad norm: 0.08341346889371928\n",
      "Iteration 2479, BCE loss: 57.75787870778939, Acc: 0.8197, Grad norm: 0.15898088317632766\n",
      "Iteration 2480, BCE loss: 57.757626582005486, Acc: 0.8197, Grad norm: 0.11153789064368294\n",
      "Iteration 2481, BCE loss: 57.757566388009984, Acc: 0.8197, Grad norm: 0.10381549682523802\n",
      "Iteration 2482, BCE loss: 57.75781091349601, Acc: 0.8197, Grad norm: 0.13805413701000827\n",
      "Iteration 2483, BCE loss: 57.757701820202485, Acc: 0.8197, Grad norm: 0.12810870473210711\n",
      "Iteration 2484, BCE loss: 57.75755144476399, Acc: 0.8197, Grad norm: 0.10737228464402103\n",
      "Iteration 2485, BCE loss: 57.75756638389363, Acc: 0.8196, Grad norm: 0.11333102117836674\n",
      "Iteration 2486, BCE loss: 57.75774986043447, Acc: 0.8197, Grad norm: 0.14429305067724763\n",
      "Iteration 2487, BCE loss: 57.75762117329803, Acc: 0.8197, Grad norm: 0.11407366906284347\n",
      "Iteration 2488, BCE loss: 57.75763859014668, Acc: 0.8197, Grad norm: 0.11859053986379622\n",
      "Iteration 2489, BCE loss: 57.75806206611277, Acc: 0.8198, Grad norm: 0.17917009894860803\n",
      "Iteration 2490, BCE loss: 57.75783313130227, Acc: 0.8197, Grad norm: 0.15378065244521472\n",
      "Iteration 2491, BCE loss: 57.75781690774801, Acc: 0.8197, Grad norm: 0.14780324762762218\n",
      "Iteration 2492, BCE loss: 57.75793205983703, Acc: 0.8198, Grad norm: 0.1685404774719146\n",
      "Iteration 2493, BCE loss: 57.757703586163075, Acc: 0.8197, Grad norm: 0.13502196553439627\n",
      "Iteration 2494, BCE loss: 57.75795005078716, Acc: 0.8197, Grad norm: 0.17809957695316572\n",
      "Iteration 2495, BCE loss: 57.75763085727055, Acc: 0.8197, Grad norm: 0.12444333909248523\n",
      "Iteration 2496, BCE loss: 57.75773325524806, Acc: 0.8197, Grad norm: 0.13911841573203848\n",
      "Iteration 2497, BCE loss: 57.75774823215362, Acc: 0.8197, Grad norm: 0.14729950667147015\n",
      "Iteration 2498, BCE loss: 57.757963410114, Acc: 0.8197, Grad norm: 0.1774939564324591\n",
      "Iteration 2499, BCE loss: 57.758000412441945, Acc: 0.8197, Grad norm: 0.179083298789799\n",
      "Iteration 2500, BCE loss: 57.7578863363014, Acc: 0.8197, Grad norm: 0.16158227186019966\n",
      "Iteration 2501, BCE loss: 57.75781737089923, Acc: 0.8197, Grad norm: 0.15036940565996673\n",
      "Iteration 2502, BCE loss: 57.757850666686615, Acc: 0.8197, Grad norm: 0.1547108892356704\n",
      "Iteration 2503, BCE loss: 57.757603379035984, Acc: 0.8197, Grad norm: 0.11523275430621047\n",
      "Iteration 2504, BCE loss: 57.75766593457578, Acc: 0.8197, Grad norm: 0.12611375238083994\n",
      "Iteration 2505, BCE loss: 57.75755604123758, Acc: 0.8197, Grad norm: 0.10612990561889109\n",
      "Iteration 2506, BCE loss: 57.75757498683035, Acc: 0.8196, Grad norm: 0.10683567782057732\n",
      "Iteration 2507, BCE loss: 57.75756978300289, Acc: 0.8196, Grad norm: 0.11017321931251396\n",
      "Iteration 2508, BCE loss: 57.757591220747116, Acc: 0.8196, Grad norm: 0.11761858014130466\n",
      "Iteration 2509, BCE loss: 57.75765597783575, Acc: 0.8196, Grad norm: 0.12012834102135404\n",
      "Iteration 2510, BCE loss: 57.75766005892995, Acc: 0.8196, Grad norm: 0.11899489981916381\n",
      "Iteration 2511, BCE loss: 57.75759194490309, Acc: 0.8195, Grad norm: 0.10155277490129255\n",
      "Iteration 2512, BCE loss: 57.75775072465426, Acc: 0.8195, Grad norm: 0.12552787949295607\n",
      "Iteration 2513, BCE loss: 57.75781872968946, Acc: 0.8194, Grad norm: 0.14502295494859868\n",
      "Iteration 2514, BCE loss: 57.757660072681404, Acc: 0.8195, Grad norm: 0.1154906753670813\n",
      "Iteration 2515, BCE loss: 57.75759849810173, Acc: 0.8195, Grad norm: 0.10771104249319303\n",
      "Iteration 2516, BCE loss: 57.757584054526596, Acc: 0.8195, Grad norm: 0.10822418297567903\n",
      "Iteration 2517, BCE loss: 57.75766016655476, Acc: 0.8195, Grad norm: 0.12603329034436625\n",
      "Iteration 2518, BCE loss: 57.75774290429021, Acc: 0.8195, Grad norm: 0.1369499939922403\n",
      "Iteration 2519, BCE loss: 57.75795401674039, Acc: 0.8195, Grad norm: 0.1628046312057204\n",
      "Iteration 2520, BCE loss: 57.75770608904506, Acc: 0.8194, Grad norm: 0.13124863836113546\n",
      "Iteration 2521, BCE loss: 57.7575802168913, Acc: 0.8195, Grad norm: 0.11331386722966698\n",
      "Iteration 2522, BCE loss: 57.75750226693476, Acc: 0.8195, Grad norm: 0.09738235035296552\n",
      "Iteration 2523, BCE loss: 57.75750038419946, Acc: 0.8196, Grad norm: 0.09071760107322094\n",
      "Iteration 2524, BCE loss: 57.75740880868112, Acc: 0.8195, Grad norm: 0.07055806793979698\n",
      "Iteration 2525, BCE loss: 57.75739724652773, Acc: 0.8195, Grad norm: 0.06849590089583736\n",
      "Iteration 2526, BCE loss: 57.757524784315464, Acc: 0.8196, Grad norm: 0.10103299386462604\n",
      "Iteration 2527, BCE loss: 57.75770644416059, Acc: 0.8195, Grad norm: 0.13013972047474995\n",
      "Iteration 2528, BCE loss: 57.75776147095305, Acc: 0.8196, Grad norm: 0.144235504220721\n",
      "Iteration 2529, BCE loss: 57.757707922646446, Acc: 0.8196, Grad norm: 0.12733472344342334\n",
      "Iteration 2530, BCE loss: 57.757629730644, Acc: 0.8195, Grad norm: 0.11042330154146726\n",
      "Iteration 2531, BCE loss: 57.757779471002664, Acc: 0.8196, Grad norm: 0.1401564750108206\n",
      "Iteration 2532, BCE loss: 57.757578107802544, Acc: 0.8195, Grad norm: 0.10210975799623578\n",
      "Iteration 2533, BCE loss: 57.75770551851083, Acc: 0.8196, Grad norm: 0.12925379320762315\n",
      "Iteration 2534, BCE loss: 57.757674046410095, Acc: 0.8196, Grad norm: 0.12811504866778098\n",
      "Iteration 2535, BCE loss: 57.75809087213389, Acc: 0.8196, Grad norm: 0.1883778991684541\n",
      "Iteration 2536, BCE loss: 57.757783018515354, Acc: 0.8196, Grad norm: 0.14689245460182562\n",
      "Iteration 2537, BCE loss: 57.758260573109666, Acc: 0.8196, Grad norm: 0.20826104829193862\n",
      "Iteration 2538, BCE loss: 57.7580751889584, Acc: 0.8196, Grad norm: 0.18498133573324854\n",
      "Iteration 2539, BCE loss: 57.75821403101348, Acc: 0.8197, Grad norm: 0.20551871422222728\n",
      "Iteration 2540, BCE loss: 57.75860531636772, Acc: 0.8197, Grad norm: 0.2475968675293675\n",
      "Iteration 2541, BCE loss: 57.75825668990427, Acc: 0.8196, Grad norm: 0.21602124006747903\n",
      "Iteration 2542, BCE loss: 57.75801109753782, Acc: 0.8196, Grad norm: 0.18566459335308452\n",
      "Iteration 2543, BCE loss: 57.75800393494501, Acc: 0.8195, Grad norm: 0.18396251128016283\n",
      "Iteration 2544, BCE loss: 57.75835538451714, Acc: 0.8195, Grad norm: 0.21853516133783052\n",
      "Iteration 2545, BCE loss: 57.758250968747674, Acc: 0.8195, Grad norm: 0.20958757368648545\n",
      "Iteration 2546, BCE loss: 57.75816341416866, Acc: 0.8195, Grad norm: 0.19510200382909082\n",
      "Iteration 2547, BCE loss: 57.75850631195763, Acc: 0.8196, Grad norm: 0.2357342974829095\n",
      "Iteration 2548, BCE loss: 57.75784373738763, Acc: 0.8196, Grad norm: 0.15937903827678365\n",
      "Iteration 2549, BCE loss: 57.7576222634347, Acc: 0.8196, Grad norm: 0.12030499417592891\n",
      "Iteration 2550, BCE loss: 57.757777217677415, Acc: 0.8196, Grad norm: 0.1454199721057673\n",
      "Iteration 2551, BCE loss: 57.75796002172123, Acc: 0.8196, Grad norm: 0.1669278921428347\n",
      "Iteration 2552, BCE loss: 57.75817778673189, Acc: 0.8196, Grad norm: 0.193685547207068\n",
      "Iteration 2553, BCE loss: 57.75790597243416, Acc: 0.8196, Grad norm: 0.15868851405944237\n",
      "Iteration 2554, BCE loss: 57.75783022762785, Acc: 0.8196, Grad norm: 0.15740800330103305\n",
      "Iteration 2555, BCE loss: 57.75786738124561, Acc: 0.8196, Grad norm: 0.15719700739028894\n",
      "Iteration 2556, BCE loss: 57.758109881191885, Acc: 0.8196, Grad norm: 0.18834003933482935\n",
      "Iteration 2557, BCE loss: 57.75787669027089, Acc: 0.8196, Grad norm: 0.15643182416884674\n",
      "Iteration 2558, BCE loss: 57.75813915418566, Acc: 0.8196, Grad norm: 0.19334414598349578\n",
      "Iteration 2559, BCE loss: 57.757978134010315, Acc: 0.8195, Grad norm: 0.17260004386774794\n",
      "Iteration 2560, BCE loss: 57.758028254545636, Acc: 0.8196, Grad norm: 0.17965013074060165\n",
      "Iteration 2561, BCE loss: 57.757927760079156, Acc: 0.8196, Grad norm: 0.1679112026379625\n",
      "Iteration 2562, BCE loss: 57.75813190519135, Acc: 0.8196, Grad norm: 0.1960894373651563\n",
      "Iteration 2563, BCE loss: 57.757664307363456, Acc: 0.8196, Grad norm: 0.12804136896024867\n",
      "Iteration 2564, BCE loss: 57.757824458980224, Acc: 0.8195, Grad norm: 0.15556726344575236\n",
      "Iteration 2565, BCE loss: 57.75779502000424, Acc: 0.8196, Grad norm: 0.15046523250261526\n",
      "Iteration 2566, BCE loss: 57.75792431046645, Acc: 0.8196, Grad norm: 0.1675657986062231\n",
      "Iteration 2567, BCE loss: 57.75781162681001, Acc: 0.8196, Grad norm: 0.1570157527940765\n",
      "Iteration 2568, BCE loss: 57.757740738355, Acc: 0.8196, Grad norm: 0.14374812507583637\n",
      "Iteration 2569, BCE loss: 57.757728728551264, Acc: 0.8196, Grad norm: 0.13616824093158428\n",
      "Iteration 2570, BCE loss: 57.757726436347355, Acc: 0.8195, Grad norm: 0.13180299960798017\n",
      "Iteration 2571, BCE loss: 57.75826561247531, Acc: 0.8195, Grad norm: 0.20193950293204388\n",
      "Iteration 2572, BCE loss: 57.75853991257942, Acc: 0.8196, Grad norm: 0.22903851122264038\n",
      "Iteration 2573, BCE loss: 57.75815191787405, Acc: 0.8196, Grad norm: 0.1871643147819258\n",
      "Iteration 2574, BCE loss: 57.758605248764056, Acc: 0.8197, Grad norm: 0.23583415417300768\n",
      "Iteration 2575, BCE loss: 57.758673471883064, Acc: 0.8197, Grad norm: 0.24616318185593067\n",
      "Iteration 2576, BCE loss: 57.75805545592054, Acc: 0.8196, Grad norm: 0.18834083069524488\n",
      "Iteration 2577, BCE loss: 57.757563851099704, Acc: 0.8196, Grad norm: 0.106954417545603\n",
      "Iteration 2578, BCE loss: 57.75775626541251, Acc: 0.8196, Grad norm: 0.13550750164369177\n",
      "Iteration 2579, BCE loss: 57.75789773013962, Acc: 0.8196, Grad norm: 0.15975606795567712\n",
      "Iteration 2580, BCE loss: 57.75794415550089, Acc: 0.8197, Grad norm: 0.16629704569823234\n",
      "Iteration 2581, BCE loss: 57.757756989379814, Acc: 0.8196, Grad norm: 0.13721589922570204\n",
      "Iteration 2582, BCE loss: 57.75768100089022, Acc: 0.8197, Grad norm: 0.12889414738430163\n",
      "Iteration 2583, BCE loss: 57.757910836782116, Acc: 0.8197, Grad norm: 0.16834359385897366\n",
      "Iteration 2584, BCE loss: 57.75783610961324, Acc: 0.8197, Grad norm: 0.15910478479648849\n",
      "Iteration 2585, BCE loss: 57.75771250700686, Acc: 0.8197, Grad norm: 0.14022758824685036\n",
      "Iteration 2586, BCE loss: 57.75754456803151, Acc: 0.8196, Grad norm: 0.11086757600569536\n",
      "Iteration 2587, BCE loss: 57.75774881988405, Acc: 0.8196, Grad norm: 0.14476799944445023\n",
      "Iteration 2588, BCE loss: 57.75753628869613, Acc: 0.8195, Grad norm: 0.10480004899418552\n",
      "Iteration 2589, BCE loss: 57.7574793829666, Acc: 0.8195, Grad norm: 0.09347913234749032\n",
      "Iteration 2590, BCE loss: 57.7577076057489, Acc: 0.8195, Grad norm: 0.13246199362799843\n",
      "Iteration 2591, BCE loss: 57.75776022983314, Acc: 0.8194, Grad norm: 0.13898267144346324\n",
      "Iteration 2592, BCE loss: 57.757707214158636, Acc: 0.8195, Grad norm: 0.13321548144142215\n",
      "Iteration 2593, BCE loss: 57.75771699892356, Acc: 0.8195, Grad norm: 0.13766210956032335\n",
      "Iteration 2594, BCE loss: 57.75765562364032, Acc: 0.8195, Grad norm: 0.12345564987895603\n",
      "Iteration 2595, BCE loss: 57.75772436509776, Acc: 0.8196, Grad norm: 0.12946847958204313\n",
      "Iteration 2596, BCE loss: 57.75743280756993, Acc: 0.8196, Grad norm: 0.0756477981868054\n",
      "Iteration 2597, BCE loss: 57.75743511090538, Acc: 0.8195, Grad norm: 0.07689304117936949\n",
      "Iteration 2598, BCE loss: 57.75752394364607, Acc: 0.8195, Grad norm: 0.09872128369611993\n",
      "Iteration 2599, BCE loss: 57.757637564032066, Acc: 0.8195, Grad norm: 0.12327933360402171\n",
      "Iteration 2600, BCE loss: 57.7575451374467, Acc: 0.8196, Grad norm: 0.10378415233570325\n",
      "Iteration 2601, BCE loss: 57.75782199514617, Acc: 0.8196, Grad norm: 0.14354777212500844\n",
      "Iteration 2602, BCE loss: 57.75757549212159, Acc: 0.8196, Grad norm: 0.10541552788658863\n",
      "Iteration 2603, BCE loss: 57.75758311723986, Acc: 0.8196, Grad norm: 0.1071310612843012\n",
      "Iteration 2604, BCE loss: 57.75751699428969, Acc: 0.8196, Grad norm: 0.0960218338061685\n",
      "Iteration 2605, BCE loss: 57.75774747941972, Acc: 0.8196, Grad norm: 0.135632366120552\n",
      "Iteration 2606, BCE loss: 57.75772636171287, Acc: 0.8196, Grad norm: 0.12995531452684023\n",
      "Iteration 2607, BCE loss: 57.7576109057906, Acc: 0.8196, Grad norm: 0.11154023440907282\n",
      "Iteration 2608, BCE loss: 57.75749846936121, Acc: 0.8196, Grad norm: 0.09287395253636395\n",
      "Iteration 2609, BCE loss: 57.757916524351984, Acc: 0.8197, Grad norm: 0.1621420175709711\n",
      "Iteration 2610, BCE loss: 57.75781643561632, Acc: 0.8196, Grad norm: 0.14670908264117685\n",
      "Iteration 2611, BCE loss: 57.757759543986595, Acc: 0.8196, Grad norm: 0.14318789344941865\n",
      "Iteration 2612, BCE loss: 57.75782603393701, Acc: 0.8197, Grad norm: 0.1475952089845727\n",
      "Iteration 2613, BCE loss: 57.75807326301941, Acc: 0.8197, Grad norm: 0.17721411942766552\n",
      "Iteration 2614, BCE loss: 57.75804491112479, Acc: 0.8197, Grad norm: 0.17474293577545885\n",
      "Iteration 2615, BCE loss: 57.75777330432898, Acc: 0.8197, Grad norm: 0.1432818461469141\n",
      "Iteration 2616, BCE loss: 57.7578799203, Acc: 0.8197, Grad norm: 0.1645721371659882\n",
      "Iteration 2617, BCE loss: 57.757811593604075, Acc: 0.8197, Grad norm: 0.1495535275811232\n",
      "Iteration 2618, BCE loss: 57.757525278200816, Acc: 0.8196, Grad norm: 0.0962306213065995\n",
      "Iteration 2619, BCE loss: 57.75746600840522, Acc: 0.8196, Grad norm: 0.08438898349172902\n",
      "Iteration 2620, BCE loss: 57.75749979287163, Acc: 0.8195, Grad norm: 0.09707301504015874\n",
      "Iteration 2621, BCE loss: 57.75762212522564, Acc: 0.8196, Grad norm: 0.12326183945216024\n",
      "Iteration 2622, BCE loss: 57.75759653394594, Acc: 0.8195, Grad norm: 0.1240049596977771\n",
      "Iteration 2623, BCE loss: 57.75766253194907, Acc: 0.8195, Grad norm: 0.1323841322195297\n",
      "Iteration 2624, BCE loss: 57.757745296397644, Acc: 0.8195, Grad norm: 0.13930736881446898\n",
      "Iteration 2625, BCE loss: 57.757902086888436, Acc: 0.8195, Grad norm: 0.1648040306694354\n",
      "Iteration 2626, BCE loss: 57.75765453189355, Acc: 0.8195, Grad norm: 0.12676279867718968\n",
      "Iteration 2627, BCE loss: 57.75765008108277, Acc: 0.8196, Grad norm: 0.11713915276768556\n",
      "Iteration 2628, BCE loss: 57.75778320471594, Acc: 0.8197, Grad norm: 0.13529156197744766\n",
      "Iteration 2629, BCE loss: 57.75790139101894, Acc: 0.8197, Grad norm: 0.15040397162607166\n",
      "Iteration 2630, BCE loss: 57.757825624591156, Acc: 0.8197, Grad norm: 0.14470693021514291\n",
      "Iteration 2631, BCE loss: 57.75786535913814, Acc: 0.8197, Grad norm: 0.15313843658709395\n",
      "Iteration 2632, BCE loss: 57.757867243087276, Acc: 0.8197, Grad norm: 0.14910482274383216\n",
      "Iteration 2633, BCE loss: 57.75777467586004, Acc: 0.8197, Grad norm: 0.14058685076761576\n",
      "Iteration 2634, BCE loss: 57.757493563555975, Acc: 0.8196, Grad norm: 0.09344326224096151\n",
      "Iteration 2635, BCE loss: 57.75759386568937, Acc: 0.8196, Grad norm: 0.11756986156616249\n",
      "Iteration 2636, BCE loss: 57.75763870944121, Acc: 0.8197, Grad norm: 0.12427713563104394\n",
      "Iteration 2637, BCE loss: 57.75752910910519, Acc: 0.8196, Grad norm: 0.10386904213190114\n",
      "Iteration 2638, BCE loss: 57.75745905555388, Acc: 0.8196, Grad norm: 0.0908514188434058\n",
      "Iteration 2639, BCE loss: 57.757504064480656, Acc: 0.8196, Grad norm: 0.09589947491036355\n",
      "Iteration 2640, BCE loss: 57.75758437425386, Acc: 0.8196, Grad norm: 0.10893500770891268\n",
      "Iteration 2641, BCE loss: 57.757686841832935, Acc: 0.8195, Grad norm: 0.1290093987309497\n",
      "Iteration 2642, BCE loss: 57.75784910886815, Acc: 0.8196, Grad norm: 0.15076843918221092\n",
      "Iteration 2643, BCE loss: 57.75774993612916, Acc: 0.8196, Grad norm: 0.13490149540190238\n",
      "Iteration 2644, BCE loss: 57.75758887845487, Acc: 0.8196, Grad norm: 0.1091917878373655\n",
      "Iteration 2645, BCE loss: 57.75773173861035, Acc: 0.8196, Grad norm: 0.13500733039843968\n",
      "Iteration 2646, BCE loss: 57.758023529699365, Acc: 0.8196, Grad norm: 0.1725941763725628\n",
      "Iteration 2647, BCE loss: 57.75811304620569, Acc: 0.8196, Grad norm: 0.18269711307535913\n",
      "Iteration 2648, BCE loss: 57.75771088942218, Acc: 0.8195, Grad norm: 0.1280640633211734\n",
      "Iteration 2649, BCE loss: 57.75777157854638, Acc: 0.8195, Grad norm: 0.13891654194688507\n",
      "Iteration 2650, BCE loss: 57.75775204882057, Acc: 0.8195, Grad norm: 0.14040127818631004\n",
      "Iteration 2651, BCE loss: 57.75791429983181, Acc: 0.8195, Grad norm: 0.16335900929309596\n",
      "Iteration 2652, BCE loss: 57.757917820797644, Acc: 0.8196, Grad norm: 0.15602580943877778\n",
      "Iteration 2653, BCE loss: 57.75784021697983, Acc: 0.8195, Grad norm: 0.1411011689131614\n",
      "Iteration 2654, BCE loss: 57.75776996500984, Acc: 0.8195, Grad norm: 0.13225354284459798\n",
      "Iteration 2655, BCE loss: 57.75761085640454, Acc: 0.8196, Grad norm: 0.10883008174090522\n",
      "Iteration 2656, BCE loss: 57.75759282765705, Acc: 0.8196, Grad norm: 0.10492549791341632\n",
      "Iteration 2657, BCE loss: 57.757628880664214, Acc: 0.8196, Grad norm: 0.11274367026352251\n",
      "Iteration 2658, BCE loss: 57.757813936382604, Acc: 0.8196, Grad norm: 0.13894801420523858\n",
      "Iteration 2659, BCE loss: 57.757697367596975, Acc: 0.8196, Grad norm: 0.12188507171712268\n",
      "Iteration 2660, BCE loss: 57.75778047640518, Acc: 0.8197, Grad norm: 0.14151671809142935\n",
      "Iteration 2661, BCE loss: 57.75786334904061, Acc: 0.8197, Grad norm: 0.15312017184366486\n",
      "Iteration 2662, BCE loss: 57.757673534036044, Acc: 0.8197, Grad norm: 0.11781091672644922\n",
      "Iteration 2663, BCE loss: 57.75760404395774, Acc: 0.8196, Grad norm: 0.1078686814718722\n",
      "Iteration 2664, BCE loss: 57.757606992464275, Acc: 0.8196, Grad norm: 0.11072852023354642\n",
      "Iteration 2665, BCE loss: 57.757518747449325, Acc: 0.8196, Grad norm: 0.08948825537155418\n",
      "Iteration 2666, BCE loss: 57.757500752921516, Acc: 0.8196, Grad norm: 0.08434058252287334\n",
      "Iteration 2667, BCE loss: 57.75758002406786, Acc: 0.8196, Grad norm: 0.10684924240361346\n",
      "Iteration 2668, BCE loss: 57.757802035406385, Acc: 0.8196, Grad norm: 0.1507738757114151\n",
      "Iteration 2669, BCE loss: 57.757501204915975, Acc: 0.8196, Grad norm: 0.102040315026124\n",
      "Iteration 2670, BCE loss: 57.75750226428681, Acc: 0.8196, Grad norm: 0.09982176853842466\n",
      "Iteration 2671, BCE loss: 57.7574981645306, Acc: 0.8196, Grad norm: 0.10248725439978451\n",
      "Iteration 2672, BCE loss: 57.7577312129861, Acc: 0.8195, Grad norm: 0.1453448019811366\n",
      "Iteration 2673, BCE loss: 57.75770343570707, Acc: 0.8196, Grad norm: 0.14311205708186792\n",
      "Iteration 2674, BCE loss: 57.75777417341787, Acc: 0.8196, Grad norm: 0.15344280825216702\n",
      "Iteration 2675, BCE loss: 57.75793245087006, Acc: 0.8195, Grad norm: 0.17486434549936727\n",
      "Iteration 2676, BCE loss: 57.758018547775634, Acc: 0.8196, Grad norm: 0.18728061044893263\n",
      "Iteration 2677, BCE loss: 57.7578182035308, Acc: 0.8196, Grad norm: 0.1572428206245272\n",
      "Iteration 2678, BCE loss: 57.757617797530365, Acc: 0.8196, Grad norm: 0.12856370309374482\n",
      "Iteration 2679, BCE loss: 57.75765608175548, Acc: 0.8197, Grad norm: 0.13451738042381461\n",
      "Iteration 2680, BCE loss: 57.757572771366384, Acc: 0.8197, Grad norm: 0.11961793289776203\n",
      "Iteration 2681, BCE loss: 57.757772237092844, Acc: 0.8197, Grad norm: 0.1546508983952978\n",
      "Iteration 2682, BCE loss: 57.75777631229933, Acc: 0.8197, Grad norm: 0.15453012898715648\n",
      "Iteration 2683, BCE loss: 57.75778239893592, Acc: 0.8196, Grad norm: 0.1550209648675836\n",
      "Iteration 2684, BCE loss: 57.757746026872624, Acc: 0.8196, Grad norm: 0.14869199826184454\n",
      "Iteration 2685, BCE loss: 57.75797318315149, Acc: 0.8196, Grad norm: 0.17825292968161835\n",
      "Iteration 2686, BCE loss: 57.75758613827391, Acc: 0.8196, Grad norm: 0.12277956825460214\n",
      "Iteration 2687, BCE loss: 57.75749071002054, Acc: 0.8196, Grad norm: 0.09697185227290178\n",
      "Iteration 2688, BCE loss: 57.75746173732628, Acc: 0.8196, Grad norm: 0.09021600649425185\n",
      "Iteration 2689, BCE loss: 57.75748031225951, Acc: 0.8195, Grad norm: 0.0928254758699894\n",
      "Iteration 2690, BCE loss: 57.757568084775485, Acc: 0.8195, Grad norm: 0.11275032343195876\n",
      "Iteration 2691, BCE loss: 57.75751661692923, Acc: 0.8195, Grad norm: 0.09868269528849385\n",
      "Iteration 2692, BCE loss: 57.757446452052896, Acc: 0.8195, Grad norm: 0.08225727976610563\n",
      "Iteration 2693, BCE loss: 57.757459344383975, Acc: 0.8195, Grad norm: 0.08956399417854746\n",
      "Iteration 2694, BCE loss: 57.75742741346211, Acc: 0.8195, Grad norm: 0.07909319142819397\n",
      "Iteration 2695, BCE loss: 57.75743915502529, Acc: 0.8195, Grad norm: 0.08417635784376915\n",
      "Iteration 2696, BCE loss: 57.75791225376284, Acc: 0.8195, Grad norm: 0.17003582917111026\n",
      "Iteration 2697, BCE loss: 57.75774271752637, Acc: 0.8196, Grad norm: 0.1409632003687672\n",
      "Iteration 2698, BCE loss: 57.757931079749724, Acc: 0.8196, Grad norm: 0.17123373871165518\n",
      "Iteration 2699, BCE loss: 57.75783615524417, Acc: 0.8196, Grad norm: 0.1598730092295693\n",
      "Iteration 2700, BCE loss: 57.75768121436049, Acc: 0.8196, Grad norm: 0.12984868644064382\n",
      "Iteration 2701, BCE loss: 57.757421829234005, Acc: 0.8196, Grad norm: 0.07848673798595066\n",
      "Iteration 2702, BCE loss: 57.75760592999383, Acc: 0.8196, Grad norm: 0.11972542493264411\n",
      "Iteration 2703, BCE loss: 57.757630015880565, Acc: 0.8195, Grad norm: 0.12161628645565323\n",
      "Iteration 2704, BCE loss: 57.75787832110615, Acc: 0.8195, Grad norm: 0.15785436230805439\n",
      "Iteration 2705, BCE loss: 57.75777363859295, Acc: 0.8195, Grad norm: 0.14680841624741536\n",
      "Iteration 2706, BCE loss: 57.757843695856224, Acc: 0.8195, Grad norm: 0.160898528638429\n",
      "Iteration 2707, BCE loss: 57.75776179459493, Acc: 0.8195, Grad norm: 0.14683118499642858\n",
      "Iteration 2708, BCE loss: 57.75752924481604, Acc: 0.8195, Grad norm: 0.10950990175271792\n",
      "Iteration 2709, BCE loss: 57.757461447166364, Acc: 0.8195, Grad norm: 0.0913890770242015\n",
      "Iteration 2710, BCE loss: 57.757903034733246, Acc: 0.8195, Grad norm: 0.16297217188179636\n",
      "Iteration 2711, BCE loss: 57.757812769310306, Acc: 0.8195, Grad norm: 0.15177799451874188\n",
      "Iteration 2712, BCE loss: 57.7577109395521, Acc: 0.8195, Grad norm: 0.13343727749629755\n",
      "Iteration 2713, BCE loss: 57.757856013426746, Acc: 0.8195, Grad norm: 0.1543447586357464\n",
      "Iteration 2714, BCE loss: 57.7578800481285, Acc: 0.8195, Grad norm: 0.15938371441186244\n",
      "Iteration 2715, BCE loss: 57.75785283218084, Acc: 0.8195, Grad norm: 0.15501116171248278\n",
      "Iteration 2716, BCE loss: 57.757870668190584, Acc: 0.8195, Grad norm: 0.15806119327997933\n",
      "Iteration 2717, BCE loss: 57.7578062953907, Acc: 0.8195, Grad norm: 0.1489001186648897\n",
      "Iteration 2718, BCE loss: 57.75753831482314, Acc: 0.8196, Grad norm: 0.10671262649446203\n",
      "Iteration 2719, BCE loss: 57.757391159747435, Acc: 0.8195, Grad norm: 0.07440708662556815\n",
      "Iteration 2720, BCE loss: 57.75757768454349, Acc: 0.8196, Grad norm: 0.11826569849986077\n",
      "Iteration 2721, BCE loss: 57.75747978950044, Acc: 0.8195, Grad norm: 0.09254462622910689\n",
      "Iteration 2722, BCE loss: 57.75742217178304, Acc: 0.8195, Grad norm: 0.07983515973980004\n",
      "Iteration 2723, BCE loss: 57.75744018829498, Acc: 0.8195, Grad norm: 0.08449003247751394\n",
      "Iteration 2724, BCE loss: 57.75759844709509, Acc: 0.8195, Grad norm: 0.12315927158349876\n",
      "Iteration 2725, BCE loss: 57.75750707610737, Acc: 0.8195, Grad norm: 0.09983110959640831\n",
      "Iteration 2726, BCE loss: 57.75756588123511, Acc: 0.8194, Grad norm: 0.10651122659046876\n",
      "Iteration 2727, BCE loss: 57.75764648115042, Acc: 0.8194, Grad norm: 0.12032357362524201\n",
      "Iteration 2728, BCE loss: 57.75791524094345, Acc: 0.8195, Grad norm: 0.16003526453920586\n",
      "Iteration 2729, BCE loss: 57.757815902889405, Acc: 0.8195, Grad norm: 0.14537923765191987\n",
      "Iteration 2730, BCE loss: 57.75795869153124, Acc: 0.8195, Grad norm: 0.1689917182681551\n",
      "Iteration 2731, BCE loss: 57.75759875578591, Acc: 0.8195, Grad norm: 0.11799572949299529\n",
      "Iteration 2732, BCE loss: 57.75748892886379, Acc: 0.8195, Grad norm: 0.08862743164474603\n",
      "Iteration 2733, BCE loss: 57.757685549564286, Acc: 0.8195, Grad norm: 0.11907121024355526\n",
      "Iteration 2734, BCE loss: 57.75755828724435, Acc: 0.8195, Grad norm: 0.10312436649462607\n",
      "Iteration 2735, BCE loss: 57.75751697507948, Acc: 0.8195, Grad norm: 0.0936252307086631\n",
      "Iteration 2736, BCE loss: 57.75750845054418, Acc: 0.8195, Grad norm: 0.09105765070385587\n",
      "Iteration 2737, BCE loss: 57.757539281308226, Acc: 0.8195, Grad norm: 0.10189815744051957\n",
      "Iteration 2738, BCE loss: 57.7576120514676, Acc: 0.8195, Grad norm: 0.11793003608790166\n",
      "Iteration 2739, BCE loss: 57.75757636947789, Acc: 0.8195, Grad norm: 0.11136051473898313\n",
      "Iteration 2740, BCE loss: 57.75762206769797, Acc: 0.8195, Grad norm: 0.11257812368094136\n",
      "Iteration 2741, BCE loss: 57.75763703142091, Acc: 0.8195, Grad norm: 0.11766272992896264\n",
      "Iteration 2742, BCE loss: 57.757909516780416, Acc: 0.8195, Grad norm: 0.15745320719810885\n",
      "Iteration 2743, BCE loss: 57.75764792387185, Acc: 0.8195, Grad norm: 0.11672962808722762\n",
      "Iteration 2744, BCE loss: 57.75770659488803, Acc: 0.8196, Grad norm: 0.13181886642329374\n",
      "Iteration 2745, BCE loss: 57.75767694198285, Acc: 0.8196, Grad norm: 0.13299380059887642\n",
      "Iteration 2746, BCE loss: 57.7574580300452, Acc: 0.8196, Grad norm: 0.08712829244326455\n",
      "Iteration 2747, BCE loss: 57.75761384148183, Acc: 0.8196, Grad norm: 0.1262649489764869\n",
      "Iteration 2748, BCE loss: 57.75751554196665, Acc: 0.8196, Grad norm: 0.10480639204635146\n",
      "Iteration 2749, BCE loss: 57.75767169918143, Acc: 0.8196, Grad norm: 0.13062150891863186\n",
      "Iteration 2750, BCE loss: 57.75754070354142, Acc: 0.8195, Grad norm: 0.09844371657567784\n",
      "Iteration 2751, BCE loss: 57.75750194492363, Acc: 0.8195, Grad norm: 0.09266641245803034\n",
      "Iteration 2752, BCE loss: 57.757647681563284, Acc: 0.8195, Grad norm: 0.12466913744585362\n",
      "Iteration 2753, BCE loss: 57.757569898563276, Acc: 0.8195, Grad norm: 0.11159553076183494\n",
      "Iteration 2754, BCE loss: 57.75749674952445, Acc: 0.8195, Grad norm: 0.09684763190425723\n",
      "Iteration 2755, BCE loss: 57.757561178347785, Acc: 0.8195, Grad norm: 0.10514766335481698\n",
      "Iteration 2756, BCE loss: 57.75746202525276, Acc: 0.8195, Grad norm: 0.0897933623108741\n",
      "Iteration 2757, BCE loss: 57.757551032190946, Acc: 0.8195, Grad norm: 0.10573120346362663\n",
      "Iteration 2758, BCE loss: 57.75751810459799, Acc: 0.8195, Grad norm: 0.09855591246367715\n",
      "Iteration 2759, BCE loss: 57.75768804296729, Acc: 0.8195, Grad norm: 0.1338038487868357\n",
      "Iteration 2760, BCE loss: 57.75736965237152, Acc: 0.8195, Grad norm: 0.06433057401276485\n",
      "Iteration 2761, BCE loss: 57.757528967261344, Acc: 0.8196, Grad norm: 0.10746789814833778\n",
      "Iteration 2762, BCE loss: 57.75730598500205, Acc: 0.8196, Grad norm: 0.04325733476527598\n",
      "Iteration 2763, BCE loss: 57.75730140454232, Acc: 0.8196, Grad norm: 0.03793375871574903\n",
      "Iteration 2764, BCE loss: 57.757438503791334, Acc: 0.8196, Grad norm: 0.08522783605162776\n",
      "Iteration 2765, BCE loss: 57.757367945719636, Acc: 0.8196, Grad norm: 0.06669760126055589\n",
      "Iteration 2766, BCE loss: 57.75738093757981, Acc: 0.8196, Grad norm: 0.06567955654821514\n",
      "Iteration 2767, BCE loss: 57.757348689032355, Acc: 0.8196, Grad norm: 0.053411016907876266\n",
      "Iteration 2768, BCE loss: 57.75767115013299, Acc: 0.8196, Grad norm: 0.13406950028913642\n",
      "Iteration 2769, BCE loss: 57.757587832011026, Acc: 0.8196, Grad norm: 0.12152816708534826\n",
      "Iteration 2770, BCE loss: 57.7576915327389, Acc: 0.8196, Grad norm: 0.13827962821513193\n",
      "Iteration 2771, BCE loss: 57.75751570438631, Acc: 0.8196, Grad norm: 0.10325893171371738\n",
      "Iteration 2772, BCE loss: 57.75746792789212, Acc: 0.8196, Grad norm: 0.09101322704636317\n",
      "Iteration 2773, BCE loss: 57.75782233371893, Acc: 0.8196, Grad norm: 0.15765393007819564\n",
      "Iteration 2774, BCE loss: 57.757807407670526, Acc: 0.8196, Grad norm: 0.14781961216356598\n",
      "Iteration 2775, BCE loss: 57.75793474677485, Acc: 0.8196, Grad norm: 0.16820124224080862\n",
      "Iteration 2776, BCE loss: 57.757568366723916, Acc: 0.8196, Grad norm: 0.11089266536195352\n",
      "Iteration 2777, BCE loss: 57.75768920916739, Acc: 0.8196, Grad norm: 0.13521240047369265\n",
      "Iteration 2778, BCE loss: 57.75767942585394, Acc: 0.8196, Grad norm: 0.13196937605618891\n",
      "Iteration 2779, BCE loss: 57.757636818946, Acc: 0.8196, Grad norm: 0.12363664859446179\n",
      "Iteration 2780, BCE loss: 57.75771784474786, Acc: 0.8197, Grad norm: 0.13364123854934645\n",
      "Iteration 2781, BCE loss: 57.75773837376526, Acc: 0.8196, Grad norm: 0.13868658313335874\n",
      "Iteration 2782, BCE loss: 57.75780687205842, Acc: 0.8197, Grad norm: 0.14774885162106227\n",
      "Iteration 2783, BCE loss: 57.757651774225835, Acc: 0.8196, Grad norm: 0.12252163098690678\n",
      "Iteration 2784, BCE loss: 57.757697318587276, Acc: 0.8197, Grad norm: 0.13418136583186438\n",
      "Iteration 2785, BCE loss: 57.757545922651744, Acc: 0.8196, Grad norm: 0.10534800199539017\n",
      "Iteration 2786, BCE loss: 57.75750631350971, Acc: 0.8196, Grad norm: 0.09489385648125843\n",
      "Iteration 2787, BCE loss: 57.7575408391766, Acc: 0.8196, Grad norm: 0.10155668333744546\n",
      "Iteration 2788, BCE loss: 57.75752540618131, Acc: 0.8196, Grad norm: 0.10219170929967657\n",
      "Iteration 2789, BCE loss: 57.7578060567656, Acc: 0.8196, Grad norm: 0.15403644563669888\n",
      "Iteration 2790, BCE loss: 57.757616418248546, Acc: 0.8195, Grad norm: 0.11549495139821075\n",
      "Iteration 2791, BCE loss: 57.757607324198986, Acc: 0.8196, Grad norm: 0.11562365186187991\n",
      "Iteration 2792, BCE loss: 57.75764179087079, Acc: 0.8196, Grad norm: 0.11525371359411234\n",
      "Iteration 2793, BCE loss: 57.75752680106196, Acc: 0.8196, Grad norm: 0.09684045276175463\n",
      "Iteration 2794, BCE loss: 57.75746529189921, Acc: 0.8196, Grad norm: 0.07929387679319998\n",
      "Iteration 2795, BCE loss: 57.757593677218594, Acc: 0.8195, Grad norm: 0.10738418686030371\n",
      "Iteration 2796, BCE loss: 57.757664036775815, Acc: 0.8196, Grad norm: 0.12885178191060548\n",
      "Iteration 2797, BCE loss: 57.75769626008359, Acc: 0.8196, Grad norm: 0.1277461059605791\n",
      "Iteration 2798, BCE loss: 57.757534131347484, Acc: 0.8196, Grad norm: 0.09356326490689236\n",
      "Iteration 2799, BCE loss: 57.75763584825886, Acc: 0.8196, Grad norm: 0.10965973159681067\n",
      "Iteration 2800, BCE loss: 57.75749117167247, Acc: 0.8195, Grad norm: 0.0847791335828309\n",
      "Iteration 2801, BCE loss: 57.7577599152976, Acc: 0.8196, Grad norm: 0.13659293305846684\n",
      "Iteration 2802, BCE loss: 57.75783251386656, Acc: 0.8195, Grad norm: 0.1467744589744214\n",
      "Iteration 2803, BCE loss: 57.75790872675786, Acc: 0.8195, Grad norm: 0.15812843839987545\n",
      "Iteration 2804, BCE loss: 57.757943081304276, Acc: 0.8195, Grad norm: 0.16039678469134724\n",
      "Iteration 2805, BCE loss: 57.7578094267143, Acc: 0.8195, Grad norm: 0.14489434840562954\n",
      "Iteration 2806, BCE loss: 57.757896771928046, Acc: 0.8195, Grad norm: 0.1584390717359457\n",
      "Iteration 2807, BCE loss: 57.75795363822503, Acc: 0.8196, Grad norm: 0.17502143094810688\n",
      "Iteration 2808, BCE loss: 57.7577463585889, Acc: 0.8196, Grad norm: 0.14709360582829273\n",
      "Iteration 2809, BCE loss: 57.757707352744944, Acc: 0.8196, Grad norm: 0.13881424495908898\n",
      "Iteration 2810, BCE loss: 57.75761793522961, Acc: 0.8196, Grad norm: 0.1174121926428666\n",
      "Iteration 2811, BCE loss: 57.75755104168763, Acc: 0.8196, Grad norm: 0.10351804149991904\n",
      "Iteration 2812, BCE loss: 57.75774469195292, Acc: 0.8196, Grad norm: 0.14132564734663064\n",
      "Iteration 2813, BCE loss: 57.75781568819703, Acc: 0.8196, Grad norm: 0.15184662854832226\n",
      "Iteration 2814, BCE loss: 57.75800210404461, Acc: 0.8196, Grad norm: 0.17503289457614485\n",
      "Iteration 2815, BCE loss: 57.75770009562427, Acc: 0.8196, Grad norm: 0.13347471989298726\n",
      "Iteration 2816, BCE loss: 57.7576605535173, Acc: 0.8196, Grad norm: 0.1285094360373468\n",
      "Iteration 2817, BCE loss: 57.75766546290316, Acc: 0.8196, Grad norm: 0.13279801690436757\n",
      "Iteration 2818, BCE loss: 57.757582685293805, Acc: 0.8196, Grad norm: 0.12024116316869107\n",
      "Iteration 2819, BCE loss: 57.757680756937944, Acc: 0.8196, Grad norm: 0.1360922250598854\n",
      "Iteration 2820, BCE loss: 57.757926932454694, Acc: 0.8197, Grad norm: 0.17381252827527333\n",
      "Iteration 2821, BCE loss: 57.757847362405414, Acc: 0.8196, Grad norm: 0.16304527543607833\n",
      "Iteration 2822, BCE loss: 57.757756240880894, Acc: 0.8196, Grad norm: 0.15067143914209605\n",
      "Iteration 2823, BCE loss: 57.757430683320464, Acc: 0.8196, Grad norm: 0.08386375495805831\n",
      "Iteration 2824, BCE loss: 57.75746897452393, Acc: 0.8196, Grad norm: 0.09079167070972786\n",
      "Iteration 2825, BCE loss: 57.75754684982091, Acc: 0.8196, Grad norm: 0.10905629289317689\n",
      "Iteration 2826, BCE loss: 57.757530829315236, Acc: 0.8196, Grad norm: 0.10354195667793141\n",
      "Iteration 2827, BCE loss: 57.75738521481192, Acc: 0.8195, Grad norm: 0.06260119731765582\n",
      "Iteration 2828, BCE loss: 57.75749977574703, Acc: 0.8196, Grad norm: 0.09366951997572437\n",
      "Iteration 2829, BCE loss: 57.75763631112583, Acc: 0.8196, Grad norm: 0.11905756205352605\n",
      "Iteration 2830, BCE loss: 57.75790750367651, Acc: 0.8196, Grad norm: 0.16101157704906968\n",
      "Iteration 2831, BCE loss: 57.75780176737524, Acc: 0.8196, Grad norm: 0.14580336924733428\n",
      "Iteration 2832, BCE loss: 57.75792775302169, Acc: 0.8196, Grad norm: 0.16565717130377652\n",
      "Iteration 2833, BCE loss: 57.75807088972684, Acc: 0.8196, Grad norm: 0.18670294850513916\n",
      "Iteration 2834, BCE loss: 57.758244625512525, Acc: 0.8195, Grad norm: 0.20943502601955544\n",
      "Iteration 2835, BCE loss: 57.757820763779876, Acc: 0.8196, Grad norm: 0.1541614549778732\n",
      "Iteration 2836, BCE loss: 57.7579347669882, Acc: 0.8195, Grad norm: 0.17078884037031797\n",
      "Iteration 2837, BCE loss: 57.75805193503942, Acc: 0.8196, Grad norm: 0.1824468307830863\n",
      "Iteration 2838, BCE loss: 57.757801510227985, Acc: 0.8196, Grad norm: 0.1475347351648061\n",
      "Iteration 2839, BCE loss: 57.757733914742886, Acc: 0.8196, Grad norm: 0.13878429914882548\n",
      "Iteration 2840, BCE loss: 57.757485060192664, Acc: 0.8196, Grad norm: 0.09392428085225923\n",
      "Iteration 2841, BCE loss: 57.75757375138395, Acc: 0.8196, Grad norm: 0.11239079963052752\n",
      "Iteration 2842, BCE loss: 57.757626293913646, Acc: 0.8196, Grad norm: 0.1219545426291766\n",
      "Iteration 2843, BCE loss: 57.75752549396431, Acc: 0.8196, Grad norm: 0.10108890337663752\n",
      "Iteration 2844, BCE loss: 57.75746113420793, Acc: 0.8196, Grad norm: 0.08769901705597778\n",
      "Iteration 2845, BCE loss: 57.757389773274525, Acc: 0.8196, Grad norm: 0.06501791246931968\n",
      "Iteration 2846, BCE loss: 57.75753519937996, Acc: 0.8196, Grad norm: 0.10064230080639368\n",
      "Iteration 2847, BCE loss: 57.757522755586415, Acc: 0.8196, Grad norm: 0.09753071074052982\n",
      "Iteration 2848, BCE loss: 57.75754210651086, Acc: 0.8196, Grad norm: 0.10179699534170647\n",
      "Iteration 2849, BCE loss: 57.75762517625675, Acc: 0.8196, Grad norm: 0.11820213628796061\n",
      "Iteration 2850, BCE loss: 57.75769425122151, Acc: 0.8196, Grad norm: 0.1265842419021256\n",
      "Iteration 2851, BCE loss: 57.75770965802033, Acc: 0.8196, Grad norm: 0.13296622273039072\n",
      "Iteration 2852, BCE loss: 57.75746671920511, Acc: 0.8196, Grad norm: 0.08926247639160793\n",
      "Iteration 2853, BCE loss: 57.757498009933116, Acc: 0.8196, Grad norm: 0.09809531265956116\n",
      "Iteration 2854, BCE loss: 57.757582874132105, Acc: 0.8196, Grad norm: 0.11660889178238382\n",
      "Iteration 2855, BCE loss: 57.75740618990088, Acc: 0.8196, Grad norm: 0.07456906107535523\n",
      "Iteration 2856, BCE loss: 57.75739166371146, Acc: 0.8195, Grad norm: 0.06768847081633848\n",
      "Iteration 2857, BCE loss: 57.75735584711348, Acc: 0.8195, Grad norm: 0.05500988696573225\n",
      "Iteration 2858, BCE loss: 57.75739286712576, Acc: 0.8196, Grad norm: 0.07102400406596837\n",
      "Iteration 2859, BCE loss: 57.757383623912496, Acc: 0.8196, Grad norm: 0.07199453387404815\n",
      "Iteration 2860, BCE loss: 57.75769807667294, Acc: 0.8196, Grad norm: 0.1412775700937974\n",
      "Iteration 2861, BCE loss: 57.757611087364694, Acc: 0.8196, Grad norm: 0.12432723703017588\n",
      "Iteration 2862, BCE loss: 57.75754368804831, Acc: 0.8196, Grad norm: 0.11274338994681771\n",
      "Iteration 2863, BCE loss: 57.757513387516994, Acc: 0.8196, Grad norm: 0.10603024667575826\n",
      "Iteration 2864, BCE loss: 57.75744150473825, Acc: 0.8196, Grad norm: 0.08741621310360925\n",
      "Iteration 2865, BCE loss: 57.757305740737706, Acc: 0.8196, Grad norm: 0.04088159324939943\n",
      "Iteration 2866, BCE loss: 57.757408959452945, Acc: 0.8196, Grad norm: 0.07673375010998194\n",
      "Iteration 2867, BCE loss: 57.757418413497064, Acc: 0.8195, Grad norm: 0.07773879881919296\n",
      "Iteration 2868, BCE loss: 57.757560668850516, Acc: 0.8196, Grad norm: 0.10712902446994506\n",
      "Iteration 2869, BCE loss: 57.75764028591097, Acc: 0.8196, Grad norm: 0.12535565910027865\n",
      "Iteration 2870, BCE loss: 57.75764219290073, Acc: 0.8196, Grad norm: 0.12039054605994014\n",
      "Iteration 2871, BCE loss: 57.7578205311257, Acc: 0.8196, Grad norm: 0.15638028469494858\n",
      "Iteration 2872, BCE loss: 57.75774034712661, Acc: 0.8196, Grad norm: 0.14331700346636347\n",
      "Iteration 2873, BCE loss: 57.757691055154524, Acc: 0.8195, Grad norm: 0.13360596503210806\n",
      "Iteration 2874, BCE loss: 57.757394308391696, Acc: 0.8195, Grad norm: 0.0666840402037869\n",
      "Iteration 2875, BCE loss: 57.75740088054357, Acc: 0.8195, Grad norm: 0.06953384329538684\n",
      "Iteration 2876, BCE loss: 57.75737004356937, Acc: 0.8195, Grad norm: 0.06057671198555042\n",
      "Iteration 2877, BCE loss: 57.7575057914795, Acc: 0.8196, Grad norm: 0.10129716865378156\n",
      "Iteration 2878, BCE loss: 57.75766116954104, Acc: 0.8195, Grad norm: 0.12978588772484118\n",
      "Iteration 2879, BCE loss: 57.757787773546134, Acc: 0.8195, Grad norm: 0.1494523572071593\n",
      "Iteration 2880, BCE loss: 57.75748386052581, Acc: 0.8196, Grad norm: 0.09609961368951556\n",
      "Iteration 2881, BCE loss: 57.75770766332914, Acc: 0.8196, Grad norm: 0.14313019025220622\n",
      "Iteration 2882, BCE loss: 57.75766759069597, Acc: 0.8196, Grad norm: 0.13741173765943573\n",
      "Iteration 2883, BCE loss: 57.75792152964473, Acc: 0.8196, Grad norm: 0.17406143133912522\n",
      "Iteration 2884, BCE loss: 57.75780389409219, Acc: 0.8196, Grad norm: 0.15947357769314915\n",
      "Iteration 2885, BCE loss: 57.75768046646667, Acc: 0.8196, Grad norm: 0.13777424353505724\n",
      "Iteration 2886, BCE loss: 57.7577656258551, Acc: 0.8195, Grad norm: 0.14884093939283702\n",
      "Iteration 2887, BCE loss: 57.757976297699955, Acc: 0.8196, Grad norm: 0.17567932880274445\n",
      "Iteration 2888, BCE loss: 57.757922041979256, Acc: 0.8196, Grad norm: 0.1689718248108596\n",
      "Iteration 2889, BCE loss: 57.75768741097668, Acc: 0.8196, Grad norm: 0.1300658397342895\n",
      "Iteration 2890, BCE loss: 57.757876512221316, Acc: 0.8196, Grad norm: 0.15608887213999856\n",
      "Iteration 2891, BCE loss: 57.75790421298929, Acc: 0.8196, Grad norm: 0.15190434384754803\n",
      "Iteration 2892, BCE loss: 57.757742540789266, Acc: 0.8196, Grad norm: 0.13116684132925754\n",
      "Iteration 2893, BCE loss: 57.75758215806519, Acc: 0.8196, Grad norm: 0.11308733992381281\n",
      "Iteration 2894, BCE loss: 57.757491516901, Acc: 0.8196, Grad norm: 0.09082733288643102\n",
      "Iteration 2895, BCE loss: 57.75760583130198, Acc: 0.8197, Grad norm: 0.11485774666884464\n",
      "Iteration 2896, BCE loss: 57.75750905638503, Acc: 0.8196, Grad norm: 0.09388652006636261\n",
      "Iteration 2897, BCE loss: 57.75749143135537, Acc: 0.8196, Grad norm: 0.09934833452294091\n",
      "Iteration 2898, BCE loss: 57.7574173594049, Acc: 0.8196, Grad norm: 0.07774958098285902\n",
      "Iteration 2899, BCE loss: 57.75749511560039, Acc: 0.8196, Grad norm: 0.09254982544304215\n",
      "Iteration 2900, BCE loss: 57.7573557405194, Acc: 0.8196, Grad norm: 0.056863136308719434\n",
      "Iteration 2901, BCE loss: 57.75746303136097, Acc: 0.8197, Grad norm: 0.08687659175843969\n",
      "Iteration 2902, BCE loss: 57.75746667458699, Acc: 0.8196, Grad norm: 0.08479354770404732\n",
      "Iteration 2903, BCE loss: 57.757373539725336, Acc: 0.8196, Grad norm: 0.06521876795187109\n",
      "Iteration 2904, BCE loss: 57.757370243069715, Acc: 0.8195, Grad norm: 0.06664063573892699\n",
      "Iteration 2905, BCE loss: 57.757408994298096, Acc: 0.8195, Grad norm: 0.07375765272191001\n",
      "Iteration 2906, BCE loss: 57.75764761994318, Acc: 0.8196, Grad norm: 0.1300321800748061\n",
      "Iteration 2907, BCE loss: 57.757369340935874, Acc: 0.8195, Grad norm: 0.05945473537689525\n",
      "Iteration 2908, BCE loss: 57.757443865000866, Acc: 0.8196, Grad norm: 0.08348890905484355\n",
      "Iteration 2909, BCE loss: 57.75753207925219, Acc: 0.8196, Grad norm: 0.10672032119658474\n",
      "Iteration 2910, BCE loss: 57.757754777910606, Acc: 0.8195, Grad norm: 0.14433107685885022\n",
      "Iteration 2911, BCE loss: 57.757536699273686, Acc: 0.8196, Grad norm: 0.10409166664431585\n",
      "Iteration 2912, BCE loss: 57.75752546676297, Acc: 0.8196, Grad norm: 0.10349002062690123\n",
      "Iteration 2913, BCE loss: 57.757667313632965, Acc: 0.8196, Grad norm: 0.13363946122964732\n",
      "Iteration 2914, BCE loss: 57.75758153473129, Acc: 0.8196, Grad norm: 0.11444996645659772\n",
      "Iteration 2915, BCE loss: 57.757675057620375, Acc: 0.8196, Grad norm: 0.12984729800883496\n",
      "Iteration 2916, BCE loss: 57.75752011313136, Acc: 0.8195, Grad norm: 0.09838988341828962\n",
      "Iteration 2917, BCE loss: 57.757568495154146, Acc: 0.8195, Grad norm: 0.1060122056961821\n",
      "Iteration 2918, BCE loss: 57.75752656678094, Acc: 0.8195, Grad norm: 0.09923056431780776\n",
      "Iteration 2919, BCE loss: 57.75760830762512, Acc: 0.8195, Grad norm: 0.11795898432820187\n",
      "Iteration 2920, BCE loss: 57.75776150158897, Acc: 0.8196, Grad norm: 0.1413493239222166\n",
      "Iteration 2921, BCE loss: 57.757557680638556, Acc: 0.8196, Grad norm: 0.10486885975093248\n",
      "Iteration 2922, BCE loss: 57.75746706593563, Acc: 0.8196, Grad norm: 0.08981848499579526\n",
      "Iteration 2923, BCE loss: 57.75762834518683, Acc: 0.8196, Grad norm: 0.11939209939174356\n",
      "Iteration 2924, BCE loss: 57.757869558384606, Acc: 0.8196, Grad norm: 0.16238215938542463\n",
      "Iteration 2925, BCE loss: 57.757969452580014, Acc: 0.8197, Grad norm: 0.17775117443543773\n",
      "Iteration 2926, BCE loss: 57.757876909400224, Acc: 0.8197, Grad norm: 0.16728901795273313\n",
      "Iteration 2927, BCE loss: 57.75752836506166, Acc: 0.8196, Grad norm: 0.10042330211542634\n",
      "Iteration 2928, BCE loss: 57.757430695009205, Acc: 0.8196, Grad norm: 0.07780669574738994\n",
      "Iteration 2929, BCE loss: 57.75751364655762, Acc: 0.8196, Grad norm: 0.10500678239943184\n",
      "Iteration 2930, BCE loss: 57.757501771932745, Acc: 0.8196, Grad norm: 0.10189007176695745\n",
      "Iteration 2931, BCE loss: 57.75742398275399, Acc: 0.8196, Grad norm: 0.07935963017859446\n",
      "Iteration 2932, BCE loss: 57.75744093062842, Acc: 0.8196, Grad norm: 0.0838243609234686\n",
      "Iteration 2933, BCE loss: 57.75763711504984, Acc: 0.8197, Grad norm: 0.1225494838626977\n",
      "Iteration 2934, BCE loss: 57.757689745990874, Acc: 0.8196, Grad norm: 0.13624605351948849\n",
      "Iteration 2935, BCE loss: 57.75751139235793, Acc: 0.8197, Grad norm: 0.1004661528536366\n",
      "Iteration 2936, BCE loss: 57.75748233209106, Acc: 0.8196, Grad norm: 0.09580756367424764\n",
      "Iteration 2937, BCE loss: 57.75751673433504, Acc: 0.8196, Grad norm: 0.10506574653057336\n",
      "Iteration 2938, BCE loss: 57.75762300204865, Acc: 0.8197, Grad norm: 0.12366882975494146\n",
      "Iteration 2939, BCE loss: 57.75754360409151, Acc: 0.8196, Grad norm: 0.1053564725756398\n",
      "Iteration 2940, BCE loss: 57.75749020215965, Acc: 0.8196, Grad norm: 0.09235744120171073\n",
      "Iteration 2941, BCE loss: 57.757491798028965, Acc: 0.8196, Grad norm: 0.09269364083983966\n",
      "Iteration 2942, BCE loss: 57.75739991406515, Acc: 0.8195, Grad norm: 0.07001998337557534\n",
      "Iteration 2943, BCE loss: 57.757587031012676, Acc: 0.8195, Grad norm: 0.1107742262917483\n",
      "Iteration 2944, BCE loss: 57.75746559777324, Acc: 0.8195, Grad norm: 0.0844312015049241\n",
      "Iteration 2945, BCE loss: 57.75765486227879, Acc: 0.8195, Grad norm: 0.12532165347427474\n",
      "Iteration 2946, BCE loss: 57.7576621734694, Acc: 0.8195, Grad norm: 0.128976394074296\n",
      "Iteration 2947, BCE loss: 57.75792457272216, Acc: 0.8195, Grad norm: 0.17124470712425344\n",
      "Iteration 2948, BCE loss: 57.757617923471415, Acc: 0.8196, Grad norm: 0.12447817239673185\n",
      "Iteration 2949, BCE loss: 57.75762852082498, Acc: 0.8196, Grad norm: 0.12452396211976394\n",
      "Iteration 2950, BCE loss: 57.75772495764694, Acc: 0.8197, Grad norm: 0.1401109144726891\n",
      "Iteration 2951, BCE loss: 57.75764073729427, Acc: 0.8197, Grad norm: 0.12596225468420816\n",
      "Iteration 2952, BCE loss: 57.757514520707446, Acc: 0.8196, Grad norm: 0.10041089569296806\n",
      "Iteration 2953, BCE loss: 57.75745562639452, Acc: 0.8196, Grad norm: 0.0864307845814306\n",
      "Iteration 2954, BCE loss: 57.75742574671385, Acc: 0.8196, Grad norm: 0.07718553392556804\n",
      "Iteration 2955, BCE loss: 57.757417678179095, Acc: 0.8196, Grad norm: 0.07273884785541422\n",
      "Iteration 2956, BCE loss: 57.75749460641921, Acc: 0.8196, Grad norm: 0.092861369112851\n",
      "Iteration 2957, BCE loss: 57.75749504458014, Acc: 0.8196, Grad norm: 0.09501054858329236\n",
      "Iteration 2958, BCE loss: 57.75757758706457, Acc: 0.8196, Grad norm: 0.10857773512581262\n",
      "Iteration 2959, BCE loss: 57.75750394005826, Acc: 0.8196, Grad norm: 0.09994251128903008\n",
      "Iteration 2960, BCE loss: 57.75753374580617, Acc: 0.8196, Grad norm: 0.10316330293567273\n",
      "Iteration 2961, BCE loss: 57.757560150561915, Acc: 0.8196, Grad norm: 0.10674795401221894\n",
      "Iteration 2962, BCE loss: 57.7575998927617, Acc: 0.8196, Grad norm: 0.11448045922548677\n",
      "Iteration 2963, BCE loss: 57.75765195942773, Acc: 0.8197, Grad norm: 0.11815642062491447\n",
      "Iteration 2964, BCE loss: 57.75772606389462, Acc: 0.8197, Grad norm: 0.13188050748462363\n",
      "Iteration 2965, BCE loss: 57.757568321062706, Acc: 0.8196, Grad norm: 0.09954374641280865\n",
      "Iteration 2966, BCE loss: 57.75757042814736, Acc: 0.8197, Grad norm: 0.10435097518021207\n",
      "Iteration 2967, BCE loss: 57.75749572398189, Acc: 0.8196, Grad norm: 0.08603182886230769\n",
      "Iteration 2968, BCE loss: 57.75774311329044, Acc: 0.8196, Grad norm: 0.13706316215386358\n",
      "Iteration 2969, BCE loss: 57.757567855509365, Acc: 0.8196, Grad norm: 0.10656911957941495\n",
      "Iteration 2970, BCE loss: 57.75747518903201, Acc: 0.8196, Grad norm: 0.0843880365263745\n",
      "Iteration 2971, BCE loss: 57.75758531610407, Acc: 0.8196, Grad norm: 0.10853295414391566\n",
      "Iteration 2972, BCE loss: 57.75742339573296, Acc: 0.8196, Grad norm: 0.07863063460534828\n",
      "Iteration 2973, BCE loss: 57.75747296500636, Acc: 0.8197, Grad norm: 0.09007380927747119\n",
      "Iteration 2974, BCE loss: 57.75754562300885, Acc: 0.8196, Grad norm: 0.10959605740279585\n",
      "Iteration 2975, BCE loss: 57.75739987421183, Acc: 0.8196, Grad norm: 0.07254054012524638\n",
      "Iteration 2976, BCE loss: 57.7575124903433, Acc: 0.8196, Grad norm: 0.10448508093606355\n",
      "Iteration 2977, BCE loss: 57.75749071251393, Acc: 0.8196, Grad norm: 0.09609444971263499\n",
      "Iteration 2978, BCE loss: 57.757460098603666, Acc: 0.8197, Grad norm: 0.08802622144717001\n",
      "Iteration 2979, BCE loss: 57.75780292708363, Acc: 0.8197, Grad norm: 0.1506122801898437\n",
      "Iteration 2980, BCE loss: 57.75801746345091, Acc: 0.8197, Grad norm: 0.18171854984786262\n",
      "Iteration 2981, BCE loss: 57.758267200791735, Acc: 0.8197, Grad norm: 0.21310208972015188\n",
      "Iteration 2982, BCE loss: 57.75763416100159, Acc: 0.8196, Grad norm: 0.12731816418317818\n",
      "Iteration 2983, BCE loss: 57.75746129523286, Acc: 0.8196, Grad norm: 0.08986727507690383\n",
      "Iteration 2984, BCE loss: 57.75740301832862, Acc: 0.8196, Grad norm: 0.0752371644685515\n",
      "Iteration 2985, BCE loss: 57.75752982250757, Acc: 0.8197, Grad norm: 0.10392395601987696\n",
      "Iteration 2986, BCE loss: 57.75765772938703, Acc: 0.8197, Grad norm: 0.12614050876315988\n",
      "Iteration 2987, BCE loss: 57.757810076586665, Acc: 0.8197, Grad norm: 0.1509083149987049\n",
      "Iteration 2988, BCE loss: 57.757485025335, Acc: 0.8197, Grad norm: 0.0959934978705972\n",
      "Iteration 2989, BCE loss: 57.75751851768143, Acc: 0.8196, Grad norm: 0.10650923071528651\n",
      "Iteration 2990, BCE loss: 57.757478600019866, Acc: 0.8196, Grad norm: 0.09767700331174979\n",
      "Iteration 2991, BCE loss: 57.75764245765582, Acc: 0.8196, Grad norm: 0.13114450701742372\n",
      "Iteration 2992, BCE loss: 57.75755305873152, Acc: 0.8196, Grad norm: 0.11162057312431134\n",
      "Iteration 2993, BCE loss: 57.75748909371251, Acc: 0.8196, Grad norm: 0.09781691735307348\n",
      "Iteration 2994, BCE loss: 57.757721004139476, Acc: 0.8196, Grad norm: 0.14243769718226373\n",
      "Iteration 2995, BCE loss: 57.75741023748944, Acc: 0.8196, Grad norm: 0.07411245794284374\n",
      "Iteration 2996, BCE loss: 57.75735670103742, Acc: 0.8196, Grad norm: 0.05646454668942038\n",
      "Iteration 2997, BCE loss: 57.75756312896138, Acc: 0.8196, Grad norm: 0.11431064115700303\n",
      "Iteration 2998, BCE loss: 57.75752701906984, Acc: 0.8196, Grad norm: 0.10337963990385397\n",
      "Iteration 2999, BCE loss: 57.757492648498676, Acc: 0.8196, Grad norm: 0.09513003238079479\n",
      "Iteration 3000, BCE loss: 57.75767619133785, Acc: 0.8195, Grad norm: 0.1312406575647666\n",
      "Iteration 3001, BCE loss: 57.75750163391418, Acc: 0.8195, Grad norm: 0.09575293943887389\n",
      "Iteration 3002, BCE loss: 57.75757572298502, Acc: 0.8195, Grad norm: 0.11339854805309131\n",
      "Iteration 3003, BCE loss: 57.75764096476901, Acc: 0.8195, Grad norm: 0.1213088611693968\n",
      "Iteration 3004, BCE loss: 57.7576587954765, Acc: 0.8194, Grad norm: 0.12837414966787666\n",
      "Iteration 3005, BCE loss: 57.75747830943461, Acc: 0.8195, Grad norm: 0.09690918194110101\n",
      "Iteration 3006, BCE loss: 57.75754912865892, Acc: 0.8195, Grad norm: 0.10895518514961314\n",
      "Iteration 3007, BCE loss: 57.757550753943946, Acc: 0.8195, Grad norm: 0.1156544452362083\n",
      "Iteration 3008, BCE loss: 57.757324793107195, Acc: 0.8196, Grad norm: 0.04978196759556659\n",
      "Iteration 3009, BCE loss: 57.75740802314949, Acc: 0.8196, Grad norm: 0.07844366055955483\n",
      "Iteration 3010, BCE loss: 57.757435733904416, Acc: 0.8196, Grad norm: 0.08024215290054403\n",
      "Iteration 3011, BCE loss: 57.757373698355195, Acc: 0.8196, Grad norm: 0.06308714847682431\n",
      "Iteration 3012, BCE loss: 57.757427393336314, Acc: 0.8196, Grad norm: 0.08124553171962708\n",
      "Iteration 3013, BCE loss: 57.75744911716654, Acc: 0.8196, Grad norm: 0.08651434060557044\n",
      "Iteration 3014, BCE loss: 57.75737851428309, Acc: 0.8196, Grad norm: 0.06508864200172233\n",
      "Iteration 3015, BCE loss: 57.75766014625748, Acc: 0.8197, Grad norm: 0.1323052470768904\n",
      "Iteration 3016, BCE loss: 57.75749587304011, Acc: 0.8197, Grad norm: 0.1003523688491587\n",
      "Iteration 3017, BCE loss: 57.75740546241681, Acc: 0.8196, Grad norm: 0.0768019578090263\n",
      "Iteration 3018, BCE loss: 57.757349080783705, Acc: 0.8196, Grad norm: 0.05032497370676797\n",
      "Iteration 3019, BCE loss: 57.757544904616054, Acc: 0.8196, Grad norm: 0.1052873989866487\n",
      "Iteration 3020, BCE loss: 57.757509738803165, Acc: 0.8196, Grad norm: 0.10158132433995758\n",
      "Iteration 3021, BCE loss: 57.7575856171489, Acc: 0.8196, Grad norm: 0.11422766460229736\n",
      "Iteration 3022, BCE loss: 57.757473163696176, Acc: 0.8196, Grad norm: 0.09300723231199623\n",
      "Iteration 3023, BCE loss: 57.75750028793584, Acc: 0.8196, Grad norm: 0.09819274131823985\n",
      "Iteration 3024, BCE loss: 57.75758784145282, Acc: 0.8195, Grad norm: 0.11244975485282774\n",
      "Iteration 3025, BCE loss: 57.75744378279926, Acc: 0.8195, Grad norm: 0.084251176851457\n",
      "Iteration 3026, BCE loss: 57.757578170685434, Acc: 0.8196, Grad norm: 0.11093399946821869\n",
      "Iteration 3027, BCE loss: 57.757487754118486, Acc: 0.8196, Grad norm: 0.09161847799770421\n",
      "Iteration 3028, BCE loss: 57.757694787308914, Acc: 0.8196, Grad norm: 0.13144433169299402\n",
      "Iteration 3029, BCE loss: 57.7573998707616, Acc: 0.8196, Grad norm: 0.07241222627499388\n",
      "Iteration 3030, BCE loss: 57.75735449280999, Acc: 0.8196, Grad norm: 0.05703120549319016\n",
      "Iteration 3031, BCE loss: 57.757768852945276, Acc: 0.8195, Grad norm: 0.15222810766737274\n",
      "Iteration 3032, BCE loss: 57.75794780099083, Acc: 0.8195, Grad norm: 0.1759189597888682\n",
      "Iteration 3033, BCE loss: 57.75783005935532, Acc: 0.8196, Grad norm: 0.15898358114099406\n",
      "Iteration 3034, BCE loss: 57.75783633027115, Acc: 0.8196, Grad norm: 0.15522931079486033\n",
      "Iteration 3035, BCE loss: 57.75760162477985, Acc: 0.8196, Grad norm: 0.11491278796768034\n",
      "Iteration 3036, BCE loss: 57.75768187976614, Acc: 0.8196, Grad norm: 0.1293325576709009\n",
      "Iteration 3037, BCE loss: 57.75754123865951, Acc: 0.8196, Grad norm: 0.10237284022984779\n",
      "Iteration 3038, BCE loss: 57.757465746989496, Acc: 0.8196, Grad norm: 0.09193348097544302\n",
      "Iteration 3039, BCE loss: 57.75735816116909, Acc: 0.8196, Grad norm: 0.061692744815206424\n",
      "Iteration 3040, BCE loss: 57.75735340617968, Acc: 0.8196, Grad norm: 0.06329492424087325\n",
      "Iteration 3041, BCE loss: 57.75734084621887, Acc: 0.8196, Grad norm: 0.05579898315417831\n",
      "Iteration 3042, BCE loss: 57.75734981191154, Acc: 0.8196, Grad norm: 0.06009488731843704\n",
      "Iteration 3043, BCE loss: 57.75742997748848, Acc: 0.8196, Grad norm: 0.08600470559180294\n",
      "Iteration 3044, BCE loss: 57.75748745138909, Acc: 0.8195, Grad norm: 0.09607633516896692\n",
      "Iteration 3045, BCE loss: 57.75751159641868, Acc: 0.8195, Grad norm: 0.0965926711872254\n",
      "Iteration 3046, BCE loss: 57.75760225440949, Acc: 0.8195, Grad norm: 0.1156926514009669\n",
      "Iteration 3047, BCE loss: 57.757456981435354, Acc: 0.8195, Grad norm: 0.08828031852945176\n",
      "Iteration 3048, BCE loss: 57.757545283796034, Acc: 0.8195, Grad norm: 0.10771280010326051\n",
      "Iteration 3049, BCE loss: 57.75749592797354, Acc: 0.8195, Grad norm: 0.09288902166160093\n",
      "Iteration 3050, BCE loss: 57.75737719360424, Acc: 0.8195, Grad norm: 0.06442875402050198\n",
      "Iteration 3051, BCE loss: 57.75738432351418, Acc: 0.8195, Grad norm: 0.06687292258996434\n",
      "Iteration 3052, BCE loss: 57.75737498620153, Acc: 0.8195, Grad norm: 0.06189862753789904\n",
      "Iteration 3053, BCE loss: 57.75750726879444, Acc: 0.8195, Grad norm: 0.09611657368675901\n",
      "Iteration 3054, BCE loss: 57.757584528176736, Acc: 0.8195, Grad norm: 0.11414055555025861\n",
      "Iteration 3055, BCE loss: 57.75754253089935, Acc: 0.8195, Grad norm: 0.10538712480620809\n",
      "Iteration 3056, BCE loss: 57.75777873341803, Acc: 0.8195, Grad norm: 0.14737836367608265\n",
      "Iteration 3057, BCE loss: 57.7574555126876, Acc: 0.8195, Grad norm: 0.08627895829047004\n",
      "Iteration 3058, BCE loss: 57.75752032934184, Acc: 0.8195, Grad norm: 0.09774332680241471\n",
      "Iteration 3059, BCE loss: 57.75763183011793, Acc: 0.8196, Grad norm: 0.12127798441691633\n",
      "Iteration 3060, BCE loss: 57.75747939071455, Acc: 0.8195, Grad norm: 0.09650647740025123\n",
      "Iteration 3061, BCE loss: 57.75755166180458, Acc: 0.8195, Grad norm: 0.11062647379177447\n",
      "Iteration 3062, BCE loss: 57.757514418910674, Acc: 0.8195, Grad norm: 0.10468899266268293\n",
      "Iteration 3063, BCE loss: 57.75758960109297, Acc: 0.8196, Grad norm: 0.11900443397397018\n",
      "Iteration 3064, BCE loss: 57.757685316868105, Acc: 0.8196, Grad norm: 0.13060317514382583\n",
      "Iteration 3065, BCE loss: 57.7576258364843, Acc: 0.8196, Grad norm: 0.12373601141300337\n",
      "Iteration 3066, BCE loss: 57.75753891740729, Acc: 0.8196, Grad norm: 0.10897943843410601\n",
      "Iteration 3067, BCE loss: 57.75746383347651, Acc: 0.8196, Grad norm: 0.09087783070720278\n",
      "Iteration 3068, BCE loss: 57.75763171713133, Acc: 0.8196, Grad norm: 0.12277623191407387\n",
      "Iteration 3069, BCE loss: 57.7575501871682, Acc: 0.8196, Grad norm: 0.10826135182159667\n",
      "Iteration 3070, BCE loss: 57.75756790881184, Acc: 0.8196, Grad norm: 0.11264526877125014\n",
      "Iteration 3071, BCE loss: 57.75762972015003, Acc: 0.8195, Grad norm: 0.12122764640354497\n",
      "Iteration 3072, BCE loss: 57.757672922768364, Acc: 0.8196, Grad norm: 0.12887764591106474\n",
      "Iteration 3073, BCE loss: 57.75772790026488, Acc: 0.8196, Grad norm: 0.14060651229050317\n",
      "Iteration 3074, BCE loss: 57.75759138184361, Acc: 0.8195, Grad norm: 0.1143385372268552\n",
      "Iteration 3075, BCE loss: 57.757624654141395, Acc: 0.8195, Grad norm: 0.12392074430625784\n",
      "Iteration 3076, BCE loss: 57.75757194903157, Acc: 0.8195, Grad norm: 0.11233876311470518\n",
      "Iteration 3077, BCE loss: 57.75750315856837, Acc: 0.8196, Grad norm: 0.09200993005240908\n",
      "Iteration 3078, BCE loss: 57.757472563059785, Acc: 0.8196, Grad norm: 0.0873619248340361\n",
      "Iteration 3079, BCE loss: 57.757431681938684, Acc: 0.8196, Grad norm: 0.08051253579190883\n",
      "Iteration 3080, BCE loss: 57.757458051289234, Acc: 0.8196, Grad norm: 0.09045903988372425\n",
      "Iteration 3081, BCE loss: 57.75742597509563, Acc: 0.8195, Grad norm: 0.08451657782595245\n",
      "Iteration 3082, BCE loss: 57.757425528850696, Acc: 0.8195, Grad norm: 0.07811101760805945\n",
      "Iteration 3083, BCE loss: 57.757454371937435, Acc: 0.8195, Grad norm: 0.0858338281212788\n",
      "Iteration 3084, BCE loss: 57.75750175802907, Acc: 0.8195, Grad norm: 0.10272774869951015\n",
      "Iteration 3085, BCE loss: 57.757489046260424, Acc: 0.8195, Grad norm: 0.10035599906781306\n",
      "Iteration 3086, BCE loss: 57.75739488493494, Acc: 0.8195, Grad norm: 0.07716259471816306\n",
      "Iteration 3087, BCE loss: 57.75736668275892, Acc: 0.8195, Grad norm: 0.06652144841344862\n",
      "Iteration 3088, BCE loss: 57.757495282882765, Acc: 0.8196, Grad norm: 0.09844272488820287\n",
      "Iteration 3089, BCE loss: 57.75737202710124, Acc: 0.8195, Grad norm: 0.06658182544731041\n",
      "Iteration 3090, BCE loss: 57.757343350761104, Acc: 0.8195, Grad norm: 0.0582231545088985\n",
      "Iteration 3091, BCE loss: 57.757317968892835, Acc: 0.8195, Grad norm: 0.043889965494911864\n",
      "Iteration 3092, BCE loss: 57.75743833663728, Acc: 0.8196, Grad norm: 0.07940925631375347\n",
      "Iteration 3093, BCE loss: 57.75754570806712, Acc: 0.8195, Grad norm: 0.10358984230906437\n",
      "Iteration 3094, BCE loss: 57.757593638879136, Acc: 0.8195, Grad norm: 0.1122707012408595\n",
      "Iteration 3095, BCE loss: 57.7578635183474, Acc: 0.8195, Grad norm: 0.1617890244082084\n",
      "Iteration 3096, BCE loss: 57.75754885827689, Acc: 0.8195, Grad norm: 0.10870235200817135\n",
      "Iteration 3097, BCE loss: 57.75804998866266, Acc: 0.8195, Grad norm: 0.1886707336585081\n",
      "Iteration 3098, BCE loss: 57.75811303944512, Acc: 0.8195, Grad norm: 0.19713766683671943\n",
      "Iteration 3099, BCE loss: 57.75825939046658, Acc: 0.8195, Grad norm: 0.21051559016338972\n",
      "Iteration 3100, BCE loss: 57.758007292937506, Acc: 0.8195, Grad norm: 0.18101776995315944\n",
      "Iteration 3101, BCE loss: 57.757952874694475, Acc: 0.8195, Grad norm: 0.1703804486422216\n",
      "Iteration 3102, BCE loss: 57.757885557106555, Acc: 0.8195, Grad norm: 0.16167362715830577\n",
      "Iteration 3103, BCE loss: 57.75793026961202, Acc: 0.8195, Grad norm: 0.16361712698983705\n",
      "Iteration 3104, BCE loss: 57.75765752802309, Acc: 0.8195, Grad norm: 0.12553496709306536\n",
      "Iteration 3105, BCE loss: 57.75778822377844, Acc: 0.8195, Grad norm: 0.14354471851647885\n",
      "Iteration 3106, BCE loss: 57.75767959992319, Acc: 0.8195, Grad norm: 0.12903000333942638\n",
      "Iteration 3107, BCE loss: 57.75756460084273, Acc: 0.8196, Grad norm: 0.10787347499345211\n",
      "Iteration 3108, BCE loss: 57.757540560435864, Acc: 0.8196, Grad norm: 0.10104613885638175\n",
      "Iteration 3109, BCE loss: 57.7575356437541, Acc: 0.8195, Grad norm: 0.10081115758890209\n",
      "Iteration 3110, BCE loss: 57.75761434518813, Acc: 0.8196, Grad norm: 0.1212391395088088\n",
      "Iteration 3111, BCE loss: 57.757550749869026, Acc: 0.8196, Grad norm: 0.10764422064408519\n",
      "Iteration 3112, BCE loss: 57.75745171000361, Acc: 0.8195, Grad norm: 0.08236596611067082\n",
      "Iteration 3113, BCE loss: 57.757555920093864, Acc: 0.8195, Grad norm: 0.10268525767368776\n",
      "Iteration 3114, BCE loss: 57.75766162378159, Acc: 0.8195, Grad norm: 0.12511381738145888\n",
      "Iteration 3115, BCE loss: 57.75775890992976, Acc: 0.8196, Grad norm: 0.14173299914635049\n",
      "Iteration 3116, BCE loss: 57.75779182777927, Acc: 0.8196, Grad norm: 0.14765974264103\n",
      "Iteration 3117, BCE loss: 57.75785543483823, Acc: 0.8196, Grad norm: 0.1513819609814726\n",
      "Iteration 3118, BCE loss: 57.7577452498725, Acc: 0.8196, Grad norm: 0.1324176721418518\n",
      "Iteration 3119, BCE loss: 57.75786344859344, Acc: 0.8196, Grad norm: 0.14888844664379022\n",
      "Iteration 3120, BCE loss: 57.75768342606996, Acc: 0.8196, Grad norm: 0.12118734974166383\n",
      "Iteration 3121, BCE loss: 57.75793818804076, Acc: 0.8196, Grad norm: 0.15780603425993706\n",
      "Iteration 3122, BCE loss: 57.75799777901307, Acc: 0.8196, Grad norm: 0.16110999425936715\n",
      "Iteration 3123, BCE loss: 57.7578556793696, Acc: 0.8195, Grad norm: 0.144682781279234\n",
      "Iteration 3124, BCE loss: 57.75760151297002, Acc: 0.8195, Grad norm: 0.10575773465174067\n",
      "Iteration 3125, BCE loss: 57.75758536047732, Acc: 0.8196, Grad norm: 0.1059319174713204\n",
      "Iteration 3126, BCE loss: 57.757657472034666, Acc: 0.8195, Grad norm: 0.11611031004653152\n",
      "Iteration 3127, BCE loss: 57.75760292560126, Acc: 0.8195, Grad norm: 0.10686596270278781\n",
      "Iteration 3128, BCE loss: 57.757612417219065, Acc: 0.8195, Grad norm: 0.10957935054274565\n",
      "Iteration 3129, BCE loss: 57.757800159003935, Acc: 0.8195, Grad norm: 0.14212316350456444\n",
      "Iteration 3130, BCE loss: 57.757757599988885, Acc: 0.8195, Grad norm: 0.1338957452928645\n",
      "Iteration 3131, BCE loss: 57.75756242573732, Acc: 0.8196, Grad norm: 0.1068608287880106\n",
      "Iteration 3132, BCE loss: 57.75742214107019, Acc: 0.8196, Grad norm: 0.07655069364604773\n",
      "Iteration 3133, BCE loss: 57.75749017213862, Acc: 0.8196, Grad norm: 0.09935078004443607\n",
      "Iteration 3134, BCE loss: 57.75734508933924, Acc: 0.8196, Grad norm: 0.056495363167171325\n",
      "Iteration 3135, BCE loss: 57.75735161329689, Acc: 0.8196, Grad norm: 0.05564890184647743\n",
      "Iteration 3136, BCE loss: 57.75730588167773, Acc: 0.8196, Grad norm: 0.03636898864055989\n",
      "Iteration 3137, BCE loss: 57.757507564447764, Acc: 0.8196, Grad norm: 0.10482686994184466\n",
      "Iteration 3138, BCE loss: 57.75761012347344, Acc: 0.8196, Grad norm: 0.12400647262632777\n",
      "Iteration 3139, BCE loss: 57.75761568198041, Acc: 0.8196, Grad norm: 0.12615718725809738\n",
      "Iteration 3140, BCE loss: 57.75748892529501, Acc: 0.8196, Grad norm: 0.0949736085453437\n",
      "Iteration 3141, BCE loss: 57.757541237091175, Acc: 0.8196, Grad norm: 0.10321359168907675\n",
      "Iteration 3142, BCE loss: 57.75750843391822, Acc: 0.8196, Grad norm: 0.10047728838508883\n",
      "Iteration 3143, BCE loss: 57.757759167339174, Acc: 0.8196, Grad norm: 0.1417310162698136\n",
      "Iteration 3144, BCE loss: 57.75765372386513, Acc: 0.8196, Grad norm: 0.1307394998095189\n",
      "Iteration 3145, BCE loss: 57.757475318059306, Acc: 0.8195, Grad norm: 0.09023278006776926\n",
      "Iteration 3146, BCE loss: 57.7576913849139, Acc: 0.8195, Grad norm: 0.13196843846067446\n",
      "Iteration 3147, BCE loss: 57.757703096082594, Acc: 0.8195, Grad norm: 0.13141365589455628\n",
      "Iteration 3148, BCE loss: 57.75789805111789, Acc: 0.8196, Grad norm: 0.16366041988797728\n",
      "Iteration 3149, BCE loss: 57.7578401411848, Acc: 0.8196, Grad norm: 0.15530088291429883\n",
      "Iteration 3150, BCE loss: 57.75809231380126, Acc: 0.8195, Grad norm: 0.18626314303835925\n",
      "Iteration 3151, BCE loss: 57.75790295611466, Acc: 0.8195, Grad norm: 0.16315603782763138\n",
      "Iteration 3152, BCE loss: 57.7581515412286, Acc: 0.8195, Grad norm: 0.19387177805516073\n",
      "Iteration 3153, BCE loss: 57.757895060047474, Acc: 0.8195, Grad norm: 0.16247293510071625\n",
      "Iteration 3154, BCE loss: 57.75777164458145, Acc: 0.8196, Grad norm: 0.14694400623163356\n",
      "Iteration 3155, BCE loss: 57.757986639491804, Acc: 0.8195, Grad norm: 0.1727177123986677\n",
      "Iteration 3156, BCE loss: 57.75801839557059, Acc: 0.8195, Grad norm: 0.17548159413409792\n",
      "Iteration 3157, BCE loss: 57.7579089549388, Acc: 0.8195, Grad norm: 0.16017029316922776\n",
      "Iteration 3158, BCE loss: 57.75789209404712, Acc: 0.8195, Grad norm: 0.15808509518912192\n",
      "Iteration 3159, BCE loss: 57.75788448628373, Acc: 0.8196, Grad norm: 0.1608896618114006\n",
      "Iteration 3160, BCE loss: 57.75787336789051, Acc: 0.8196, Grad norm: 0.16238084064630626\n",
      "Iteration 3161, BCE loss: 57.75755293716574, Acc: 0.8196, Grad norm: 0.11158710300063703\n",
      "Iteration 3162, BCE loss: 57.75752737609896, Acc: 0.8196, Grad norm: 0.10298746468219963\n",
      "Iteration 3163, BCE loss: 57.7574780550661, Acc: 0.8196, Grad norm: 0.09101925587189956\n",
      "Iteration 3164, BCE loss: 57.75760801942464, Acc: 0.8195, Grad norm: 0.11371330869040182\n",
      "Iteration 3165, BCE loss: 57.75759569294998, Acc: 0.8195, Grad norm: 0.11644581956440805\n",
      "Iteration 3166, BCE loss: 57.75755297621581, Acc: 0.8196, Grad norm: 0.11128706368286932\n",
      "Iteration 3167, BCE loss: 57.7574336655074, Acc: 0.8196, Grad norm: 0.08602466953097718\n",
      "Iteration 3168, BCE loss: 57.75766346904587, Acc: 0.8196, Grad norm: 0.134574487778353\n",
      "Iteration 3169, BCE loss: 57.757748389334616, Acc: 0.8197, Grad norm: 0.14580865945341012\n",
      "Iteration 3170, BCE loss: 57.757798585239115, Acc: 0.8197, Grad norm: 0.15340432069932053\n",
      "Iteration 3171, BCE loss: 57.757516930529334, Acc: 0.8196, Grad norm: 0.10037144652278367\n",
      "Iteration 3172, BCE loss: 57.75762681229838, Acc: 0.8196, Grad norm: 0.12790485983972918\n",
      "Iteration 3173, BCE loss: 57.75746110888146, Acc: 0.8196, Grad norm: 0.0943684841883278\n",
      "Iteration 3174, BCE loss: 57.75745752029924, Acc: 0.8196, Grad norm: 0.09317585958080582\n",
      "Iteration 3175, BCE loss: 57.757655648931255, Acc: 0.8196, Grad norm: 0.1347450184646849\n",
      "Iteration 3176, BCE loss: 57.75785453563981, Acc: 0.8197, Grad norm: 0.16536989238072106\n",
      "Iteration 3177, BCE loss: 57.757694847853955, Acc: 0.8196, Grad norm: 0.1394823026492174\n",
      "Iteration 3178, BCE loss: 57.75776898193725, Acc: 0.8196, Grad norm: 0.14951488289276504\n",
      "Iteration 3179, BCE loss: 57.75786270986301, Acc: 0.8196, Grad norm: 0.16453110545301602\n",
      "Iteration 3180, BCE loss: 57.75790464441354, Acc: 0.8196, Grad norm: 0.172500226595061\n",
      "Iteration 3181, BCE loss: 57.75777932121022, Acc: 0.8197, Grad norm: 0.15434875638700915\n",
      "Iteration 3182, BCE loss: 57.75792797589191, Acc: 0.8197, Grad norm: 0.17384737210111617\n",
      "Iteration 3183, BCE loss: 57.757902101220665, Acc: 0.8197, Grad norm: 0.16925607795701217\n",
      "Iteration 3184, BCE loss: 57.75813413142272, Acc: 0.8197, Grad norm: 0.19865576415900635\n",
      "Iteration 3185, BCE loss: 57.757997227204505, Acc: 0.8197, Grad norm: 0.17842971843060038\n",
      "Iteration 3186, BCE loss: 57.758263453947805, Acc: 0.8197, Grad norm: 0.21173826266185264\n",
      "Iteration 3187, BCE loss: 57.75787494871446, Acc: 0.8197, Grad norm: 0.1613154457971224\n",
      "Iteration 3188, BCE loss: 57.75780879021455, Acc: 0.8197, Grad norm: 0.14824891466787507\n",
      "Iteration 3189, BCE loss: 57.75770016128849, Acc: 0.8196, Grad norm: 0.13319061756508072\n",
      "Iteration 3190, BCE loss: 57.75796467941623, Acc: 0.8197, Grad norm: 0.16736708081590082\n",
      "Iteration 3191, BCE loss: 57.75769028647443, Acc: 0.8196, Grad norm: 0.12910088817289916\n",
      "Iteration 3192, BCE loss: 57.75778646475625, Acc: 0.8197, Grad norm: 0.14339556009713492\n",
      "Iteration 3193, BCE loss: 57.757659549714674, Acc: 0.8196, Grad norm: 0.12820690853374384\n",
      "Iteration 3194, BCE loss: 57.7578045430316, Acc: 0.8196, Grad norm: 0.15256459173919162\n",
      "Iteration 3195, BCE loss: 57.75793243116256, Acc: 0.8196, Grad norm: 0.17256811765025692\n",
      "Iteration 3196, BCE loss: 57.75817824544354, Acc: 0.8196, Grad norm: 0.20331688965382358\n",
      "Iteration 3197, BCE loss: 57.75831366156682, Acc: 0.8196, Grad norm: 0.21732221102705396\n",
      "Iteration 3198, BCE loss: 57.75809780254366, Acc: 0.8197, Grad norm: 0.19069923708039277\n",
      "Iteration 3199, BCE loss: 57.757864280440955, Acc: 0.8197, Grad norm: 0.15791008936083964\n",
      "Iteration 3200, BCE loss: 57.75809838662463, Acc: 0.8197, Grad norm: 0.18675446804858867\n",
      "Iteration 3201, BCE loss: 57.757974257578795, Acc: 0.8197, Grad norm: 0.1696558558693648\n",
      "Iteration 3202, BCE loss: 57.75804302690487, Acc: 0.8197, Grad norm: 0.18462035209818617\n",
      "Iteration 3203, BCE loss: 57.7579276997397, Acc: 0.8197, Grad norm: 0.16938782103051309\n",
      "Iteration 3204, BCE loss: 57.75764651715738, Acc: 0.8197, Grad norm: 0.12995430233542057\n",
      "Iteration 3205, BCE loss: 57.75742505372038, Acc: 0.8196, Grad norm: 0.08026994013466594\n",
      "Iteration 3206, BCE loss: 57.75759320381506, Acc: 0.8196, Grad norm: 0.1185491963245629\n",
      "Iteration 3207, BCE loss: 57.757532422456265, Acc: 0.8196, Grad norm: 0.10653094619029609\n",
      "Iteration 3208, BCE loss: 57.75767913397378, Acc: 0.8195, Grad norm: 0.13459379297023058\n",
      "Iteration 3209, BCE loss: 57.7575269047678, Acc: 0.8195, Grad norm: 0.10181971441522107\n",
      "Iteration 3210, BCE loss: 57.757592762282, Acc: 0.8196, Grad norm: 0.11836382727988458\n",
      "Iteration 3211, BCE loss: 57.757591442463315, Acc: 0.8196, Grad norm: 0.11635156394443309\n",
      "Iteration 3212, BCE loss: 57.75769828328771, Acc: 0.8196, Grad norm: 0.13715536287133012\n",
      "Iteration 3213, BCE loss: 57.7576470417487, Acc: 0.8196, Grad norm: 0.12273520500513876\n",
      "Iteration 3214, BCE loss: 57.75778856811159, Acc: 0.8196, Grad norm: 0.14793695091459927\n",
      "Iteration 3215, BCE loss: 57.75767469482328, Acc: 0.8196, Grad norm: 0.1338814172190225\n",
      "Iteration 3216, BCE loss: 57.757605810433404, Acc: 0.8196, Grad norm: 0.1197584458354917\n",
      "Iteration 3217, BCE loss: 57.757662295862765, Acc: 0.8196, Grad norm: 0.13288117273479386\n",
      "Iteration 3218, BCE loss: 57.75759790949421, Acc: 0.8196, Grad norm: 0.12267320463713668\n",
      "Iteration 3219, BCE loss: 57.757573912718144, Acc: 0.8196, Grad norm: 0.11755905372169402\n",
      "Iteration 3220, BCE loss: 57.75740963793825, Acc: 0.8196, Grad norm: 0.07947433775942862\n",
      "Iteration 3221, BCE loss: 57.7573971651925, Acc: 0.8196, Grad norm: 0.0766456529275064\n",
      "Iteration 3222, BCE loss: 57.757451154429376, Acc: 0.8196, Grad norm: 0.08972027697953876\n",
      "Iteration 3223, BCE loss: 57.75734455307659, Acc: 0.8196, Grad norm: 0.05805732923112412\n",
      "Iteration 3224, BCE loss: 57.75736612563141, Acc: 0.8196, Grad norm: 0.05864881762160075\n",
      "Iteration 3225, BCE loss: 57.75755104206751, Acc: 0.8196, Grad norm: 0.10850771412200541\n",
      "Iteration 3226, BCE loss: 57.757586951990035, Acc: 0.8196, Grad norm: 0.11865956976757126\n",
      "Iteration 3227, BCE loss: 57.75750313629388, Acc: 0.8196, Grad norm: 0.10140791190199389\n",
      "Iteration 3228, BCE loss: 57.757548537572276, Acc: 0.8196, Grad norm: 0.10825398228011124\n",
      "Iteration 3229, BCE loss: 57.75751695470595, Acc: 0.8196, Grad norm: 0.10153330985063136\n",
      "Iteration 3230, BCE loss: 57.757477754386066, Acc: 0.8196, Grad norm: 0.09208181191654828\n",
      "Iteration 3231, BCE loss: 57.75782622021087, Acc: 0.8196, Grad norm: 0.1591629452540569\n",
      "Iteration 3232, BCE loss: 57.757736025180506, Acc: 0.8196, Grad norm: 0.1450945275734178\n",
      "Iteration 3233, BCE loss: 57.7578110996478, Acc: 0.8196, Grad norm: 0.15481779434840784\n",
      "Iteration 3234, BCE loss: 57.75797366732071, Acc: 0.8196, Grad norm: 0.1755296092364617\n",
      "Iteration 3235, BCE loss: 57.75768895950418, Acc: 0.8196, Grad norm: 0.1350948091175538\n",
      "Iteration 3236, BCE loss: 57.75760953289313, Acc: 0.8196, Grad norm: 0.12059304769682806\n",
      "Iteration 3237, BCE loss: 57.757497617511866, Acc: 0.8196, Grad norm: 0.09843761364481565\n",
      "Iteration 3238, BCE loss: 57.7575992343869, Acc: 0.8196, Grad norm: 0.12184426466334844\n",
      "Iteration 3239, BCE loss: 57.75755064876702, Acc: 0.8196, Grad norm: 0.10914450948548209\n",
      "Iteration 3240, BCE loss: 57.757814386921915, Acc: 0.8196, Grad norm: 0.1570053077730887\n",
      "Iteration 3241, BCE loss: 57.75783956163756, Acc: 0.8196, Grad norm: 0.16430352820263205\n",
      "Iteration 3242, BCE loss: 57.7579243139615, Acc: 0.8196, Grad norm: 0.1769366991452593\n",
      "Iteration 3243, BCE loss: 57.7580536668697, Acc: 0.8197, Grad norm: 0.1930758582091692\n",
      "Iteration 3244, BCE loss: 57.758050678281194, Acc: 0.8197, Grad norm: 0.1930697237504214\n",
      "Iteration 3245, BCE loss: 57.757742510579575, Acc: 0.8196, Grad norm: 0.1490778054728269\n",
      "Iteration 3246, BCE loss: 57.75765210099876, Acc: 0.8196, Grad norm: 0.13263136450324986\n",
      "Iteration 3247, BCE loss: 57.757394416138915, Acc: 0.8196, Grad norm: 0.07373058135581143\n",
      "Iteration 3248, BCE loss: 57.75736019385723, Acc: 0.8196, Grad norm: 0.05830474050332746\n",
      "Iteration 3249, BCE loss: 57.75741073946338, Acc: 0.8196, Grad norm: 0.07399455030099193\n",
      "Iteration 3250, BCE loss: 57.757449939599496, Acc: 0.8196, Grad norm: 0.08577235600201476\n",
      "Iteration 3251, BCE loss: 57.75746054534186, Acc: 0.8195, Grad norm: 0.08343427863567124\n",
      "Iteration 3252, BCE loss: 57.75745286039312, Acc: 0.8195, Grad norm: 0.0828652598005983\n",
      "Iteration 3253, BCE loss: 57.757366586491784, Acc: 0.8195, Grad norm: 0.06185691479124725\n",
      "Iteration 3254, BCE loss: 57.757489012982, Acc: 0.8195, Grad norm: 0.09612906257172779\n",
      "Iteration 3255, BCE loss: 57.75744053656719, Acc: 0.8195, Grad norm: 0.07801167808235515\n",
      "Iteration 3256, BCE loss: 57.757481671394096, Acc: 0.8195, Grad norm: 0.09049625556562428\n",
      "Iteration 3257, BCE loss: 57.75748932876644, Acc: 0.8195, Grad norm: 0.09185040468095414\n",
      "Iteration 3258, BCE loss: 57.7574347695552, Acc: 0.8196, Grad norm: 0.0785976572248162\n",
      "Iteration 3259, BCE loss: 57.75753655705709, Acc: 0.8196, Grad norm: 0.10063962597308955\n",
      "Iteration 3260, BCE loss: 57.7575081482143, Acc: 0.8196, Grad norm: 0.09442250966885796\n",
      "Iteration 3261, BCE loss: 57.757429154346255, Acc: 0.8196, Grad norm: 0.07707593812937644\n",
      "Iteration 3262, BCE loss: 57.7574307638861, Acc: 0.8195, Grad norm: 0.08267684889496348\n",
      "Iteration 3263, BCE loss: 57.75742318524026, Acc: 0.8195, Grad norm: 0.08019235037642698\n",
      "Iteration 3264, BCE loss: 57.75753346364877, Acc: 0.8195, Grad norm: 0.10564872620433703\n",
      "Iteration 3265, BCE loss: 57.75758993894212, Acc: 0.8195, Grad norm: 0.11734552880278723\n",
      "Iteration 3266, BCE loss: 57.757486722551626, Acc: 0.8195, Grad norm: 0.09696108206304707\n",
      "Iteration 3267, BCE loss: 57.75748091523046, Acc: 0.8195, Grad norm: 0.09535890130880458\n",
      "Iteration 3268, BCE loss: 57.75743706420795, Acc: 0.8195, Grad norm: 0.08134309580737696\n",
      "Iteration 3269, BCE loss: 57.7576511326151, Acc: 0.8195, Grad norm: 0.12940867895780264\n",
      "Iteration 3270, BCE loss: 57.75761810906256, Acc: 0.8195, Grad norm: 0.12466561611877498\n",
      "Iteration 3271, BCE loss: 57.757550672077876, Acc: 0.8195, Grad norm: 0.1133072651133866\n",
      "Iteration 3272, BCE loss: 57.75755150233543, Acc: 0.8195, Grad norm: 0.11208119843663047\n",
      "Iteration 3273, BCE loss: 57.75751577964658, Acc: 0.8195, Grad norm: 0.10106625883518891\n",
      "Iteration 3274, BCE loss: 57.75756525448343, Acc: 0.8195, Grad norm: 0.11272573547291205\n",
      "Iteration 3275, BCE loss: 57.75753865385283, Acc: 0.8195, Grad norm: 0.10460402805768855\n",
      "Iteration 3276, BCE loss: 57.75754315621649, Acc: 0.8195, Grad norm: 0.10847856632910506\n",
      "Iteration 3277, BCE loss: 57.75750905283874, Acc: 0.8195, Grad norm: 0.09994330592934066\n",
      "Iteration 3278, BCE loss: 57.75743696942605, Acc: 0.8195, Grad norm: 0.0790282555334974\n",
      "Iteration 3279, BCE loss: 57.757479863729586, Acc: 0.8195, Grad norm: 0.09233825732375026\n",
      "Iteration 3280, BCE loss: 57.7574480963665, Acc: 0.8195, Grad norm: 0.08839616081265077\n",
      "Iteration 3281, BCE loss: 57.75744457447058, Acc: 0.8195, Grad norm: 0.08753979870775087\n",
      "Iteration 3282, BCE loss: 57.75742298986518, Acc: 0.8195, Grad norm: 0.08350909870652866\n",
      "Iteration 3283, BCE loss: 57.757405926845564, Acc: 0.8196, Grad norm: 0.07744022469365505\n",
      "Iteration 3284, BCE loss: 57.757391693825696, Acc: 0.8196, Grad norm: 0.07435466378753833\n",
      "Iteration 3285, BCE loss: 57.7573205455687, Acc: 0.8196, Grad norm: 0.04251611070733092\n",
      "Iteration 3286, BCE loss: 57.757375451811725, Acc: 0.8196, Grad norm: 0.061393385527571065\n",
      "Iteration 3287, BCE loss: 57.75748541730465, Acc: 0.8196, Grad norm: 0.09289034053509938\n",
      "Iteration 3288, BCE loss: 57.75753570837419, Acc: 0.8195, Grad norm: 0.10715728721909445\n",
      "Iteration 3289, BCE loss: 57.75755806331705, Acc: 0.8196, Grad norm: 0.1094634662724585\n",
      "Iteration 3290, BCE loss: 57.75768360722964, Acc: 0.8195, Grad norm: 0.13467491603419682\n",
      "Iteration 3291, BCE loss: 57.75757101521124, Acc: 0.8196, Grad norm: 0.11217573790157057\n",
      "Iteration 3292, BCE loss: 57.757430291250174, Acc: 0.8196, Grad norm: 0.07966392544894865\n",
      "Iteration 3293, BCE loss: 57.75761214355201, Acc: 0.8196, Grad norm: 0.12386465600571095\n",
      "Iteration 3294, BCE loss: 57.75770730107257, Acc: 0.8196, Grad norm: 0.13859726044878853\n",
      "Iteration 3295, BCE loss: 57.75743430741805, Acc: 0.8196, Grad norm: 0.08454990617906034\n",
      "Iteration 3296, BCE loss: 57.757332973883734, Acc: 0.8196, Grad norm: 0.04849698237546252\n",
      "Iteration 3297, BCE loss: 57.757426325859484, Acc: 0.8196, Grad norm: 0.0750509523810398\n",
      "Iteration 3298, BCE loss: 57.757425267590065, Acc: 0.8196, Grad norm: 0.07661454434275544\n",
      "Iteration 3299, BCE loss: 57.75748915300254, Acc: 0.8196, Grad norm: 0.09259437181097463\n",
      "Iteration 3300, BCE loss: 57.757622728075, Acc: 0.8196, Grad norm: 0.11894666326870114\n",
      "Iteration 3301, BCE loss: 57.75767013974702, Acc: 0.8196, Grad norm: 0.12472651369498844\n",
      "Iteration 3302, BCE loss: 57.75781204226563, Acc: 0.8196, Grad norm: 0.14831932475404852\n",
      "Iteration 3303, BCE loss: 57.75766059889947, Acc: 0.8196, Grad norm: 0.12277825678799675\n",
      "Iteration 3304, BCE loss: 57.75763104249222, Acc: 0.8196, Grad norm: 0.11906132180496694\n",
      "Iteration 3305, BCE loss: 57.75755197921183, Acc: 0.8196, Grad norm: 0.10425522487043541\n",
      "Iteration 3306, BCE loss: 57.757434354452556, Acc: 0.8196, Grad norm: 0.07562751520400873\n",
      "Iteration 3307, BCE loss: 57.75739121675881, Acc: 0.8196, Grad norm: 0.06675176095770863\n",
      "Iteration 3308, BCE loss: 57.75737052263851, Acc: 0.8196, Grad norm: 0.06509377486030593\n",
      "Iteration 3309, BCE loss: 57.75755341828426, Acc: 0.8195, Grad norm: 0.1112244712385806\n",
      "Iteration 3310, BCE loss: 57.75757697214868, Acc: 0.8195, Grad norm: 0.11666937718322454\n",
      "Iteration 3311, BCE loss: 57.75771105007922, Acc: 0.8195, Grad norm: 0.13709710259086846\n",
      "Iteration 3312, BCE loss: 57.75759693896691, Acc: 0.8195, Grad norm: 0.11451790126539074\n",
      "Iteration 3313, BCE loss: 57.757517944615586, Acc: 0.8195, Grad norm: 0.09676246135112211\n",
      "Iteration 3314, BCE loss: 57.75763578012733, Acc: 0.8195, Grad norm: 0.12130529230781871\n",
      "Iteration 3315, BCE loss: 57.75759300766795, Acc: 0.8195, Grad norm: 0.1189245559975599\n",
      "Iteration 3316, BCE loss: 57.75753562036173, Acc: 0.8195, Grad norm: 0.10967182572000023\n",
      "Iteration 3317, BCE loss: 57.757472824646, Acc: 0.8195, Grad norm: 0.09398390636979582\n",
      "Iteration 3318, BCE loss: 57.75746029197978, Acc: 0.8195, Grad norm: 0.09290262275145883\n",
      "Iteration 3319, BCE loss: 57.75761985135325, Acc: 0.8195, Grad norm: 0.12469792652444672\n",
      "Iteration 3320, BCE loss: 57.7577882526579, Acc: 0.8195, Grad norm: 0.1537823487042898\n",
      "Iteration 3321, BCE loss: 57.75763612967246, Acc: 0.8196, Grad norm: 0.12753207568530472\n",
      "Iteration 3322, BCE loss: 57.757607241415414, Acc: 0.8195, Grad norm: 0.11968100879346827\n",
      "Iteration 3323, BCE loss: 57.75747384535727, Acc: 0.8195, Grad norm: 0.09459684269562937\n",
      "Iteration 3324, BCE loss: 57.75759846078482, Acc: 0.8195, Grad norm: 0.11706653172922436\n",
      "Iteration 3325, BCE loss: 57.757388146386106, Acc: 0.8195, Grad norm: 0.0683038751627625\n",
      "Iteration 3326, BCE loss: 57.7573966324953, Acc: 0.8196, Grad norm: 0.06898379069681836\n",
      "Iteration 3327, BCE loss: 57.75746761231517, Acc: 0.8195, Grad norm: 0.08591735662493283\n",
      "Iteration 3328, BCE loss: 57.7575385316697, Acc: 0.8195, Grad norm: 0.10024084777575598\n",
      "Iteration 3329, BCE loss: 57.757551771712, Acc: 0.8196, Grad norm: 0.10218485252828369\n",
      "Iteration 3330, BCE loss: 57.75753414986879, Acc: 0.8195, Grad norm: 0.09547474883075663\n",
      "Iteration 3331, BCE loss: 57.75751207759838, Acc: 0.8195, Grad norm: 0.0917014062375652\n",
      "Iteration 3332, BCE loss: 57.75756298593295, Acc: 0.8195, Grad norm: 0.10428929717869258\n",
      "Iteration 3333, BCE loss: 57.7574486464074, Acc: 0.8195, Grad norm: 0.0787830068344592\n",
      "Iteration 3334, BCE loss: 57.75742087244355, Acc: 0.8195, Grad norm: 0.07184752963290793\n",
      "Iteration 3335, BCE loss: 57.75752699440886, Acc: 0.8195, Grad norm: 0.09702606724984617\n",
      "Iteration 3336, BCE loss: 57.75756296247695, Acc: 0.8195, Grad norm: 0.11078229683266465\n",
      "Iteration 3337, BCE loss: 57.7578066127798, Acc: 0.8194, Grad norm: 0.1532594827149928\n",
      "Iteration 3338, BCE loss: 57.75757200541531, Acc: 0.8195, Grad norm: 0.10505610693488251\n",
      "Iteration 3339, BCE loss: 57.75761252121535, Acc: 0.8196, Grad norm: 0.10773224576026896\n",
      "Iteration 3340, BCE loss: 57.757614297077566, Acc: 0.8196, Grad norm: 0.1067225048218513\n",
      "Iteration 3341, BCE loss: 57.75753409397303, Acc: 0.8196, Grad norm: 0.09289511788022393\n",
      "Iteration 3342, BCE loss: 57.75765528236401, Acc: 0.8196, Grad norm: 0.1158622782788161\n",
      "Iteration 3343, BCE loss: 57.75791491302053, Acc: 0.8196, Grad norm: 0.1601463273045421\n",
      "Iteration 3344, BCE loss: 57.757746828991955, Acc: 0.8196, Grad norm: 0.1374665393310357\n",
      "Iteration 3345, BCE loss: 57.757854925241276, Acc: 0.8196, Grad norm: 0.1532181356320253\n",
      "Iteration 3346, BCE loss: 57.757551474247606, Acc: 0.8196, Grad norm: 0.10124059906131283\n",
      "Iteration 3347, BCE loss: 57.75749166138454, Acc: 0.8195, Grad norm: 0.08717328744750277\n",
      "Iteration 3348, BCE loss: 57.75761631993352, Acc: 0.8195, Grad norm: 0.11439095930373874\n",
      "Iteration 3349, BCE loss: 57.757485405122566, Acc: 0.8196, Grad norm: 0.08895839660543206\n",
      "Iteration 3350, BCE loss: 57.75745289147759, Acc: 0.8195, Grad norm: 0.08094046543612306\n",
      "Iteration 3351, BCE loss: 57.75751604504583, Acc: 0.8195, Grad norm: 0.09258134178351098\n",
      "Iteration 3352, BCE loss: 57.75760219145044, Acc: 0.8196, Grad norm: 0.11275790810240915\n",
      "Iteration 3353, BCE loss: 57.75776622570488, Acc: 0.8196, Grad norm: 0.14654325281467676\n",
      "Iteration 3354, BCE loss: 57.75757660840592, Acc: 0.8196, Grad norm: 0.11238584680122517\n",
      "Iteration 3355, BCE loss: 57.75761710885732, Acc: 0.8196, Grad norm: 0.1197943537159817\n",
      "Iteration 3356, BCE loss: 57.75754486824857, Acc: 0.8196, Grad norm: 0.10305735855632901\n",
      "Iteration 3357, BCE loss: 57.75758000682912, Acc: 0.8196, Grad norm: 0.1092774224129291\n",
      "Iteration 3358, BCE loss: 57.75758015801682, Acc: 0.8196, Grad norm: 0.1098249306682641\n",
      "Iteration 3359, BCE loss: 57.75747042264678, Acc: 0.8196, Grad norm: 0.09029742069934284\n",
      "Iteration 3360, BCE loss: 57.757420669930596, Acc: 0.8196, Grad norm: 0.07856000706166706\n",
      "Iteration 3361, BCE loss: 57.75746924404601, Acc: 0.8195, Grad norm: 0.09112522455754217\n",
      "Iteration 3362, BCE loss: 57.75756245941983, Acc: 0.8196, Grad norm: 0.11201839005238151\n",
      "Iteration 3363, BCE loss: 57.75759669513407, Acc: 0.8196, Grad norm: 0.12094153714231876\n",
      "Iteration 3364, BCE loss: 57.757489843460306, Acc: 0.8196, Grad norm: 0.10037439893444913\n",
      "Iteration 3365, BCE loss: 57.757408876741366, Acc: 0.8196, Grad norm: 0.08084821204227699\n",
      "Iteration 3366, BCE loss: 57.757397643770645, Acc: 0.8196, Grad norm: 0.07752534940813656\n",
      "Iteration 3367, BCE loss: 57.75738069727476, Acc: 0.8196, Grad norm: 0.06807065286639677\n",
      "Iteration 3368, BCE loss: 57.75735880035148, Acc: 0.8196, Grad norm: 0.05653145183706793\n",
      "Iteration 3369, BCE loss: 57.757401189247716, Acc: 0.8196, Grad norm: 0.07769699257621028\n",
      "Iteration 3370, BCE loss: 57.75748490004703, Acc: 0.8196, Grad norm: 0.09674181752511533\n",
      "Iteration 3371, BCE loss: 57.757593985460275, Acc: 0.8196, Grad norm: 0.12098675246942008\n",
      "Iteration 3372, BCE loss: 57.757585675754065, Acc: 0.8195, Grad norm: 0.12211856753905616\n",
      "Iteration 3373, BCE loss: 57.757440954284235, Acc: 0.8196, Grad norm: 0.09014193026380247\n",
      "Iteration 3374, BCE loss: 57.75748611003266, Acc: 0.8196, Grad norm: 0.09777025756155841\n",
      "Iteration 3375, BCE loss: 57.75760805355845, Acc: 0.8196, Grad norm: 0.12403871176876251\n",
      "Iteration 3376, BCE loss: 57.75777680785609, Acc: 0.8196, Grad norm: 0.1534060338563325\n",
      "Iteration 3377, BCE loss: 57.757838894810945, Acc: 0.8197, Grad norm: 0.16264240040846384\n",
      "Iteration 3378, BCE loss: 57.75785847925663, Acc: 0.8197, Grad norm: 0.16652464995166763\n",
      "Iteration 3379, BCE loss: 57.757654493745065, Acc: 0.8197, Grad norm: 0.13043586235259022\n",
      "Iteration 3380, BCE loss: 57.75759838704555, Acc: 0.8197, Grad norm: 0.12217138735905873\n",
      "Iteration 3381, BCE loss: 57.75743081950939, Acc: 0.8196, Grad norm: 0.08591556603128675\n",
      "Iteration 3382, BCE loss: 57.75753714422956, Acc: 0.8196, Grad norm: 0.1092939108137591\n",
      "Iteration 3383, BCE loss: 57.757563374068674, Acc: 0.8196, Grad norm: 0.11143184433710263\n",
      "Iteration 3384, BCE loss: 57.757584971128914, Acc: 0.8196, Grad norm: 0.11597984146865008\n",
      "Iteration 3385, BCE loss: 57.75763177863793, Acc: 0.8195, Grad norm: 0.12262580749845103\n",
      "Iteration 3386, BCE loss: 57.757672996773636, Acc: 0.8196, Grad norm: 0.1263024505941408\n",
      "Iteration 3387, BCE loss: 57.75752072093018, Acc: 0.8196, Grad norm: 0.09303022558851722\n",
      "Iteration 3388, BCE loss: 57.757495770297126, Acc: 0.8196, Grad norm: 0.09048267168231797\n",
      "Iteration 3389, BCE loss: 57.75738774993361, Acc: 0.8196, Grad norm: 0.06613911591516596\n",
      "Iteration 3390, BCE loss: 57.757403647681905, Acc: 0.8196, Grad norm: 0.0727104148441142\n",
      "Iteration 3391, BCE loss: 57.75755030398322, Acc: 0.8196, Grad norm: 0.10514162070342817\n",
      "Iteration 3392, BCE loss: 57.75761539220652, Acc: 0.8195, Grad norm: 0.11822783813811191\n",
      "Iteration 3393, BCE loss: 57.75758299029563, Acc: 0.8196, Grad norm: 0.11043221512799921\n",
      "Iteration 3394, BCE loss: 57.75757506154185, Acc: 0.8196, Grad norm: 0.11041412289666909\n",
      "Iteration 3395, BCE loss: 57.75773089583021, Acc: 0.8196, Grad norm: 0.13814924897848227\n",
      "Iteration 3396, BCE loss: 57.757876249881434, Acc: 0.8196, Grad norm: 0.16168807731061097\n",
      "Iteration 3397, BCE loss: 57.75764063895909, Acc: 0.8196, Grad norm: 0.12851643723360157\n",
      "Iteration 3398, BCE loss: 57.75745514791707, Acc: 0.8196, Grad norm: 0.08940290580588738\n",
      "Iteration 3399, BCE loss: 57.757343157876335, Acc: 0.8196, Grad norm: 0.05386771008885747\n",
      "Iteration 3400, BCE loss: 57.757341445310246, Acc: 0.8196, Grad norm: 0.051499119981849764\n",
      "Iteration 3401, BCE loss: 57.75748530216222, Acc: 0.8196, Grad norm: 0.08822112131885121\n",
      "Iteration 3402, BCE loss: 57.757444493381385, Acc: 0.8196, Grad norm: 0.0826240211440701\n",
      "Iteration 3403, BCE loss: 57.75746965106136, Acc: 0.8196, Grad norm: 0.08700186284912963\n",
      "Iteration 3404, BCE loss: 57.757561122947706, Acc: 0.8196, Grad norm: 0.10392747647384515\n",
      "Iteration 3405, BCE loss: 57.7575949495416, Acc: 0.8196, Grad norm: 0.1098229577631933\n",
      "Iteration 3406, BCE loss: 57.7575980619658, Acc: 0.8196, Grad norm: 0.11713137230732634\n",
      "Iteration 3407, BCE loss: 57.75766336264496, Acc: 0.8196, Grad norm: 0.1295948306710962\n",
      "Iteration 3408, BCE loss: 57.757699992381475, Acc: 0.8196, Grad norm: 0.13819636324662388\n",
      "Iteration 3409, BCE loss: 57.75786264234005, Acc: 0.8196, Grad norm: 0.16549934944280145\n",
      "Iteration 3410, BCE loss: 57.75768664963112, Acc: 0.8196, Grad norm: 0.13871800775066254\n",
      "Iteration 3411, BCE loss: 57.7575467716765, Acc: 0.8196, Grad norm: 0.11113317930319185\n",
      "Iteration 3412, BCE loss: 57.75786387944443, Acc: 0.8196, Grad norm: 0.1626645586315694\n",
      "Iteration 3413, BCE loss: 57.757682377710736, Acc: 0.8196, Grad norm: 0.13563350366086718\n",
      "Iteration 3414, BCE loss: 57.75769329332908, Acc: 0.8196, Grad norm: 0.1375964917991931\n",
      "Iteration 3415, BCE loss: 57.75761323309091, Acc: 0.8196, Grad norm: 0.12265225004916813\n",
      "Iteration 3416, BCE loss: 57.75760076286076, Acc: 0.8196, Grad norm: 0.11882494818646878\n",
      "Iteration 3417, BCE loss: 57.757479261013586, Acc: 0.8196, Grad norm: 0.09197365369454921\n",
      "Iteration 3418, BCE loss: 57.757410005041685, Acc: 0.8195, Grad norm: 0.07171121875491532\n",
      "Iteration 3419, BCE loss: 57.75742202384387, Acc: 0.8195, Grad norm: 0.07313815436470661\n",
      "Iteration 3420, BCE loss: 57.75743496523896, Acc: 0.8195, Grad norm: 0.0754631767612385\n",
      "Iteration 3421, BCE loss: 57.75763428978344, Acc: 0.8196, Grad norm: 0.11833264853436949\n",
      "Iteration 3422, BCE loss: 57.75768300483427, Acc: 0.8195, Grad norm: 0.12376345845454358\n",
      "Iteration 3423, BCE loss: 57.757604745223716, Acc: 0.8195, Grad norm: 0.1120859459863143\n",
      "Iteration 3424, BCE loss: 57.75764633480276, Acc: 0.8195, Grad norm: 0.12106650602654102\n",
      "Iteration 3425, BCE loss: 57.757615992678566, Acc: 0.8195, Grad norm: 0.11969544443749715\n",
      "Iteration 3426, BCE loss: 57.75765411650636, Acc: 0.8195, Grad norm: 0.126217984088726\n",
      "Iteration 3427, BCE loss: 57.75752981246404, Acc: 0.8196, Grad norm: 0.10598996351042268\n",
      "Iteration 3428, BCE loss: 57.75752505640418, Acc: 0.8195, Grad norm: 0.10108851986040186\n",
      "Iteration 3429, BCE loss: 57.75754163238144, Acc: 0.8196, Grad norm: 0.10173609551865302\n",
      "Iteration 3430, BCE loss: 57.75764636805216, Acc: 0.8195, Grad norm: 0.12655959133475012\n",
      "Iteration 3431, BCE loss: 57.75773467991683, Acc: 0.8196, Grad norm: 0.1412149553784224\n",
      "Iteration 3432, BCE loss: 57.75799001424729, Acc: 0.8195, Grad norm: 0.1787148842195842\n",
      "Iteration 3433, BCE loss: 57.75801735177528, Acc: 0.8196, Grad norm: 0.17932570786996657\n",
      "Iteration 3434, BCE loss: 57.75779231497064, Acc: 0.8196, Grad norm: 0.14696782329813862\n",
      "Iteration 3435, BCE loss: 57.75797998751241, Acc: 0.8196, Grad norm: 0.17595139074737026\n",
      "Iteration 3436, BCE loss: 57.75778828814931, Acc: 0.8195, Grad norm: 0.150934728987262\n",
      "Iteration 3437, BCE loss: 57.75746563645804, Acc: 0.8195, Grad norm: 0.09108823149881004\n",
      "Iteration 3438, BCE loss: 57.75741985890719, Acc: 0.8195, Grad norm: 0.07271104346468192\n",
      "Iteration 3439, BCE loss: 57.757432600743144, Acc: 0.8196, Grad norm: 0.07501458660166187\n",
      "Iteration 3440, BCE loss: 57.75748469959007, Acc: 0.8196, Grad norm: 0.08559930418353898\n",
      "Iteration 3441, BCE loss: 57.757604403655314, Acc: 0.8196, Grad norm: 0.11305455673206243\n",
      "Iteration 3442, BCE loss: 57.757593227008904, Acc: 0.8196, Grad norm: 0.10686161879745631\n",
      "Iteration 3443, BCE loss: 57.75773897881534, Acc: 0.8196, Grad norm: 0.13616215161261364\n",
      "Iteration 3444, BCE loss: 57.75767090485975, Acc: 0.8196, Grad norm: 0.13170562886271242\n",
      "Iteration 3445, BCE loss: 57.75746734704672, Acc: 0.8196, Grad norm: 0.0893970579472446\n",
      "Iteration 3446, BCE loss: 57.75755879926729, Acc: 0.8196, Grad norm: 0.1060069140549632\n",
      "Iteration 3447, BCE loss: 57.75758476510347, Acc: 0.8196, Grad norm: 0.10853514702922958\n",
      "Iteration 3448, BCE loss: 57.75755359071722, Acc: 0.8196, Grad norm: 0.1043226767623591\n",
      "Iteration 3449, BCE loss: 57.75745545829324, Acc: 0.8196, Grad norm: 0.08502412097887754\n",
      "Iteration 3450, BCE loss: 57.757391250550256, Acc: 0.8195, Grad norm: 0.06651065772574423\n",
      "Iteration 3451, BCE loss: 57.75737309270099, Acc: 0.8195, Grad norm: 0.06537216442607684\n",
      "Iteration 3452, BCE loss: 57.75744941198465, Acc: 0.8195, Grad norm: 0.08722131535064759\n",
      "Iteration 3453, BCE loss: 57.75753282825295, Acc: 0.8195, Grad norm: 0.10416981346556196\n",
      "Iteration 3454, BCE loss: 57.757467774730415, Acc: 0.8195, Grad norm: 0.09328905387123879\n",
      "Iteration 3455, BCE loss: 57.75748289528346, Acc: 0.8196, Grad norm: 0.09315463570921684\n",
      "Iteration 3456, BCE loss: 57.75756030599536, Acc: 0.8196, Grad norm: 0.11006600028471546\n",
      "Iteration 3457, BCE loss: 57.7575640277152, Acc: 0.8196, Grad norm: 0.11358523271332997\n",
      "Iteration 3458, BCE loss: 57.75742148879044, Acc: 0.8196, Grad norm: 0.08052361312290754\n",
      "Iteration 3459, BCE loss: 57.75736514105617, Acc: 0.8196, Grad norm: 0.06369058719561693\n",
      "Iteration 3460, BCE loss: 57.75732157151485, Acc: 0.8196, Grad norm: 0.04815112861089537\n",
      "Iteration 3461, BCE loss: 57.757294006163534, Acc: 0.8196, Grad norm: 0.03199792351701327\n",
      "Iteration 3462, BCE loss: 57.75733886503703, Acc: 0.8196, Grad norm: 0.05547042322043464\n",
      "Iteration 3463, BCE loss: 57.75739935865549, Acc: 0.8196, Grad norm: 0.07684747808821044\n",
      "Iteration 3464, BCE loss: 57.75747848837054, Acc: 0.8196, Grad norm: 0.09620790303806862\n",
      "Iteration 3465, BCE loss: 57.757482963418965, Acc: 0.8196, Grad norm: 0.09707526029095222\n",
      "Iteration 3466, BCE loss: 57.757580154487165, Acc: 0.8197, Grad norm: 0.11418225290560192\n",
      "Iteration 3467, BCE loss: 57.75742799038647, Acc: 0.8196, Grad norm: 0.08166286747369932\n",
      "Iteration 3468, BCE loss: 57.75763663726247, Acc: 0.8196, Grad norm: 0.12969588868743798\n",
      "Iteration 3469, BCE loss: 57.75762949227245, Acc: 0.8197, Grad norm: 0.12470582567707081\n",
      "Iteration 3470, BCE loss: 57.75761164334247, Acc: 0.8196, Grad norm: 0.11834777763189949\n",
      "Iteration 3471, BCE loss: 57.75757308213734, Acc: 0.8196, Grad norm: 0.10810298652046459\n",
      "Iteration 3472, BCE loss: 57.757483445298575, Acc: 0.8196, Grad norm: 0.08941426917294157\n",
      "Iteration 3473, BCE loss: 57.75754919482856, Acc: 0.8196, Grad norm: 0.10382465806620447\n",
      "Iteration 3474, BCE loss: 57.757501956161114, Acc: 0.8196, Grad norm: 0.0932931323939534\n",
      "Iteration 3475, BCE loss: 57.75766738452029, Acc: 0.8196, Grad norm: 0.13004448750383055\n",
      "Iteration 3476, BCE loss: 57.75770191490864, Acc: 0.8195, Grad norm: 0.136831531178171\n",
      "Iteration 3477, BCE loss: 57.757487164736304, Acc: 0.8195, Grad norm: 0.09619382801535069\n",
      "Iteration 3478, BCE loss: 57.75763744266985, Acc: 0.8195, Grad norm: 0.1290045099806251\n",
      "Iteration 3479, BCE loss: 57.757544632013676, Acc: 0.8196, Grad norm: 0.11001680119920931\n",
      "Iteration 3480, BCE loss: 57.757605068099465, Acc: 0.8196, Grad norm: 0.12020837586597802\n",
      "Iteration 3481, BCE loss: 57.75751355098417, Acc: 0.8195, Grad norm: 0.09961176852799342\n",
      "Iteration 3482, BCE loss: 57.757437016090705, Acc: 0.8195, Grad norm: 0.07839392425226256\n",
      "Iteration 3483, BCE loss: 57.757514924306626, Acc: 0.8196, Grad norm: 0.0997291039273103\n",
      "Iteration 3484, BCE loss: 57.75757859932415, Acc: 0.8196, Grad norm: 0.11306449730472684\n",
      "Iteration 3485, BCE loss: 57.75759818803691, Acc: 0.8196, Grad norm: 0.11161060196192643\n",
      "Iteration 3486, BCE loss: 57.75761424133402, Acc: 0.8196, Grad norm: 0.11620852147859055\n",
      "Iteration 3487, BCE loss: 57.7575542770775, Acc: 0.8195, Grad norm: 0.10384824521148535\n",
      "Iteration 3488, BCE loss: 57.75771032448471, Acc: 0.8195, Grad norm: 0.1396022550796883\n",
      "Iteration 3489, BCE loss: 57.75760125675332, Acc: 0.8195, Grad norm: 0.11716723658737986\n",
      "Iteration 3490, BCE loss: 57.75760189818487, Acc: 0.8195, Grad norm: 0.11369720510416231\n",
      "Iteration 3491, BCE loss: 57.75758481381054, Acc: 0.8195, Grad norm: 0.10602593303664931\n",
      "Iteration 3492, BCE loss: 57.75780068125583, Acc: 0.8196, Grad norm: 0.13768128461252874\n",
      "Iteration 3493, BCE loss: 57.75771254774176, Acc: 0.8196, Grad norm: 0.12687766049371113\n",
      "Iteration 3494, BCE loss: 57.75754586997867, Acc: 0.8195, Grad norm: 0.09783614071015143\n",
      "Iteration 3495, BCE loss: 57.75743273878883, Acc: 0.8195, Grad norm: 0.07838544296948084\n",
      "Iteration 3496, BCE loss: 57.75744164824076, Acc: 0.8195, Grad norm: 0.08298414785414475\n",
      "Iteration 3497, BCE loss: 57.75742885818822, Acc: 0.8195, Grad norm: 0.08245665298228479\n",
      "Iteration 3498, BCE loss: 57.75743207889675, Acc: 0.8196, Grad norm: 0.07793568080828302\n",
      "Iteration 3499, BCE loss: 57.75748828917247, Acc: 0.8196, Grad norm: 0.0922028325919773\n",
      "Iteration 3500, BCE loss: 57.757402057412996, Acc: 0.8196, Grad norm: 0.07474891668137282\n",
      "Iteration 3501, BCE loss: 57.75735696590729, Acc: 0.8196, Grad norm: 0.06022491149900046\n",
      "Iteration 3502, BCE loss: 57.75762657225401, Acc: 0.8196, Grad norm: 0.12343625309760642\n",
      "Iteration 3503, BCE loss: 57.757619379511915, Acc: 0.8196, Grad norm: 0.11944782408922439\n",
      "Iteration 3504, BCE loss: 57.757677880498335, Acc: 0.8196, Grad norm: 0.12691438842281128\n",
      "Iteration 3505, BCE loss: 57.75759936182635, Acc: 0.8196, Grad norm: 0.1124367476754253\n",
      "Iteration 3506, BCE loss: 57.75755752463858, Acc: 0.8196, Grad norm: 0.10499429478583974\n",
      "Iteration 3507, BCE loss: 57.75753603281143, Acc: 0.8196, Grad norm: 0.1000701490266855\n",
      "Iteration 3508, BCE loss: 57.75763266145901, Acc: 0.8196, Grad norm: 0.12015108775134571\n",
      "Iteration 3509, BCE loss: 57.75764551737122, Acc: 0.8195, Grad norm: 0.12802774689116253\n",
      "Iteration 3510, BCE loss: 57.75756013849213, Acc: 0.8195, Grad norm: 0.11053956692379696\n",
      "Iteration 3511, BCE loss: 57.757522796497966, Acc: 0.8195, Grad norm: 0.10081977413788493\n",
      "Iteration 3512, BCE loss: 57.757592355265935, Acc: 0.8196, Grad norm: 0.1151767289652984\n",
      "Iteration 3513, BCE loss: 57.75771074264064, Acc: 0.8196, Grad norm: 0.13481497226533987\n",
      "Iteration 3514, BCE loss: 57.75779281848251, Acc: 0.8196, Grad norm: 0.14632965618182464\n",
      "Iteration 3515, BCE loss: 57.75774008648824, Acc: 0.8196, Grad norm: 0.13968532470685271\n",
      "Iteration 3516, BCE loss: 57.758002294313115, Acc: 0.8196, Grad norm: 0.17473562389812566\n",
      "Iteration 3517, BCE loss: 57.75776635214041, Acc: 0.8196, Grad norm: 0.14339273329222163\n",
      "Iteration 3518, BCE loss: 57.75775206785591, Acc: 0.8196, Grad norm: 0.14447017978904458\n",
      "Iteration 3519, BCE loss: 57.75777735183553, Acc: 0.8196, Grad norm: 0.14995392194856255\n",
      "Iteration 3520, BCE loss: 57.757943420062816, Acc: 0.8196, Grad norm: 0.17492664443673694\n",
      "Iteration 3521, BCE loss: 57.75795861134584, Acc: 0.8196, Grad norm: 0.1758586221869456\n",
      "Iteration 3522, BCE loss: 57.75779819546061, Acc: 0.8195, Grad norm: 0.1529499270420714\n",
      "Iteration 3523, BCE loss: 57.75772268364321, Acc: 0.8196, Grad norm: 0.14194380659427433\n",
      "Iteration 3524, BCE loss: 57.75765659546524, Acc: 0.8196, Grad norm: 0.13165395875722613\n",
      "Iteration 3525, BCE loss: 57.75750062290538, Acc: 0.8196, Grad norm: 0.1018211803761627\n",
      "Iteration 3526, BCE loss: 57.75738835227597, Acc: 0.8196, Grad norm: 0.06965859363284\n",
      "Iteration 3527, BCE loss: 57.75738910355709, Acc: 0.8196, Grad norm: 0.07301890269149545\n",
      "Iteration 3528, BCE loss: 57.75739891453668, Acc: 0.8196, Grad norm: 0.06956345125612665\n",
      "Iteration 3529, BCE loss: 57.757427592787636, Acc: 0.8196, Grad norm: 0.0792600545974235\n",
      "Iteration 3530, BCE loss: 57.757423078683956, Acc: 0.8196, Grad norm: 0.07846697040712902\n",
      "Iteration 3531, BCE loss: 57.75737596902016, Acc: 0.8196, Grad norm: 0.06264573720811456\n",
      "Iteration 3532, BCE loss: 57.75737631780497, Acc: 0.8196, Grad norm: 0.06139373043152539\n",
      "Iteration 3533, BCE loss: 57.75740013939429, Acc: 0.8196, Grad norm: 0.07212912730958083\n",
      "Iteration 3534, BCE loss: 57.757359656392595, Acc: 0.8196, Grad norm: 0.05869272320994739\n",
      "Iteration 3535, BCE loss: 57.757442463064784, Acc: 0.8196, Grad norm: 0.08068301257041839\n",
      "Iteration 3536, BCE loss: 57.75739462300031, Acc: 0.8196, Grad norm: 0.06557622327439802\n",
      "Iteration 3537, BCE loss: 57.757366597847465, Acc: 0.8196, Grad norm: 0.05921548467606617\n",
      "Iteration 3538, BCE loss: 57.7573654194731, Acc: 0.8196, Grad norm: 0.06434120489307685\n",
      "Iteration 3539, BCE loss: 57.75741289979111, Acc: 0.8196, Grad norm: 0.0799826054620614\n",
      "Iteration 3540, BCE loss: 57.75740479561422, Acc: 0.8196, Grad norm: 0.07805001584013825\n",
      "Iteration 3541, BCE loss: 57.75730475914737, Acc: 0.8196, Grad norm: 0.036083394646031865\n",
      "Iteration 3542, BCE loss: 57.75735002349731, Acc: 0.8196, Grad norm: 0.053552368223494316\n",
      "Iteration 3543, BCE loss: 57.757356210479614, Acc: 0.8196, Grad norm: 0.05922585486544178\n",
      "Iteration 3544, BCE loss: 57.75737051206714, Acc: 0.8196, Grad norm: 0.06378226432673854\n",
      "Iteration 3545, BCE loss: 57.757450769736366, Acc: 0.8196, Grad norm: 0.08960634925674107\n",
      "Iteration 3546, BCE loss: 57.75747095004499, Acc: 0.8196, Grad norm: 0.09156037981037471\n",
      "Iteration 3547, BCE loss: 57.75747143885771, Acc: 0.8196, Grad norm: 0.09017087921687827\n",
      "Iteration 3548, BCE loss: 57.7574413211823, Acc: 0.8196, Grad norm: 0.08028695397930397\n",
      "Iteration 3549, BCE loss: 57.75743364319648, Acc: 0.8196, Grad norm: 0.08137574648744123\n",
      "Iteration 3550, BCE loss: 57.75750936429171, Acc: 0.8196, Grad norm: 0.10050782095167315\n",
      "Iteration 3551, BCE loss: 57.75748293826932, Acc: 0.8196, Grad norm: 0.09284604120124595\n",
      "Iteration 3552, BCE loss: 57.75763257598799, Acc: 0.8197, Grad norm: 0.1179965458170183\n",
      "Iteration 3553, BCE loss: 57.75766203547383, Acc: 0.8197, Grad norm: 0.12327743506578337\n",
      "Iteration 3554, BCE loss: 57.75749216543871, Acc: 0.8196, Grad norm: 0.09039154259272249\n",
      "Iteration 3555, BCE loss: 57.75747541793576, Acc: 0.8196, Grad norm: 0.08585934253933057\n",
      "Iteration 3556, BCE loss: 57.75750583892804, Acc: 0.8196, Grad norm: 0.09456168821459605\n",
      "Iteration 3557, BCE loss: 57.75759712358105, Acc: 0.8196, Grad norm: 0.11328441583376107\n",
      "Iteration 3558, BCE loss: 57.75772682215267, Acc: 0.8196, Grad norm: 0.1354951605628502\n",
      "Iteration 3559, BCE loss: 57.7577124055846, Acc: 0.8196, Grad norm: 0.13913413653495083\n",
      "Iteration 3560, BCE loss: 57.757632821501744, Acc: 0.8196, Grad norm: 0.12232527479126773\n",
      "Iteration 3561, BCE loss: 57.75767402005755, Acc: 0.8196, Grad norm: 0.128642849152957\n",
      "Iteration 3562, BCE loss: 57.75771627761668, Acc: 0.8196, Grad norm: 0.1321185182559254\n",
      "Iteration 3563, BCE loss: 57.757753829555064, Acc: 0.8196, Grad norm: 0.1372453402197487\n",
      "Iteration 3564, BCE loss: 57.75779154222588, Acc: 0.8196, Grad norm: 0.14412038596551155\n",
      "Iteration 3565, BCE loss: 57.757701513452844, Acc: 0.8196, Grad norm: 0.12619547366541098\n",
      "Iteration 3566, BCE loss: 57.75766911152313, Acc: 0.8196, Grad norm: 0.12273993210090121\n",
      "Iteration 3567, BCE loss: 57.75761418561036, Acc: 0.8196, Grad norm: 0.1154790435222385\n",
      "Iteration 3568, BCE loss: 57.75758615050756, Acc: 0.8196, Grad norm: 0.11055031975444833\n",
      "Iteration 3569, BCE loss: 57.75769707877012, Acc: 0.8196, Grad norm: 0.12668841221010985\n",
      "Iteration 3570, BCE loss: 57.75779704589934, Acc: 0.8196, Grad norm: 0.1430291389662064\n",
      "Iteration 3571, BCE loss: 57.75777877052286, Acc: 0.8196, Grad norm: 0.1446167180931196\n",
      "Iteration 3572, BCE loss: 57.7577574429703, Acc: 0.8196, Grad norm: 0.14301039643967955\n",
      "Iteration 3573, BCE loss: 57.75781687625627, Acc: 0.8196, Grad norm: 0.15427175802900536\n",
      "Iteration 3574, BCE loss: 57.75774481982705, Acc: 0.8196, Grad norm: 0.14036284110818506\n",
      "Iteration 3575, BCE loss: 57.7579301561427, Acc: 0.8196, Grad norm: 0.16463181744713554\n",
      "Iteration 3576, BCE loss: 57.75785054247612, Acc: 0.8196, Grad norm: 0.15734336350899544\n",
      "Iteration 3577, BCE loss: 57.75762409834351, Acc: 0.8196, Grad norm: 0.12234450240074908\n",
      "Iteration 3578, BCE loss: 57.75758253286418, Acc: 0.8196, Grad norm: 0.1187933820630878\n",
      "Iteration 3579, BCE loss: 57.75743215397289, Acc: 0.8196, Grad norm: 0.08466090014629084\n",
      "Iteration 3580, BCE loss: 57.75740995893898, Acc: 0.8196, Grad norm: 0.07649702431998448\n",
      "Iteration 3581, BCE loss: 57.75754186107354, Acc: 0.8196, Grad norm: 0.10384558453546688\n",
      "Iteration 3582, BCE loss: 57.757487385534965, Acc: 0.8196, Grad norm: 0.09186257042805882\n",
      "Iteration 3583, BCE loss: 57.75741088614765, Acc: 0.8196, Grad norm: 0.07341511237005072\n",
      "Iteration 3584, BCE loss: 57.75745448119106, Acc: 0.8196, Grad norm: 0.08864396145958182\n",
      "Iteration 3585, BCE loss: 57.75750743818932, Acc: 0.8196, Grad norm: 0.10068132507100186\n",
      "Iteration 3586, BCE loss: 57.75744538136233, Acc: 0.8196, Grad norm: 0.09082401169980472\n",
      "Iteration 3587, BCE loss: 57.75753532902915, Acc: 0.8196, Grad norm: 0.11181004385854647\n",
      "Iteration 3588, BCE loss: 57.75755643613123, Acc: 0.8195, Grad norm: 0.11218990590617882\n",
      "Iteration 3589, BCE loss: 57.75744147073651, Acc: 0.8195, Grad norm: 0.08874298749442167\n",
      "Iteration 3590, BCE loss: 57.75742939072807, Acc: 0.8196, Grad norm: 0.08459160987593532\n",
      "Iteration 3591, BCE loss: 57.757375267899334, Acc: 0.8196, Grad norm: 0.06535324327507441\n",
      "Iteration 3592, BCE loss: 57.757425778016014, Acc: 0.8195, Grad norm: 0.08069277892367342\n",
      "Iteration 3593, BCE loss: 57.75746775319067, Acc: 0.8195, Grad norm: 0.09520212606823382\n",
      "Iteration 3594, BCE loss: 57.757504636338204, Acc: 0.8195, Grad norm: 0.10058777740308868\n",
      "Iteration 3595, BCE loss: 57.75741601752986, Acc: 0.8195, Grad norm: 0.07897854766465204\n",
      "Iteration 3596, BCE loss: 57.75737066005381, Acc: 0.8196, Grad norm: 0.06398007916960542\n",
      "Iteration 3597, BCE loss: 57.7573527658505, Acc: 0.8196, Grad norm: 0.05918899307426108\n",
      "Iteration 3598, BCE loss: 57.75743992361359, Acc: 0.8196, Grad norm: 0.08940296862293774\n",
      "Iteration 3599, BCE loss: 57.75761414291367, Acc: 0.8196, Grad norm: 0.12677689245617454\n",
      "Iteration 3600, BCE loss: 57.75750727433616, Acc: 0.8196, Grad norm: 0.10629004716124076\n",
      "Iteration 3601, BCE loss: 57.757396763372206, Acc: 0.8196, Grad norm: 0.07622770280129236\n",
      "Iteration 3602, BCE loss: 57.75730681457376, Acc: 0.8196, Grad norm: 0.04303364729848644\n",
      "Iteration 3603, BCE loss: 57.75731057974808, Acc: 0.8196, Grad norm: 0.04457643809142138\n",
      "Iteration 3604, BCE loss: 57.75732271744463, Acc: 0.8196, Grad norm: 0.0487717717171679\n",
      "Iteration 3605, BCE loss: 57.757354667398005, Acc: 0.8196, Grad norm: 0.058046862194733685\n",
      "Iteration 3606, BCE loss: 57.75745355561309, Acc: 0.8196, Grad norm: 0.08425670829042256\n",
      "Iteration 3607, BCE loss: 57.75735885018632, Acc: 0.8196, Grad norm: 0.05710651464497065\n",
      "Iteration 3608, BCE loss: 57.75740527647645, Acc: 0.8196, Grad norm: 0.07185489273606106\n",
      "Iteration 3609, BCE loss: 57.75751516375057, Acc: 0.8196, Grad norm: 0.098545202448622\n",
      "Iteration 3610, BCE loss: 57.75753944162899, Acc: 0.8196, Grad norm: 0.10621249752928676\n",
      "Iteration 3611, BCE loss: 57.757380016522184, Acc: 0.8196, Grad norm: 0.06759664484745534\n",
      "Iteration 3612, BCE loss: 57.757445179017026, Acc: 0.8196, Grad norm: 0.08616095216521613\n",
      "Iteration 3613, BCE loss: 57.75739030673532, Acc: 0.8196, Grad norm: 0.06783865659563978\n",
      "Iteration 3614, BCE loss: 57.75738344376293, Acc: 0.8195, Grad norm: 0.06759272317920945\n",
      "Iteration 3615, BCE loss: 57.75735803698487, Acc: 0.8195, Grad norm: 0.06065830442459765\n",
      "Iteration 3616, BCE loss: 57.757362569627844, Acc: 0.8195, Grad norm: 0.06434517314467578\n",
      "Iteration 3617, BCE loss: 57.75743970363557, Acc: 0.8195, Grad norm: 0.08989666195607807\n",
      "Iteration 3618, BCE loss: 57.757337910965965, Acc: 0.8195, Grad norm: 0.04945270745547843\n",
      "Iteration 3619, BCE loss: 57.75737596126308, Acc: 0.8195, Grad norm: 0.06471695452425368\n",
      "Iteration 3620, BCE loss: 57.7576250925399, Acc: 0.8195, Grad norm: 0.12886693152583129\n",
      "Iteration 3621, BCE loss: 57.75748089776785, Acc: 0.8195, Grad norm: 0.09717433222745835\n",
      "Iteration 3622, BCE loss: 57.757490568963995, Acc: 0.8195, Grad norm: 0.09762938485762741\n",
      "Iteration 3623, BCE loss: 57.75752440442646, Acc: 0.8195, Grad norm: 0.10717749548402095\n",
      "Iteration 3624, BCE loss: 57.757505413496084, Acc: 0.8195, Grad norm: 0.10184521465671605\n",
      "Iteration 3625, BCE loss: 57.75743433394736, Acc: 0.8196, Grad norm: 0.08185837336511775\n",
      "Iteration 3626, BCE loss: 57.757399047522455, Acc: 0.8196, Grad norm: 0.07264262478708747\n",
      "Iteration 3627, BCE loss: 57.7573430542364, Acc: 0.8195, Grad norm: 0.05616566280855339\n",
      "Iteration 3628, BCE loss: 57.75734082381764, Acc: 0.8196, Grad norm: 0.05470396344364208\n",
      "Iteration 3629, BCE loss: 57.75732097063978, Acc: 0.8195, Grad norm: 0.04621430799319818\n",
      "Iteration 3630, BCE loss: 57.757418751660026, Acc: 0.8195, Grad norm: 0.08385831728392984\n",
      "Iteration 3631, BCE loss: 57.75737968463284, Acc: 0.8195, Grad norm: 0.07019730559592216\n",
      "Iteration 3632, BCE loss: 57.75738888682123, Acc: 0.8195, Grad norm: 0.06704682693127122\n",
      "Iteration 3633, BCE loss: 57.757433704757986, Acc: 0.8195, Grad norm: 0.07682241667037629\n",
      "Iteration 3634, BCE loss: 57.757409907879534, Acc: 0.8195, Grad norm: 0.07330436567053202\n",
      "Iteration 3635, BCE loss: 57.75745269352245, Acc: 0.8195, Grad norm: 0.09095748100691559\n",
      "Iteration 3636, BCE loss: 57.757354055705704, Acc: 0.8195, Grad norm: 0.058821703358632633\n",
      "Iteration 3637, BCE loss: 57.757342810276896, Acc: 0.8196, Grad norm: 0.05204427079509224\n",
      "Iteration 3638, BCE loss: 57.75753571219035, Acc: 0.8196, Grad norm: 0.10730946030567005\n",
      "Iteration 3639, BCE loss: 57.757441910817974, Acc: 0.8196, Grad norm: 0.08315808432019246\n",
      "Iteration 3640, BCE loss: 57.75758766271575, Acc: 0.8195, Grad norm: 0.113467134132008\n",
      "Iteration 3641, BCE loss: 57.75742719527803, Acc: 0.8195, Grad norm: 0.07718884567131114\n",
      "Iteration 3642, BCE loss: 57.75748657627307, Acc: 0.8195, Grad norm: 0.09102515642559503\n",
      "Iteration 3643, BCE loss: 57.75764769097324, Acc: 0.8195, Grad norm: 0.1273211497627835\n",
      "Iteration 3644, BCE loss: 57.757647893520996, Acc: 0.8196, Grad norm: 0.12838892627893578\n",
      "Iteration 3645, BCE loss: 57.7576233549926, Acc: 0.8195, Grad norm: 0.12035990170793427\n",
      "Iteration 3646, BCE loss: 57.75752790212246, Acc: 0.8195, Grad norm: 0.10630216350946335\n",
      "Iteration 3647, BCE loss: 57.75771742859025, Acc: 0.8195, Grad norm: 0.14083970219361056\n",
      "Iteration 3648, BCE loss: 57.757631157611996, Acc: 0.8195, Grad norm: 0.12521818715866784\n",
      "Iteration 3649, BCE loss: 57.75749282466815, Acc: 0.8196, Grad norm: 0.09755474553171614\n",
      "Iteration 3650, BCE loss: 57.75744194824895, Acc: 0.8195, Grad norm: 0.0813649367671996\n",
      "Iteration 3651, BCE loss: 57.75745646238646, Acc: 0.8195, Grad norm: 0.08070675778220421\n",
      "Iteration 3652, BCE loss: 57.757422525880465, Acc: 0.8195, Grad norm: 0.07049485860983247\n",
      "Iteration 3653, BCE loss: 57.75751976993193, Acc: 0.8195, Grad norm: 0.09485170310408735\n",
      "Iteration 3654, BCE loss: 57.75740727563472, Acc: 0.8195, Grad norm: 0.06696557561918202\n",
      "Iteration 3655, BCE loss: 57.75749650081713, Acc: 0.8196, Grad norm: 0.08743080189624387\n",
      "Iteration 3656, BCE loss: 57.75754127291136, Acc: 0.8196, Grad norm: 0.09464226994868208\n",
      "Iteration 3657, BCE loss: 57.75763437915385, Acc: 0.8196, Grad norm: 0.1124539498410553\n",
      "Iteration 3658, BCE loss: 57.75773013869886, Acc: 0.8196, Grad norm: 0.1274608904937768\n",
      "Iteration 3659, BCE loss: 57.75761005223866, Acc: 0.8196, Grad norm: 0.10537537877264619\n",
      "Iteration 3660, BCE loss: 57.75759759989567, Acc: 0.8196, Grad norm: 0.10367351605397614\n",
      "Iteration 3661, BCE loss: 57.75764211563642, Acc: 0.8195, Grad norm: 0.1178005893895486\n",
      "Iteration 3662, BCE loss: 57.757553210396225, Acc: 0.8196, Grad norm: 0.09833376743670133\n",
      "Iteration 3663, BCE loss: 57.757607741589055, Acc: 0.8196, Grad norm: 0.11375608293239564\n",
      "Iteration 3664, BCE loss: 57.757433036397046, Acc: 0.8195, Grad norm: 0.07335128642543967\n",
      "Iteration 3665, BCE loss: 57.75739709572381, Acc: 0.8195, Grad norm: 0.07072209330347323\n",
      "Iteration 3666, BCE loss: 57.75735237499892, Acc: 0.8196, Grad norm: 0.05602088289828493\n",
      "Iteration 3667, BCE loss: 57.75740457995413, Acc: 0.8196, Grad norm: 0.07211398390260815\n",
      "Iteration 3668, BCE loss: 57.757410399489004, Acc: 0.8196, Grad norm: 0.06989170682361312\n",
      "Iteration 3669, BCE loss: 57.75740331308971, Acc: 0.8196, Grad norm: 0.06966224054239957\n",
      "Iteration 3670, BCE loss: 57.75738489877315, Acc: 0.8196, Grad norm: 0.0676252543679292\n",
      "Iteration 3671, BCE loss: 57.75745025984935, Acc: 0.8196, Grad norm: 0.08093741585646196\n",
      "Iteration 3672, BCE loss: 57.75742259319066, Acc: 0.8195, Grad norm: 0.07130915335126166\n",
      "Iteration 3673, BCE loss: 57.75751170739024, Acc: 0.8195, Grad norm: 0.09632122731363751\n",
      "Iteration 3674, BCE loss: 57.75749920326446, Acc: 0.8195, Grad norm: 0.09645571467162217\n",
      "Iteration 3675, BCE loss: 57.757498167544966, Acc: 0.8195, Grad norm: 0.10452058045647233\n",
      "Iteration 3676, BCE loss: 57.757538587502665, Acc: 0.8195, Grad norm: 0.11200907494486989\n",
      "Iteration 3677, BCE loss: 57.75747170032861, Acc: 0.8195, Grad norm: 0.09745538539437265\n",
      "Iteration 3678, BCE loss: 57.75741845341197, Acc: 0.8195, Grad norm: 0.08434380655421428\n",
      "Iteration 3679, BCE loss: 57.757303978455965, Acc: 0.8195, Grad norm: 0.041984645420023926\n",
      "Iteration 3680, BCE loss: 57.75732263019745, Acc: 0.8196, Grad norm: 0.05002066455162617\n",
      "Iteration 3681, BCE loss: 57.757380276346495, Acc: 0.8196, Grad norm: 0.06704356428026714\n",
      "Iteration 3682, BCE loss: 57.75735523230966, Acc: 0.8196, Grad norm: 0.05694051004520897\n",
      "Iteration 3683, BCE loss: 57.75738725156609, Acc: 0.8196, Grad norm: 0.07115900123709029\n",
      "Iteration 3684, BCE loss: 57.757413710232825, Acc: 0.8196, Grad norm: 0.07577595111238096\n",
      "Iteration 3685, BCE loss: 57.757429274016665, Acc: 0.8196, Grad norm: 0.07579655809372124\n",
      "Iteration 3686, BCE loss: 57.757517653729714, Acc: 0.8196, Grad norm: 0.0988904718399447\n",
      "Iteration 3687, BCE loss: 57.75747917763744, Acc: 0.8196, Grad norm: 0.08817335138369178\n",
      "Iteration 3688, BCE loss: 57.757444464159526, Acc: 0.8196, Grad norm: 0.08125963068930415\n",
      "Iteration 3689, BCE loss: 57.757445553121215, Acc: 0.8196, Grad norm: 0.08004898898484146\n",
      "Iteration 3690, BCE loss: 57.757496673997665, Acc: 0.8196, Grad norm: 0.09437725045147621\n",
      "Iteration 3691, BCE loss: 57.75740663912415, Acc: 0.8196, Grad norm: 0.07392534927645314\n",
      "Iteration 3692, BCE loss: 57.75741839283174, Acc: 0.8196, Grad norm: 0.07892232419241747\n",
      "Iteration 3693, BCE loss: 57.75732532612476, Acc: 0.8196, Grad norm: 0.04662176980348694\n",
      "Iteration 3694, BCE loss: 57.75736086055711, Acc: 0.8196, Grad norm: 0.057074654014460775\n",
      "Iteration 3695, BCE loss: 57.7573717941567, Acc: 0.8196, Grad norm: 0.05982715410958465\n",
      "Iteration 3696, BCE loss: 57.75741248391916, Acc: 0.8196, Grad norm: 0.07054231558560202\n",
      "Iteration 3697, BCE loss: 57.7574737044648, Acc: 0.8195, Grad norm: 0.08708926957407993\n",
      "Iteration 3698, BCE loss: 57.7575675575071, Acc: 0.8195, Grad norm: 0.11292785497566009\n",
      "Iteration 3699, BCE loss: 57.75768774875611, Acc: 0.8195, Grad norm: 0.13935279619313506\n",
      "Iteration 3700, BCE loss: 57.75754291260218, Acc: 0.8195, Grad norm: 0.10945356622343698\n",
      "Iteration 3701, BCE loss: 57.757598382925806, Acc: 0.8195, Grad norm: 0.11731340602597226\n",
      "Iteration 3702, BCE loss: 57.75775175763107, Acc: 0.8195, Grad norm: 0.14453325100106035\n",
      "Iteration 3703, BCE loss: 57.7574679438373, Acc: 0.8195, Grad norm: 0.09157217332361546\n",
      "Iteration 3704, BCE loss: 57.757491494343576, Acc: 0.8195, Grad norm: 0.09959550603340137\n",
      "Iteration 3705, BCE loss: 57.75758349331341, Acc: 0.8195, Grad norm: 0.1179799433470816\n",
      "Iteration 3706, BCE loss: 57.75752938744212, Acc: 0.8196, Grad norm: 0.10436661485551642\n",
      "Iteration 3707, BCE loss: 57.75745346731895, Acc: 0.8196, Grad norm: 0.08946767256878937\n",
      "Iteration 3708, BCE loss: 57.757496712043064, Acc: 0.8196, Grad norm: 0.0983575755137304\n",
      "Iteration 3709, BCE loss: 57.75754200982531, Acc: 0.8196, Grad norm: 0.1027192209751369\n",
      "Iteration 3710, BCE loss: 57.75768337287619, Acc: 0.8196, Grad norm: 0.1286415766996247\n",
      "Iteration 3711, BCE loss: 57.757674210932926, Acc: 0.8196, Grad norm: 0.1252483055558113\n",
      "Iteration 3712, BCE loss: 57.75746854480658, Acc: 0.8196, Grad norm: 0.08676045520218587\n",
      "Iteration 3713, BCE loss: 57.75746043095867, Acc: 0.8196, Grad norm: 0.08517119585182925\n",
      "Iteration 3714, BCE loss: 57.75742375076932, Acc: 0.8196, Grad norm: 0.07863670291763131\n",
      "Iteration 3715, BCE loss: 57.757369736117795, Acc: 0.8196, Grad norm: 0.06426524445650059\n",
      "Iteration 3716, BCE loss: 57.75745855490828, Acc: 0.8196, Grad norm: 0.08269316130565897\n",
      "Iteration 3717, BCE loss: 57.75748228633928, Acc: 0.8196, Grad norm: 0.09219443502380999\n",
      "Iteration 3718, BCE loss: 57.75750425995656, Acc: 0.8195, Grad norm: 0.09971455531841662\n",
      "Iteration 3719, BCE loss: 57.757494616935446, Acc: 0.8195, Grad norm: 0.09860694570771464\n",
      "Iteration 3720, BCE loss: 57.75737679472451, Acc: 0.8196, Grad norm: 0.07097878634441347\n",
      "Iteration 3721, BCE loss: 57.7573718373947, Acc: 0.8195, Grad norm: 0.06696675076010972\n",
      "Iteration 3722, BCE loss: 57.757404982634725, Acc: 0.8195, Grad norm: 0.0780609819516421\n",
      "Iteration 3723, BCE loss: 57.75739548438749, Acc: 0.8195, Grad norm: 0.07369182607663768\n",
      "Iteration 3724, BCE loss: 57.757409424262825, Acc: 0.8195, Grad norm: 0.07601545514892652\n",
      "Iteration 3725, BCE loss: 57.757447753620454, Acc: 0.8196, Grad norm: 0.08033012013916005\n",
      "Iteration 3726, BCE loss: 57.7576334427451, Acc: 0.8196, Grad norm: 0.11713691123492984\n",
      "Iteration 3727, BCE loss: 57.75768415767526, Acc: 0.8196, Grad norm: 0.12785917695956325\n",
      "Iteration 3728, BCE loss: 57.757784884978776, Acc: 0.8197, Grad norm: 0.14655379220018905\n",
      "Iteration 3729, BCE loss: 57.75797033129391, Acc: 0.8197, Grad norm: 0.17342132049617795\n",
      "Iteration 3730, BCE loss: 57.75781409040593, Acc: 0.8197, Grad norm: 0.15103448707584322\n",
      "Iteration 3731, BCE loss: 57.7577463801596, Acc: 0.8197, Grad norm: 0.1381453801499041\n",
      "Iteration 3732, BCE loss: 57.75781317111546, Acc: 0.8197, Grad norm: 0.15138209073699838\n",
      "Iteration 3733, BCE loss: 57.75770312336533, Acc: 0.8197, Grad norm: 0.1318027312805901\n",
      "Iteration 3734, BCE loss: 57.75767233860378, Acc: 0.8197, Grad norm: 0.1335155487329881\n",
      "Iteration 3735, BCE loss: 57.75775494565321, Acc: 0.8196, Grad norm: 0.1459621522679534\n",
      "Iteration 3736, BCE loss: 57.75784946874847, Acc: 0.8197, Grad norm: 0.15874336693068822\n",
      "Iteration 3737, BCE loss: 57.757779645172604, Acc: 0.8197, Grad norm: 0.14589995702841435\n",
      "Iteration 3738, BCE loss: 57.75766172867209, Acc: 0.8197, Grad norm: 0.1260666672162614\n",
      "Iteration 3739, BCE loss: 57.75761581686206, Acc: 0.8197, Grad norm: 0.12078251645770567\n",
      "Iteration 3740, BCE loss: 57.757558007789925, Acc: 0.8197, Grad norm: 0.11281238011889523\n",
      "Iteration 3741, BCE loss: 57.75762843769817, Acc: 0.8197, Grad norm: 0.12781822768516535\n",
      "Iteration 3742, BCE loss: 57.75741356686091, Acc: 0.8196, Grad norm: 0.07952493127726604\n",
      "Iteration 3743, BCE loss: 57.7574144114379, Acc: 0.8196, Grad norm: 0.08079341348227466\n",
      "Iteration 3744, BCE loss: 57.75753105053027, Acc: 0.8196, Grad norm: 0.10601415126348447\n",
      "Iteration 3745, BCE loss: 57.75753434146387, Acc: 0.8197, Grad norm: 0.10776478231893091\n",
      "Iteration 3746, BCE loss: 57.757484600963984, Acc: 0.8197, Grad norm: 0.09769742291304898\n",
      "Iteration 3747, BCE loss: 57.75741415417716, Acc: 0.8196, Grad norm: 0.07783542775736364\n",
      "Iteration 3748, BCE loss: 57.75746101498875, Acc: 0.8196, Grad norm: 0.08990329677880883\n",
      "Iteration 3749, BCE loss: 57.75750956729449, Acc: 0.8196, Grad norm: 0.10234023722346376\n",
      "Iteration 3750, BCE loss: 57.75750823984579, Acc: 0.8196, Grad norm: 0.09790432340564822\n",
      "Iteration 3751, BCE loss: 57.757549564889594, Acc: 0.8196, Grad norm: 0.11007133695070159\n",
      "Iteration 3752, BCE loss: 57.75746426109066, Acc: 0.8196, Grad norm: 0.09212988072849768\n",
      "Iteration 3753, BCE loss: 57.75757834846246, Acc: 0.8196, Grad norm: 0.11376507092916999\n",
      "Iteration 3754, BCE loss: 57.75748238482422, Acc: 0.8196, Grad norm: 0.09466292444472373\n",
      "Iteration 3755, BCE loss: 57.75748673461826, Acc: 0.8196, Grad norm: 0.09702167124878676\n",
      "Iteration 3756, BCE loss: 57.75744289620896, Acc: 0.8196, Grad norm: 0.08173916162161082\n",
      "Iteration 3757, BCE loss: 57.75753229622207, Acc: 0.8196, Grad norm: 0.1035788172090335\n",
      "Iteration 3758, BCE loss: 57.757496724120294, Acc: 0.8196, Grad norm: 0.09527959156822535\n",
      "Iteration 3759, BCE loss: 57.75761055784889, Acc: 0.8196, Grad norm: 0.11538616860410872\n",
      "Iteration 3760, BCE loss: 57.757601722836014, Acc: 0.8196, Grad norm: 0.1126956016567463\n",
      "Iteration 3761, BCE loss: 57.75745611259755, Acc: 0.8196, Grad norm: 0.07957553117983429\n",
      "Iteration 3762, BCE loss: 57.7574984995323, Acc: 0.8196, Grad norm: 0.08552150517798185\n",
      "Iteration 3763, BCE loss: 57.75745057207554, Acc: 0.8196, Grad norm: 0.07732093837046358\n",
      "Iteration 3764, BCE loss: 57.75768995454393, Acc: 0.8196, Grad norm: 0.12580355697079182\n",
      "Iteration 3765, BCE loss: 57.75758212437856, Acc: 0.8196, Grad norm: 0.10957495678625573\n",
      "Iteration 3766, BCE loss: 57.7576189244885, Acc: 0.8196, Grad norm: 0.1157230721761259\n",
      "Iteration 3767, BCE loss: 57.75751300316466, Acc: 0.8196, Grad norm: 0.09388694336290568\n",
      "Iteration 3768, BCE loss: 57.75760709075519, Acc: 0.8197, Grad norm: 0.11583961320964846\n",
      "Iteration 3769, BCE loss: 57.757498219607996, Acc: 0.8196, Grad norm: 0.09835454203573792\n",
      "Iteration 3770, BCE loss: 57.75744678047729, Acc: 0.8196, Grad norm: 0.08740178253886159\n",
      "Iteration 3771, BCE loss: 57.75747161823618, Acc: 0.8197, Grad norm: 0.09090374019717387\n",
      "Iteration 3772, BCE loss: 57.7574876375279, Acc: 0.8197, Grad norm: 0.09728551229548361\n",
      "Iteration 3773, BCE loss: 57.757480648274786, Acc: 0.8197, Grad norm: 0.09073270007442306\n",
      "Iteration 3774, BCE loss: 57.757440313254506, Acc: 0.8196, Grad norm: 0.08689425219209482\n",
      "Iteration 3775, BCE loss: 57.75734827869563, Acc: 0.8196, Grad norm: 0.05868060577410721\n",
      "Iteration 3776, BCE loss: 57.75732907473605, Acc: 0.8196, Grad norm: 0.0452317678111\n",
      "Iteration 3777, BCE loss: 57.75741724786319, Acc: 0.8196, Grad norm: 0.07126854858121211\n",
      "Iteration 3778, BCE loss: 57.7573509910133, Acc: 0.8196, Grad norm: 0.05375310296615315\n",
      "Iteration 3779, BCE loss: 57.7573425876605, Acc: 0.8196, Grad norm: 0.051277237511166855\n",
      "Iteration 3780, BCE loss: 57.75751500634658, Acc: 0.8196, Grad norm: 0.10217892233862576\n",
      "Iteration 3781, BCE loss: 57.75752342942407, Acc: 0.8196, Grad norm: 0.10549282391787684\n",
      "Iteration 3782, BCE loss: 57.75744597405986, Acc: 0.8196, Grad norm: 0.08934878427292624\n",
      "Iteration 3783, BCE loss: 57.757487282108656, Acc: 0.8196, Grad norm: 0.09798882106403821\n",
      "Iteration 3784, BCE loss: 57.757504665645484, Acc: 0.8196, Grad norm: 0.10331177927503234\n",
      "Iteration 3785, BCE loss: 57.75760661431962, Acc: 0.8195, Grad norm: 0.12168582758978043\n",
      "Iteration 3786, BCE loss: 57.757536363215614, Acc: 0.8195, Grad norm: 0.10584158102488257\n",
      "Iteration 3787, BCE loss: 57.75746638850896, Acc: 0.8195, Grad norm: 0.09065212252754137\n",
      "Iteration 3788, BCE loss: 57.75749659781388, Acc: 0.8196, Grad norm: 0.09846968564761338\n",
      "Iteration 3789, BCE loss: 57.75752673391382, Acc: 0.8196, Grad norm: 0.10638024756079824\n",
      "Iteration 3790, BCE loss: 57.75755954868353, Acc: 0.8196, Grad norm: 0.1165718477447954\n",
      "Iteration 3791, BCE loss: 57.757538495423276, Acc: 0.8195, Grad norm: 0.10952861020284052\n",
      "Iteration 3792, BCE loss: 57.75749125117626, Acc: 0.8196, Grad norm: 0.10005748832448058\n",
      "Iteration 3793, BCE loss: 57.75743756032119, Acc: 0.8196, Grad norm: 0.0906064010747827\n",
      "Iteration 3794, BCE loss: 57.75759196100894, Acc: 0.8196, Grad norm: 0.12552669000598804\n",
      "Iteration 3795, BCE loss: 57.75745039995942, Acc: 0.8196, Grad norm: 0.09152139348713262\n",
      "Iteration 3796, BCE loss: 57.757505046543, Acc: 0.8196, Grad norm: 0.10429874752793819\n",
      "Iteration 3797, BCE loss: 57.7574739529416, Acc: 0.8196, Grad norm: 0.09636939697674912\n",
      "Iteration 3798, BCE loss: 57.757346354774825, Acc: 0.8196, Grad norm: 0.05732420881791142\n",
      "Iteration 3799, BCE loss: 57.75738951808728, Acc: 0.8196, Grad norm: 0.0737815576277309\n",
      "Iteration 3800, BCE loss: 57.75733230857038, Acc: 0.8196, Grad norm: 0.04798342658527773\n",
      "Iteration 3801, BCE loss: 57.757482217848604, Acc: 0.8196, Grad norm: 0.0935519262468341\n",
      "Iteration 3802, BCE loss: 57.75751994860763, Acc: 0.8196, Grad norm: 0.09969024677246406\n",
      "Iteration 3803, BCE loss: 57.7576352544945, Acc: 0.8196, Grad norm: 0.12093278578057362\n",
      "Iteration 3804, BCE loss: 57.757607302669825, Acc: 0.8195, Grad norm: 0.11122437339322559\n",
      "Iteration 3805, BCE loss: 57.75750855515781, Acc: 0.8196, Grad norm: 0.09500332885799069\n",
      "Iteration 3806, BCE loss: 57.757650131323395, Acc: 0.8196, Grad norm: 0.12865786393742604\n",
      "Iteration 3807, BCE loss: 57.75760459341279, Acc: 0.8196, Grad norm: 0.11826311464902109\n",
      "Iteration 3808, BCE loss: 57.75762149331524, Acc: 0.8196, Grad norm: 0.12316481209851324\n",
      "Iteration 3809, BCE loss: 57.757670420962974, Acc: 0.8196, Grad norm: 0.13039808575014195\n",
      "Iteration 3810, BCE loss: 57.75754276003684, Acc: 0.8196, Grad norm: 0.10843926970874257\n",
      "Iteration 3811, BCE loss: 57.75759336891875, Acc: 0.8196, Grad norm: 0.12018961463819217\n",
      "Iteration 3812, BCE loss: 57.75748286968495, Acc: 0.8196, Grad norm: 0.09474985462741486\n",
      "Iteration 3813, BCE loss: 57.75758665736111, Acc: 0.8196, Grad norm: 0.11609162446034131\n",
      "Iteration 3814, BCE loss: 57.75741797619871, Acc: 0.8196, Grad norm: 0.08053190098180726\n",
      "Iteration 3815, BCE loss: 57.75748480780763, Acc: 0.8196, Grad norm: 0.09487595068072155\n",
      "Iteration 3816, BCE loss: 57.757670412851304, Acc: 0.8196, Grad norm: 0.1263835626066955\n",
      "Iteration 3817, BCE loss: 57.757611742208866, Acc: 0.8196, Grad norm: 0.11651355336728382\n",
      "Iteration 3818, BCE loss: 57.7575028424314, Acc: 0.8196, Grad norm: 0.09526983363622087\n",
      "Iteration 3819, BCE loss: 57.75755485308862, Acc: 0.8196, Grad norm: 0.10448739401755638\n",
      "Iteration 3820, BCE loss: 57.757573312639096, Acc: 0.8196, Grad norm: 0.10956126629646057\n",
      "Iteration 3821, BCE loss: 57.7576800949914, Acc: 0.8196, Grad norm: 0.1291135722501251\n",
      "Iteration 3822, BCE loss: 57.75760858065544, Acc: 0.8196, Grad norm: 0.11753420221124683\n",
      "Iteration 3823, BCE loss: 57.757511716624265, Acc: 0.8196, Grad norm: 0.09648915451884128\n",
      "Iteration 3824, BCE loss: 57.75759297521077, Acc: 0.8196, Grad norm: 0.11420018372871665\n",
      "Iteration 3825, BCE loss: 57.757718748531374, Acc: 0.8196, Grad norm: 0.12985640009731936\n",
      "Iteration 3826, BCE loss: 57.75757105658749, Acc: 0.8196, Grad norm: 0.10101590818880461\n",
      "Iteration 3827, BCE loss: 57.75751309938978, Acc: 0.8196, Grad norm: 0.08901535474839077\n",
      "Iteration 3828, BCE loss: 57.75753537673562, Acc: 0.8196, Grad norm: 0.09559262595151119\n",
      "Iteration 3829, BCE loss: 57.75749033953676, Acc: 0.8196, Grad norm: 0.08588273380411716\n",
      "Iteration 3830, BCE loss: 57.75753548088825, Acc: 0.8196, Grad norm: 0.09788887693110185\n",
      "Iteration 3831, BCE loss: 57.7574120292565, Acc: 0.8196, Grad norm: 0.06809973546031982\n",
      "Iteration 3832, BCE loss: 57.757508867941375, Acc: 0.8196, Grad norm: 0.09466734072110179\n",
      "Iteration 3833, BCE loss: 57.75759003098838, Acc: 0.8195, Grad norm: 0.11449792687494709\n",
      "Iteration 3834, BCE loss: 57.75748086723737, Acc: 0.8196, Grad norm: 0.09288286602511155\n",
      "Iteration 3835, BCE loss: 57.75753327921173, Acc: 0.8196, Grad norm: 0.1062979143904088\n",
      "Iteration 3836, BCE loss: 57.7575095912145, Acc: 0.8196, Grad norm: 0.10347382909994479\n",
      "Iteration 3837, BCE loss: 57.75757965212056, Acc: 0.8196, Grad norm: 0.11532342483686048\n",
      "Iteration 3838, BCE loss: 57.757736038862745, Acc: 0.8196, Grad norm: 0.14073048207720967\n",
      "Iteration 3839, BCE loss: 57.75759918021596, Acc: 0.8196, Grad norm: 0.11436016931177181\n",
      "Iteration 3840, BCE loss: 57.75769793536949, Acc: 0.8196, Grad norm: 0.13401094345344108\n",
      "Iteration 3841, BCE loss: 57.75774689489505, Acc: 0.8196, Grad norm: 0.14428961041461597\n",
      "Iteration 3842, BCE loss: 57.75760077007601, Acc: 0.8195, Grad norm: 0.1189017138037201\n",
      "Iteration 3843, BCE loss: 57.75746792532452, Acc: 0.8195, Grad norm: 0.08976962640853889\n",
      "Iteration 3844, BCE loss: 57.7574196294826, Acc: 0.8196, Grad norm: 0.08011749664559181\n",
      "Iteration 3845, BCE loss: 57.75738333858803, Acc: 0.8195, Grad norm: 0.06641770066035484\n",
      "Iteration 3846, BCE loss: 57.757412868488686, Acc: 0.8196, Grad norm: 0.07399603499056769\n",
      "Iteration 3847, BCE loss: 57.757454145407024, Acc: 0.8196, Grad norm: 0.08429147281894124\n",
      "Iteration 3848, BCE loss: 57.75746611938698, Acc: 0.8196, Grad norm: 0.0869616072838849\n",
      "Iteration 3849, BCE loss: 57.757424819285575, Acc: 0.8196, Grad norm: 0.07347823231006775\n",
      "Iteration 3850, BCE loss: 57.757495364821494, Acc: 0.8196, Grad norm: 0.0894192608574874\n",
      "Iteration 3851, BCE loss: 57.75764386844209, Acc: 0.8195, Grad norm: 0.11921079140996706\n",
      "Iteration 3852, BCE loss: 57.75754966931849, Acc: 0.8195, Grad norm: 0.10325361254281641\n",
      "Iteration 3853, BCE loss: 57.75757454056729, Acc: 0.8195, Grad norm: 0.11028647184505644\n",
      "Iteration 3854, BCE loss: 57.757568747739725, Acc: 0.8196, Grad norm: 0.110061986053529\n",
      "Iteration 3855, BCE loss: 57.757448555237005, Acc: 0.8196, Grad norm: 0.082089863468078\n",
      "Iteration 3856, BCE loss: 57.75747822698477, Acc: 0.8195, Grad norm: 0.08765286663394213\n",
      "Iteration 3857, BCE loss: 57.757441892142324, Acc: 0.8196, Grad norm: 0.0818263864484224\n",
      "Iteration 3858, BCE loss: 57.75753494914703, Acc: 0.8195, Grad norm: 0.10114958464457763\n",
      "Iteration 3859, BCE loss: 57.75747289198442, Acc: 0.8195, Grad norm: 0.08716816029311372\n",
      "Iteration 3860, BCE loss: 57.7575919392569, Acc: 0.8195, Grad norm: 0.11368318887544972\n",
      "Iteration 3861, BCE loss: 57.75753868893244, Acc: 0.8195, Grad norm: 0.10062850264840666\n",
      "Iteration 3862, BCE loss: 57.75760624689503, Acc: 0.8195, Grad norm: 0.11731569806440963\n",
      "Iteration 3863, BCE loss: 57.75752847927451, Acc: 0.8195, Grad norm: 0.09724191894230118\n",
      "Iteration 3864, BCE loss: 57.757605075021075, Acc: 0.8196, Grad norm: 0.11175279123459927\n",
      "Iteration 3865, BCE loss: 57.75747872453555, Acc: 0.8196, Grad norm: 0.08545777495354788\n",
      "Iteration 3866, BCE loss: 57.75759658682579, Acc: 0.8196, Grad norm: 0.11319261974811462\n",
      "Iteration 3867, BCE loss: 57.757538543301074, Acc: 0.8196, Grad norm: 0.09628021808447812\n",
      "Iteration 3868, BCE loss: 57.7575735334276, Acc: 0.8196, Grad norm: 0.10961899015671804\n",
      "Iteration 3869, BCE loss: 57.75780846094369, Acc: 0.8195, Grad norm: 0.15341184003209796\n",
      "Iteration 3870, BCE loss: 57.75754738820393, Acc: 0.8196, Grad norm: 0.10347774541535111\n",
      "Iteration 3871, BCE loss: 57.757566965692476, Acc: 0.8196, Grad norm: 0.10427050602573273\n",
      "Iteration 3872, BCE loss: 57.75755575216555, Acc: 0.8196, Grad norm: 0.0984425146514356\n",
      "Iteration 3873, BCE loss: 57.757489491464185, Acc: 0.8195, Grad norm: 0.09012729498262698\n",
      "Iteration 3874, BCE loss: 57.7576045347679, Acc: 0.8195, Grad norm: 0.11618659052135791\n",
      "Iteration 3875, BCE loss: 57.75770628665024, Acc: 0.8195, Grad norm: 0.13317369656600572\n",
      "Iteration 3876, BCE loss: 57.757697720303256, Acc: 0.8195, Grad norm: 0.13309186619995345\n",
      "Iteration 3877, BCE loss: 57.75760978043338, Acc: 0.8195, Grad norm: 0.1187281218571519\n",
      "Iteration 3878, BCE loss: 57.75753149464168, Acc: 0.8195, Grad norm: 0.10682708126031372\n",
      "Iteration 3879, BCE loss: 57.75750931964399, Acc: 0.8195, Grad norm: 0.10249661859497927\n",
      "Iteration 3880, BCE loss: 57.75756865717068, Acc: 0.8196, Grad norm: 0.11364541343791436\n",
      "Iteration 3881, BCE loss: 57.75761814962502, Acc: 0.8196, Grad norm: 0.12283354587528797\n",
      "Iteration 3882, BCE loss: 57.75757670297132, Acc: 0.8196, Grad norm: 0.11463178016698965\n",
      "Iteration 3883, BCE loss: 57.757663469257075, Acc: 0.8196, Grad norm: 0.1255296361790788\n",
      "Iteration 3884, BCE loss: 57.757816806702486, Acc: 0.8196, Grad norm: 0.14899451064927222\n",
      "Iteration 3885, BCE loss: 57.75770785742401, Acc: 0.8196, Grad norm: 0.12953909439681238\n",
      "Iteration 3886, BCE loss: 57.757786565666, Acc: 0.8196, Grad norm: 0.1468731509293087\n",
      "Iteration 3887, BCE loss: 57.75763721633212, Acc: 0.8196, Grad norm: 0.12225515885484782\n",
      "Iteration 3888, BCE loss: 57.75770062845852, Acc: 0.8196, Grad norm: 0.13002710291133285\n",
      "Iteration 3889, BCE loss: 57.757499017581665, Acc: 0.8196, Grad norm: 0.09304637917013728\n",
      "Iteration 3890, BCE loss: 57.75742578551711, Acc: 0.8196, Grad norm: 0.08044021264485841\n",
      "Iteration 3891, BCE loss: 57.75749685909612, Acc: 0.8196, Grad norm: 0.09692086978604182\n",
      "Iteration 3892, BCE loss: 57.7573950920241, Acc: 0.8196, Grad norm: 0.06853020856035226\n",
      "Iteration 3893, BCE loss: 57.75738939621477, Acc: 0.8196, Grad norm: 0.06557835740139634\n",
      "Iteration 3894, BCE loss: 57.75737908424965, Acc: 0.8196, Grad norm: 0.06522075486603571\n",
      "Iteration 3895, BCE loss: 57.75743703591393, Acc: 0.8196, Grad norm: 0.0813765216658747\n",
      "Iteration 3896, BCE loss: 57.75741104716744, Acc: 0.8196, Grad norm: 0.0741923451632982\n",
      "Iteration 3897, BCE loss: 57.757488505412184, Acc: 0.8196, Grad norm: 0.09362436471608425\n",
      "Iteration 3898, BCE loss: 57.757380191228066, Acc: 0.8196, Grad norm: 0.06135225889200858\n",
      "Iteration 3899, BCE loss: 57.757551320152714, Acc: 0.8196, Grad norm: 0.10857619661404304\n",
      "Iteration 3900, BCE loss: 57.7573700218536, Acc: 0.8196, Grad norm: 0.061157746176254406\n",
      "Iteration 3901, BCE loss: 57.75736854761928, Acc: 0.8196, Grad norm: 0.06156719915901453\n",
      "Iteration 3902, BCE loss: 57.75735639550673, Acc: 0.8196, Grad norm: 0.05879717721397714\n",
      "Iteration 3903, BCE loss: 57.757424362202165, Acc: 0.8196, Grad norm: 0.07896834382203913\n",
      "Iteration 3904, BCE loss: 57.75755050412599, Acc: 0.8195, Grad norm: 0.1117597594054466\n",
      "Iteration 3905, BCE loss: 57.75745723799409, Acc: 0.8195, Grad norm: 0.09106736989028401\n",
      "Iteration 3906, BCE loss: 57.75748041072602, Acc: 0.8195, Grad norm: 0.0976973472296326\n",
      "Iteration 3907, BCE loss: 57.757494888963606, Acc: 0.8195, Grad norm: 0.10146454214369703\n",
      "Iteration 3908, BCE loss: 57.75739171931576, Acc: 0.8196, Grad norm: 0.07460881962208721\n",
      "Iteration 3909, BCE loss: 57.75733982890701, Acc: 0.8196, Grad norm: 0.05395303773446984\n",
      "Iteration 3910, BCE loss: 57.75738938932054, Acc: 0.8195, Grad norm: 0.072635611078846\n",
      "Iteration 3911, BCE loss: 57.75736391047343, Acc: 0.8196, Grad norm: 0.06077399705759942\n",
      "Iteration 3912, BCE loss: 57.75737233350107, Acc: 0.8196, Grad norm: 0.06252215743775622\n",
      "Iteration 3913, BCE loss: 57.75735682376725, Acc: 0.8196, Grad norm: 0.05961115261752435\n",
      "Iteration 3914, BCE loss: 57.75733317622512, Acc: 0.8196, Grad norm: 0.050542689587788264\n",
      "Iteration 3915, BCE loss: 57.75733705378592, Acc: 0.8196, Grad norm: 0.05599816635068813\n",
      "Iteration 3916, BCE loss: 57.75733086115103, Acc: 0.8196, Grad norm: 0.051281460837692076\n",
      "Iteration 3917, BCE loss: 57.7573990385641, Acc: 0.8196, Grad norm: 0.07565212308858033\n",
      "Iteration 3918, BCE loss: 57.75742041337148, Acc: 0.8195, Grad norm: 0.08267713676438739\n",
      "Iteration 3919, BCE loss: 57.7574112445106, Acc: 0.8195, Grad norm: 0.07392696167625021\n",
      "Iteration 3920, BCE loss: 57.757377593139935, Acc: 0.8195, Grad norm: 0.06679597903152451\n",
      "Iteration 3921, BCE loss: 57.757333696768306, Acc: 0.8196, Grad norm: 0.05247826734181081\n",
      "Iteration 3922, BCE loss: 57.757349456835925, Acc: 0.8196, Grad norm: 0.05930478910698299\n",
      "Iteration 3923, BCE loss: 57.757370971699935, Acc: 0.8196, Grad norm: 0.0650659539256545\n",
      "Iteration 3924, BCE loss: 57.75757902500357, Acc: 0.8195, Grad norm: 0.11462953358455705\n",
      "Iteration 3925, BCE loss: 57.75743123420583, Acc: 0.8195, Grad norm: 0.0830747134753228\n",
      "Iteration 3926, BCE loss: 57.75740336909185, Acc: 0.8195, Grad norm: 0.07434845958743816\n",
      "Iteration 3927, BCE loss: 57.75739785817872, Acc: 0.8196, Grad norm: 0.07039391270344894\n",
      "Iteration 3928, BCE loss: 57.75731705285547, Acc: 0.8196, Grad norm: 0.043664844803003644\n",
      "Iteration 3929, BCE loss: 57.75735223791147, Acc: 0.8196, Grad norm: 0.05554270800187052\n",
      "Iteration 3930, BCE loss: 57.757361864364796, Acc: 0.8196, Grad norm: 0.059917494245479805\n",
      "Iteration 3931, BCE loss: 57.757301365971344, Acc: 0.8195, Grad norm: 0.037907300945438975\n",
      "Iteration 3932, BCE loss: 57.757361138481926, Acc: 0.8196, Grad norm: 0.06101214789322297\n",
      "Iteration 3933, BCE loss: 57.757360847601575, Acc: 0.8196, Grad norm: 0.060663827953272444\n",
      "Iteration 3934, BCE loss: 57.757355347858905, Acc: 0.8196, Grad norm: 0.05614755617016805\n",
      "Iteration 3935, BCE loss: 57.75736997812592, Acc: 0.8196, Grad norm: 0.06313371408658218\n",
      "Iteration 3936, BCE loss: 57.7573844628482, Acc: 0.8196, Grad norm: 0.06558252554159699\n",
      "Iteration 3937, BCE loss: 57.75737222190661, Acc: 0.8196, Grad norm: 0.06280961182651366\n",
      "Iteration 3938, BCE loss: 57.75734375850866, Acc: 0.8196, Grad norm: 0.05755731231930296\n",
      "Iteration 3939, BCE loss: 57.757452123959204, Acc: 0.8196, Grad norm: 0.08950100011632896\n",
      "Iteration 3940, BCE loss: 57.75751985863272, Acc: 0.8196, Grad norm: 0.10535500954477937\n",
      "Iteration 3941, BCE loss: 57.75766565388129, Acc: 0.8196, Grad norm: 0.13498433895447254\n",
      "Iteration 3942, BCE loss: 57.75758808696364, Acc: 0.8196, Grad norm: 0.12045551685641244\n",
      "Iteration 3943, BCE loss: 57.75741423718665, Acc: 0.8196, Grad norm: 0.07748801851467459\n",
      "Iteration 3944, BCE loss: 57.75736748249311, Acc: 0.8195, Grad norm: 0.05837704912323248\n",
      "Iteration 3945, BCE loss: 57.757466187571595, Acc: 0.8195, Grad norm: 0.08317952947599064\n",
      "Iteration 3946, BCE loss: 57.757485431885314, Acc: 0.8195, Grad norm: 0.08860950649289706\n",
      "Iteration 3947, BCE loss: 57.757487722235766, Acc: 0.8196, Grad norm: 0.09065623490906421\n",
      "Iteration 3948, BCE loss: 57.757586556945164, Acc: 0.8196, Grad norm: 0.11033662937284289\n",
      "Iteration 3949, BCE loss: 57.757562666993046, Acc: 0.8196, Grad norm: 0.10782823978282462\n",
      "Iteration 3950, BCE loss: 57.757635715347476, Acc: 0.8195, Grad norm: 0.11883858718812372\n",
      "Iteration 3951, BCE loss: 57.75751957404928, Acc: 0.8196, Grad norm: 0.09391791682023415\n",
      "Iteration 3952, BCE loss: 57.75749604960109, Acc: 0.8196, Grad norm: 0.09233787975097055\n",
      "Iteration 3953, BCE loss: 57.7576433681667, Acc: 0.8196, Grad norm: 0.12389728766680114\n",
      "Iteration 3954, BCE loss: 57.757649917515934, Acc: 0.8195, Grad norm: 0.12506172117300954\n",
      "Iteration 3955, BCE loss: 57.75769572288223, Acc: 0.8195, Grad norm: 0.1269860877219301\n",
      "Iteration 3956, BCE loss: 57.75764091921431, Acc: 0.8196, Grad norm: 0.11548646987671565\n",
      "Iteration 3957, BCE loss: 57.75774220045925, Acc: 0.8195, Grad norm: 0.1341493083463586\n",
      "Iteration 3958, BCE loss: 57.757867448863706, Acc: 0.8195, Grad norm: 0.15525076053524783\n",
      "Iteration 3959, BCE loss: 57.75768563764056, Acc: 0.8195, Grad norm: 0.12776195904428336\n",
      "Iteration 3960, BCE loss: 57.757800919680776, Acc: 0.8196, Grad norm: 0.1498073853468554\n",
      "Iteration 3961, BCE loss: 57.75787647150878, Acc: 0.8195, Grad norm: 0.16003699326930754\n",
      "Iteration 3962, BCE loss: 57.75788871875914, Acc: 0.8195, Grad norm: 0.16348348412305097\n",
      "Iteration 3963, BCE loss: 57.75764378403217, Acc: 0.8195, Grad norm: 0.12763993007947141\n",
      "Iteration 3964, BCE loss: 57.75771535809979, Acc: 0.8195, Grad norm: 0.13896461828139645\n",
      "Iteration 3965, BCE loss: 57.75751768912298, Acc: 0.8195, Grad norm: 0.1037290555810147\n",
      "Iteration 3966, BCE loss: 57.75743529776274, Acc: 0.8195, Grad norm: 0.08370406515577393\n",
      "Iteration 3967, BCE loss: 57.75748763489972, Acc: 0.8195, Grad norm: 0.09808771740087062\n",
      "Iteration 3968, BCE loss: 57.757464077667805, Acc: 0.8195, Grad norm: 0.09443058044430429\n",
      "Iteration 3969, BCE loss: 57.7573861263313, Acc: 0.8195, Grad norm: 0.07230486356387772\n",
      "Iteration 3970, BCE loss: 57.75741206312887, Acc: 0.8195, Grad norm: 0.08071156853328437\n",
      "Iteration 3971, BCE loss: 57.75745011173921, Acc: 0.8195, Grad norm: 0.09013112729276813\n",
      "Iteration 3972, BCE loss: 57.7574280468279, Acc: 0.8195, Grad norm: 0.08453185870572387\n",
      "Iteration 3973, BCE loss: 57.75755851146332, Acc: 0.8195, Grad norm: 0.11564752030083716\n",
      "Iteration 3974, BCE loss: 57.75746917693874, Acc: 0.8195, Grad norm: 0.09280985706154904\n",
      "Iteration 3975, BCE loss: 57.75739664462185, Acc: 0.8195, Grad norm: 0.07534044757625523\n",
      "Iteration 3976, BCE loss: 57.75732276365153, Acc: 0.8195, Grad norm: 0.04654907009537641\n",
      "Iteration 3977, BCE loss: 57.75735244512436, Acc: 0.8195, Grad norm: 0.05916219060560146\n",
      "Iteration 3978, BCE loss: 57.757434338825306, Acc: 0.8195, Grad norm: 0.08223709373639279\n",
      "Iteration 3979, BCE loss: 57.7574721527274, Acc: 0.8195, Grad norm: 0.0893833623388993\n",
      "Iteration 3980, BCE loss: 57.75757227825588, Acc: 0.8195, Grad norm: 0.11036073373489166\n",
      "Iteration 3981, BCE loss: 57.75744162067332, Acc: 0.8195, Grad norm: 0.08146770763198832\n",
      "Iteration 3982, BCE loss: 57.757551930956566, Acc: 0.8195, Grad norm: 0.10442014100322246\n",
      "Iteration 3983, BCE loss: 57.75750286886921, Acc: 0.8196, Grad norm: 0.09520463389922415\n",
      "Iteration 3984, BCE loss: 57.757717802686614, Acc: 0.8196, Grad norm: 0.1376493669093339\n",
      "Iteration 3985, BCE loss: 57.75779584911129, Acc: 0.8196, Grad norm: 0.1488471576375798\n",
      "Iteration 3986, BCE loss: 57.75765920206915, Acc: 0.8196, Grad norm: 0.12789857398397037\n",
      "Iteration 3987, BCE loss: 57.75767416226668, Acc: 0.8196, Grad norm: 0.13137369760305367\n",
      "Iteration 3988, BCE loss: 57.757590266286584, Acc: 0.8196, Grad norm: 0.11630784119512487\n",
      "Iteration 3989, BCE loss: 57.75750719651924, Acc: 0.8196, Grad norm: 0.09412824437888957\n",
      "Iteration 3990, BCE loss: 57.75756301329676, Acc: 0.8196, Grad norm: 0.10210755986047537\n",
      "Iteration 3991, BCE loss: 57.75757899029293, Acc: 0.8196, Grad norm: 0.11415046035229529\n",
      "Iteration 3992, BCE loss: 57.75739473959186, Acc: 0.8196, Grad norm: 0.06750319014969451\n",
      "Iteration 3993, BCE loss: 57.757450650901475, Acc: 0.8196, Grad norm: 0.08455570616893375\n",
      "Iteration 3994, BCE loss: 57.75750522919479, Acc: 0.8196, Grad norm: 0.09839678111289425\n",
      "Iteration 3995, BCE loss: 57.75764703395494, Acc: 0.8196, Grad norm: 0.12294442499596214\n",
      "Iteration 3996, BCE loss: 57.75759530453952, Acc: 0.8196, Grad norm: 0.11504739398514909\n",
      "Iteration 3997, BCE loss: 57.757466737006425, Acc: 0.8196, Grad norm: 0.08919956416240926\n",
      "Iteration 3998, BCE loss: 57.75744057936656, Acc: 0.8196, Grad norm: 0.08436841685428373\n",
      "Iteration 3999, BCE loss: 57.75744817578452, Acc: 0.8196, Grad norm: 0.08266656219897577\n",
      "Iteration 4000, BCE loss: 57.75741402606786, Acc: 0.8196, Grad norm: 0.0722024822635184\n",
      "Iteration 4001, BCE loss: 57.757519045115615, Acc: 0.8196, Grad norm: 0.09939656048699809\n",
      "Iteration 4002, BCE loss: 57.757522504557215, Acc: 0.8196, Grad norm: 0.10293707756016168\n",
      "Iteration 4003, BCE loss: 57.75745701970258, Acc: 0.8196, Grad norm: 0.08421331396906687\n",
      "Iteration 4004, BCE loss: 57.75740362172654, Acc: 0.8196, Grad norm: 0.07233885861575433\n",
      "Iteration 4005, BCE loss: 57.757434069090536, Acc: 0.8196, Grad norm: 0.08010111722706548\n",
      "Iteration 4006, BCE loss: 57.75748936578853, Acc: 0.8196, Grad norm: 0.09679779313215024\n",
      "Iteration 4007, BCE loss: 57.75740506064699, Acc: 0.8196, Grad norm: 0.07497476102655573\n",
      "Iteration 4008, BCE loss: 57.75740320531266, Acc: 0.8196, Grad norm: 0.07229388613414688\n",
      "Iteration 4009, BCE loss: 57.757422352714485, Acc: 0.8196, Grad norm: 0.07836157680637845\n",
      "Iteration 4010, BCE loss: 57.75752121077963, Acc: 0.8196, Grad norm: 0.10248869698500941\n",
      "Iteration 4011, BCE loss: 57.757829088532155, Acc: 0.8196, Grad norm: 0.15532454829503753\n",
      "Iteration 4012, BCE loss: 57.75765799241768, Acc: 0.8195, Grad norm: 0.12382581515750234\n",
      "Iteration 4013, BCE loss: 57.75767399436723, Acc: 0.8195, Grad norm: 0.12821916984086315\n",
      "Iteration 4014, BCE loss: 57.75771214539809, Acc: 0.8196, Grad norm: 0.13729513570329552\n",
      "Iteration 4015, BCE loss: 57.75768564040921, Acc: 0.8196, Grad norm: 0.13200527723509795\n",
      "Iteration 4016, BCE loss: 57.75754010743307, Acc: 0.8196, Grad norm: 0.10359041799722805\n",
      "Iteration 4017, BCE loss: 57.75754446051694, Acc: 0.8196, Grad norm: 0.10609274044265518\n",
      "Iteration 4018, BCE loss: 57.75744991508347, Acc: 0.8196, Grad norm: 0.08007672940770388\n",
      "Iteration 4019, BCE loss: 57.75742613616225, Acc: 0.8195, Grad norm: 0.07724076782252248\n",
      "Iteration 4020, BCE loss: 57.757426039150616, Acc: 0.8195, Grad norm: 0.07708852025882733\n",
      "Iteration 4021, BCE loss: 57.75755597233096, Acc: 0.8195, Grad norm: 0.10267758109807068\n",
      "Iteration 4022, BCE loss: 57.75762822379002, Acc: 0.8196, Grad norm: 0.11782731426013608\n",
      "Iteration 4023, BCE loss: 57.75764295811477, Acc: 0.8196, Grad norm: 0.12201660573763537\n",
      "Iteration 4024, BCE loss: 57.75764882176102, Acc: 0.8196, Grad norm: 0.12140656508107159\n",
      "Iteration 4025, BCE loss: 57.757558908187264, Acc: 0.8196, Grad norm: 0.1060971397538277\n",
      "Iteration 4026, BCE loss: 57.75759438894556, Acc: 0.8196, Grad norm: 0.11464322681862392\n",
      "Iteration 4027, BCE loss: 57.75763246186081, Acc: 0.8196, Grad norm: 0.12659209200339228\n",
      "Iteration 4028, BCE loss: 57.757543724349645, Acc: 0.8196, Grad norm: 0.10631604329789364\n",
      "Iteration 4029, BCE loss: 57.75748257486086, Acc: 0.8195, Grad norm: 0.09083439584925525\n",
      "Iteration 4030, BCE loss: 57.75745073398573, Acc: 0.8195, Grad norm: 0.0786984064941142\n",
      "Iteration 4031, BCE loss: 57.75741263411458, Acc: 0.8195, Grad norm: 0.07036082869549205\n",
      "Iteration 4032, BCE loss: 57.757427346707566, Acc: 0.8195, Grad norm: 0.07303795309749031\n",
      "Iteration 4033, BCE loss: 57.75742420425602, Acc: 0.8195, Grad norm: 0.07614923894235337\n",
      "Iteration 4034, BCE loss: 57.75739945054686, Acc: 0.8195, Grad norm: 0.07016570679089616\n",
      "Iteration 4035, BCE loss: 57.757459072502584, Acc: 0.8195, Grad norm: 0.08806264813033012\n",
      "Iteration 4036, BCE loss: 57.75761124900941, Acc: 0.8194, Grad norm: 0.11510724733268772\n",
      "Iteration 4037, BCE loss: 57.75762646312913, Acc: 0.8194, Grad norm: 0.12253242094489142\n",
      "Iteration 4038, BCE loss: 57.757604560628245, Acc: 0.8194, Grad norm: 0.12029763564250102\n",
      "Iteration 4039, BCE loss: 57.757422836509974, Acc: 0.8195, Grad norm: 0.07889198192623349\n",
      "Iteration 4040, BCE loss: 57.75737024857324, Acc: 0.8195, Grad norm: 0.06449913860901003\n",
      "Iteration 4041, BCE loss: 57.757570784820274, Acc: 0.8195, Grad norm: 0.11634200291477544\n",
      "Iteration 4042, BCE loss: 57.757530376402286, Acc: 0.8195, Grad norm: 0.10733517333815625\n",
      "Iteration 4043, BCE loss: 57.757513608017476, Acc: 0.8195, Grad norm: 0.10295588569024593\n",
      "Iteration 4044, BCE loss: 57.75761040767394, Acc: 0.8195, Grad norm: 0.12179360119057459\n",
      "Iteration 4045, BCE loss: 57.757668374992775, Acc: 0.8195, Grad norm: 0.1323175272153267\n",
      "Iteration 4046, BCE loss: 57.757572126368515, Acc: 0.8195, Grad norm: 0.11498757585839081\n",
      "Iteration 4047, BCE loss: 57.75757243440887, Acc: 0.8195, Grad norm: 0.11500104538533305\n",
      "Iteration 4048, BCE loss: 57.75749074355965, Acc: 0.8195, Grad norm: 0.09667617730436798\n",
      "Iteration 4049, BCE loss: 57.757551390565425, Acc: 0.8195, Grad norm: 0.11114872378056569\n",
      "Iteration 4050, BCE loss: 57.75744116706413, Acc: 0.8195, Grad norm: 0.08498282280113795\n",
      "Iteration 4051, BCE loss: 57.75738485809762, Acc: 0.8195, Grad norm: 0.0688614180932724\n",
      "Iteration 4052, BCE loss: 57.757389130017145, Acc: 0.8196, Grad norm: 0.06873899649145346\n",
      "Iteration 4053, BCE loss: 57.75735476267906, Acc: 0.8196, Grad norm: 0.055650298345210604\n",
      "Iteration 4054, BCE loss: 57.757358242877615, Acc: 0.8196, Grad norm: 0.05946406887122647\n",
      "Iteration 4055, BCE loss: 57.757459141501144, Acc: 0.8196, Grad norm: 0.08914035686979302\n",
      "Iteration 4056, BCE loss: 57.75753159663081, Acc: 0.8197, Grad norm: 0.10309239943200046\n",
      "Iteration 4057, BCE loss: 57.75748768724184, Acc: 0.8196, Grad norm: 0.09426022792035432\n",
      "Iteration 4058, BCE loss: 57.757502748732776, Acc: 0.8196, Grad norm: 0.09561078871000729\n",
      "Iteration 4059, BCE loss: 57.75750336742131, Acc: 0.8197, Grad norm: 0.09826112162902023\n",
      "Iteration 4060, BCE loss: 57.757507250755886, Acc: 0.8196, Grad norm: 0.10482355086112623\n",
      "Iteration 4061, BCE loss: 57.75749182820437, Acc: 0.8196, Grad norm: 0.100671912153823\n",
      "Iteration 4062, BCE loss: 57.75757922443593, Acc: 0.8196, Grad norm: 0.11686908137307528\n",
      "Iteration 4063, BCE loss: 57.75744658270284, Acc: 0.8196, Grad norm: 0.08305511855297322\n",
      "Iteration 4064, BCE loss: 57.75736268931374, Acc: 0.8196, Grad norm: 0.06194683510640872\n",
      "Iteration 4065, BCE loss: 57.75746388313015, Acc: 0.8196, Grad norm: 0.09303568702673162\n",
      "Iteration 4066, BCE loss: 57.75759211583957, Acc: 0.8196, Grad norm: 0.11704395303376668\n",
      "Iteration 4067, BCE loss: 57.757602779460264, Acc: 0.8196, Grad norm: 0.11611676330706198\n",
      "Iteration 4068, BCE loss: 57.7574118388485, Acc: 0.8196, Grad norm: 0.07053708918148924\n",
      "Iteration 4069, BCE loss: 57.75739673515484, Acc: 0.8196, Grad norm: 0.06937736200136398\n",
      "Iteration 4070, BCE loss: 57.75738989069583, Acc: 0.8196, Grad norm: 0.06828303951333839\n",
      "Iteration 4071, BCE loss: 57.75735036184023, Acc: 0.8195, Grad norm: 0.056489187519462414\n",
      "Iteration 4072, BCE loss: 57.75736925036726, Acc: 0.8195, Grad norm: 0.06537880883839965\n",
      "Iteration 4073, BCE loss: 57.75746069609834, Acc: 0.8195, Grad norm: 0.08930642268883997\n",
      "Iteration 4074, BCE loss: 57.75748864116565, Acc: 0.8195, Grad norm: 0.09765650618448833\n",
      "Iteration 4075, BCE loss: 57.75750265661365, Acc: 0.8195, Grad norm: 0.09965392610590607\n",
      "Iteration 4076, BCE loss: 57.757502131768504, Acc: 0.8195, Grad norm: 0.09930710873836533\n",
      "Iteration 4077, BCE loss: 57.75748853899278, Acc: 0.8196, Grad norm: 0.09918706135968325\n",
      "Iteration 4078, BCE loss: 57.7574977961242, Acc: 0.8196, Grad norm: 0.1032669890616689\n",
      "Iteration 4079, BCE loss: 57.757463250402346, Acc: 0.8196, Grad norm: 0.09344700092273014\n",
      "Iteration 4080, BCE loss: 57.757511126776805, Acc: 0.8196, Grad norm: 0.10348576039405338\n",
      "Iteration 4081, BCE loss: 57.75750665050411, Acc: 0.8196, Grad norm: 0.10572862518870865\n",
      "Iteration 4082, BCE loss: 57.757403195558695, Acc: 0.8196, Grad norm: 0.07768677577061091\n",
      "Iteration 4083, BCE loss: 57.757393529558684, Acc: 0.8196, Grad norm: 0.07752125681360018\n",
      "Iteration 4084, BCE loss: 57.75749456362223, Acc: 0.8196, Grad norm: 0.10017709470085666\n",
      "Iteration 4085, BCE loss: 57.75744217117004, Acc: 0.8196, Grad norm: 0.08483701559783687\n",
      "Iteration 4086, BCE loss: 57.757312008811326, Acc: 0.8196, Grad norm: 0.04161979097235296\n",
      "Iteration 4087, BCE loss: 57.757329561675235, Acc: 0.8195, Grad norm: 0.04817173767284098\n",
      "Iteration 4088, BCE loss: 57.757346271410924, Acc: 0.8195, Grad norm: 0.05512485920749356\n",
      "Iteration 4089, BCE loss: 57.75736932121323, Acc: 0.8196, Grad norm: 0.06304408072714017\n",
      "Iteration 4090, BCE loss: 57.757366865067546, Acc: 0.8196, Grad norm: 0.06277848944181044\n",
      "Iteration 4091, BCE loss: 57.757473475967856, Acc: 0.8196, Grad norm: 0.09108422539562261\n",
      "Iteration 4092, BCE loss: 57.75744069924008, Acc: 0.8195, Grad norm: 0.08572047576459839\n",
      "Iteration 4093, BCE loss: 57.757452209198604, Acc: 0.8195, Grad norm: 0.08864618845295513\n",
      "Iteration 4094, BCE loss: 57.75746555041026, Acc: 0.8195, Grad norm: 0.09270452393245616\n",
      "Iteration 4095, BCE loss: 57.7574424740475, Acc: 0.8195, Grad norm: 0.08742535263023957\n",
      "Iteration 4096, BCE loss: 57.75737702601, Acc: 0.8195, Grad norm: 0.06521437951483558\n",
      "Iteration 4097, BCE loss: 57.757553564678226, Acc: 0.8195, Grad norm: 0.10860411352710926\n",
      "Iteration 4098, BCE loss: 57.757409028372976, Acc: 0.8195, Grad norm: 0.07444090594482417\n",
      "Iteration 4099, BCE loss: 57.7573766690399, Acc: 0.8195, Grad norm: 0.06403013402718066\n",
      "Iteration 4100, BCE loss: 57.757462973458026, Acc: 0.8195, Grad norm: 0.08736272269548583\n",
      "Iteration 4101, BCE loss: 57.75749633997602, Acc: 0.8195, Grad norm: 0.09617915133245068\n",
      "Iteration 4102, BCE loss: 57.75742942682126, Acc: 0.8195, Grad norm: 0.07678476728728321\n",
      "Iteration 4103, BCE loss: 57.75748059460437, Acc: 0.8195, Grad norm: 0.08926565777291373\n",
      "Iteration 4104, BCE loss: 57.75760687443039, Acc: 0.8196, Grad norm: 0.11719018388493513\n",
      "Iteration 4105, BCE loss: 57.757509048003655, Acc: 0.8196, Grad norm: 0.09156160047240092\n",
      "Iteration 4106, BCE loss: 57.75756978656598, Acc: 0.8196, Grad norm: 0.09952057141026358\n",
      "Iteration 4107, BCE loss: 57.75762159308453, Acc: 0.8196, Grad norm: 0.11103916038193859\n",
      "Iteration 4108, BCE loss: 57.757652253970676, Acc: 0.8196, Grad norm: 0.12083187735328492\n",
      "Iteration 4109, BCE loss: 57.75754510148741, Acc: 0.8196, Grad norm: 0.10364525778909008\n",
      "Iteration 4110, BCE loss: 57.7575086972756, Acc: 0.8196, Grad norm: 0.09156774019452936\n",
      "Iteration 4111, BCE loss: 57.7574262614106, Acc: 0.8196, Grad norm: 0.07328161338166095\n",
      "Iteration 4112, BCE loss: 57.75737027326191, Acc: 0.8196, Grad norm: 0.05828483329614996\n",
      "Iteration 4113, BCE loss: 57.7573613077314, Acc: 0.8195, Grad norm: 0.0625173888938877\n",
      "Iteration 4114, BCE loss: 57.757326052820524, Acc: 0.8196, Grad norm: 0.04737392976534071\n",
      "Iteration 4115, BCE loss: 57.757402781701785, Acc: 0.8196, Grad norm: 0.06745360436155316\n",
      "Iteration 4116, BCE loss: 57.75747997424625, Acc: 0.8196, Grad norm: 0.09244037104391453\n",
      "Iteration 4117, BCE loss: 57.75754666632943, Acc: 0.8196, Grad norm: 0.10767158081763514\n",
      "Iteration 4118, BCE loss: 57.75764579961849, Acc: 0.8196, Grad norm: 0.12825052522092767\n",
      "Iteration 4119, BCE loss: 57.757533177728405, Acc: 0.8196, Grad norm: 0.109971636788446\n",
      "Iteration 4120, BCE loss: 57.75750286962693, Acc: 0.8196, Grad norm: 0.10242114270190628\n",
      "Iteration 4121, BCE loss: 57.75756372609825, Acc: 0.8196, Grad norm: 0.11490927287009343\n",
      "Iteration 4122, BCE loss: 57.75749618250565, Acc: 0.8195, Grad norm: 0.10090352738426422\n",
      "Iteration 4123, BCE loss: 57.75746350793387, Acc: 0.8195, Grad norm: 0.09234223881228353\n",
      "Iteration 4124, BCE loss: 57.75746232593133, Acc: 0.8196, Grad norm: 0.08762029760554339\n",
      "Iteration 4125, BCE loss: 57.75754935350155, Acc: 0.8195, Grad norm: 0.10956725446173442\n",
      "Iteration 4126, BCE loss: 57.75759308716955, Acc: 0.8195, Grad norm: 0.1176319391086912\n",
      "Iteration 4127, BCE loss: 57.75776234788266, Acc: 0.8196, Grad norm: 0.1428725711655085\n",
      "Iteration 4128, BCE loss: 57.75777823987021, Acc: 0.8196, Grad norm: 0.14480450670971456\n",
      "Iteration 4129, BCE loss: 57.75761463104021, Acc: 0.8196, Grad norm: 0.12037562023168867\n",
      "Iteration 4130, BCE loss: 57.75744827486235, Acc: 0.8196, Grad norm: 0.08809686917804846\n",
      "Iteration 4131, BCE loss: 57.757537782804505, Acc: 0.8195, Grad norm: 0.11145809957478071\n",
      "Iteration 4132, BCE loss: 57.757526824875384, Acc: 0.8195, Grad norm: 0.11125804217186025\n",
      "Iteration 4133, BCE loss: 57.75767884958886, Acc: 0.8195, Grad norm: 0.1369780381116904\n",
      "Iteration 4134, BCE loss: 57.757888680711616, Acc: 0.8195, Grad norm: 0.16714799539242564\n",
      "Iteration 4135, BCE loss: 57.75765672644512, Acc: 0.8196, Grad norm: 0.1301163341095609\n",
      "Iteration 4136, BCE loss: 57.75758792408313, Acc: 0.8196, Grad norm: 0.11772136685872678\n",
      "Iteration 4137, BCE loss: 57.757553527198624, Acc: 0.8196, Grad norm: 0.10793876755597166\n",
      "Iteration 4138, BCE loss: 57.75740567128685, Acc: 0.8196, Grad norm: 0.07802554276472544\n",
      "Iteration 4139, BCE loss: 57.75742767247728, Acc: 0.8196, Grad norm: 0.08072089505597825\n",
      "Iteration 4140, BCE loss: 57.75741711677652, Acc: 0.8196, Grad norm: 0.08047430822203595\n",
      "Iteration 4141, BCE loss: 57.75728678411086, Acc: 0.8196, Grad norm: 0.03190350216199708\n",
      "Iteration 4142, BCE loss: 57.75733539099867, Acc: 0.8196, Grad norm: 0.0551938288841747\n",
      "Iteration 4143, BCE loss: 57.757365083153445, Acc: 0.8196, Grad norm: 0.06835453177855443\n",
      "Iteration 4144, BCE loss: 57.75741525819058, Acc: 0.8195, Grad norm: 0.08415148073872349\n",
      "Iteration 4145, BCE loss: 57.75746506334957, Acc: 0.8195, Grad norm: 0.09375130980434683\n",
      "Iteration 4146, BCE loss: 57.75757377415064, Acc: 0.8195, Grad norm: 0.11424261625891988\n",
      "Iteration 4147, BCE loss: 57.75741001993886, Acc: 0.8195, Grad norm: 0.07606816674965837\n",
      "Iteration 4148, BCE loss: 57.75735259505615, Acc: 0.8195, Grad norm: 0.058060821863716826\n",
      "Iteration 4149, BCE loss: 57.757434350382916, Acc: 0.8195, Grad norm: 0.08216147584765969\n",
      "Iteration 4150, BCE loss: 57.7574482254678, Acc: 0.8195, Grad norm: 0.08461422498014927\n",
      "Iteration 4151, BCE loss: 57.757465140234984, Acc: 0.8195, Grad norm: 0.09246421057008374\n",
      "Iteration 4152, BCE loss: 57.75736623473776, Acc: 0.8195, Grad norm: 0.06436828070460889\n",
      "Iteration 4153, BCE loss: 57.75738785947492, Acc: 0.8195, Grad norm: 0.07290804595076283\n",
      "Iteration 4154, BCE loss: 57.757384944190335, Acc: 0.8195, Grad norm: 0.0691471480913508\n",
      "Iteration 4155, BCE loss: 57.75734384472801, Acc: 0.8195, Grad norm: 0.05718089932375018\n",
      "Iteration 4156, BCE loss: 57.75739168789128, Acc: 0.8195, Grad norm: 0.07294750326707762\n",
      "Iteration 4157, BCE loss: 57.75742902005191, Acc: 0.8195, Grad norm: 0.08792071855304213\n",
      "Iteration 4158, BCE loss: 57.757390192204056, Acc: 0.8195, Grad norm: 0.07546435623385239\n",
      "Iteration 4159, BCE loss: 57.75742152638087, Acc: 0.8195, Grad norm: 0.08123632030155063\n",
      "Iteration 4160, BCE loss: 57.757429771380224, Acc: 0.8195, Grad norm: 0.0857486619229889\n",
      "Iteration 4161, BCE loss: 57.75746980702333, Acc: 0.8195, Grad norm: 0.09816141822892353\n",
      "Iteration 4162, BCE loss: 57.75745530266613, Acc: 0.8195, Grad norm: 0.09383563341710419\n",
      "Iteration 4163, BCE loss: 57.75746062209659, Acc: 0.8195, Grad norm: 0.09445174672132486\n",
      "Iteration 4164, BCE loss: 57.75743963731162, Acc: 0.8195, Grad norm: 0.08596852953842159\n",
      "Iteration 4165, BCE loss: 57.75740135346079, Acc: 0.8195, Grad norm: 0.07550844931236358\n",
      "Iteration 4166, BCE loss: 57.757480250237066, Acc: 0.8195, Grad norm: 0.09395932097748358\n",
      "Iteration 4167, BCE loss: 57.75768035305429, Acc: 0.8195, Grad norm: 0.13580194700523404\n",
      "Iteration 4168, BCE loss: 57.75768830591712, Acc: 0.8195, Grad norm: 0.13464999044020798\n",
      "Iteration 4169, BCE loss: 57.75779760816061, Acc: 0.8195, Grad norm: 0.1548147258328572\n",
      "Iteration 4170, BCE loss: 57.757712968065746, Acc: 0.8195, Grad norm: 0.14188935792638382\n",
      "Iteration 4171, BCE loss: 57.757545603511154, Acc: 0.8196, Grad norm: 0.11440690652152014\n",
      "Iteration 4172, BCE loss: 57.75755272227762, Acc: 0.8196, Grad norm: 0.11750561681870945\n",
      "Iteration 4173, BCE loss: 57.75740336725856, Acc: 0.8196, Grad norm: 0.08097853397365183\n",
      "Iteration 4174, BCE loss: 57.75740676281676, Acc: 0.8196, Grad norm: 0.08029592023144251\n",
      "Iteration 4175, BCE loss: 57.75755529426171, Acc: 0.8196, Grad norm: 0.11600171136768342\n",
      "Iteration 4176, BCE loss: 57.75746217047401, Acc: 0.8196, Grad norm: 0.09643923825362276\n",
      "Iteration 4177, BCE loss: 57.757555214285816, Acc: 0.8196, Grad norm: 0.11641167174985818\n",
      "Iteration 4178, BCE loss: 57.757560971101, Acc: 0.8196, Grad norm: 0.1177055154028894\n",
      "Iteration 4179, BCE loss: 57.75760477497759, Acc: 0.8196, Grad norm: 0.12687493710537495\n",
      "Iteration 4180, BCE loss: 57.75774864560055, Acc: 0.8196, Grad norm: 0.15107636587159612\n",
      "Iteration 4181, BCE loss: 57.75752738960041, Acc: 0.8196, Grad norm: 0.10714175764322845\n",
      "Iteration 4182, BCE loss: 57.757620297140676, Acc: 0.8195, Grad norm: 0.12263786498526763\n",
      "Iteration 4183, BCE loss: 57.75747428376383, Acc: 0.8195, Grad norm: 0.09257364337580437\n",
      "Iteration 4184, BCE loss: 57.757493209141586, Acc: 0.8195, Grad norm: 0.09291664979446965\n",
      "Iteration 4185, BCE loss: 57.757650600501734, Acc: 0.8196, Grad norm: 0.12622933119839158\n",
      "Iteration 4186, BCE loss: 57.757581533350034, Acc: 0.8195, Grad norm: 0.11554455172516537\n",
      "Iteration 4187, BCE loss: 57.75767422412781, Acc: 0.8195, Grad norm: 0.1349540391288567\n",
      "Iteration 4188, BCE loss: 57.75770905230158, Acc: 0.8196, Grad norm: 0.14437769609985995\n",
      "Iteration 4189, BCE loss: 57.757727620119, Acc: 0.8195, Grad norm: 0.14282536553219005\n",
      "Iteration 4190, BCE loss: 57.75761968785996, Acc: 0.8195, Grad norm: 0.12430383376711407\n",
      "Iteration 4191, BCE loss: 57.75775499310131, Acc: 0.8195, Grad norm: 0.1456598867167986\n",
      "Iteration 4192, BCE loss: 57.757710852436844, Acc: 0.8195, Grad norm: 0.1355052772026159\n",
      "Iteration 4193, BCE loss: 57.75765728012369, Acc: 0.8196, Grad norm: 0.13127022768300453\n",
      "Iteration 4194, BCE loss: 57.757571371378916, Acc: 0.8196, Grad norm: 0.11571457301640953\n",
      "Iteration 4195, BCE loss: 57.7574700400733, Acc: 0.8196, Grad norm: 0.09597924142607714\n",
      "Iteration 4196, BCE loss: 57.75745435537043, Acc: 0.8196, Grad norm: 0.08962395324373319\n",
      "Iteration 4197, BCE loss: 57.757439390160314, Acc: 0.8196, Grad norm: 0.0867983037083171\n",
      "Iteration 4198, BCE loss: 57.757635398931875, Acc: 0.8197, Grad norm: 0.12905788157019557\n",
      "Iteration 4199, BCE loss: 57.75777044823948, Acc: 0.8197, Grad norm: 0.15158356561326689\n",
      "Iteration 4200, BCE loss: 57.75762091058171, Acc: 0.8196, Grad norm: 0.1274082907767153\n",
      "Iteration 4201, BCE loss: 57.75742786086193, Acc: 0.8196, Grad norm: 0.08004510176149196\n",
      "Iteration 4202, BCE loss: 57.75744060067031, Acc: 0.8196, Grad norm: 0.08394701911380105\n",
      "Iteration 4203, BCE loss: 57.75740711986275, Acc: 0.8196, Grad norm: 0.07959257175411052\n",
      "Iteration 4204, BCE loss: 57.75739652664027, Acc: 0.8196, Grad norm: 0.07637328043338444\n",
      "Iteration 4205, BCE loss: 57.757380797723165, Acc: 0.8196, Grad norm: 0.07037327735126835\n",
      "Iteration 4206, BCE loss: 57.75733757023919, Acc: 0.8196, Grad norm: 0.05330518847072826\n",
      "Iteration 4207, BCE loss: 57.7573989209906, Acc: 0.8196, Grad norm: 0.07286652152584829\n",
      "Iteration 4208, BCE loss: 57.75747185266512, Acc: 0.8196, Grad norm: 0.09343505352078027\n",
      "Iteration 4209, BCE loss: 57.75756341409584, Acc: 0.8196, Grad norm: 0.11366423205984977\n",
      "Iteration 4210, BCE loss: 57.757488317100325, Acc: 0.8196, Grad norm: 0.09591282878614245\n",
      "Iteration 4211, BCE loss: 57.75742183015161, Acc: 0.8196, Grad norm: 0.07980257310372399\n",
      "Iteration 4212, BCE loss: 57.757445572170056, Acc: 0.8196, Grad norm: 0.08528000118304788\n",
      "Iteration 4213, BCE loss: 57.75759360054663, Acc: 0.8196, Grad norm: 0.1175439536474009\n",
      "Iteration 4214, BCE loss: 57.7575681736323, Acc: 0.8196, Grad norm: 0.11127914429446793\n",
      "Iteration 4215, BCE loss: 57.75787648930598, Acc: 0.8196, Grad norm: 0.1586467142364781\n",
      "Iteration 4216, BCE loss: 57.75761624932908, Acc: 0.8196, Grad norm: 0.11669699023908343\n",
      "Iteration 4217, BCE loss: 57.75755065753789, Acc: 0.8196, Grad norm: 0.10100577071668979\n",
      "Iteration 4218, BCE loss: 57.75766456616967, Acc: 0.8196, Grad norm: 0.1231147458784231\n",
      "Iteration 4219, BCE loss: 57.75762561622412, Acc: 0.8196, Grad norm: 0.11588262030765269\n",
      "Iteration 4220, BCE loss: 57.7575939352573, Acc: 0.8196, Grad norm: 0.11224065407058836\n",
      "Iteration 4221, BCE loss: 57.757614631144335, Acc: 0.8196, Grad norm: 0.11944362627494666\n",
      "Iteration 4222, BCE loss: 57.75770419206121, Acc: 0.8196, Grad norm: 0.1360393242447835\n",
      "Iteration 4223, BCE loss: 57.757737767684695, Acc: 0.8196, Grad norm: 0.14032400355039612\n",
      "Iteration 4224, BCE loss: 57.7576463230386, Acc: 0.8196, Grad norm: 0.12901229972638747\n",
      "Iteration 4225, BCE loss: 57.75772395418828, Acc: 0.8196, Grad norm: 0.1455630188327491\n",
      "Iteration 4226, BCE loss: 57.75748789389024, Acc: 0.8196, Grad norm: 0.10096220424189672\n",
      "Iteration 4227, BCE loss: 57.7574167035159, Acc: 0.8196, Grad norm: 0.08354009179478909\n",
      "Iteration 4228, BCE loss: 57.75750899541444, Acc: 0.8196, Grad norm: 0.10580781258774959\n",
      "Iteration 4229, BCE loss: 57.75759836523858, Acc: 0.8196, Grad norm: 0.1199945700979239\n",
      "Iteration 4230, BCE loss: 57.75753671207326, Acc: 0.8196, Grad norm: 0.10713930755229549\n",
      "Iteration 4231, BCE loss: 57.75759627123409, Acc: 0.8196, Grad norm: 0.12003551345692728\n",
      "Iteration 4232, BCE loss: 57.757511906582906, Acc: 0.8196, Grad norm: 0.10310704649523238\n",
      "Iteration 4233, BCE loss: 57.75755290894979, Acc: 0.8196, Grad norm: 0.111458588347839\n",
      "Iteration 4234, BCE loss: 57.757494216900255, Acc: 0.8196, Grad norm: 0.09876810883538631\n",
      "Iteration 4235, BCE loss: 57.75760097653175, Acc: 0.8196, Grad norm: 0.12251496937333804\n",
      "Iteration 4236, BCE loss: 57.757453909845296, Acc: 0.8196, Grad norm: 0.08895050657382164\n",
      "Iteration 4237, BCE loss: 57.757388735244646, Acc: 0.8196, Grad norm: 0.06973699048752201\n",
      "Iteration 4238, BCE loss: 57.757564065625886, Acc: 0.8196, Grad norm: 0.11122785950468532\n",
      "Iteration 4239, BCE loss: 57.75750492502408, Acc: 0.8196, Grad norm: 0.09942548837149819\n",
      "Iteration 4240, BCE loss: 57.757444487595365, Acc: 0.8196, Grad norm: 0.0837145823377068\n",
      "Iteration 4241, BCE loss: 57.75738175634112, Acc: 0.8196, Grad norm: 0.06574841111348895\n",
      "Iteration 4242, BCE loss: 57.75735094079707, Acc: 0.8196, Grad norm: 0.05828988357516529\n",
      "Iteration 4243, BCE loss: 57.75741233175077, Acc: 0.8196, Grad norm: 0.07613105089751492\n",
      "Iteration 4244, BCE loss: 57.757438926799196, Acc: 0.8196, Grad norm: 0.08295683775090225\n",
      "Iteration 4245, BCE loss: 57.75747557759731, Acc: 0.8196, Grad norm: 0.09433228514113029\n",
      "Iteration 4246, BCE loss: 57.75742262386656, Acc: 0.8196, Grad norm: 0.08272006103778476\n",
      "Iteration 4247, BCE loss: 57.7574808406122, Acc: 0.8196, Grad norm: 0.09931591680483161\n",
      "Iteration 4248, BCE loss: 57.757385309800966, Acc: 0.8196, Grad norm: 0.07067707432258816\n",
      "Iteration 4249, BCE loss: 57.75752757229049, Acc: 0.8196, Grad norm: 0.10610793370302199\n",
      "Iteration 4250, BCE loss: 57.75740948036695, Acc: 0.8196, Grad norm: 0.07670399814115725\n",
      "Iteration 4251, BCE loss: 57.75748982030875, Acc: 0.8196, Grad norm: 0.09824359263121973\n",
      "Iteration 4252, BCE loss: 57.75752254463384, Acc: 0.8196, Grad norm: 0.10117829481014\n",
      "Iteration 4253, BCE loss: 57.75756285899651, Acc: 0.8197, Grad norm: 0.10777343071942329\n",
      "Iteration 4254, BCE loss: 57.75742517496339, Acc: 0.8196, Grad norm: 0.07523223445398404\n",
      "Iteration 4255, BCE loss: 57.757405517065216, Acc: 0.8196, Grad norm: 0.07322380824038364\n",
      "Iteration 4256, BCE loss: 57.75743507879777, Acc: 0.8196, Grad norm: 0.07619438580767057\n",
      "Iteration 4257, BCE loss: 57.757398312485776, Acc: 0.8196, Grad norm: 0.07019987432118517\n",
      "Iteration 4258, BCE loss: 57.757407076846135, Acc: 0.8196, Grad norm: 0.07418749140994745\n",
      "Iteration 4259, BCE loss: 57.75753412850442, Acc: 0.8196, Grad norm: 0.107043231506275\n",
      "Iteration 4260, BCE loss: 57.75771257136151, Acc: 0.8196, Grad norm: 0.13886721046598685\n",
      "Iteration 4261, BCE loss: 57.75792874090513, Acc: 0.8197, Grad norm: 0.17215903025643162\n",
      "Iteration 4262, BCE loss: 57.757863264123486, Acc: 0.8196, Grad norm: 0.16158082835744084\n",
      "Iteration 4263, BCE loss: 57.7576816949522, Acc: 0.8196, Grad norm: 0.13635065465427157\n",
      "Iteration 4264, BCE loss: 57.757583627859105, Acc: 0.8196, Grad norm: 0.11950349509906179\n",
      "Iteration 4265, BCE loss: 57.75746525606685, Acc: 0.8196, Grad norm: 0.08709100962932459\n",
      "Iteration 4266, BCE loss: 57.757464200864035, Acc: 0.8196, Grad norm: 0.08674725606285312\n",
      "Iteration 4267, BCE loss: 57.757444385812036, Acc: 0.8196, Grad norm: 0.08208677391844064\n",
      "Iteration 4268, BCE loss: 57.75740073182341, Acc: 0.8195, Grad norm: 0.07232716329943781\n",
      "Iteration 4269, BCE loss: 57.75741269061746, Acc: 0.8196, Grad norm: 0.07303202519154821\n",
      "Iteration 4270, BCE loss: 57.757411921764664, Acc: 0.8196, Grad norm: 0.07802402006218434\n",
      "Iteration 4271, BCE loss: 57.757310131016986, Acc: 0.8196, Grad norm: 0.044534264575132904\n",
      "Iteration 4272, BCE loss: 57.75735648908252, Acc: 0.8196, Grad norm: 0.0641233726673837\n",
      "Iteration 4273, BCE loss: 57.757423888733314, Acc: 0.8195, Grad norm: 0.08278835546650133\n",
      "Iteration 4274, BCE loss: 57.757461985270716, Acc: 0.8195, Grad norm: 0.09405607279662107\n",
      "Iteration 4275, BCE loss: 57.75747966205691, Acc: 0.8195, Grad norm: 0.09939487240278758\n",
      "Iteration 4276, BCE loss: 57.757464649887524, Acc: 0.8195, Grad norm: 0.09463632115593895\n",
      "Iteration 4277, BCE loss: 57.757554242705446, Acc: 0.8195, Grad norm: 0.11128961305772517\n",
      "Iteration 4278, BCE loss: 57.75754467199951, Acc: 0.8196, Grad norm: 0.10914925690590417\n",
      "Iteration 4279, BCE loss: 57.757676640606775, Acc: 0.8196, Grad norm: 0.1323072881839317\n",
      "Iteration 4280, BCE loss: 57.757641572470625, Acc: 0.8196, Grad norm: 0.1250440702203938\n",
      "Iteration 4281, BCE loss: 57.75750980349011, Acc: 0.8196, Grad norm: 0.09934322635046504\n",
      "Iteration 4282, BCE loss: 57.757475842650024, Acc: 0.8196, Grad norm: 0.09218834056156934\n",
      "Iteration 4283, BCE loss: 57.7575044797772, Acc: 0.8196, Grad norm: 0.09911460539162817\n",
      "Iteration 4284, BCE loss: 57.757585666338244, Acc: 0.8196, Grad norm: 0.1147842253400632\n",
      "Iteration 4285, BCE loss: 57.75742469725323, Acc: 0.8196, Grad norm: 0.07974951064071145\n",
      "Iteration 4286, BCE loss: 57.757558533347634, Acc: 0.8196, Grad norm: 0.10936320643809237\n",
      "Iteration 4287, BCE loss: 57.75757821214532, Acc: 0.8196, Grad norm: 0.11448866387924227\n",
      "Iteration 4288, BCE loss: 57.75748360997456, Acc: 0.8196, Grad norm: 0.09477626889006781\n",
      "Iteration 4289, BCE loss: 57.75739368417014, Acc: 0.8196, Grad norm: 0.07399282078665904\n",
      "Iteration 4290, BCE loss: 57.757329737035874, Acc: 0.8196, Grad norm: 0.0486387927989482\n",
      "Iteration 4291, BCE loss: 57.75736395960037, Acc: 0.8196, Grad norm: 0.06394413220380239\n",
      "Iteration 4292, BCE loss: 57.75730320545604, Acc: 0.8196, Grad norm: 0.03757067110426221\n",
      "Iteration 4293, BCE loss: 57.7573088560812, Acc: 0.8196, Grad norm: 0.04244090369443892\n",
      "Iteration 4294, BCE loss: 57.757335957088515, Acc: 0.8196, Grad norm: 0.05514922571003623\n",
      "Iteration 4295, BCE loss: 57.757347605335205, Acc: 0.8196, Grad norm: 0.0614996545141009\n",
      "Iteration 4296, BCE loss: 57.75738876535411, Acc: 0.8196, Grad norm: 0.07512417547014222\n",
      "Iteration 4297, BCE loss: 57.75743490402263, Acc: 0.8196, Grad norm: 0.08758255495782055\n",
      "Iteration 4298, BCE loss: 57.757366090814855, Acc: 0.8196, Grad norm: 0.0660955525024744\n",
      "Iteration 4299, BCE loss: 57.75736237090267, Acc: 0.8196, Grad norm: 0.06313672273863458\n",
      "Iteration 4300, BCE loss: 57.75737484717962, Acc: 0.8195, Grad norm: 0.06850640332166555\n",
      "Iteration 4301, BCE loss: 57.75742130797744, Acc: 0.8196, Grad norm: 0.0813349164611309\n",
      "Iteration 4302, BCE loss: 57.75750201892434, Acc: 0.8195, Grad norm: 0.10108652573582398\n",
      "Iteration 4303, BCE loss: 57.75747024401263, Acc: 0.8196, Grad norm: 0.09080877269221088\n",
      "Iteration 4304, BCE loss: 57.75738530873582, Acc: 0.8195, Grad norm: 0.0697204620844289\n",
      "Iteration 4305, BCE loss: 57.75732564930448, Acc: 0.8195, Grad norm: 0.049775339286851746\n",
      "Iteration 4306, BCE loss: 57.75733646446494, Acc: 0.8195, Grad norm: 0.05551919445340502\n",
      "Iteration 4307, BCE loss: 57.757330858518934, Acc: 0.8195, Grad norm: 0.04921877448512043\n",
      "Iteration 4308, BCE loss: 57.75733322491479, Acc: 0.8196, Grad norm: 0.05127493088812166\n",
      "Iteration 4309, BCE loss: 57.7573612705895, Acc: 0.8196, Grad norm: 0.06059224114183378\n",
      "Iteration 4310, BCE loss: 57.757462119395484, Acc: 0.8195, Grad norm: 0.09230797568958143\n",
      "Iteration 4311, BCE loss: 57.75756236768923, Acc: 0.8195, Grad norm: 0.1101585560578134\n",
      "Iteration 4312, BCE loss: 57.7576201203149, Acc: 0.8196, Grad norm: 0.12438663198389886\n",
      "Iteration 4313, BCE loss: 57.75748530403126, Acc: 0.8196, Grad norm: 0.09657081115839862\n",
      "Iteration 4314, BCE loss: 57.75744118373299, Acc: 0.8196, Grad norm: 0.08459443904950809\n",
      "Iteration 4315, BCE loss: 57.757500030503564, Acc: 0.8195, Grad norm: 0.10029610520022945\n",
      "Iteration 4316, BCE loss: 57.75748190488535, Acc: 0.8196, Grad norm: 0.09717135751365874\n",
      "Iteration 4317, BCE loss: 57.757511145197554, Acc: 0.8196, Grad norm: 0.10106143308137978\n",
      "Iteration 4318, BCE loss: 57.757576306439, Acc: 0.8196, Grad norm: 0.11656369049975035\n",
      "Iteration 4319, BCE loss: 57.75758174276477, Acc: 0.8196, Grad norm: 0.11900749413922146\n",
      "Iteration 4320, BCE loss: 57.75748261551582, Acc: 0.8196, Grad norm: 0.09597342185882346\n",
      "Iteration 4321, BCE loss: 57.75759338838704, Acc: 0.8196, Grad norm: 0.11790895054439372\n",
      "Iteration 4322, BCE loss: 57.75753978154077, Acc: 0.8196, Grad norm: 0.10933293666807906\n",
      "Iteration 4323, BCE loss: 57.75750872550131, Acc: 0.8196, Grad norm: 0.10287296968586349\n",
      "Iteration 4324, BCE loss: 57.75742253683608, Acc: 0.8196, Grad norm: 0.0814151189228516\n",
      "Iteration 4325, BCE loss: 57.757373118672525, Acc: 0.8196, Grad norm: 0.06498786503987448\n",
      "Iteration 4326, BCE loss: 57.75739793328694, Acc: 0.8195, Grad norm: 0.07202757203209832\n",
      "Iteration 4327, BCE loss: 57.75735877662184, Acc: 0.8195, Grad norm: 0.06140426211823534\n",
      "Iteration 4328, BCE loss: 57.75752927064699, Acc: 0.8195, Grad norm: 0.10594029490258544\n",
      "Iteration 4329, BCE loss: 57.75775043468623, Acc: 0.8195, Grad norm: 0.1440212495482962\n",
      "Iteration 4330, BCE loss: 57.757698913510815, Acc: 0.8195, Grad norm: 0.1368251493260726\n",
      "Iteration 4331, BCE loss: 57.757722872322425, Acc: 0.8195, Grad norm: 0.14189437912531375\n",
      "Iteration 4332, BCE loss: 57.75763846483594, Acc: 0.8195, Grad norm: 0.1272368478793714\n",
      "Iteration 4333, BCE loss: 57.757661197126765, Acc: 0.8195, Grad norm: 0.1273979184985931\n",
      "Iteration 4334, BCE loss: 57.75767826054406, Acc: 0.8195, Grad norm: 0.13366362636412008\n",
      "Iteration 4335, BCE loss: 57.757556338856254, Acc: 0.8195, Grad norm: 0.11040026668214598\n",
      "Iteration 4336, BCE loss: 57.75761802095871, Acc: 0.8195, Grad norm: 0.12453404579480942\n",
      "Iteration 4337, BCE loss: 57.757461873639215, Acc: 0.8196, Grad norm: 0.09130017637404947\n",
      "Iteration 4338, BCE loss: 57.75740991932156, Acc: 0.8196, Grad norm: 0.08101055424964049\n",
      "Iteration 4339, BCE loss: 57.75732943107546, Acc: 0.8196, Grad norm: 0.05128076687894779\n",
      "Iteration 4340, BCE loss: 57.75736711275475, Acc: 0.8196, Grad norm: 0.06656290835896311\n",
      "Iteration 4341, BCE loss: 57.75746608750663, Acc: 0.8195, Grad norm: 0.09473962724485516\n",
      "Iteration 4342, BCE loss: 57.757364015594966, Acc: 0.8196, Grad norm: 0.06177472194858242\n",
      "Iteration 4343, BCE loss: 57.75744549388092, Acc: 0.8195, Grad norm: 0.08645528255577396\n",
      "Iteration 4344, BCE loss: 57.75741982280947, Acc: 0.8195, Grad norm: 0.0818058912976629\n",
      "Iteration 4345, BCE loss: 57.757420002497035, Acc: 0.8195, Grad norm: 0.08029768441660076\n",
      "Iteration 4346, BCE loss: 57.75746701674734, Acc: 0.8195, Grad norm: 0.09171112746365746\n",
      "Iteration 4347, BCE loss: 57.75753681438482, Acc: 0.8196, Grad norm: 0.10582647864095919\n",
      "Iteration 4348, BCE loss: 57.75742252596027, Acc: 0.8195, Grad norm: 0.07879861095684308\n",
      "Iteration 4349, BCE loss: 57.75736321475298, Acc: 0.8195, Grad norm: 0.06246968730778553\n",
      "Iteration 4350, BCE loss: 57.757429881960306, Acc: 0.8196, Grad norm: 0.08328087807811999\n",
      "Iteration 4351, BCE loss: 57.757504501688764, Acc: 0.8195, Grad norm: 0.10027474035198376\n",
      "Iteration 4352, BCE loss: 57.757597035818634, Acc: 0.8196, Grad norm: 0.11886183153543228\n",
      "Iteration 4353, BCE loss: 57.757463518835, Acc: 0.8196, Grad norm: 0.09325027861594824\n",
      "Iteration 4354, BCE loss: 57.75741699304148, Acc: 0.8196, Grad norm: 0.0809133186755683\n",
      "Iteration 4355, BCE loss: 57.75742341777794, Acc: 0.8196, Grad norm: 0.08475050403550526\n",
      "Iteration 4356, BCE loss: 57.75741964545233, Acc: 0.8196, Grad norm: 0.08396212542167865\n",
      "Iteration 4357, BCE loss: 57.75741105321687, Acc: 0.8196, Grad norm: 0.07757923579043426\n",
      "Iteration 4358, BCE loss: 57.75734954582502, Acc: 0.8196, Grad norm: 0.057987117534526345\n",
      "Iteration 4359, BCE loss: 57.7574131685581, Acc: 0.8196, Grad norm: 0.07989972998905917\n",
      "Iteration 4360, BCE loss: 57.75744692637474, Acc: 0.8196, Grad norm: 0.089853164353728\n",
      "Iteration 4361, BCE loss: 57.75744324791995, Acc: 0.8196, Grad norm: 0.09062085250887492\n",
      "Iteration 4362, BCE loss: 57.757422662424574, Acc: 0.8196, Grad norm: 0.08329654456539078\n",
      "Iteration 4363, BCE loss: 57.757365129503555, Acc: 0.8196, Grad norm: 0.063636594869595\n",
      "Iteration 4364, BCE loss: 57.75739582746695, Acc: 0.8196, Grad norm: 0.07390542183002083\n",
      "Iteration 4365, BCE loss: 57.75741923556207, Acc: 0.8196, Grad norm: 0.08419697991090035\n",
      "Iteration 4366, BCE loss: 57.75739369097282, Acc: 0.8196, Grad norm: 0.07537657227295409\n",
      "Iteration 4367, BCE loss: 57.75734728599349, Acc: 0.8195, Grad norm: 0.056225772014062175\n",
      "Iteration 4368, BCE loss: 57.75733750354762, Acc: 0.8196, Grad norm: 0.049903235680524995\n",
      "Iteration 4369, BCE loss: 57.757363025620705, Acc: 0.8195, Grad norm: 0.05700751270913552\n",
      "Iteration 4370, BCE loss: 57.757393642774204, Acc: 0.8195, Grad norm: 0.06929842989944704\n",
      "Iteration 4371, BCE loss: 57.75740602164595, Acc: 0.8196, Grad norm: 0.07188114653188984\n",
      "Iteration 4372, BCE loss: 57.75754385284507, Acc: 0.8196, Grad norm: 0.10571510244452446\n",
      "Iteration 4373, BCE loss: 57.75750600679524, Acc: 0.8196, Grad norm: 0.09726175272721772\n",
      "Iteration 4374, BCE loss: 57.75772067680742, Acc: 0.8196, Grad norm: 0.1388027751621792\n",
      "Iteration 4375, BCE loss: 57.75762866571047, Acc: 0.8196, Grad norm: 0.1204035544886892\n",
      "Iteration 4376, BCE loss: 57.757479731520974, Acc: 0.8196, Grad norm: 0.08887181941699782\n",
      "Iteration 4377, BCE loss: 57.75751856820412, Acc: 0.8196, Grad norm: 0.09537128455929011\n",
      "Iteration 4378, BCE loss: 57.757521690011075, Acc: 0.8196, Grad norm: 0.09466501818947101\n",
      "Iteration 4379, BCE loss: 57.75759117347782, Acc: 0.8196, Grad norm: 0.10400802353788442\n",
      "Iteration 4380, BCE loss: 57.757637888023346, Acc: 0.8196, Grad norm: 0.11745500466648598\n",
      "Iteration 4381, BCE loss: 57.75772848524153, Acc: 0.8196, Grad norm: 0.13340600828177718\n",
      "Iteration 4382, BCE loss: 57.75771598870108, Acc: 0.8195, Grad norm: 0.1294904124746095\n",
      "Iteration 4383, BCE loss: 57.757729169208204, Acc: 0.8195, Grad norm: 0.13137277267565434\n",
      "Iteration 4384, BCE loss: 57.75790294442059, Acc: 0.8195, Grad norm: 0.15472478537870002\n",
      "Iteration 4385, BCE loss: 57.75785666513636, Acc: 0.8195, Grad norm: 0.14938419938792466\n",
      "Iteration 4386, BCE loss: 57.757706016259206, Acc: 0.8195, Grad norm: 0.12889194426746695\n",
      "Iteration 4387, BCE loss: 57.75777466091227, Acc: 0.8196, Grad norm: 0.13951788453805622\n",
      "Iteration 4388, BCE loss: 57.757668822867736, Acc: 0.8196, Grad norm: 0.12983855334420158\n",
      "Iteration 4389, BCE loss: 57.75784265252534, Acc: 0.8196, Grad norm: 0.16194018448606087\n",
      "Iteration 4390, BCE loss: 57.7576128412736, Acc: 0.8196, Grad norm: 0.12614055136265315\n",
      "Iteration 4391, BCE loss: 57.75769913550553, Acc: 0.8196, Grad norm: 0.14203687670612206\n",
      "Iteration 4392, BCE loss: 57.7577756307236, Acc: 0.8197, Grad norm: 0.15428152334454284\n",
      "Iteration 4393, BCE loss: 57.75766126106506, Acc: 0.8196, Grad norm: 0.13506643571666155\n",
      "Iteration 4394, BCE loss: 57.75755547072537, Acc: 0.8196, Grad norm: 0.1140405862031313\n",
      "Iteration 4395, BCE loss: 57.757572919781296, Acc: 0.8196, Grad norm: 0.11553555589967572\n",
      "Iteration 4396, BCE loss: 57.75743434125103, Acc: 0.8196, Grad norm: 0.08410475948096156\n",
      "Iteration 4397, BCE loss: 57.757445958367605, Acc: 0.8196, Grad norm: 0.08937499143584877\n",
      "Iteration 4398, BCE loss: 57.75744939674167, Acc: 0.8196, Grad norm: 0.09007229262701624\n",
      "Iteration 4399, BCE loss: 57.75747661350333, Acc: 0.8196, Grad norm: 0.09493599863797426\n",
      "Iteration 4400, BCE loss: 57.75750041846268, Acc: 0.8196, Grad norm: 0.10488228811055107\n",
      "Iteration 4401, BCE loss: 57.757440335592634, Acc: 0.8196, Grad norm: 0.08733114159153961\n",
      "Iteration 4402, BCE loss: 57.75741632363001, Acc: 0.8196, Grad norm: 0.08011674103486446\n",
      "Iteration 4403, BCE loss: 57.75744901346003, Acc: 0.8196, Grad norm: 0.08402689064028186\n",
      "Iteration 4404, BCE loss: 57.75746123154772, Acc: 0.8196, Grad norm: 0.09396315844828021\n",
      "Iteration 4405, BCE loss: 57.757539708010626, Acc: 0.8196, Grad norm: 0.11047559301853585\n",
      "Iteration 4406, BCE loss: 57.75754404560098, Acc: 0.8196, Grad norm: 0.11185483163700737\n",
      "Iteration 4407, BCE loss: 57.757493567641305, Acc: 0.8195, Grad norm: 0.09907233910473295\n",
      "Iteration 4408, BCE loss: 57.7575411250387, Acc: 0.8196, Grad norm: 0.11079689811621107\n",
      "Iteration 4409, BCE loss: 57.75752352086971, Acc: 0.8196, Grad norm: 0.10370436082257826\n",
      "Iteration 4410, BCE loss: 57.757476619012124, Acc: 0.8195, Grad norm: 0.09378410673264154\n",
      "Iteration 4411, BCE loss: 57.75753014668382, Acc: 0.8196, Grad norm: 0.10948873849284065\n",
      "Iteration 4412, BCE loss: 57.75742927015446, Acc: 0.8196, Grad norm: 0.08663537759111004\n",
      "Iteration 4413, BCE loss: 57.75743805584983, Acc: 0.8196, Grad norm: 0.08584165214612331\n",
      "Iteration 4414, BCE loss: 57.75750027283661, Acc: 0.8196, Grad norm: 0.10056315273803376\n",
      "Iteration 4415, BCE loss: 57.757520344208544, Acc: 0.8196, Grad norm: 0.10566835180406325\n",
      "Iteration 4416, BCE loss: 57.757490334640465, Acc: 0.8195, Grad norm: 0.09657431433509454\n",
      "Iteration 4417, BCE loss: 57.75752686828576, Acc: 0.8196, Grad norm: 0.10726969592331882\n",
      "Iteration 4418, BCE loss: 57.75762907754596, Acc: 0.8195, Grad norm: 0.12858404821693079\n",
      "Iteration 4419, BCE loss: 57.7577202887195, Acc: 0.8195, Grad norm: 0.14526617967802286\n",
      "Iteration 4420, BCE loss: 57.75756647452393, Acc: 0.8196, Grad norm: 0.11630807346041439\n",
      "Iteration 4421, BCE loss: 57.757548038611176, Acc: 0.8196, Grad norm: 0.11115112853459214\n",
      "Iteration 4422, BCE loss: 57.75759817341097, Acc: 0.8196, Grad norm: 0.11995950970458201\n",
      "Iteration 4423, BCE loss: 57.75750030785246, Acc: 0.8196, Grad norm: 0.10095314628806003\n",
      "Iteration 4424, BCE loss: 57.75762340225343, Acc: 0.8196, Grad norm: 0.1219014571975841\n",
      "Iteration 4425, BCE loss: 57.757563330441684, Acc: 0.8196, Grad norm: 0.10819295539113333\n",
      "Iteration 4426, BCE loss: 57.75764660100437, Acc: 0.8196, Grad norm: 0.12815338588447175\n",
      "Iteration 4427, BCE loss: 57.757549752243506, Acc: 0.8196, Grad norm: 0.10855468210388824\n",
      "Iteration 4428, BCE loss: 57.75756937721896, Acc: 0.8196, Grad norm: 0.1097377279969868\n",
      "Iteration 4429, BCE loss: 57.75743549763127, Acc: 0.8196, Grad norm: 0.07345927445909886\n",
      "Iteration 4430, BCE loss: 57.75735809573778, Acc: 0.8196, Grad norm: 0.05547931860597746\n",
      "Iteration 4431, BCE loss: 57.7574220825268, Acc: 0.8196, Grad norm: 0.07702989356339474\n",
      "Iteration 4432, BCE loss: 57.75743199700516, Acc: 0.8196, Grad norm: 0.08144875805907215\n",
      "Iteration 4433, BCE loss: 57.75742816047534, Acc: 0.8196, Grad norm: 0.0801003311519667\n",
      "Iteration 4434, BCE loss: 57.75736804098146, Acc: 0.8196, Grad norm: 0.060391728216404726\n",
      "Iteration 4435, BCE loss: 57.75739360105143, Acc: 0.8196, Grad norm: 0.06505369148869514\n",
      "Iteration 4436, BCE loss: 57.75743686686644, Acc: 0.8196, Grad norm: 0.08062821709158884\n",
      "Iteration 4437, BCE loss: 57.7575060123313, Acc: 0.8196, Grad norm: 0.09739206692607681\n",
      "Iteration 4438, BCE loss: 57.757479468298556, Acc: 0.8196, Grad norm: 0.09761340548415617\n",
      "Iteration 4439, BCE loss: 57.757487469038836, Acc: 0.8197, Grad norm: 0.09689307341665984\n",
      "Iteration 4440, BCE loss: 57.75746899551441, Acc: 0.8197, Grad norm: 0.0916100047365047\n",
      "Iteration 4441, BCE loss: 57.757447629391535, Acc: 0.8197, Grad norm: 0.08726987899212961\n",
      "Iteration 4442, BCE loss: 57.757651592572, Acc: 0.8197, Grad norm: 0.12979175858216943\n",
      "Iteration 4443, BCE loss: 57.7575612984316, Acc: 0.8197, Grad norm: 0.11563903746771335\n",
      "Iteration 4444, BCE loss: 57.75768332780869, Acc: 0.8197, Grad norm: 0.13575083680625194\n",
      "Iteration 4445, BCE loss: 57.75765287011693, Acc: 0.8197, Grad norm: 0.13107532437662295\n",
      "Iteration 4446, BCE loss: 57.75750105377554, Acc: 0.8196, Grad norm: 0.1016011415672795\n",
      "Iteration 4447, BCE loss: 57.75754831736112, Acc: 0.8196, Grad norm: 0.11156819400680466\n",
      "Iteration 4448, BCE loss: 57.7573790002445, Acc: 0.8196, Grad norm: 0.06860528120732298\n",
      "Iteration 4449, BCE loss: 57.7573664861564, Acc: 0.8196, Grad norm: 0.06018558041085128\n",
      "Iteration 4450, BCE loss: 57.75740924023313, Acc: 0.8196, Grad norm: 0.07369669235079236\n",
      "Iteration 4451, BCE loss: 57.757440751317496, Acc: 0.8196, Grad norm: 0.08273421116523537\n",
      "Iteration 4452, BCE loss: 57.75743916155287, Acc: 0.8196, Grad norm: 0.08333448940265309\n",
      "Iteration 4453, BCE loss: 57.7573739385691, Acc: 0.8196, Grad norm: 0.06234413223756403\n",
      "Iteration 4454, BCE loss: 57.75734504605313, Acc: 0.8196, Grad norm: 0.049921673058723776\n",
      "Iteration 4455, BCE loss: 57.75738223962979, Acc: 0.8196, Grad norm: 0.06373602344542416\n",
      "Iteration 4456, BCE loss: 57.757473215930446, Acc: 0.8196, Grad norm: 0.08945745671501493\n",
      "Iteration 4457, BCE loss: 57.75742988551869, Acc: 0.8196, Grad norm: 0.07624115894819414\n",
      "Iteration 4458, BCE loss: 57.7575200837821, Acc: 0.8196, Grad norm: 0.10201018647131117\n",
      "Iteration 4459, BCE loss: 57.75754455390869, Acc: 0.8196, Grad norm: 0.1087182508477869\n",
      "Iteration 4460, BCE loss: 57.75739816471839, Acc: 0.8196, Grad norm: 0.07189432076922656\n",
      "Iteration 4461, BCE loss: 57.75759605728295, Acc: 0.8196, Grad norm: 0.12021746823291074\n",
      "Iteration 4462, BCE loss: 57.75757737654554, Acc: 0.8196, Grad norm: 0.11949139398774014\n",
      "Iteration 4463, BCE loss: 57.757491911120844, Acc: 0.8197, Grad norm: 0.10061193433112008\n",
      "Iteration 4464, BCE loss: 57.757465850713956, Acc: 0.8196, Grad norm: 0.09812825185815453\n",
      "Iteration 4465, BCE loss: 57.75756472139742, Acc: 0.8196, Grad norm: 0.1203179665943454\n",
      "Iteration 4466, BCE loss: 57.757864384813445, Acc: 0.8196, Grad norm: 0.16869934133629239\n",
      "Iteration 4467, BCE loss: 57.75785870359472, Acc: 0.8197, Grad norm: 0.16695966252650565\n",
      "Iteration 4468, BCE loss: 57.75762250673422, Acc: 0.8196, Grad norm: 0.1291592709808436\n",
      "Iteration 4469, BCE loss: 57.75740642539611, Acc: 0.8196, Grad norm: 0.08086334492774701\n",
      "Iteration 4470, BCE loss: 57.75736112938441, Acc: 0.8196, Grad norm: 0.06319811783813314\n",
      "Iteration 4471, BCE loss: 57.75739780537586, Acc: 0.8196, Grad norm: 0.07694337850119976\n",
      "Iteration 4472, BCE loss: 57.75746233233313, Acc: 0.8196, Grad norm: 0.09108031071283473\n",
      "Iteration 4473, BCE loss: 57.75742115783158, Acc: 0.8196, Grad norm: 0.08093360440417786\n",
      "Iteration 4474, BCE loss: 57.75732113594944, Acc: 0.8195, Grad norm: 0.04847641433965947\n",
      "Iteration 4475, BCE loss: 57.7573116023791, Acc: 0.8195, Grad norm: 0.0433705492799275\n",
      "Iteration 4476, BCE loss: 57.75728921613815, Acc: 0.8195, Grad norm: 0.030108512741506558\n",
      "Iteration 4477, BCE loss: 57.75729148102925, Acc: 0.8195, Grad norm: 0.03297571781782462\n",
      "Iteration 4478, BCE loss: 57.75737826654295, Acc: 0.8196, Grad norm: 0.0684520826415817\n",
      "Iteration 4479, BCE loss: 57.75735918019973, Acc: 0.8195, Grad norm: 0.059507574762477713\n",
      "Iteration 4480, BCE loss: 57.75734029546615, Acc: 0.8196, Grad norm: 0.05749353952087521\n",
      "Iteration 4481, BCE loss: 57.75734235051108, Acc: 0.8196, Grad norm: 0.05791305685577604\n",
      "Iteration 4482, BCE loss: 57.75733516763174, Acc: 0.8196, Grad norm: 0.05280191649892089\n",
      "Iteration 4483, BCE loss: 57.757375698586515, Acc: 0.8195, Grad norm: 0.06654157107311744\n",
      "Iteration 4484, BCE loss: 57.75737800949512, Acc: 0.8195, Grad norm: 0.06906097524435215\n",
      "Iteration 4485, BCE loss: 57.75738789740601, Acc: 0.8195, Grad norm: 0.07256420996620438\n",
      "Iteration 4486, BCE loss: 57.75738205707594, Acc: 0.8195, Grad norm: 0.06882486926285372\n",
      "Iteration 4487, BCE loss: 57.75741234348125, Acc: 0.8195, Grad norm: 0.07947336376983809\n",
      "Iteration 4488, BCE loss: 57.757415211111336, Acc: 0.8195, Grad norm: 0.08057147050562845\n",
      "Iteration 4489, BCE loss: 57.75732482647423, Acc: 0.8195, Grad norm: 0.05258772918660338\n",
      "Iteration 4490, BCE loss: 57.75733212771499, Acc: 0.8195, Grad norm: 0.054357744903445936\n",
      "Iteration 4491, BCE loss: 57.75738761821077, Acc: 0.8195, Grad norm: 0.0692163232901377\n",
      "Iteration 4492, BCE loss: 57.757518860701985, Acc: 0.8195, Grad norm: 0.10184719525410842\n",
      "Iteration 4493, BCE loss: 57.757549471617835, Acc: 0.8195, Grad norm: 0.10863215976183173\n",
      "Iteration 4494, BCE loss: 57.75763766525983, Acc: 0.8196, Grad norm: 0.12156321754356514\n",
      "Iteration 4495, BCE loss: 57.757701540666744, Acc: 0.8196, Grad norm: 0.13329788678349452\n",
      "Iteration 4496, BCE loss: 57.757563887681854, Acc: 0.8196, Grad norm: 0.10786834318086758\n",
      "Iteration 4497, BCE loss: 57.7575070079681, Acc: 0.8196, Grad norm: 0.09420034813232356\n",
      "Iteration 4498, BCE loss: 57.75756933825554, Acc: 0.8196, Grad norm: 0.10768089668888814\n",
      "Iteration 4499, BCE loss: 57.757544095708155, Acc: 0.8195, Grad norm: 0.10178589826048985\n",
      "Iteration 4500, BCE loss: 57.75750244327376, Acc: 0.8195, Grad norm: 0.09831967311701206\n",
      "Iteration 4501, BCE loss: 57.75748769654554, Acc: 0.8196, Grad norm: 0.08968827057474099\n",
      "Iteration 4502, BCE loss: 57.757417418284696, Acc: 0.8196, Grad norm: 0.07477394634122936\n",
      "Iteration 4503, BCE loss: 57.75746182142267, Acc: 0.8196, Grad norm: 0.0856254475571135\n",
      "Iteration 4504, BCE loss: 57.75743488319149, Acc: 0.8196, Grad norm: 0.0797085463423077\n",
      "Iteration 4505, BCE loss: 57.75742215396903, Acc: 0.8196, Grad norm: 0.07536118000272515\n",
      "Iteration 4506, BCE loss: 57.757338083309854, Acc: 0.8196, Grad norm: 0.05385871018622466\n",
      "Iteration 4507, BCE loss: 57.757402035253904, Acc: 0.8196, Grad norm: 0.07844915897136512\n",
      "Iteration 4508, BCE loss: 57.75744777051231, Acc: 0.8196, Grad norm: 0.0915354408175507\n",
      "Iteration 4509, BCE loss: 57.75732834045573, Acc: 0.8195, Grad norm: 0.05131323247768127\n",
      "Iteration 4510, BCE loss: 57.757329975330066, Acc: 0.8195, Grad norm: 0.0507113447954261\n",
      "Iteration 4511, BCE loss: 57.75732109554375, Acc: 0.8195, Grad norm: 0.045992359050779835\n",
      "Iteration 4512, BCE loss: 57.75729487950871, Acc: 0.8195, Grad norm: 0.034584920121653796\n",
      "Iteration 4513, BCE loss: 57.75736242882894, Acc: 0.8195, Grad norm: 0.06381112371750111\n",
      "Iteration 4514, BCE loss: 57.75736059508862, Acc: 0.8195, Grad norm: 0.06459704382126424\n",
      "Iteration 4515, BCE loss: 57.75735900940355, Acc: 0.8195, Grad norm: 0.06259949686745828\n",
      "Iteration 4516, BCE loss: 57.75734542942132, Acc: 0.8195, Grad norm: 0.057779042160117325\n",
      "Iteration 4517, BCE loss: 57.75734872203002, Acc: 0.8195, Grad norm: 0.056052290151714146\n",
      "Iteration 4518, BCE loss: 57.75739682433616, Acc: 0.8196, Grad norm: 0.06879127823682836\n",
      "Iteration 4519, BCE loss: 57.75746353816679, Acc: 0.8195, Grad norm: 0.08319196117619776\n",
      "Iteration 4520, BCE loss: 57.75750646746676, Acc: 0.8195, Grad norm: 0.09530333588210067\n",
      "Iteration 4521, BCE loss: 57.75760788645066, Acc: 0.8195, Grad norm: 0.11623830099929687\n",
      "Iteration 4522, BCE loss: 57.757577330954845, Acc: 0.8195, Grad norm: 0.10924308622782858\n",
      "Iteration 4523, BCE loss: 57.757538432088225, Acc: 0.8195, Grad norm: 0.09889331726159975\n",
      "Iteration 4524, BCE loss: 57.75765861345433, Acc: 0.8195, Grad norm: 0.1270760521953417\n",
      "Iteration 4525, BCE loss: 57.75753654706818, Acc: 0.8195, Grad norm: 0.10570570973014061\n",
      "Iteration 4526, BCE loss: 57.7575461575344, Acc: 0.8195, Grad norm: 0.10868403031350099\n",
      "Iteration 4527, BCE loss: 57.75750792545021, Acc: 0.8195, Grad norm: 0.09685371214912447\n",
      "Iteration 4528, BCE loss: 57.75739794830977, Acc: 0.8195, Grad norm: 0.0678874025092265\n",
      "Iteration 4529, BCE loss: 57.75734513159455, Acc: 0.8195, Grad norm: 0.05132576865342388\n",
      "Iteration 4530, BCE loss: 57.75742287687238, Acc: 0.8195, Grad norm: 0.07551424593991206\n",
      "Iteration 4531, BCE loss: 57.75737295695971, Acc: 0.8195, Grad norm: 0.06238729284469804\n",
      "Iteration 4532, BCE loss: 57.75747754547466, Acc: 0.8195, Grad norm: 0.09110517250310692\n",
      "Iteration 4533, BCE loss: 57.75745298965548, Acc: 0.8195, Grad norm: 0.08439178370232557\n",
      "Iteration 4534, BCE loss: 57.75744255310438, Acc: 0.8195, Grad norm: 0.08019471347550124\n",
      "Iteration 4535, BCE loss: 57.757407224994466, Acc: 0.8195, Grad norm: 0.0664351719795882\n",
      "Iteration 4536, BCE loss: 57.75738850911314, Acc: 0.8196, Grad norm: 0.06157851473309825\n",
      "Iteration 4537, BCE loss: 57.75736046822358, Acc: 0.8196, Grad norm: 0.05674724621424389\n",
      "Iteration 4538, BCE loss: 57.757351222665335, Acc: 0.8196, Grad norm: 0.05308919036767545\n",
      "Iteration 4539, BCE loss: 57.757338074077296, Acc: 0.8196, Grad norm: 0.04999967216390564\n",
      "Iteration 4540, BCE loss: 57.75737901677871, Acc: 0.8195, Grad norm: 0.06219160823799467\n",
      "Iteration 4541, BCE loss: 57.75733125530651, Acc: 0.8196, Grad norm: 0.04888972627001512\n",
      "Iteration 4542, BCE loss: 57.75734824315048, Acc: 0.8196, Grad norm: 0.05379779613735339\n",
      "Iteration 4543, BCE loss: 57.75734758240759, Acc: 0.8196, Grad norm: 0.054769989389069226\n",
      "Iteration 4544, BCE loss: 57.75735984753704, Acc: 0.8196, Grad norm: 0.060294918842518426\n",
      "Iteration 4545, BCE loss: 57.757433971200946, Acc: 0.8196, Grad norm: 0.08021445804222047\n",
      "Iteration 4546, BCE loss: 57.757491855993244, Acc: 0.8196, Grad norm: 0.09525541884820796\n",
      "Iteration 4547, BCE loss: 57.757462978737806, Acc: 0.8196, Grad norm: 0.09189884382095345\n",
      "Iteration 4548, BCE loss: 57.757458979761765, Acc: 0.8196, Grad norm: 0.08942810846322093\n",
      "Iteration 4549, BCE loss: 57.75737360493196, Acc: 0.8196, Grad norm: 0.0667056751768058\n",
      "Iteration 4550, BCE loss: 57.75745266288365, Acc: 0.8196, Grad norm: 0.08812666272263561\n",
      "Iteration 4551, BCE loss: 57.75756875672994, Acc: 0.8196, Grad norm: 0.11756480821589717\n",
      "Iteration 4552, BCE loss: 57.75753098930831, Acc: 0.8196, Grad norm: 0.10296295032660369\n",
      "Iteration 4553, BCE loss: 57.75751153102814, Acc: 0.8196, Grad norm: 0.10152640098502046\n",
      "Iteration 4554, BCE loss: 57.75743686439004, Acc: 0.8196, Grad norm: 0.08559827188626033\n",
      "Iteration 4555, BCE loss: 57.75745213510085, Acc: 0.8196, Grad norm: 0.08554740855393866\n",
      "Iteration 4556, BCE loss: 57.75746931407974, Acc: 0.8196, Grad norm: 0.09308839616755428\n",
      "Iteration 4557, BCE loss: 57.75757317447241, Acc: 0.8196, Grad norm: 0.11651868140470789\n",
      "Iteration 4558, BCE loss: 57.75740565381142, Acc: 0.8196, Grad norm: 0.07716315241961012\n",
      "Iteration 4559, BCE loss: 57.75748480649544, Acc: 0.8196, Grad norm: 0.09848084509922121\n",
      "Iteration 4560, BCE loss: 57.757403220403575, Acc: 0.8196, Grad norm: 0.07800039106127436\n",
      "Iteration 4561, BCE loss: 57.757370351482976, Acc: 0.8196, Grad norm: 0.06628224857388194\n",
      "Iteration 4562, BCE loss: 57.75738511893285, Acc: 0.8196, Grad norm: 0.0677286314750711\n",
      "Iteration 4563, BCE loss: 57.757371312361215, Acc: 0.8196, Grad norm: 0.06347925193628194\n",
      "Iteration 4564, BCE loss: 57.75734446377279, Acc: 0.8196, Grad norm: 0.054509360355914256\n",
      "Iteration 4565, BCE loss: 57.757371498482435, Acc: 0.8196, Grad norm: 0.06443584881290422\n",
      "Iteration 4566, BCE loss: 57.757351573343726, Acc: 0.8196, Grad norm: 0.05962130966883063\n",
      "Iteration 4567, BCE loss: 57.757308724744064, Acc: 0.8195, Grad norm: 0.0421571877425857\n",
      "Iteration 4568, BCE loss: 57.757391379538674, Acc: 0.8195, Grad norm: 0.07245570183496959\n",
      "Iteration 4569, BCE loss: 57.757321614490394, Acc: 0.8195, Grad norm: 0.04751310575821667\n",
      "Iteration 4570, BCE loss: 57.75735156071461, Acc: 0.8195, Grad norm: 0.06169933429941366\n",
      "Iteration 4571, BCE loss: 57.75741356906288, Acc: 0.8195, Grad norm: 0.08296463064663138\n",
      "Iteration 4572, BCE loss: 57.75739001062912, Acc: 0.8195, Grad norm: 0.07284388579854852\n",
      "Iteration 4573, BCE loss: 57.75746815374085, Acc: 0.8195, Grad norm: 0.09391248110401008\n",
      "Iteration 4574, BCE loss: 57.75748666014993, Acc: 0.8196, Grad norm: 0.10067166077684733\n",
      "Iteration 4575, BCE loss: 57.75757336938332, Acc: 0.8195, Grad norm: 0.11707001390240478\n",
      "Iteration 4576, BCE loss: 57.75747447438691, Acc: 0.8196, Grad norm: 0.09471281738927831\n",
      "Iteration 4577, BCE loss: 57.757403383286245, Acc: 0.8195, Grad norm: 0.07413369388825596\n",
      "Iteration 4578, BCE loss: 57.75738969763272, Acc: 0.8195, Grad norm: 0.07052011597434929\n",
      "Iteration 4579, BCE loss: 57.7573516560737, Acc: 0.8195, Grad norm: 0.057170880322115064\n",
      "Iteration 4580, BCE loss: 57.757373266309344, Acc: 0.8195, Grad norm: 0.06590660692855169\n",
      "Iteration 4581, BCE loss: 57.757372445636236, Acc: 0.8195, Grad norm: 0.06723981020800956\n",
      "Iteration 4582, BCE loss: 57.757362609556374, Acc: 0.8195, Grad norm: 0.06649398138268454\n",
      "Iteration 4583, BCE loss: 57.7573483184613, Acc: 0.8195, Grad norm: 0.058601388920796915\n",
      "Iteration 4584, BCE loss: 57.757368898129755, Acc: 0.8195, Grad norm: 0.06084507830162453\n",
      "Iteration 4585, BCE loss: 57.75732440240968, Acc: 0.8195, Grad norm: 0.04479298534911372\n",
      "Iteration 4586, BCE loss: 57.757331772293576, Acc: 0.8195, Grad norm: 0.04658578639429594\n",
      "Iteration 4587, BCE loss: 57.757350056652285, Acc: 0.8195, Grad norm: 0.05582985659859758\n",
      "Iteration 4588, BCE loss: 57.75737642003971, Acc: 0.8195, Grad norm: 0.06412890099111794\n",
      "Iteration 4589, BCE loss: 57.75742845897402, Acc: 0.8195, Grad norm: 0.08242314221033434\n",
      "Iteration 4590, BCE loss: 57.75740903804228, Acc: 0.8195, Grad norm: 0.07830517437941788\n",
      "Iteration 4591, BCE loss: 57.75738896247452, Acc: 0.8195, Grad norm: 0.07421914805567469\n",
      "Iteration 4592, BCE loss: 57.75735869895937, Acc: 0.8195, Grad norm: 0.06357495981138753\n",
      "Iteration 4593, BCE loss: 57.7573447749658, Acc: 0.8195, Grad norm: 0.0593246985481213\n",
      "Iteration 4594, BCE loss: 57.757316111692546, Acc: 0.8195, Grad norm: 0.046302228911501965\n",
      "Iteration 4595, BCE loss: 57.75733150084524, Acc: 0.8195, Grad norm: 0.05335236510950867\n",
      "Iteration 4596, BCE loss: 57.75740903943705, Acc: 0.8196, Grad norm: 0.07889185501394548\n",
      "Iteration 4597, BCE loss: 57.75740420911868, Acc: 0.8196, Grad norm: 0.0757959438648501\n",
      "Iteration 4598, BCE loss: 57.75746917930368, Acc: 0.8196, Grad norm: 0.08992925422596712\n",
      "Iteration 4599, BCE loss: 57.75736152698921, Acc: 0.8196, Grad norm: 0.06267585025254865\n",
      "Iteration 4600, BCE loss: 57.75740452403298, Acc: 0.8195, Grad norm: 0.07182579162560176\n",
      "Iteration 4601, BCE loss: 57.757396643094296, Acc: 0.8196, Grad norm: 0.07009745812347588\n",
      "Iteration 4602, BCE loss: 57.75742132839412, Acc: 0.8196, Grad norm: 0.08022243581037808\n",
      "Iteration 4603, BCE loss: 57.75740975603396, Acc: 0.8195, Grad norm: 0.07515126154458555\n",
      "Iteration 4604, BCE loss: 57.75735869023075, Acc: 0.8196, Grad norm: 0.0618095132448555\n",
      "Iteration 4605, BCE loss: 57.75737797672075, Acc: 0.8195, Grad norm: 0.06621664207470104\n",
      "Iteration 4606, BCE loss: 57.75739735176171, Acc: 0.8196, Grad norm: 0.07556272646338473\n",
      "Iteration 4607, BCE loss: 57.757387344124766, Acc: 0.8196, Grad norm: 0.06918056686821474\n",
      "Iteration 4608, BCE loss: 57.7573689304169, Acc: 0.8195, Grad norm: 0.0648918045610541\n",
      "Iteration 4609, BCE loss: 57.75736014821482, Acc: 0.8196, Grad norm: 0.05980529127268349\n",
      "Iteration 4610, BCE loss: 57.75735229418813, Acc: 0.8196, Grad norm: 0.05749178119926356\n",
      "Iteration 4611, BCE loss: 57.75735733031892, Acc: 0.8196, Grad norm: 0.0609540535335146\n",
      "Iteration 4612, BCE loss: 57.75739715624952, Acc: 0.8196, Grad norm: 0.07132196796018109\n",
      "Iteration 4613, BCE loss: 57.75742286651409, Acc: 0.8196, Grad norm: 0.07834400869621365\n",
      "Iteration 4614, BCE loss: 57.757503824781246, Acc: 0.8196, Grad norm: 0.098460839831471\n",
      "Iteration 4615, BCE loss: 57.75738484088128, Acc: 0.8196, Grad norm: 0.07070747380950469\n",
      "Iteration 4616, BCE loss: 57.75737353398792, Acc: 0.8196, Grad norm: 0.06712494147067324\n",
      "Iteration 4617, BCE loss: 57.75734380755782, Acc: 0.8196, Grad norm: 0.05172637237708164\n",
      "Iteration 4618, BCE loss: 57.75741802404923, Acc: 0.8196, Grad norm: 0.07906765574246553\n",
      "Iteration 4619, BCE loss: 57.757490780742316, Acc: 0.8196, Grad norm: 0.09564590053512936\n",
      "Iteration 4620, BCE loss: 57.757537191050744, Acc: 0.8196, Grad norm: 0.10871971702792409\n",
      "Iteration 4621, BCE loss: 57.75746873642231, Acc: 0.8196, Grad norm: 0.09357507230681386\n",
      "Iteration 4622, BCE loss: 57.75742181255332, Acc: 0.8196, Grad norm: 0.07989986370505167\n",
      "Iteration 4623, BCE loss: 57.7574885351265, Acc: 0.8196, Grad norm: 0.09665836982133123\n",
      "Iteration 4624, BCE loss: 57.75750381442116, Acc: 0.8196, Grad norm: 0.10240982076777802\n",
      "Iteration 4625, BCE loss: 57.75740596334401, Acc: 0.8196, Grad norm: 0.0789603803712533\n",
      "Iteration 4626, BCE loss: 57.75739906592888, Acc: 0.8196, Grad norm: 0.07946851598274612\n",
      "Iteration 4627, BCE loss: 57.757294367116, Acc: 0.8196, Grad norm: 0.03849664169554636\n",
      "Iteration 4628, BCE loss: 57.757276027745334, Acc: 0.8196, Grad norm: 0.027144954006436524\n",
      "Iteration 4629, BCE loss: 57.75732135443566, Acc: 0.8196, Grad norm: 0.04948110589036626\n",
      "Iteration 4630, BCE loss: 57.75729928651466, Acc: 0.8196, Grad norm: 0.04019365429972846\n",
      "Iteration 4631, BCE loss: 57.75735929910768, Acc: 0.8196, Grad norm: 0.06684087857481345\n",
      "Iteration 4632, BCE loss: 57.757347553295745, Acc: 0.8196, Grad norm: 0.057828889906131505\n",
      "Iteration 4633, BCE loss: 57.75744161102429, Acc: 0.8196, Grad norm: 0.09144683782532723\n",
      "Iteration 4634, BCE loss: 57.757399070327125, Acc: 0.8195, Grad norm: 0.07774168610088966\n",
      "Iteration 4635, BCE loss: 57.7573473757415, Acc: 0.8195, Grad norm: 0.06088943525588574\n",
      "Iteration 4636, BCE loss: 57.7574000500245, Acc: 0.8196, Grad norm: 0.07740938419582435\n",
      "Iteration 4637, BCE loss: 57.757563658450344, Acc: 0.8195, Grad norm: 0.11650402532285242\n",
      "Iteration 4638, BCE loss: 57.75763653447743, Acc: 0.8195, Grad norm: 0.13069179575049988\n",
      "Iteration 4639, BCE loss: 57.7577398475522, Acc: 0.8195, Grad norm: 0.14655895845872072\n",
      "Iteration 4640, BCE loss: 57.75755247410788, Acc: 0.8195, Grad norm: 0.11371260320379917\n",
      "Iteration 4641, BCE loss: 57.75760119468659, Acc: 0.8196, Grad norm: 0.12494058708289993\n",
      "Iteration 4642, BCE loss: 57.75741428538294, Acc: 0.8195, Grad norm: 0.07976829903638173\n",
      "Iteration 4643, BCE loss: 57.75738548607771, Acc: 0.8196, Grad norm: 0.07189775488420577\n",
      "Iteration 4644, BCE loss: 57.757356251551855, Acc: 0.8196, Grad norm: 0.059208261323199826\n",
      "Iteration 4645, BCE loss: 57.75732529562496, Acc: 0.8195, Grad norm: 0.048645819411265785\n",
      "Iteration 4646, BCE loss: 57.7573501395175, Acc: 0.8196, Grad norm: 0.060799308396725246\n",
      "Iteration 4647, BCE loss: 57.757347271716355, Acc: 0.8196, Grad norm: 0.05905253254773217\n",
      "Iteration 4648, BCE loss: 57.75743601999378, Acc: 0.8196, Grad norm: 0.0871713876400043\n",
      "Iteration 4649, BCE loss: 57.75755046988931, Acc: 0.8196, Grad norm: 0.1148486812821005\n",
      "Iteration 4650, BCE loss: 57.75751752220653, Acc: 0.8196, Grad norm: 0.10774801177623662\n",
      "Iteration 4651, BCE loss: 57.757521123921364, Acc: 0.8196, Grad norm: 0.11027807609247767\n",
      "Iteration 4652, BCE loss: 57.75748128353905, Acc: 0.8196, Grad norm: 0.09908446360057932\n",
      "Iteration 4653, BCE loss: 57.75752280332073, Acc: 0.8196, Grad norm: 0.10940324618358951\n",
      "Iteration 4654, BCE loss: 57.75752209319004, Acc: 0.8195, Grad norm: 0.10799385351152736\n",
      "Iteration 4655, BCE loss: 57.757399028825034, Acc: 0.8195, Grad norm: 0.07556778282012722\n",
      "Iteration 4656, BCE loss: 57.757385586413626, Acc: 0.8195, Grad norm: 0.07254510985001776\n",
      "Iteration 4657, BCE loss: 57.75744559202781, Acc: 0.8195, Grad norm: 0.08898297919671559\n",
      "Iteration 4658, BCE loss: 57.757409807562524, Acc: 0.8195, Grad norm: 0.07744825900508505\n",
      "Iteration 4659, BCE loss: 57.75746468727095, Acc: 0.8195, Grad norm: 0.09474534803929942\n",
      "Iteration 4660, BCE loss: 57.75752863854471, Acc: 0.8195, Grad norm: 0.11111910277863174\n",
      "Iteration 4661, BCE loss: 57.75751708638306, Acc: 0.8195, Grad norm: 0.10601972346655983\n",
      "Iteration 4662, BCE loss: 57.75745879660768, Acc: 0.8195, Grad norm: 0.089896891524635\n",
      "Iteration 4663, BCE loss: 57.757366989133175, Acc: 0.8196, Grad norm: 0.06339419379421539\n",
      "Iteration 4664, BCE loss: 57.75740427651995, Acc: 0.8195, Grad norm: 0.07607396011939034\n",
      "Iteration 4665, BCE loss: 57.75737582373007, Acc: 0.8196, Grad norm: 0.06456719443814687\n",
      "Iteration 4666, BCE loss: 57.75745915139868, Acc: 0.8196, Grad norm: 0.08520079433426465\n",
      "Iteration 4667, BCE loss: 57.75762388412235, Acc: 0.8195, Grad norm: 0.12325587649110552\n",
      "Iteration 4668, BCE loss: 57.75760221322179, Acc: 0.8195, Grad norm: 0.12037187993948742\n",
      "Iteration 4669, BCE loss: 57.75746960884247, Acc: 0.8195, Grad norm: 0.09246774432332341\n",
      "Iteration 4670, BCE loss: 57.75753851010599, Acc: 0.8195, Grad norm: 0.10977951943011256\n",
      "Iteration 4671, BCE loss: 57.75744002437821, Acc: 0.8195, Grad norm: 0.08686837037782671\n",
      "Iteration 4672, BCE loss: 57.75745894489581, Acc: 0.8195, Grad norm: 0.09273499880037013\n",
      "Iteration 4673, BCE loss: 57.757387218357835, Acc: 0.8195, Grad norm: 0.07253124664485612\n",
      "Iteration 4674, BCE loss: 57.757348438964286, Acc: 0.8195, Grad norm: 0.06121994107772801\n",
      "Iteration 4675, BCE loss: 57.75736047165471, Acc: 0.8195, Grad norm: 0.06679302113517285\n",
      "Iteration 4676, BCE loss: 57.757331092668004, Acc: 0.8195, Grad norm: 0.05495948785303165\n",
      "Iteration 4677, BCE loss: 57.7573190408513, Acc: 0.8195, Grad norm: 0.047112528860195214\n",
      "Iteration 4678, BCE loss: 57.75738145895132, Acc: 0.8196, Grad norm: 0.07008929169198194\n",
      "Iteration 4679, BCE loss: 57.75731669816923, Acc: 0.8196, Grad norm: 0.04457240877100665\n",
      "Iteration 4680, BCE loss: 57.757359108657475, Acc: 0.8196, Grad norm: 0.0665561361894485\n",
      "Iteration 4681, BCE loss: 57.757433348018296, Acc: 0.8195, Grad norm: 0.08870787768448402\n",
      "Iteration 4682, BCE loss: 57.75750874438, Acc: 0.8195, Grad norm: 0.10512664024767947\n",
      "Iteration 4683, BCE loss: 57.757549437037724, Acc: 0.8196, Grad norm: 0.11291109238319268\n",
      "Iteration 4684, BCE loss: 57.75754415232586, Acc: 0.8196, Grad norm: 0.1109203330178599\n",
      "Iteration 4685, BCE loss: 57.75746051183341, Acc: 0.8196, Grad norm: 0.09171329962749397\n",
      "Iteration 4686, BCE loss: 57.75737413490731, Acc: 0.8196, Grad norm: 0.06617682050082065\n",
      "Iteration 4687, BCE loss: 57.75735672106269, Acc: 0.8195, Grad norm: 0.05817528048743972\n",
      "Iteration 4688, BCE loss: 57.757364095435726, Acc: 0.8196, Grad norm: 0.060122544493265886\n",
      "Iteration 4689, BCE loss: 57.75735677673905, Acc: 0.8196, Grad norm: 0.05897119312842518\n",
      "Iteration 4690, BCE loss: 57.757380199883336, Acc: 0.8196, Grad norm: 0.06749059705108902\n",
      "Iteration 4691, BCE loss: 57.75734196307805, Acc: 0.8195, Grad norm: 0.05217887882686278\n",
      "Iteration 4692, BCE loss: 57.75732635886031, Acc: 0.8195, Grad norm: 0.048454655854141285\n",
      "Iteration 4693, BCE loss: 57.7573727337787, Acc: 0.8195, Grad norm: 0.06728392396806439\n",
      "Iteration 4694, BCE loss: 57.75738117749526, Acc: 0.8195, Grad norm: 0.06740664500018848\n",
      "Iteration 4695, BCE loss: 57.7574267209981, Acc: 0.8195, Grad norm: 0.08209535149662445\n",
      "Iteration 4696, BCE loss: 57.75748851050146, Acc: 0.8195, Grad norm: 0.0938074562714782\n",
      "Iteration 4697, BCE loss: 57.75751095638528, Acc: 0.8195, Grad norm: 0.09456060203861749\n",
      "Iteration 4698, BCE loss: 57.75753610821779, Acc: 0.8195, Grad norm: 0.09857342823504502\n",
      "Iteration 4699, BCE loss: 57.757505005919306, Acc: 0.8195, Grad norm: 0.09271488935503232\n",
      "Iteration 4700, BCE loss: 57.75763339681306, Acc: 0.8195, Grad norm: 0.11873020212631029\n",
      "Iteration 4701, BCE loss: 57.757684744920496, Acc: 0.8195, Grad norm: 0.12576953414794825\n",
      "Iteration 4702, BCE loss: 57.75766950795398, Acc: 0.8195, Grad norm: 0.12432368159353742\n",
      "Iteration 4703, BCE loss: 57.757677625511135, Acc: 0.8196, Grad norm: 0.12964688115847536\n",
      "Iteration 4704, BCE loss: 57.7575111374358, Acc: 0.8196, Grad norm: 0.0995435166656046\n",
      "Iteration 4705, BCE loss: 57.75754274778424, Acc: 0.8196, Grad norm: 0.10710364463337106\n",
      "Iteration 4706, BCE loss: 57.75747817594625, Acc: 0.8196, Grad norm: 0.09079412874703009\n",
      "Iteration 4707, BCE loss: 57.75760496079856, Acc: 0.8196, Grad norm: 0.11689632913021034\n",
      "Iteration 4708, BCE loss: 57.75773142464662, Acc: 0.8196, Grad norm: 0.13671498251974074\n",
      "Iteration 4709, BCE loss: 57.75780000844129, Acc: 0.8196, Grad norm: 0.14786169705229243\n",
      "Iteration 4710, BCE loss: 57.75782183247227, Acc: 0.8196, Grad norm: 0.14883567748637844\n",
      "Iteration 4711, BCE loss: 57.75776341272454, Acc: 0.8196, Grad norm: 0.1396017412872485\n",
      "Iteration 4712, BCE loss: 57.75768556728383, Acc: 0.8196, Grad norm: 0.12742818835768777\n",
      "Iteration 4713, BCE loss: 57.757648787657715, Acc: 0.8196, Grad norm: 0.12438594130092973\n",
      "Iteration 4714, BCE loss: 57.75756674632048, Acc: 0.8196, Grad norm: 0.1092594949491383\n",
      "Iteration 4715, BCE loss: 57.75760410273367, Acc: 0.8196, Grad norm: 0.11569805698944234\n",
      "Iteration 4716, BCE loss: 57.75746203098491, Acc: 0.8196, Grad norm: 0.08895909449708807\n",
      "Iteration 4717, BCE loss: 57.75747801913802, Acc: 0.8195, Grad norm: 0.0894950883180868\n",
      "Iteration 4718, BCE loss: 57.75746891746944, Acc: 0.8195, Grad norm: 0.08995755435295538\n",
      "Iteration 4719, BCE loss: 57.757492656777615, Acc: 0.8196, Grad norm: 0.09789176081571616\n",
      "Iteration 4720, BCE loss: 57.75756616110192, Acc: 0.8196, Grad norm: 0.11329255599926447\n",
      "Iteration 4721, BCE loss: 57.75756186009991, Acc: 0.8196, Grad norm: 0.11432494464374053\n",
      "Iteration 4722, BCE loss: 57.75760996933775, Acc: 0.8195, Grad norm: 0.12081028794065851\n",
      "Iteration 4723, BCE loss: 57.75754818494963, Acc: 0.8195, Grad norm: 0.10712499311189516\n",
      "Iteration 4724, BCE loss: 57.7575896561939, Acc: 0.8195, Grad norm: 0.11741457314983367\n",
      "Iteration 4725, BCE loss: 57.757674597467926, Acc: 0.8195, Grad norm: 0.13472934914871165\n",
      "Iteration 4726, BCE loss: 57.75781016232908, Acc: 0.8196, Grad norm: 0.15763118581000812\n",
      "Iteration 4727, BCE loss: 57.75781800935841, Acc: 0.8196, Grad norm: 0.15912920208774\n",
      "Iteration 4728, BCE loss: 57.75770862842144, Acc: 0.8196, Grad norm: 0.14034633918675687\n",
      "Iteration 4729, BCE loss: 57.757572906665196, Acc: 0.8196, Grad norm: 0.11396083637020947\n",
      "Iteration 4730, BCE loss: 57.757421140703315, Acc: 0.8196, Grad norm: 0.07915406040436693\n",
      "Iteration 4731, BCE loss: 57.75746878370036, Acc: 0.8196, Grad norm: 0.09453045517363488\n",
      "Iteration 4732, BCE loss: 57.75741567191365, Acc: 0.8196, Grad norm: 0.07927796074981243\n",
      "Iteration 4733, BCE loss: 57.75745545852006, Acc: 0.8196, Grad norm: 0.08838403199676965\n",
      "Iteration 4734, BCE loss: 57.7574256348914, Acc: 0.8196, Grad norm: 0.08184532054281529\n",
      "Iteration 4735, BCE loss: 57.757494568536124, Acc: 0.8196, Grad norm: 0.0988717555716157\n",
      "Iteration 4736, BCE loss: 57.75770358152555, Acc: 0.8197, Grad norm: 0.1375000992387745\n",
      "Iteration 4737, BCE loss: 57.757679902944005, Acc: 0.8196, Grad norm: 0.13178469241132815\n",
      "Iteration 4738, BCE loss: 57.757590549704666, Acc: 0.8196, Grad norm: 0.11586107921196627\n",
      "Iteration 4739, BCE loss: 57.75750183468335, Acc: 0.8196, Grad norm: 0.09657670575832548\n",
      "Iteration 4740, BCE loss: 57.757548140643934, Acc: 0.8196, Grad norm: 0.1057363964705733\n",
      "Iteration 4741, BCE loss: 57.75752866013879, Acc: 0.8196, Grad norm: 0.10322655951390629\n",
      "Iteration 4742, BCE loss: 57.75756364659132, Acc: 0.8196, Grad norm: 0.10852259737298732\n",
      "Iteration 4743, BCE loss: 57.75747838691264, Acc: 0.8196, Grad norm: 0.09254749550257807\n",
      "Iteration 4744, BCE loss: 57.757388410456386, Acc: 0.8196, Grad norm: 0.06882901641487256\n",
      "Iteration 4745, BCE loss: 57.757375639158724, Acc: 0.8195, Grad norm: 0.06658251951959555\n",
      "Iteration 4746, BCE loss: 57.757497354015186, Acc: 0.8195, Grad norm: 0.09617013353332811\n",
      "Iteration 4747, BCE loss: 57.7575195029375, Acc: 0.8195, Grad norm: 0.0992586166934461\n",
      "Iteration 4748, BCE loss: 57.757460707303466, Acc: 0.8195, Grad norm: 0.08671924942719593\n",
      "Iteration 4749, BCE loss: 57.757522289007056, Acc: 0.8195, Grad norm: 0.1011953009754631\n",
      "Iteration 4750, BCE loss: 57.75743744987465, Acc: 0.8195, Grad norm: 0.07925452694333049\n",
      "Iteration 4751, BCE loss: 57.75747906243117, Acc: 0.8195, Grad norm: 0.09105816662188647\n",
      "Iteration 4752, BCE loss: 57.75743027002849, Acc: 0.8195, Grad norm: 0.0785230380835504\n",
      "Iteration 4753, BCE loss: 57.757429777189415, Acc: 0.8195, Grad norm: 0.07677176739886044\n",
      "Iteration 4754, BCE loss: 57.75738045663313, Acc: 0.8196, Grad norm: 0.06497118136989756\n",
      "Iteration 4755, BCE loss: 57.75734520236637, Acc: 0.8196, Grad norm: 0.05541737926687138\n",
      "Iteration 4756, BCE loss: 57.757312059667086, Acc: 0.8196, Grad norm: 0.04476690302798911\n",
      "Iteration 4757, BCE loss: 57.75733284824439, Acc: 0.8195, Grad norm: 0.05291014186901792\n",
      "Iteration 4758, BCE loss: 57.757318734355216, Acc: 0.8196, Grad norm: 0.044027151121407905\n",
      "Iteration 4759, BCE loss: 57.75732826423332, Acc: 0.8195, Grad norm: 0.05093231108071816\n",
      "Iteration 4760, BCE loss: 57.757357465123626, Acc: 0.8196, Grad norm: 0.06586535895813891\n",
      "Iteration 4761, BCE loss: 57.75734558973037, Acc: 0.8195, Grad norm: 0.06257746823094604\n",
      "Iteration 4762, BCE loss: 57.75737233443809, Acc: 0.8195, Grad norm: 0.0718074964997908\n",
      "Iteration 4763, BCE loss: 57.75736801450292, Acc: 0.8195, Grad norm: 0.06911806290268112\n",
      "Iteration 4764, BCE loss: 57.75734251716359, Acc: 0.8195, Grad norm: 0.055466858144696825\n",
      "Iteration 4765, BCE loss: 57.75734240580093, Acc: 0.8195, Grad norm: 0.05910933275589112\n",
      "Iteration 4766, BCE loss: 57.757409937982096, Acc: 0.8195, Grad norm: 0.08166288019756889\n",
      "Iteration 4767, BCE loss: 57.757426446165894, Acc: 0.8195, Grad norm: 0.08515204409721183\n",
      "Iteration 4768, BCE loss: 57.75753132931521, Acc: 0.8195, Grad norm: 0.11085703963549282\n",
      "Iteration 4769, BCE loss: 57.75740354325055, Acc: 0.8195, Grad norm: 0.07754480120042904\n",
      "Iteration 4770, BCE loss: 57.75751654352247, Acc: 0.8195, Grad norm: 0.10699555789888945\n",
      "Iteration 4771, BCE loss: 57.75752168425929, Acc: 0.8195, Grad norm: 0.10819987438270676\n",
      "Iteration 4772, BCE loss: 57.757497327030926, Acc: 0.8195, Grad norm: 0.10287499430997883\n",
      "Iteration 4773, BCE loss: 57.75741660294477, Acc: 0.8195, Grad norm: 0.0832881037484355\n",
      "Iteration 4774, BCE loss: 57.757367417049636, Acc: 0.8195, Grad norm: 0.06493834996590814\n",
      "Iteration 4775, BCE loss: 57.75743268762994, Acc: 0.8195, Grad norm: 0.08546923217960556\n",
      "Iteration 4776, BCE loss: 57.757437280366574, Acc: 0.8196, Grad norm: 0.08562989502900915\n",
      "Iteration 4777, BCE loss: 57.757493380976165, Acc: 0.8196, Grad norm: 0.09744672330028509\n",
      "Iteration 4778, BCE loss: 57.75739340437106, Acc: 0.8195, Grad norm: 0.07006010076434145\n",
      "Iteration 4779, BCE loss: 57.757372254906116, Acc: 0.8195, Grad norm: 0.06544257519367491\n",
      "Iteration 4780, BCE loss: 57.75730597115519, Acc: 0.8195, Grad norm: 0.04062643965964193\n",
      "Iteration 4781, BCE loss: 57.75732729370574, Acc: 0.8196, Grad norm: 0.04846345598193931\n",
      "Iteration 4782, BCE loss: 57.7573204694688, Acc: 0.8195, Grad norm: 0.049480187284507204\n",
      "Iteration 4783, BCE loss: 57.75734620032735, Acc: 0.8196, Grad norm: 0.059131939969878186\n",
      "Iteration 4784, BCE loss: 57.75734982761479, Acc: 0.8196, Grad norm: 0.060458182859230186\n",
      "Iteration 4785, BCE loss: 57.75735771557388, Acc: 0.8196, Grad norm: 0.06549773655144207\n",
      "Iteration 4786, BCE loss: 57.75744729885602, Acc: 0.8196, Grad norm: 0.09234306397038276\n",
      "Iteration 4787, BCE loss: 57.75740146759267, Acc: 0.8196, Grad norm: 0.07705518619515923\n",
      "Iteration 4788, BCE loss: 57.757323715522475, Acc: 0.8196, Grad norm: 0.04363139522069465\n",
      "Iteration 4789, BCE loss: 57.757368130514784, Acc: 0.8196, Grad norm: 0.06298683054082943\n",
      "Iteration 4790, BCE loss: 57.7574070772294, Acc: 0.8195, Grad norm: 0.07368249448873093\n",
      "Iteration 4791, BCE loss: 57.75735772802388, Acc: 0.8196, Grad norm: 0.05792872995801155\n",
      "Iteration 4792, BCE loss: 57.75732628968497, Acc: 0.8196, Grad norm: 0.04627536026703275\n",
      "Iteration 4793, BCE loss: 57.75739061494122, Acc: 0.8196, Grad norm: 0.06927598306338285\n",
      "Iteration 4794, BCE loss: 57.75737678530661, Acc: 0.8196, Grad norm: 0.06549290644300804\n",
      "Iteration 4795, BCE loss: 57.757311631538784, Acc: 0.8196, Grad norm: 0.04307660094972036\n",
      "Iteration 4796, BCE loss: 57.75734474059901, Acc: 0.8195, Grad norm: 0.05605720377332942\n",
      "Iteration 4797, BCE loss: 57.75740884139336, Acc: 0.8196, Grad norm: 0.07401218231416255\n",
      "Iteration 4798, BCE loss: 57.75734222419443, Acc: 0.8196, Grad norm: 0.05122363336497877\n",
      "Iteration 4799, BCE loss: 57.75738337927524, Acc: 0.8195, Grad norm: 0.06386170567171187\n",
      "Iteration 4800, BCE loss: 57.75742420185203, Acc: 0.8195, Grad norm: 0.07652911154647864\n",
      "Iteration 4801, BCE loss: 57.75743780866301, Acc: 0.8196, Grad norm: 0.08267468121895181\n",
      "Iteration 4802, BCE loss: 57.757442389761806, Acc: 0.8195, Grad norm: 0.08062124184236416\n",
      "Iteration 4803, BCE loss: 57.75748294656273, Acc: 0.8195, Grad norm: 0.09280047143682589\n",
      "Iteration 4804, BCE loss: 57.757519841599674, Acc: 0.8195, Grad norm: 0.1048246313322747\n",
      "Iteration 4805, BCE loss: 57.757546153436756, Acc: 0.8195, Grad norm: 0.10862546575141767\n",
      "Iteration 4806, BCE loss: 57.757520949818115, Acc: 0.8195, Grad norm: 0.10556905243310072\n",
      "Iteration 4807, BCE loss: 57.75762391488956, Acc: 0.8195, Grad norm: 0.12387609738305598\n",
      "Iteration 4808, BCE loss: 57.757525604986526, Acc: 0.8195, Grad norm: 0.1073882320747971\n",
      "Iteration 4809, BCE loss: 57.75757799566157, Acc: 0.8196, Grad norm: 0.11745156199048722\n",
      "Iteration 4810, BCE loss: 57.75756584064597, Acc: 0.8196, Grad norm: 0.11538321336278981\n",
      "Iteration 4811, BCE loss: 57.75752142929265, Acc: 0.8196, Grad norm: 0.10241890589330573\n",
      "Iteration 4812, BCE loss: 57.757498254832925, Acc: 0.8196, Grad norm: 0.09435956200774033\n",
      "Iteration 4813, BCE loss: 57.75742195250649, Acc: 0.8196, Grad norm: 0.07710186344766776\n",
      "Iteration 4814, BCE loss: 57.75741274277862, Acc: 0.8196, Grad norm: 0.07326986811547347\n",
      "Iteration 4815, BCE loss: 57.75735295480143, Acc: 0.8196, Grad norm: 0.052704359070074584\n",
      "Iteration 4816, BCE loss: 57.75735485992593, Acc: 0.8195, Grad norm: 0.054373995030635544\n",
      "Iteration 4817, BCE loss: 57.75740030574393, Acc: 0.8195, Grad norm: 0.06873652991829551\n",
      "Iteration 4818, BCE loss: 57.757372138474366, Acc: 0.8195, Grad norm: 0.06093365898253525\n",
      "Iteration 4819, BCE loss: 57.75732213000433, Acc: 0.8195, Grad norm: 0.047878220033575725\n",
      "Iteration 4820, BCE loss: 57.75732558531568, Acc: 0.8195, Grad norm: 0.046590405847228684\n",
      "Iteration 4821, BCE loss: 57.75734984879669, Acc: 0.8195, Grad norm: 0.05402088454022382\n",
      "Iteration 4822, BCE loss: 57.757362563598306, Acc: 0.8195, Grad norm: 0.06456607318839852\n",
      "Iteration 4823, BCE loss: 57.75739450207159, Acc: 0.8196, Grad norm: 0.07335970621299578\n",
      "Iteration 4824, BCE loss: 57.75747486917817, Acc: 0.8196, Grad norm: 0.09423142154443558\n",
      "Iteration 4825, BCE loss: 57.757540938182366, Acc: 0.8196, Grad norm: 0.11004337533270828\n",
      "Iteration 4826, BCE loss: 57.757667725041216, Acc: 0.8196, Grad norm: 0.13269854708007542\n",
      "Iteration 4827, BCE loss: 57.75782109606489, Acc: 0.8196, Grad norm: 0.1560959467509095\n",
      "Iteration 4828, BCE loss: 57.75795047287568, Acc: 0.8196, Grad norm: 0.17296000002075837\n",
      "Iteration 4829, BCE loss: 57.7577011519073, Acc: 0.8196, Grad norm: 0.13614282071715506\n",
      "Iteration 4830, BCE loss: 57.757911496868374, Acc: 0.8197, Grad norm: 0.168884974830392\n",
      "Iteration 4831, BCE loss: 57.75773732927465, Acc: 0.8197, Grad norm: 0.14375533662776663\n",
      "Iteration 4832, BCE loss: 57.75767148023464, Acc: 0.8197, Grad norm: 0.13053016850055524\n",
      "Iteration 4833, BCE loss: 57.75769580698931, Acc: 0.8197, Grad norm: 0.131975407904842\n",
      "Iteration 4834, BCE loss: 57.75761425911768, Acc: 0.8197, Grad norm: 0.11812418203862299\n",
      "Iteration 4835, BCE loss: 57.75752453994627, Acc: 0.8196, Grad norm: 0.10024090897752728\n",
      "Iteration 4836, BCE loss: 57.75763827719391, Acc: 0.8196, Grad norm: 0.12248804891771035\n",
      "Iteration 4837, BCE loss: 57.75769407302401, Acc: 0.8196, Grad norm: 0.13492303214241608\n",
      "Iteration 4838, BCE loss: 57.75773084956859, Acc: 0.8197, Grad norm: 0.142875118163293\n",
      "Iteration 4839, BCE loss: 57.75759863246995, Acc: 0.8196, Grad norm: 0.12080353943187717\n",
      "Iteration 4840, BCE loss: 57.75758295725811, Acc: 0.8196, Grad norm: 0.11823359374932622\n",
      "Iteration 4841, BCE loss: 57.75757462966186, Acc: 0.8196, Grad norm: 0.11723448152266426\n",
      "Iteration 4842, BCE loss: 57.75757454787782, Acc: 0.8196, Grad norm: 0.11739478042331679\n",
      "Iteration 4843, BCE loss: 57.75746598552796, Acc: 0.8196, Grad norm: 0.09585345233289476\n",
      "Iteration 4844, BCE loss: 57.75741813130401, Acc: 0.8196, Grad norm: 0.08225532087463587\n",
      "Iteration 4845, BCE loss: 57.75730452427558, Acc: 0.8196, Grad norm: 0.03837028547746945\n",
      "Iteration 4846, BCE loss: 57.75731179316906, Acc: 0.8196, Grad norm: 0.04646079834948294\n",
      "Iteration 4847, BCE loss: 57.75729269771759, Acc: 0.8196, Grad norm: 0.0352024855971883\n",
      "Iteration 4848, BCE loss: 57.75730575334328, Acc: 0.8196, Grad norm: 0.04071232446246965\n",
      "Iteration 4849, BCE loss: 57.75732057923301, Acc: 0.8195, Grad norm: 0.049706040367189744\n",
      "Iteration 4850, BCE loss: 57.75730217032664, Acc: 0.8195, Grad norm: 0.03965272915649316\n",
      "Iteration 4851, BCE loss: 57.75735439433245, Acc: 0.8195, Grad norm: 0.06404082061512617\n",
      "Iteration 4852, BCE loss: 57.757434244381784, Acc: 0.8195, Grad norm: 0.08768858017695973\n",
      "Iteration 4853, BCE loss: 57.757357516442, Acc: 0.8195, Grad norm: 0.06488226075955056\n",
      "Iteration 4854, BCE loss: 57.757523750926715, Acc: 0.8196, Grad norm: 0.11110980101432942\n",
      "Iteration 4855, BCE loss: 57.757474638590836, Acc: 0.8196, Grad norm: 0.10037662582975573\n",
      "Iteration 4856, BCE loss: 57.75745190703935, Acc: 0.8195, Grad norm: 0.09270023382754092\n",
      "Iteration 4857, BCE loss: 57.75736459774734, Acc: 0.8195, Grad norm: 0.06841197825168178\n",
      "Iteration 4858, BCE loss: 57.757341440439674, Acc: 0.8195, Grad norm: 0.05899275121079502\n",
      "Iteration 4859, BCE loss: 57.75732687983832, Acc: 0.8196, Grad norm: 0.05100616637046147\n",
      "Iteration 4860, BCE loss: 57.757299395204825, Acc: 0.8196, Grad norm: 0.03703695350341914\n",
      "Iteration 4861, BCE loss: 57.75730015673951, Acc: 0.8196, Grad norm: 0.035063746886099555\n",
      "Iteration 4862, BCE loss: 57.757310745729335, Acc: 0.8195, Grad norm: 0.04382620463537765\n",
      "Iteration 4863, BCE loss: 57.75734646275512, Acc: 0.8195, Grad norm: 0.05393068691475817\n",
      "Iteration 4864, BCE loss: 57.75735692044071, Acc: 0.8195, Grad norm: 0.060811801115205365\n",
      "Iteration 4865, BCE loss: 57.757417811598614, Acc: 0.8195, Grad norm: 0.08072722174483066\n",
      "Iteration 4866, BCE loss: 57.757469921989, Acc: 0.8195, Grad norm: 0.09107668601707158\n",
      "Iteration 4867, BCE loss: 57.75758137047326, Acc: 0.8195, Grad norm: 0.11691330476474358\n",
      "Iteration 4868, BCE loss: 57.757475601468244, Acc: 0.8195, Grad norm: 0.09377254431768206\n",
      "Iteration 4869, BCE loss: 57.757522428887725, Acc: 0.8195, Grad norm: 0.1033549803435015\n",
      "Iteration 4870, BCE loss: 57.75739775877834, Acc: 0.8195, Grad norm: 0.07348628236986147\n",
      "Iteration 4871, BCE loss: 57.75733725048278, Acc: 0.8195, Grad norm: 0.0503193943728901\n",
      "Iteration 4872, BCE loss: 57.75740118194614, Acc: 0.8195, Grad norm: 0.07141777627096135\n",
      "Iteration 4873, BCE loss: 57.757333607242415, Acc: 0.8195, Grad norm: 0.052184892880459015\n",
      "Iteration 4874, BCE loss: 57.757333627773306, Acc: 0.8195, Grad norm: 0.05461300990603881\n",
      "Iteration 4875, BCE loss: 57.75729217105804, Acc: 0.8195, Grad norm: 0.03251374072665643\n",
      "Iteration 4876, BCE loss: 57.757287988823705, Acc: 0.8195, Grad norm: 0.03191466620106436\n",
      "Iteration 4877, BCE loss: 57.75727683726059, Acc: 0.8196, Grad norm: 0.024294864986450396\n",
      "Iteration 4878, BCE loss: 57.75732812687127, Acc: 0.8196, Grad norm: 0.05267871665958483\n",
      "Iteration 4879, BCE loss: 57.75737837043526, Acc: 0.8195, Grad norm: 0.07439221659780014\n",
      "Iteration 4880, BCE loss: 57.757300911067105, Acc: 0.8195, Grad norm: 0.043500311547585976\n",
      "Iteration 4881, BCE loss: 57.757357737099326, Acc: 0.8195, Grad norm: 0.06564254704027393\n",
      "Iteration 4882, BCE loss: 57.75733759285684, Acc: 0.8195, Grad norm: 0.05802209478957622\n",
      "Iteration 4883, BCE loss: 57.757339757117464, Acc: 0.8195, Grad norm: 0.05812229010994314\n",
      "Iteration 4884, BCE loss: 57.75735946051816, Acc: 0.8195, Grad norm: 0.062172219593949606\n",
      "Iteration 4885, BCE loss: 57.75738684523718, Acc: 0.8195, Grad norm: 0.07344493673647695\n",
      "Iteration 4886, BCE loss: 57.757376875475586, Acc: 0.8195, Grad norm: 0.07289890291490288\n",
      "Iteration 4887, BCE loss: 57.75743501706245, Acc: 0.8195, Grad norm: 0.087537267910332\n",
      "Iteration 4888, BCE loss: 57.75743233977508, Acc: 0.8195, Grad norm: 0.0875100671355339\n",
      "Iteration 4889, BCE loss: 57.75734692324503, Acc: 0.8195, Grad norm: 0.0610556302161739\n",
      "Iteration 4890, BCE loss: 57.757342960860655, Acc: 0.8195, Grad norm: 0.05975132010595055\n",
      "Iteration 4891, BCE loss: 57.7573814258205, Acc: 0.8195, Grad norm: 0.07301930821006887\n",
      "Iteration 4892, BCE loss: 57.757370870284845, Acc: 0.8195, Grad norm: 0.07074565215232861\n",
      "Iteration 4893, BCE loss: 57.75732944837246, Acc: 0.8195, Grad norm: 0.053253679070051255\n",
      "Iteration 4894, BCE loss: 57.75734052607276, Acc: 0.8196, Grad norm: 0.052791196012471224\n",
      "Iteration 4895, BCE loss: 57.757357930799174, Acc: 0.8196, Grad norm: 0.057886821086004\n",
      "Iteration 4896, BCE loss: 57.757382206165346, Acc: 0.8195, Grad norm: 0.06918402375790172\n",
      "Iteration 4897, BCE loss: 57.75748075860571, Acc: 0.8195, Grad norm: 0.09903017996765699\n",
      "Iteration 4898, BCE loss: 57.75751980480172, Acc: 0.8195, Grad norm: 0.1076966871670349\n",
      "Iteration 4899, BCE loss: 57.757504099664416, Acc: 0.8195, Grad norm: 0.10461400227416554\n",
      "Iteration 4900, BCE loss: 57.75742696553182, Acc: 0.8196, Grad norm: 0.08404187954349074\n",
      "Iteration 4901, BCE loss: 57.7574259100662, Acc: 0.8196, Grad norm: 0.08609115361211574\n",
      "Iteration 4902, BCE loss: 57.75734386994574, Acc: 0.8195, Grad norm: 0.06044091397592376\n",
      "Iteration 4903, BCE loss: 57.75737608545561, Acc: 0.8195, Grad norm: 0.0704706934365646\n",
      "Iteration 4904, BCE loss: 57.75739993812701, Acc: 0.8196, Grad norm: 0.07600105627412579\n",
      "Iteration 4905, BCE loss: 57.7574283518795, Acc: 0.8196, Grad norm: 0.08632448234206981\n",
      "Iteration 4906, BCE loss: 57.757398388683995, Acc: 0.8195, Grad norm: 0.07811687357477004\n",
      "Iteration 4907, BCE loss: 57.75732453993943, Acc: 0.8195, Grad norm: 0.05381523212476713\n",
      "Iteration 4908, BCE loss: 57.75735789940031, Acc: 0.8195, Grad norm: 0.06472852572461098\n",
      "Iteration 4909, BCE loss: 57.75730356530737, Acc: 0.8195, Grad norm: 0.0428048245388293\n",
      "Iteration 4910, BCE loss: 57.75735521842706, Acc: 0.8195, Grad norm: 0.06402618278318685\n",
      "Iteration 4911, BCE loss: 57.75735519608796, Acc: 0.8195, Grad norm: 0.06440255524146017\n",
      "Iteration 4912, BCE loss: 57.75747310416746, Acc: 0.8195, Grad norm: 0.09895114793260164\n",
      "Iteration 4913, BCE loss: 57.75739230156204, Acc: 0.8195, Grad norm: 0.07508516697834104\n",
      "Iteration 4914, BCE loss: 57.75735823980256, Acc: 0.8196, Grad norm: 0.06398158628332812\n",
      "Iteration 4915, BCE loss: 57.75740591888194, Acc: 0.8195, Grad norm: 0.07787637309755292\n",
      "Iteration 4916, BCE loss: 57.75744529261011, Acc: 0.8195, Grad norm: 0.08603940108241016\n",
      "Iteration 4917, BCE loss: 57.757349069320355, Acc: 0.8195, Grad norm: 0.06017274966218194\n",
      "Iteration 4918, BCE loss: 57.75736438072363, Acc: 0.8195, Grad norm: 0.06454723540071004\n",
      "Iteration 4919, BCE loss: 57.757355945656386, Acc: 0.8195, Grad norm: 0.06141275812316183\n",
      "Iteration 4920, BCE loss: 57.75748521744444, Acc: 0.8195, Grad norm: 0.09542398430773155\n",
      "Iteration 4921, BCE loss: 57.75755284860145, Acc: 0.8196, Grad norm: 0.10998176519731657\n",
      "Iteration 4922, BCE loss: 57.75755988622973, Acc: 0.8195, Grad norm: 0.11000447887507273\n",
      "Iteration 4923, BCE loss: 57.75749068719808, Acc: 0.8195, Grad norm: 0.09725764870370351\n",
      "Iteration 4924, BCE loss: 57.757475811816064, Acc: 0.8195, Grad norm: 0.09585331361959205\n",
      "Iteration 4925, BCE loss: 57.757484150222396, Acc: 0.8195, Grad norm: 0.09500712492812195\n",
      "Iteration 4926, BCE loss: 57.75739334869882, Acc: 0.8196, Grad norm: 0.07038021007763468\n",
      "Iteration 4927, BCE loss: 57.757469854479275, Acc: 0.8195, Grad norm: 0.09406796739613778\n",
      "Iteration 4928, BCE loss: 57.75741293924993, Acc: 0.8195, Grad norm: 0.07939538220509808\n",
      "Iteration 4929, BCE loss: 57.757450497110646, Acc: 0.8195, Grad norm: 0.08648879758341242\n",
      "Iteration 4930, BCE loss: 57.7573626478803, Acc: 0.8195, Grad norm: 0.05654778956242069\n",
      "Iteration 4931, BCE loss: 57.75732947812216, Acc: 0.8195, Grad norm: 0.049155963147220004\n",
      "Iteration 4932, BCE loss: 57.75732071595083, Acc: 0.8195, Grad norm: 0.047153583739835025\n",
      "Iteration 4933, BCE loss: 57.757332639338685, Acc: 0.8195, Grad norm: 0.050443869645133185\n",
      "Iteration 4934, BCE loss: 57.757342123096265, Acc: 0.8195, Grad norm: 0.053450208638566685\n",
      "Iteration 4935, BCE loss: 57.75729296506947, Acc: 0.8196, Grad norm: 0.03301870275234982\n",
      "Iteration 4936, BCE loss: 57.75731080769643, Acc: 0.8195, Grad norm: 0.041876886995121675\n",
      "Iteration 4937, BCE loss: 57.75733017454041, Acc: 0.8196, Grad norm: 0.05155881868508066\n",
      "Iteration 4938, BCE loss: 57.75734094001662, Acc: 0.8195, Grad norm: 0.05424305733686503\n",
      "Iteration 4939, BCE loss: 57.757345301347286, Acc: 0.8195, Grad norm: 0.0535274981689052\n",
      "Iteration 4940, BCE loss: 57.75744468232496, Acc: 0.8195, Grad norm: 0.08212003338407561\n",
      "Iteration 4941, BCE loss: 57.75740930663571, Acc: 0.8195, Grad norm: 0.07472910976450278\n",
      "Iteration 4942, BCE loss: 57.75738436092415, Acc: 0.8195, Grad norm: 0.06651887607373952\n",
      "Iteration 4943, BCE loss: 57.75749495335751, Acc: 0.8195, Grad norm: 0.09079207725973829\n",
      "Iteration 4944, BCE loss: 57.75742849005601, Acc: 0.8195, Grad norm: 0.07542812718498952\n",
      "Iteration 4945, BCE loss: 57.75746004040776, Acc: 0.8195, Grad norm: 0.08355697587544876\n",
      "Iteration 4946, BCE loss: 57.757634884825976, Acc: 0.8195, Grad norm: 0.11830800668571541\n",
      "Iteration 4947, BCE loss: 57.75764632799598, Acc: 0.8194, Grad norm: 0.12322268060567673\n",
      "Iteration 4948, BCE loss: 57.75756308275752, Acc: 0.8195, Grad norm: 0.10622495623323516\n",
      "Iteration 4949, BCE loss: 57.7575147440764, Acc: 0.8195, Grad norm: 0.09675576104380026\n",
      "Iteration 4950, BCE loss: 57.75755859035611, Acc: 0.8194, Grad norm: 0.10944424008795826\n",
      "Iteration 4951, BCE loss: 57.75748815725072, Acc: 0.8195, Grad norm: 0.09358962545848895\n",
      "Iteration 4952, BCE loss: 57.75746264403439, Acc: 0.8195, Grad norm: 0.08770179546217455\n",
      "Iteration 4953, BCE loss: 57.757506188848375, Acc: 0.8195, Grad norm: 0.09466338168150643\n",
      "Iteration 4954, BCE loss: 57.75746948339104, Acc: 0.8195, Grad norm: 0.08798600728298266\n",
      "Iteration 4955, BCE loss: 57.75758499134979, Acc: 0.8195, Grad norm: 0.11451831004061602\n",
      "Iteration 4956, BCE loss: 57.75757861014688, Acc: 0.8195, Grad norm: 0.11402308135128685\n",
      "Iteration 4957, BCE loss: 57.757456188608515, Acc: 0.8195, Grad norm: 0.08857490336812732\n",
      "Iteration 4958, BCE loss: 57.7574661754745, Acc: 0.8195, Grad norm: 0.09447387704757676\n",
      "Iteration 4959, BCE loss: 57.75741993680504, Acc: 0.8196, Grad norm: 0.07939044822709972\n",
      "Iteration 4960, BCE loss: 57.75752697959729, Acc: 0.8196, Grad norm: 0.10329398794877497\n",
      "Iteration 4961, BCE loss: 57.7575672977495, Acc: 0.8196, Grad norm: 0.1133082845082072\n",
      "Iteration 4962, BCE loss: 57.75744226263702, Acc: 0.8196, Grad norm: 0.08451642117702793\n",
      "Iteration 4963, BCE loss: 57.75745636911801, Acc: 0.8195, Grad norm: 0.08818665465754896\n",
      "Iteration 4964, BCE loss: 57.75752085607073, Acc: 0.8195, Grad norm: 0.10420686288514457\n",
      "Iteration 4965, BCE loss: 57.75749896767173, Acc: 0.8196, Grad norm: 0.1006239523110352\n",
      "Iteration 4966, BCE loss: 57.757519782535155, Acc: 0.8195, Grad norm: 0.1033164380512778\n",
      "Iteration 4967, BCE loss: 57.75745346586896, Acc: 0.8196, Grad norm: 0.08744342774142849\n",
      "Iteration 4968, BCE loss: 57.75747047333486, Acc: 0.8195, Grad norm: 0.08962359782831283\n",
      "Iteration 4969, BCE loss: 57.75742060059059, Acc: 0.8195, Grad norm: 0.07811283626679687\n",
      "Iteration 4970, BCE loss: 57.757459718776595, Acc: 0.8195, Grad norm: 0.08459328864050424\n",
      "Iteration 4971, BCE loss: 57.75749811939481, Acc: 0.8195, Grad norm: 0.09359520993640073\n",
      "Iteration 4972, BCE loss: 57.757452789728404, Acc: 0.8195, Grad norm: 0.08620374466692537\n",
      "Iteration 4973, BCE loss: 57.757476915460344, Acc: 0.8195, Grad norm: 0.09193845465105964\n",
      "Iteration 4974, BCE loss: 57.757466909872164, Acc: 0.8195, Grad norm: 0.09062775851726809\n",
      "Iteration 4975, BCE loss: 57.75751510431911, Acc: 0.8195, Grad norm: 0.10097701823399893\n",
      "Iteration 4976, BCE loss: 57.757507928642745, Acc: 0.8195, Grad norm: 0.09860737742021528\n",
      "Iteration 4977, BCE loss: 57.757387320304204, Acc: 0.8195, Grad norm: 0.06729313723159833\n",
      "Iteration 4978, BCE loss: 57.75750322468785, Acc: 0.8196, Grad norm: 0.09840019162196033\n",
      "Iteration 4979, BCE loss: 57.75744390129337, Acc: 0.8196, Grad norm: 0.08368490313435721\n",
      "Iteration 4980, BCE loss: 57.75744465996608, Acc: 0.8196, Grad norm: 0.08251119461405958\n",
      "Iteration 4981, BCE loss: 57.75736843993509, Acc: 0.8196, Grad norm: 0.05933765613104867\n",
      "Iteration 4982, BCE loss: 57.75732207156267, Acc: 0.8196, Grad norm: 0.043883105834066176\n",
      "Iteration 4983, BCE loss: 57.75740397245552, Acc: 0.8196, Grad norm: 0.07576325403575428\n",
      "Iteration 4984, BCE loss: 57.75750355088435, Acc: 0.8196, Grad norm: 0.09975915372636494\n",
      "Iteration 4985, BCE loss: 57.75737198547343, Acc: 0.8196, Grad norm: 0.06510721690725352\n",
      "Iteration 4986, BCE loss: 57.757411282587924, Acc: 0.8196, Grad norm: 0.0769055819610636\n",
      "Iteration 4987, BCE loss: 57.75734283851405, Acc: 0.8195, Grad norm: 0.05538931256433285\n",
      "Iteration 4988, BCE loss: 57.75737374086182, Acc: 0.8196, Grad norm: 0.06712284283067037\n",
      "Iteration 4989, BCE loss: 57.75733494756547, Acc: 0.8196, Grad norm: 0.05356764669642865\n",
      "Iteration 4990, BCE loss: 57.757434092230895, Acc: 0.8196, Grad norm: 0.08763219336240707\n",
      "Iteration 4991, BCE loss: 57.75745041158562, Acc: 0.8196, Grad norm: 0.09241491440032885\n",
      "Iteration 4992, BCE loss: 57.75746581850052, Acc: 0.8196, Grad norm: 0.09502848640018689\n",
      "Iteration 4993, BCE loss: 57.75735946081389, Acc: 0.8196, Grad norm: 0.06415657233051307\n",
      "Iteration 4994, BCE loss: 57.75736325377494, Acc: 0.8196, Grad norm: 0.06674881771619567\n",
      "Iteration 4995, BCE loss: 57.75732606835638, Acc: 0.8195, Grad norm: 0.05134540762186211\n",
      "Iteration 4996, BCE loss: 57.75740323449331, Acc: 0.8196, Grad norm: 0.07824398613931374\n",
      "Iteration 4997, BCE loss: 57.75751691436267, Acc: 0.8196, Grad norm: 0.10780447900908399\n",
      "Iteration 4998, BCE loss: 57.75739360002046, Acc: 0.8196, Grad norm: 0.07753222678113031\n",
      "Iteration 4999, BCE loss: 57.75748447753067, Acc: 0.8196, Grad norm: 0.09988244903053492\n",
      "Iteration 5000, BCE loss: 57.75732623652212, Acc: 0.8196, Grad norm: 0.05171575992958288\n",
      "Iteration 5001, BCE loss: 57.7573301917863, Acc: 0.8196, Grad norm: 0.055837116004808944\n",
      "Iteration 5002, BCE loss: 57.757465541772504, Acc: 0.8196, Grad norm: 0.0980264740807352\n",
      "Iteration 5003, BCE loss: 57.757592871433296, Acc: 0.8197, Grad norm: 0.12522174534901734\n",
      "Iteration 5004, BCE loss: 57.75759548651455, Acc: 0.8196, Grad norm: 0.12455790697805448\n",
      "Iteration 5005, BCE loss: 57.75759001412136, Acc: 0.8196, Grad norm: 0.12314930014247319\n",
      "Iteration 5006, BCE loss: 57.75754343100264, Acc: 0.8196, Grad norm: 0.1130800562489067\n",
      "Iteration 5007, BCE loss: 57.75757164720258, Acc: 0.8197, Grad norm: 0.11903527791418934\n",
      "Iteration 5008, BCE loss: 57.75751102045284, Acc: 0.8196, Grad norm: 0.10575514289337673\n",
      "Iteration 5009, BCE loss: 57.7576029804018, Acc: 0.8196, Grad norm: 0.12440501404777919\n",
      "Iteration 5010, BCE loss: 57.757367820544516, Acc: 0.8196, Grad norm: 0.06558947271034947\n",
      "Iteration 5011, BCE loss: 57.75737611037273, Acc: 0.8196, Grad norm: 0.06681893276389762\n",
      "Iteration 5012, BCE loss: 57.757378515327616, Acc: 0.8196, Grad norm: 0.06799711823987457\n",
      "Iteration 5013, BCE loss: 57.757349473636374, Acc: 0.8196, Grad norm: 0.059815021651666\n",
      "Iteration 5014, BCE loss: 57.757361636542264, Acc: 0.8196, Grad norm: 0.06396940324782156\n",
      "Iteration 5015, BCE loss: 57.75735133701198, Acc: 0.8196, Grad norm: 0.05953118966355\n",
      "Iteration 5016, BCE loss: 57.75735830886117, Acc: 0.8195, Grad norm: 0.06408006518173899\n",
      "Iteration 5017, BCE loss: 57.75743263178051, Acc: 0.8195, Grad norm: 0.0864616783177612\n",
      "Iteration 5018, BCE loss: 57.75743826201615, Acc: 0.8195, Grad norm: 0.08473490583295505\n",
      "Iteration 5019, BCE loss: 57.75740012106381, Acc: 0.8195, Grad norm: 0.0739656612123024\n",
      "Iteration 5020, BCE loss: 57.75739919729037, Acc: 0.8196, Grad norm: 0.0753139540598834\n",
      "Iteration 5021, BCE loss: 57.75745937778489, Acc: 0.8196, Grad norm: 0.09213470776455694\n",
      "Iteration 5022, BCE loss: 57.757424943372556, Acc: 0.8196, Grad norm: 0.08348707512315193\n",
      "Iteration 5023, BCE loss: 57.757445072025774, Acc: 0.8196, Grad norm: 0.08858175678743471\n",
      "Iteration 5024, BCE loss: 57.75736403296045, Acc: 0.8196, Grad norm: 0.06564599432982675\n",
      "Iteration 5025, BCE loss: 57.75736703584286, Acc: 0.8196, Grad norm: 0.06543365290025743\n",
      "Iteration 5026, BCE loss: 57.75744770398126, Acc: 0.8196, Grad norm: 0.08745589944545991\n",
      "Iteration 5027, BCE loss: 57.75745801053514, Acc: 0.8195, Grad norm: 0.09094661232086103\n",
      "Iteration 5028, BCE loss: 57.75739765870195, Acc: 0.8195, Grad norm: 0.07537704869725823\n",
      "Iteration 5029, BCE loss: 57.757433923744195, Acc: 0.8195, Grad norm: 0.08457791748624806\n",
      "Iteration 5030, BCE loss: 57.75744508678931, Acc: 0.8195, Grad norm: 0.09051229842290177\n",
      "Iteration 5031, BCE loss: 57.757455441056734, Acc: 0.8195, Grad norm: 0.09264314981294536\n",
      "Iteration 5032, BCE loss: 57.75743904085546, Acc: 0.8195, Grad norm: 0.09025615530152334\n",
      "Iteration 5033, BCE loss: 57.75734850266549, Acc: 0.8195, Grad norm: 0.06315513940562077\n",
      "Iteration 5034, BCE loss: 57.7573678816694, Acc: 0.8195, Grad norm: 0.0684908792973797\n",
      "Iteration 5035, BCE loss: 57.75735173087671, Acc: 0.8196, Grad norm: 0.05947099828579267\n",
      "Iteration 5036, BCE loss: 57.75735380278023, Acc: 0.8196, Grad norm: 0.061038078028453464\n",
      "Iteration 5037, BCE loss: 57.75735313263828, Acc: 0.8196, Grad norm: 0.05986955609131459\n",
      "Iteration 5038, BCE loss: 57.75738256865206, Acc: 0.8196, Grad norm: 0.07000285862344427\n",
      "Iteration 5039, BCE loss: 57.75740614319836, Acc: 0.8196, Grad norm: 0.07887255897580915\n",
      "Iteration 5040, BCE loss: 57.75737616878598, Acc: 0.8196, Grad norm: 0.06962282804269354\n",
      "Iteration 5041, BCE loss: 57.757364768485814, Acc: 0.8196, Grad norm: 0.06752513535063398\n",
      "Iteration 5042, BCE loss: 57.75732888568179, Acc: 0.8195, Grad norm: 0.054850107572037614\n",
      "Iteration 5043, BCE loss: 57.757310471476174, Acc: 0.8195, Grad norm: 0.046775224787438764\n",
      "Iteration 5044, BCE loss: 57.75732693584821, Acc: 0.8195, Grad norm: 0.0511375960108567\n",
      "Iteration 5045, BCE loss: 57.75734696577952, Acc: 0.8195, Grad norm: 0.05974040468948555\n",
      "Iteration 5046, BCE loss: 57.75734795357074, Acc: 0.8195, Grad norm: 0.06258437250938538\n",
      "Iteration 5047, BCE loss: 57.75731317293018, Acc: 0.8195, Grad norm: 0.04965913809672799\n",
      "Iteration 5048, BCE loss: 57.7572901720431, Acc: 0.8195, Grad norm: 0.0350181051414656\n",
      "Iteration 5049, BCE loss: 57.75731983160554, Acc: 0.8195, Grad norm: 0.04885991794677402\n",
      "Iteration 5050, BCE loss: 57.75735132125686, Acc: 0.8195, Grad norm: 0.05938475228180017\n",
      "Iteration 5051, BCE loss: 57.75741927724176, Acc: 0.8195, Grad norm: 0.07922873029876853\n",
      "Iteration 5052, BCE loss: 57.75743777111038, Acc: 0.8195, Grad norm: 0.08516940312970994\n",
      "Iteration 5053, BCE loss: 57.7574116545007, Acc: 0.8195, Grad norm: 0.0777538618458348\n",
      "Iteration 5054, BCE loss: 57.7574651250694, Acc: 0.8195, Grad norm: 0.09322603741194721\n",
      "Iteration 5055, BCE loss: 57.75756625608446, Acc: 0.8196, Grad norm: 0.11799432018384116\n",
      "Iteration 5056, BCE loss: 57.757591181872364, Acc: 0.8195, Grad norm: 0.12139274044757226\n",
      "Iteration 5057, BCE loss: 57.75761251276431, Acc: 0.8195, Grad norm: 0.12621469195937735\n",
      "Iteration 5058, BCE loss: 57.75763491811199, Acc: 0.8196, Grad norm: 0.1304269003559475\n",
      "Iteration 5059, BCE loss: 57.757708849198494, Acc: 0.8195, Grad norm: 0.14182045778400543\n",
      "Iteration 5060, BCE loss: 57.75761537207521, Acc: 0.8195, Grad norm: 0.12586590361278624\n",
      "Iteration 5061, BCE loss: 57.75757610343349, Acc: 0.8196, Grad norm: 0.11863148210124659\n",
      "Iteration 5062, BCE loss: 57.757410183689245, Acc: 0.8195, Grad norm: 0.08118171182477758\n",
      "Iteration 5063, BCE loss: 57.75752475549723, Acc: 0.8195, Grad norm: 0.11004002505913317\n",
      "Iteration 5064, BCE loss: 57.75751060679271, Acc: 0.8195, Grad norm: 0.10653784180985183\n",
      "Iteration 5065, BCE loss: 57.75742376991901, Acc: 0.8195, Grad norm: 0.08404167585094452\n",
      "Iteration 5066, BCE loss: 57.75733009654806, Acc: 0.8195, Grad norm: 0.05186370694090555\n",
      "Iteration 5067, BCE loss: 57.75733252416838, Acc: 0.8195, Grad norm: 0.050861096478297896\n",
      "Iteration 5068, BCE loss: 57.75731100683527, Acc: 0.8195, Grad norm: 0.04330627856471643\n",
      "Iteration 5069, BCE loss: 57.75730923773335, Acc: 0.8195, Grad norm: 0.0428124859174392\n",
      "Iteration 5070, BCE loss: 57.7573644924984, Acc: 0.8196, Grad norm: 0.06743043681276223\n",
      "Iteration 5071, BCE loss: 57.75744554354284, Acc: 0.8196, Grad norm: 0.0906025268926204\n",
      "Iteration 5072, BCE loss: 57.75741665057525, Acc: 0.8196, Grad norm: 0.08042345956027933\n",
      "Iteration 5073, BCE loss: 57.757452095196, Acc: 0.8196, Grad norm: 0.089747641197301\n",
      "Iteration 5074, BCE loss: 57.757558975721295, Acc: 0.8196, Grad norm: 0.11362207657902632\n",
      "Iteration 5075, BCE loss: 57.75743542543269, Acc: 0.8196, Grad norm: 0.08331864051325476\n",
      "Iteration 5076, BCE loss: 57.75753529902684, Acc: 0.8196, Grad norm: 0.10393204360465744\n",
      "Iteration 5077, BCE loss: 57.75752946199038, Acc: 0.8196, Grad norm: 0.10161392298533871\n",
      "Iteration 5078, BCE loss: 57.7575000455475, Acc: 0.8196, Grad norm: 0.0949820997092437\n",
      "Iteration 5079, BCE loss: 57.75766301498801, Acc: 0.8196, Grad norm: 0.124393220588075\n",
      "Iteration 5080, BCE loss: 57.757630608733145, Acc: 0.8196, Grad norm: 0.12021552620276911\n",
      "Iteration 5081, BCE loss: 57.75754063415386, Acc: 0.8196, Grad norm: 0.10403797438113807\n",
      "Iteration 5082, BCE loss: 57.757692742534466, Acc: 0.8196, Grad norm: 0.13481252560057008\n",
      "Iteration 5083, BCE loss: 57.75770875786068, Acc: 0.8196, Grad norm: 0.13517656749274076\n",
      "Iteration 5084, BCE loss: 57.75762897765419, Acc: 0.8196, Grad norm: 0.12568247760292367\n",
      "Iteration 5085, BCE loss: 57.757610311792334, Acc: 0.8196, Grad norm: 0.1231447754246037\n",
      "Iteration 5086, BCE loss: 57.7576164691106, Acc: 0.8196, Grad norm: 0.12605295075706902\n",
      "Iteration 5087, BCE loss: 57.75764294520131, Acc: 0.8196, Grad norm: 0.1309855413982527\n",
      "Iteration 5088, BCE loss: 57.75755167080995, Acc: 0.8196, Grad norm: 0.11335040803831961\n",
      "Iteration 5089, BCE loss: 57.757613926322094, Acc: 0.8196, Grad norm: 0.12283056893718093\n",
      "Iteration 5090, BCE loss: 57.75762166706889, Acc: 0.8196, Grad norm: 0.12638033136688775\n",
      "Iteration 5091, BCE loss: 57.75773803477179, Acc: 0.8196, Grad norm: 0.1494022882152608\n",
      "Iteration 5092, BCE loss: 57.75755010206415, Acc: 0.8196, Grad norm: 0.11518117604040246\n",
      "Iteration 5093, BCE loss: 57.75751839448836, Acc: 0.8196, Grad norm: 0.10908280051359631\n",
      "Iteration 5094, BCE loss: 57.75742229635714, Acc: 0.8196, Grad norm: 0.08514269114153435\n",
      "Iteration 5095, BCE loss: 57.757396800862765, Acc: 0.8196, Grad norm: 0.07527956103302214\n",
      "Iteration 5096, BCE loss: 57.75748377575394, Acc: 0.8196, Grad norm: 0.09882940694771777\n",
      "Iteration 5097, BCE loss: 57.75755370563864, Acc: 0.8196, Grad norm: 0.11564605162223361\n",
      "Iteration 5098, BCE loss: 57.75740525683098, Acc: 0.8196, Grad norm: 0.07980611300981758\n",
      "Iteration 5099, BCE loss: 57.75737563443505, Acc: 0.8196, Grad norm: 0.06885113986736541\n",
      "Iteration 5100, BCE loss: 57.75734574774854, Acc: 0.8196, Grad norm: 0.058741884751419914\n",
      "Iteration 5101, BCE loss: 57.75738172040266, Acc: 0.8196, Grad norm: 0.07014487319744683\n",
      "Iteration 5102, BCE loss: 57.75740727662357, Acc: 0.8196, Grad norm: 0.07992115665590503\n",
      "Iteration 5103, BCE loss: 57.757400330349114, Acc: 0.8196, Grad norm: 0.07846303980072018\n",
      "Iteration 5104, BCE loss: 57.7575271854936, Acc: 0.8196, Grad norm: 0.1072457578432213\n",
      "Iteration 5105, BCE loss: 57.757486846273736, Acc: 0.8196, Grad norm: 0.09925044044221308\n",
      "Iteration 5106, BCE loss: 57.75760035165051, Acc: 0.8195, Grad norm: 0.1217516688616067\n",
      "Iteration 5107, BCE loss: 57.75766034837678, Acc: 0.8196, Grad norm: 0.13392149012245733\n",
      "Iteration 5108, BCE loss: 57.75763272399483, Acc: 0.8196, Grad norm: 0.1294695299411081\n",
      "Iteration 5109, BCE loss: 57.75769201362761, Acc: 0.8196, Grad norm: 0.14169967831508262\n",
      "Iteration 5110, BCE loss: 57.757741770564564, Acc: 0.8196, Grad norm: 0.14789547645186368\n",
      "Iteration 5111, BCE loss: 57.75793800878813, Acc: 0.8196, Grad norm: 0.17591985199603097\n",
      "Iteration 5112, BCE loss: 57.75776564017059, Acc: 0.8196, Grad norm: 0.15045411729664337\n",
      "Iteration 5113, BCE loss: 57.757829033857604, Acc: 0.8196, Grad norm: 0.16082414246868293\n",
      "Iteration 5114, BCE loss: 57.75784940263777, Acc: 0.8196, Grad norm: 0.16439718510147097\n",
      "Iteration 5115, BCE loss: 57.757795265671945, Acc: 0.8196, Grad norm: 0.1543949477424379\n",
      "Iteration 5116, BCE loss: 57.75780203339241, Acc: 0.8196, Grad norm: 0.15172164920683204\n",
      "Iteration 5117, BCE loss: 57.75764021906876, Acc: 0.8196, Grad norm: 0.12709490911908047\n",
      "Iteration 5118, BCE loss: 57.75764681917332, Acc: 0.8196, Grad norm: 0.1263485278113679\n",
      "Iteration 5119, BCE loss: 57.757683859090676, Acc: 0.8196, Grad norm: 0.13132250464946169\n",
      "Iteration 5120, BCE loss: 57.7576701166699, Acc: 0.8196, Grad norm: 0.13079616009005196\n",
      "Iteration 5121, BCE loss: 57.75768187657325, Acc: 0.8196, Grad norm: 0.1303080165526439\n",
      "Iteration 5122, BCE loss: 57.7576248834246, Acc: 0.8196, Grad norm: 0.12013796023083062\n",
      "Iteration 5123, BCE loss: 57.757740971544635, Acc: 0.8196, Grad norm: 0.13956326755155254\n",
      "Iteration 5124, BCE loss: 57.75775047831982, Acc: 0.8196, Grad norm: 0.14101717590019053\n",
      "Iteration 5125, BCE loss: 57.75756909331982, Acc: 0.8196, Grad norm: 0.11279347602407787\n",
      "Iteration 5126, BCE loss: 57.75765157447785, Acc: 0.8196, Grad norm: 0.12618782163679357\n",
      "Iteration 5127, BCE loss: 57.757645804406025, Acc: 0.8196, Grad norm: 0.12535288550504983\n",
      "Iteration 5128, BCE loss: 57.75768081688568, Acc: 0.8196, Grad norm: 0.12956631795711074\n",
      "Iteration 5129, BCE loss: 57.75764020005095, Acc: 0.8196, Grad norm: 0.12312316668438657\n",
      "Iteration 5130, BCE loss: 57.757586502458764, Acc: 0.8196, Grad norm: 0.11294427675191279\n",
      "Iteration 5131, BCE loss: 57.75751405932304, Acc: 0.8196, Grad norm: 0.1003599603836954\n",
      "Iteration 5132, BCE loss: 57.75745884698747, Acc: 0.8196, Grad norm: 0.08901952161143646\n",
      "Iteration 5133, BCE loss: 57.75746011473207, Acc: 0.8196, Grad norm: 0.09137271940439726\n",
      "Iteration 5134, BCE loss: 57.75749724381359, Acc: 0.8196, Grad norm: 0.10201446298437647\n",
      "Iteration 5135, BCE loss: 57.75762358573669, Acc: 0.8196, Grad norm: 0.12742179587714794\n",
      "Iteration 5136, BCE loss: 57.75762159532269, Acc: 0.8196, Grad norm: 0.12593341787431736\n",
      "Iteration 5137, BCE loss: 57.75766515295102, Acc: 0.8196, Grad norm: 0.133495671383418\n",
      "Iteration 5138, BCE loss: 57.75762152136727, Acc: 0.8196, Grad norm: 0.1262416150179305\n",
      "Iteration 5139, BCE loss: 57.757561503122425, Acc: 0.8196, Grad norm: 0.11418955014224871\n",
      "Iteration 5140, BCE loss: 57.75747390395466, Acc: 0.8196, Grad norm: 0.09620736477035066\n",
      "Iteration 5141, BCE loss: 57.75753032933943, Acc: 0.8196, Grad norm: 0.10788258360931376\n",
      "Iteration 5142, BCE loss: 57.75746607658759, Acc: 0.8196, Grad norm: 0.09621382654926809\n",
      "Iteration 5143, BCE loss: 57.75746243185611, Acc: 0.8196, Grad norm: 0.09565874928461757\n",
      "Iteration 5144, BCE loss: 57.757361884136955, Acc: 0.8196, Grad norm: 0.06527613285424018\n",
      "Iteration 5145, BCE loss: 57.7573234494497, Acc: 0.8196, Grad norm: 0.051430562201561905\n",
      "Iteration 5146, BCE loss: 57.757424928958265, Acc: 0.8196, Grad norm: 0.08633974305251935\n",
      "Iteration 5147, BCE loss: 57.757397464321954, Acc: 0.8196, Grad norm: 0.07749285880149187\n",
      "Iteration 5148, BCE loss: 57.7573694060692, Acc: 0.8196, Grad norm: 0.06740375337464397\n",
      "Iteration 5149, BCE loss: 57.75730311271687, Acc: 0.8196, Grad norm: 0.041403923535975606\n",
      "Iteration 5150, BCE loss: 57.75734088282569, Acc: 0.8196, Grad norm: 0.05988154893677655\n",
      "Iteration 5151, BCE loss: 57.757383940015615, Acc: 0.8196, Grad norm: 0.07329552951584231\n",
      "Iteration 5152, BCE loss: 57.757414512312906, Acc: 0.8196, Grad norm: 0.08107351630346707\n",
      "Iteration 5153, BCE loss: 57.7573311079928, Acc: 0.8196, Grad norm: 0.05366832052465602\n",
      "Iteration 5154, BCE loss: 57.757453156909506, Acc: 0.8196, Grad norm: 0.09219899327693017\n",
      "Iteration 5155, BCE loss: 57.75754457799656, Acc: 0.8195, Grad norm: 0.11245649625906975\n",
      "Iteration 5156, BCE loss: 57.75749813813185, Acc: 0.8195, Grad norm: 0.1017705554571565\n",
      "Iteration 5157, BCE loss: 57.75751626088981, Acc: 0.8195, Grad norm: 0.10577170938671028\n",
      "Iteration 5158, BCE loss: 57.75743731850324, Acc: 0.8195, Grad norm: 0.08478771114224279\n",
      "Iteration 5159, BCE loss: 57.75736699391027, Acc: 0.8195, Grad norm: 0.06482440940122733\n",
      "Iteration 5160, BCE loss: 57.757383815635265, Acc: 0.8196, Grad norm: 0.07048360572713464\n",
      "Iteration 5161, BCE loss: 57.75749072755583, Acc: 0.8196, Grad norm: 0.09654764993096673\n",
      "Iteration 5162, BCE loss: 57.75743546770721, Acc: 0.8196, Grad norm: 0.08541815480323335\n",
      "Iteration 5163, BCE loss: 57.75745615887334, Acc: 0.8196, Grad norm: 0.09242535359916317\n",
      "Iteration 5164, BCE loss: 57.75739675256823, Acc: 0.8196, Grad norm: 0.07447995308778972\n",
      "Iteration 5165, BCE loss: 57.75741980685632, Acc: 0.8195, Grad norm: 0.07917416909755735\n",
      "Iteration 5166, BCE loss: 57.75735695796939, Acc: 0.8195, Grad norm: 0.05938968794466579\n",
      "Iteration 5167, BCE loss: 57.75736763596052, Acc: 0.8195, Grad norm: 0.06230174149635027\n",
      "Iteration 5168, BCE loss: 57.757437317568275, Acc: 0.8195, Grad norm: 0.08144309415229117\n",
      "Iteration 5169, BCE loss: 57.75739865569374, Acc: 0.8195, Grad norm: 0.07077488742463235\n",
      "Iteration 5170, BCE loss: 57.75742975974252, Acc: 0.8195, Grad norm: 0.08088900712732351\n",
      "Iteration 5171, BCE loss: 57.75751274773251, Acc: 0.8195, Grad norm: 0.10259130690297491\n",
      "Iteration 5172, BCE loss: 57.75744477704819, Acc: 0.8195, Grad norm: 0.08513362227246214\n",
      "Iteration 5173, BCE loss: 57.75736381719409, Acc: 0.8195, Grad norm: 0.060154018454725705\n",
      "Iteration 5174, BCE loss: 57.75741586814564, Acc: 0.8196, Grad norm: 0.07335233106405442\n",
      "Iteration 5175, BCE loss: 57.757437773140396, Acc: 0.8196, Grad norm: 0.08277894867667504\n",
      "Iteration 5176, BCE loss: 57.75742695247274, Acc: 0.8196, Grad norm: 0.07761360421491031\n",
      "Iteration 5177, BCE loss: 57.75737057932553, Acc: 0.8195, Grad norm: 0.061851495444683796\n",
      "Iteration 5178, BCE loss: 57.757479313847554, Acc: 0.8195, Grad norm: 0.08944018244472973\n",
      "Iteration 5179, BCE loss: 57.757446647148456, Acc: 0.8195, Grad norm: 0.07933614078686967\n",
      "Iteration 5180, BCE loss: 57.757465096115816, Acc: 0.8195, Grad norm: 0.08829199794791995\n",
      "Iteration 5181, BCE loss: 57.75755740459896, Acc: 0.8196, Grad norm: 0.10847581324423657\n",
      "Iteration 5182, BCE loss: 57.75753439067006, Acc: 0.8195, Grad norm: 0.10062872844887025\n",
      "Iteration 5183, BCE loss: 57.757430270416066, Acc: 0.8195, Grad norm: 0.07379617366663625\n",
      "Iteration 5184, BCE loss: 57.75744064493808, Acc: 0.8195, Grad norm: 0.07518258127984945\n",
      "Iteration 5185, BCE loss: 57.75748024792603, Acc: 0.8195, Grad norm: 0.0832990180990829\n",
      "Iteration 5186, BCE loss: 57.75750040363164, Acc: 0.8195, Grad norm: 0.08646061248948975\n",
      "Iteration 5187, BCE loss: 57.757473766939384, Acc: 0.8195, Grad norm: 0.08210007761517436\n",
      "Iteration 5188, BCE loss: 57.75742072844902, Acc: 0.8195, Grad norm: 0.07292937908589918\n",
      "Iteration 5189, BCE loss: 57.75741267305266, Acc: 0.8195, Grad norm: 0.0694460099122161\n",
      "Iteration 5190, BCE loss: 57.7573864937562, Acc: 0.8195, Grad norm: 0.06335102189463848\n",
      "Iteration 5191, BCE loss: 57.75743475814923, Acc: 0.8195, Grad norm: 0.0804294506348913\n",
      "Iteration 5192, BCE loss: 57.7574038327261, Acc: 0.8195, Grad norm: 0.07305215716837868\n",
      "Iteration 5193, BCE loss: 57.75746739732572, Acc: 0.8195, Grad norm: 0.08948411365837644\n",
      "Iteration 5194, BCE loss: 57.757380509827016, Acc: 0.8195, Grad norm: 0.06701035833564441\n",
      "Iteration 5195, BCE loss: 57.75741294033164, Acc: 0.8196, Grad norm: 0.08008749563683518\n",
      "Iteration 5196, BCE loss: 57.75733876377379, Acc: 0.8195, Grad norm: 0.056295748923885246\n",
      "Iteration 5197, BCE loss: 57.75739494681618, Acc: 0.8195, Grad norm: 0.07564989984113957\n",
      "Iteration 5198, BCE loss: 57.75740186911674, Acc: 0.8195, Grad norm: 0.08002731772241214\n",
      "Iteration 5199, BCE loss: 57.75734452536944, Acc: 0.8195, Grad norm: 0.05957848600409991\n",
      "Iteration 5200, BCE loss: 57.757439553507844, Acc: 0.8195, Grad norm: 0.08971400078348613\n",
      "Iteration 5201, BCE loss: 57.757369602840896, Acc: 0.8196, Grad norm: 0.0680030002040424\n",
      "Iteration 5202, BCE loss: 57.757326545065055, Acc: 0.8196, Grad norm: 0.05310304664177784\n",
      "Iteration 5203, BCE loss: 57.75732124028485, Acc: 0.8195, Grad norm: 0.05198842462841492\n",
      "Iteration 5204, BCE loss: 57.757332269302005, Acc: 0.8195, Grad norm: 0.05382324282222104\n",
      "Iteration 5205, BCE loss: 57.75733643232762, Acc: 0.8195, Grad norm: 0.05355435430002151\n",
      "Iteration 5206, BCE loss: 57.757401887081784, Acc: 0.8195, Grad norm: 0.07603182026327673\n",
      "Iteration 5207, BCE loss: 57.757455188508736, Acc: 0.8195, Grad norm: 0.09099118602451844\n",
      "Iteration 5208, BCE loss: 57.757374698274816, Acc: 0.8195, Grad norm: 0.06926070159512215\n",
      "Iteration 5209, BCE loss: 57.757368264137284, Acc: 0.8195, Grad norm: 0.06690541181743097\n",
      "Iteration 5210, BCE loss: 57.757409693873285, Acc: 0.8195, Grad norm: 0.07957192419840027\n",
      "Iteration 5211, BCE loss: 57.757418989231184, Acc: 0.8195, Grad norm: 0.08165308810616458\n",
      "Iteration 5212, BCE loss: 57.757368430458044, Acc: 0.8195, Grad norm: 0.06744302685364947\n",
      "Iteration 5213, BCE loss: 57.757418264192445, Acc: 0.8195, Grad norm: 0.08511581292775344\n",
      "Iteration 5214, BCE loss: 57.75760549862389, Acc: 0.8195, Grad norm: 0.12657345335758327\n",
      "Iteration 5215, BCE loss: 57.75746067628646, Acc: 0.8195, Grad norm: 0.09526337414684122\n",
      "Iteration 5216, BCE loss: 57.75742210393433, Acc: 0.8195, Grad norm: 0.0842249145494597\n",
      "Iteration 5217, BCE loss: 57.7574854413872, Acc: 0.8195, Grad norm: 0.09947355474787986\n",
      "Iteration 5218, BCE loss: 57.75742915645919, Acc: 0.8195, Grad norm: 0.08628748347039539\n",
      "Iteration 5219, BCE loss: 57.757452398589805, Acc: 0.8195, Grad norm: 0.09257353824039292\n",
      "Iteration 5220, BCE loss: 57.75751574676562, Acc: 0.8196, Grad norm: 0.10753873296840177\n",
      "Iteration 5221, BCE loss: 57.75743323025823, Acc: 0.8195, Grad norm: 0.08912387122251421\n",
      "Iteration 5222, BCE loss: 57.75747923023843, Acc: 0.8195, Grad norm: 0.10090209398720378\n",
      "Iteration 5223, BCE loss: 57.7574187907785, Acc: 0.8195, Grad norm: 0.08492599444049763\n",
      "Iteration 5224, BCE loss: 57.7574047726676, Acc: 0.8195, Grad norm: 0.08048770597978828\n",
      "Iteration 5225, BCE loss: 57.75742530737711, Acc: 0.8195, Grad norm: 0.0865910857748797\n",
      "Iteration 5226, BCE loss: 57.75748420182812, Acc: 0.8195, Grad norm: 0.10071818053763\n",
      "Iteration 5227, BCE loss: 57.7574618557167, Acc: 0.8195, Grad norm: 0.09508650710731364\n",
      "Iteration 5228, BCE loss: 57.757503996253874, Acc: 0.8195, Grad norm: 0.10409629259280756\n",
      "Iteration 5229, BCE loss: 57.75744208837162, Acc: 0.8195, Grad norm: 0.08984828469629474\n",
      "Iteration 5230, BCE loss: 57.757430720625294, Acc: 0.8195, Grad norm: 0.08599151216690323\n",
      "Iteration 5231, BCE loss: 57.75744173951645, Acc: 0.8195, Grad norm: 0.0851907021608172\n",
      "Iteration 5232, BCE loss: 57.75741803784561, Acc: 0.8196, Grad norm: 0.07621336767796948\n",
      "Iteration 5233, BCE loss: 57.757487819127505, Acc: 0.8196, Grad norm: 0.09517250168429169\n",
      "Iteration 5234, BCE loss: 57.75749949837656, Acc: 0.8196, Grad norm: 0.09760828941760169\n",
      "Iteration 5235, BCE loss: 57.757465368878925, Acc: 0.8195, Grad norm: 0.09088958288727612\n",
      "Iteration 5236, BCE loss: 57.75738617805223, Acc: 0.8196, Grad norm: 0.0689561249212475\n",
      "Iteration 5237, BCE loss: 57.757399408418806, Acc: 0.8196, Grad norm: 0.07697151079373181\n",
      "Iteration 5238, BCE loss: 57.75738004120666, Acc: 0.8196, Grad norm: 0.07166662566309352\n",
      "Iteration 5239, BCE loss: 57.7575148792748, Acc: 0.8195, Grad norm: 0.10841342564894826\n",
      "Iteration 5240, BCE loss: 57.75748376801967, Acc: 0.8195, Grad norm: 0.10077200306725242\n",
      "Iteration 5241, BCE loss: 57.7574968592604, Acc: 0.8195, Grad norm: 0.10166342563644853\n",
      "Iteration 5242, BCE loss: 57.757513503879125, Acc: 0.8196, Grad norm: 0.10720650655522787\n",
      "Iteration 5243, BCE loss: 57.757491623368836, Acc: 0.8195, Grad norm: 0.10227024603340437\n",
      "Iteration 5244, BCE loss: 57.75755228277468, Acc: 0.8195, Grad norm: 0.11398572859411422\n",
      "Iteration 5245, BCE loss: 57.757490089160584, Acc: 0.8195, Grad norm: 0.09997447620975805\n",
      "Iteration 5246, BCE loss: 57.75748262538657, Acc: 0.8195, Grad norm: 0.09966977266356798\n",
      "Iteration 5247, BCE loss: 57.757432152417195, Acc: 0.8195, Grad norm: 0.08772193360094713\n",
      "Iteration 5248, BCE loss: 57.757442129138425, Acc: 0.8196, Grad norm: 0.09284510741974467\n",
      "Iteration 5249, BCE loss: 57.7573739135415, Acc: 0.8196, Grad norm: 0.07391540346169119\n",
      "Iteration 5250, BCE loss: 57.7573737169849, Acc: 0.8196, Grad norm: 0.07340301773922855\n",
      "Iteration 5251, BCE loss: 57.75736132066335, Acc: 0.8196, Grad norm: 0.0659948636374035\n",
      "Iteration 5252, BCE loss: 57.75730692680499, Acc: 0.8196, Grad norm: 0.04490429898986858\n",
      "Iteration 5253, BCE loss: 57.757337718627284, Acc: 0.8196, Grad norm: 0.06013659934655815\n",
      "Iteration 5254, BCE loss: 57.75733818737349, Acc: 0.8196, Grad norm: 0.05745786275329829\n",
      "Iteration 5255, BCE loss: 57.75732203867172, Acc: 0.8196, Grad norm: 0.04845406018448803\n",
      "Iteration 5256, BCE loss: 57.757317456440205, Acc: 0.8195, Grad norm: 0.047421399050273094\n",
      "Iteration 5257, BCE loss: 57.75729967167666, Acc: 0.8195, Grad norm: 0.03742931373543996\n",
      "Iteration 5258, BCE loss: 57.7573316178219, Acc: 0.8195, Grad norm: 0.05056750818143282\n",
      "Iteration 5259, BCE loss: 57.75733302323682, Acc: 0.8195, Grad norm: 0.05050131945293298\n",
      "Iteration 5260, BCE loss: 57.75732997833924, Acc: 0.8195, Grad norm: 0.05174053489455098\n",
      "Iteration 5261, BCE loss: 57.75736415128543, Acc: 0.8195, Grad norm: 0.06443731112786237\n",
      "Iteration 5262, BCE loss: 57.757310380166615, Acc: 0.8195, Grad norm: 0.042899909271778947\n",
      "Iteration 5263, BCE loss: 57.75735849534249, Acc: 0.8195, Grad norm: 0.06229714706125809\n",
      "Iteration 5264, BCE loss: 57.757326701850744, Acc: 0.8195, Grad norm: 0.049604290303437916\n",
      "Iteration 5265, BCE loss: 57.75732782040181, Acc: 0.8195, Grad norm: 0.04922601112481315\n",
      "Iteration 5266, BCE loss: 57.757331541099326, Acc: 0.8195, Grad norm: 0.05324878168999139\n",
      "Iteration 5267, BCE loss: 57.75732365830862, Acc: 0.8195, Grad norm: 0.0499247606020455\n",
      "Iteration 5268, BCE loss: 57.757344661771185, Acc: 0.8195, Grad norm: 0.05725602956307624\n",
      "Iteration 5269, BCE loss: 57.75736019001642, Acc: 0.8196, Grad norm: 0.06244114459088716\n",
      "Iteration 5270, BCE loss: 57.75738860495987, Acc: 0.8196, Grad norm: 0.07309730221307022\n",
      "Iteration 5271, BCE loss: 57.757370377901765, Acc: 0.8196, Grad norm: 0.06518081146524227\n",
      "Iteration 5272, BCE loss: 57.757350403536, Acc: 0.8196, Grad norm: 0.06051059583913455\n",
      "Iteration 5273, BCE loss: 57.75736247760924, Acc: 0.8196, Grad norm: 0.06419640484229147\n",
      "Iteration 5274, BCE loss: 57.757379034408174, Acc: 0.8196, Grad norm: 0.07117997193768492\n",
      "Iteration 5275, BCE loss: 57.75737739377317, Acc: 0.8196, Grad norm: 0.0697916450131571\n",
      "Iteration 5276, BCE loss: 57.757378903029235, Acc: 0.8196, Grad norm: 0.07051076825818499\n",
      "Iteration 5277, BCE loss: 57.75736036915614, Acc: 0.8196, Grad norm: 0.06344464374450207\n",
      "Iteration 5278, BCE loss: 57.757377678755915, Acc: 0.8196, Grad norm: 0.07226322972841062\n",
      "Iteration 5279, BCE loss: 57.75733382959796, Acc: 0.8196, Grad norm: 0.055010047578404185\n",
      "Iteration 5280, BCE loss: 57.75735125225212, Acc: 0.8196, Grad norm: 0.060557963543184726\n",
      "Iteration 5281, BCE loss: 57.75735138898726, Acc: 0.8196, Grad norm: 0.06280134522036969\n",
      "Iteration 5282, BCE loss: 57.757421190734405, Acc: 0.8196, Grad norm: 0.08333210965164513\n",
      "Iteration 5283, BCE loss: 57.75738826808514, Acc: 0.8196, Grad norm: 0.07443015715192149\n",
      "Iteration 5284, BCE loss: 57.75741012132795, Acc: 0.8196, Grad norm: 0.08081588347003822\n",
      "Iteration 5285, BCE loss: 57.75739796699383, Acc: 0.8196, Grad norm: 0.0785146792762767\n",
      "Iteration 5286, BCE loss: 57.75736370642961, Acc: 0.8196, Grad norm: 0.0681911863869152\n",
      "Iteration 5287, BCE loss: 57.75734622439025, Acc: 0.8196, Grad norm: 0.060323665280487195\n",
      "Iteration 5288, BCE loss: 57.75745508749094, Acc: 0.8196, Grad norm: 0.09095104978175197\n",
      "Iteration 5289, BCE loss: 57.757487338797986, Acc: 0.8196, Grad norm: 0.0974801669221663\n",
      "Iteration 5290, BCE loss: 57.75753581071754, Acc: 0.8196, Grad norm: 0.10900261029946808\n",
      "Iteration 5291, BCE loss: 57.757602726019144, Acc: 0.8195, Grad norm: 0.12337481831236236\n",
      "Iteration 5292, BCE loss: 57.75754189142676, Acc: 0.8195, Grad norm: 0.11220936731605873\n",
      "Iteration 5293, BCE loss: 57.75758501602119, Acc: 0.8195, Grad norm: 0.12085787044213918\n",
      "Iteration 5294, BCE loss: 57.75756914925049, Acc: 0.8196, Grad norm: 0.11792670953638225\n",
      "Iteration 5295, BCE loss: 57.757363236069054, Acc: 0.8196, Grad norm: 0.06623667467904998\n",
      "Iteration 5296, BCE loss: 57.757387417007095, Acc: 0.8196, Grad norm: 0.07125294450349691\n",
      "Iteration 5297, BCE loss: 57.75743265441285, Acc: 0.8196, Grad norm: 0.08542786562551903\n",
      "Iteration 5298, BCE loss: 57.75738852673848, Acc: 0.8196, Grad norm: 0.07384800374274197\n",
      "Iteration 5299, BCE loss: 57.757388848152225, Acc: 0.8196, Grad norm: 0.07510834154785537\n",
      "Iteration 5300, BCE loss: 57.75732541731443, Acc: 0.8196, Grad norm: 0.053460934457880846\n",
      "Iteration 5301, BCE loss: 57.75733284030815, Acc: 0.8196, Grad norm: 0.05705896459629395\n",
      "Iteration 5302, BCE loss: 57.75736374507636, Acc: 0.8196, Grad norm: 0.06871150420247858\n",
      "Iteration 5303, BCE loss: 57.757328077452584, Acc: 0.8196, Grad norm: 0.055266372639372535\n",
      "Iteration 5304, BCE loss: 57.7573169667519, Acc: 0.8196, Grad norm: 0.049962413559248284\n",
      "Iteration 5305, BCE loss: 57.75728518599209, Acc: 0.8196, Grad norm: 0.03334860278671726\n",
      "Iteration 5306, BCE loss: 57.75729024463412, Acc: 0.8196, Grad norm: 0.03496812241082784\n",
      "Iteration 5307, BCE loss: 57.757351386781465, Acc: 0.8196, Grad norm: 0.061746784583635195\n",
      "Iteration 5308, BCE loss: 57.75732336210899, Acc: 0.8196, Grad norm: 0.05051203524504207\n",
      "Iteration 5309, BCE loss: 57.7573315954001, Acc: 0.8196, Grad norm: 0.054015819969964955\n",
      "Iteration 5310, BCE loss: 57.75736177536603, Acc: 0.8196, Grad norm: 0.06233185996978636\n",
      "Iteration 5311, BCE loss: 57.75738009074681, Acc: 0.8196, Grad norm: 0.06885138963609573\n",
      "Iteration 5312, BCE loss: 57.75733437763583, Acc: 0.8196, Grad norm: 0.05574678299472178\n",
      "Iteration 5313, BCE loss: 57.75739601106485, Acc: 0.8196, Grad norm: 0.07778155882187976\n",
      "Iteration 5314, BCE loss: 57.757470297375406, Acc: 0.8196, Grad norm: 0.09616228629914285\n",
      "Iteration 5315, BCE loss: 57.75745339679126, Acc: 0.8196, Grad norm: 0.09248659913524909\n",
      "Iteration 5316, BCE loss: 57.75749080223969, Acc: 0.8195, Grad norm: 0.10004053703480434\n",
      "Iteration 5317, BCE loss: 57.757477605743944, Acc: 0.8196, Grad norm: 0.09745475261711291\n",
      "Iteration 5318, BCE loss: 57.75745991365969, Acc: 0.8196, Grad norm: 0.09603365786178054\n",
      "Iteration 5319, BCE loss: 57.75739925848738, Acc: 0.8195, Grad norm: 0.07865837663740624\n",
      "Iteration 5320, BCE loss: 57.75740696198598, Acc: 0.8195, Grad norm: 0.08045695027114902\n",
      "Iteration 5321, BCE loss: 57.75739656198156, Acc: 0.8196, Grad norm: 0.07279717134025107\n",
      "Iteration 5322, BCE loss: 57.757388387141575, Acc: 0.8195, Grad norm: 0.06803018422643783\n",
      "Iteration 5323, BCE loss: 57.75740711912567, Acc: 0.8195, Grad norm: 0.07244816648340391\n",
      "Iteration 5324, BCE loss: 57.75742971452324, Acc: 0.8195, Grad norm: 0.08289142343067543\n",
      "Iteration 5325, BCE loss: 57.75738362683355, Acc: 0.8195, Grad norm: 0.07221890678235442\n",
      "Iteration 5326, BCE loss: 57.75735030345329, Acc: 0.8195, Grad norm: 0.05876529681853419\n",
      "Iteration 5327, BCE loss: 57.757361347863, Acc: 0.8195, Grad norm: 0.06144843215201041\n",
      "Iteration 5328, BCE loss: 57.75737429783831, Acc: 0.8195, Grad norm: 0.06310447058052429\n",
      "Iteration 5329, BCE loss: 57.75737575511346, Acc: 0.8195, Grad norm: 0.06169239630608838\n",
      "Iteration 5330, BCE loss: 57.757365194062956, Acc: 0.8196, Grad norm: 0.05869737949478055\n",
      "Iteration 5331, BCE loss: 57.75734779925452, Acc: 0.8195, Grad norm: 0.05359977491104656\n",
      "Iteration 5332, BCE loss: 57.75733752218838, Acc: 0.8196, Grad norm: 0.05414167760585331\n",
      "Iteration 5333, BCE loss: 57.75740094780552, Acc: 0.8196, Grad norm: 0.07529868025632336\n",
      "Iteration 5334, BCE loss: 57.75737327757445, Acc: 0.8196, Grad norm: 0.0629211397174013\n",
      "Iteration 5335, BCE loss: 57.757402457084964, Acc: 0.8196, Grad norm: 0.07129571360254314\n",
      "Iteration 5336, BCE loss: 57.75742781354967, Acc: 0.8196, Grad norm: 0.07900386985145916\n",
      "Iteration 5337, BCE loss: 57.757447044211105, Acc: 0.8196, Grad norm: 0.08318542361978051\n",
      "Iteration 5338, BCE loss: 57.75740245211983, Acc: 0.8196, Grad norm: 0.07181708404977234\n",
      "Iteration 5339, BCE loss: 57.757503125603876, Acc: 0.8196, Grad norm: 0.0983063973723387\n",
      "Iteration 5340, BCE loss: 57.75749382134663, Acc: 0.8196, Grad norm: 0.09701170251133576\n",
      "Iteration 5341, BCE loss: 57.75740287578961, Acc: 0.8196, Grad norm: 0.07576894906581826\n",
      "Iteration 5342, BCE loss: 57.75741132760404, Acc: 0.8196, Grad norm: 0.07779177594329346\n",
      "Iteration 5343, BCE loss: 57.75744350965351, Acc: 0.8196, Grad norm: 0.08674429991909544\n",
      "Iteration 5344, BCE loss: 57.75750324488788, Acc: 0.8196, Grad norm: 0.09772487174916296\n",
      "Iteration 5345, BCE loss: 57.75749846000814, Acc: 0.8196, Grad norm: 0.09884570694440466\n",
      "Iteration 5346, BCE loss: 57.757492334807736, Acc: 0.8196, Grad norm: 0.09708317143834956\n",
      "Iteration 5347, BCE loss: 57.75745695616135, Acc: 0.8196, Grad norm: 0.08842026061981967\n",
      "Iteration 5348, BCE loss: 57.757468049518714, Acc: 0.8196, Grad norm: 0.09127871665012995\n",
      "Iteration 5349, BCE loss: 57.757381548029294, Acc: 0.8196, Grad norm: 0.06850432774814655\n",
      "Iteration 5350, BCE loss: 57.757395610884245, Acc: 0.8196, Grad norm: 0.07197764470487478\n",
      "Iteration 5351, BCE loss: 57.75746253945621, Acc: 0.8196, Grad norm: 0.09357858766205839\n",
      "Iteration 5352, BCE loss: 57.7574297190737, Acc: 0.8196, Grad norm: 0.08521959044807863\n",
      "Iteration 5353, BCE loss: 57.757420588218935, Acc: 0.8196, Grad norm: 0.0829248212317925\n",
      "Iteration 5354, BCE loss: 57.75741994346926, Acc: 0.8196, Grad norm: 0.07906228817428557\n",
      "Iteration 5355, BCE loss: 57.75739697761222, Acc: 0.8196, Grad norm: 0.07600840381363859\n",
      "Iteration 5356, BCE loss: 57.757332381518935, Acc: 0.8196, Grad norm: 0.05436327743018966\n",
      "Iteration 5357, BCE loss: 57.757352953380774, Acc: 0.8196, Grad norm: 0.06477565481587957\n",
      "Iteration 5358, BCE loss: 57.75734546620007, Acc: 0.8196, Grad norm: 0.060297412108374575\n",
      "Iteration 5359, BCE loss: 57.757385544381094, Acc: 0.8196, Grad norm: 0.07542374747765462\n",
      "Iteration 5360, BCE loss: 57.75734932348342, Acc: 0.8196, Grad norm: 0.0606468207260708\n",
      "Iteration 5361, BCE loss: 57.75732754939832, Acc: 0.8196, Grad norm: 0.05296090834581051\n",
      "Iteration 5362, BCE loss: 57.75733420190585, Acc: 0.8196, Grad norm: 0.0533186981411656\n",
      "Iteration 5363, BCE loss: 57.75727978545568, Acc: 0.8196, Grad norm: 0.026641164788140874\n",
      "Iteration 5364, BCE loss: 57.75730156697688, Acc: 0.8196, Grad norm: 0.04323069515176536\n",
      "Iteration 5365, BCE loss: 57.75733683809546, Acc: 0.8196, Grad norm: 0.05908588599278741\n",
      "Iteration 5366, BCE loss: 57.75738020280737, Acc: 0.8196, Grad norm: 0.07314927963746118\n",
      "Iteration 5367, BCE loss: 57.75752668671802, Acc: 0.8196, Grad norm: 0.11049671488502984\n",
      "Iteration 5368, BCE loss: 57.75751537027999, Acc: 0.8196, Grad norm: 0.10815027819143795\n",
      "Iteration 5369, BCE loss: 57.757474335662174, Acc: 0.8196, Grad norm: 0.10007513439700091\n",
      "Iteration 5370, BCE loss: 57.7574192260323, Acc: 0.8196, Grad norm: 0.08494192156767837\n",
      "Iteration 5371, BCE loss: 57.75743987499648, Acc: 0.8196, Grad norm: 0.08914326515256624\n",
      "Iteration 5372, BCE loss: 57.757479900037815, Acc: 0.8196, Grad norm: 0.09650984068906768\n",
      "Iteration 5373, BCE loss: 57.75740308337487, Acc: 0.8196, Grad norm: 0.07684322213952477\n",
      "Iteration 5374, BCE loss: 57.75737383562277, Acc: 0.8196, Grad norm: 0.0686454801901193\n",
      "Iteration 5375, BCE loss: 57.757362713564845, Acc: 0.8196, Grad norm: 0.0649166257942198\n",
      "Iteration 5376, BCE loss: 57.75736845727001, Acc: 0.8196, Grad norm: 0.06649470485353991\n",
      "Iteration 5377, BCE loss: 57.757359723601866, Acc: 0.8196, Grad norm: 0.06360943352619806\n",
      "Iteration 5378, BCE loss: 57.7573845897992, Acc: 0.8196, Grad norm: 0.07200856870827278\n",
      "Iteration 5379, BCE loss: 57.757357098176215, Acc: 0.8196, Grad norm: 0.06555755393320015\n",
      "Iteration 5380, BCE loss: 57.75735580976239, Acc: 0.8196, Grad norm: 0.06521628025091675\n",
      "Iteration 5381, BCE loss: 57.75733850890249, Acc: 0.8196, Grad norm: 0.05802145286071202\n",
      "Iteration 5382, BCE loss: 57.75729556224303, Acc: 0.8196, Grad norm: 0.03888605294811673\n",
      "Iteration 5383, BCE loss: 57.75735716117954, Acc: 0.8196, Grad norm: 0.0657549357042947\n",
      "Iteration 5384, BCE loss: 57.75741290051805, Acc: 0.8196, Grad norm: 0.08352386338163494\n",
      "Iteration 5385, BCE loss: 57.75736057133113, Acc: 0.8196, Grad norm: 0.06832570628043654\n",
      "Iteration 5386, BCE loss: 57.75731798338883, Acc: 0.8195, Grad norm: 0.048739129396210994\n",
      "Iteration 5387, BCE loss: 57.7572983711316, Acc: 0.8195, Grad norm: 0.037328022546222826\n",
      "Iteration 5388, BCE loss: 57.757340536407185, Acc: 0.8195, Grad norm: 0.054389945976742196\n",
      "Iteration 5389, BCE loss: 57.757410875065034, Acc: 0.8195, Grad norm: 0.07721567523504651\n",
      "Iteration 5390, BCE loss: 57.75744063658148, Acc: 0.8195, Grad norm: 0.08251136988266138\n",
      "Iteration 5391, BCE loss: 57.7575173633079, Acc: 0.8194, Grad norm: 0.10000541379687658\n",
      "Iteration 5392, BCE loss: 57.75744851827533, Acc: 0.8195, Grad norm: 0.0852173188272333\n",
      "Iteration 5393, BCE loss: 57.757433367289735, Acc: 0.8195, Grad norm: 0.08390043400738924\n",
      "Iteration 5394, BCE loss: 57.75739625178202, Acc: 0.8195, Grad norm: 0.07292875655164453\n",
      "Iteration 5395, BCE loss: 57.7574540164139, Acc: 0.8195, Grad norm: 0.09052514430943569\n",
      "Iteration 5396, BCE loss: 57.75745722631561, Acc: 0.8195, Grad norm: 0.0926162243128998\n",
      "Iteration 5397, BCE loss: 57.757376658311216, Acc: 0.8195, Grad norm: 0.07107768981864361\n",
      "Iteration 5398, BCE loss: 57.75737720860285, Acc: 0.8195, Grad norm: 0.07012560830930771\n",
      "Iteration 5399, BCE loss: 57.75749528136346, Acc: 0.8195, Grad norm: 0.1012142545111878\n",
      "Iteration 5400, BCE loss: 57.757504192759214, Acc: 0.8195, Grad norm: 0.10229575746212312\n",
      "Iteration 5401, BCE loss: 57.75754784520545, Acc: 0.8195, Grad norm: 0.11159414016319577\n",
      "Iteration 5402, BCE loss: 57.75755711740436, Acc: 0.8195, Grad norm: 0.11295752527829085\n",
      "Iteration 5403, BCE loss: 57.75745081923447, Acc: 0.8195, Grad norm: 0.08714521157134578\n",
      "Iteration 5404, BCE loss: 57.75742755968532, Acc: 0.8195, Grad norm: 0.07881910303507382\n",
      "Iteration 5405, BCE loss: 57.75755195990098, Acc: 0.8195, Grad norm: 0.10697196381483952\n",
      "Iteration 5406, BCE loss: 57.75747098537059, Acc: 0.8195, Grad norm: 0.08830967722524022\n",
      "Iteration 5407, BCE loss: 57.7574156440563, Acc: 0.8195, Grad norm: 0.07394214975719694\n",
      "Iteration 5408, BCE loss: 57.75748724040082, Acc: 0.8195, Grad norm: 0.09361237228670373\n",
      "Iteration 5409, BCE loss: 57.75754232486606, Acc: 0.8195, Grad norm: 0.10813341816887187\n",
      "Iteration 5410, BCE loss: 57.757508407414136, Acc: 0.8195, Grad norm: 0.0979147889226542\n",
      "Iteration 5411, BCE loss: 57.757487247259036, Acc: 0.8195, Grad norm: 0.09348717368902892\n",
      "Iteration 5412, BCE loss: 57.757523827345764, Acc: 0.8195, Grad norm: 0.1034846056913711\n",
      "Iteration 5413, BCE loss: 57.757422407341295, Acc: 0.8195, Grad norm: 0.07898820437740102\n",
      "Iteration 5414, BCE loss: 57.75745317646708, Acc: 0.8195, Grad norm: 0.0826289588579984\n",
      "Iteration 5415, BCE loss: 57.757498982104664, Acc: 0.8195, Grad norm: 0.09532901974374634\n",
      "Iteration 5416, BCE loss: 57.75755438116539, Acc: 0.8195, Grad norm: 0.10789108463314466\n",
      "Iteration 5417, BCE loss: 57.7575508646177, Acc: 0.8195, Grad norm: 0.10632181495019762\n",
      "Iteration 5418, BCE loss: 57.75752708982659, Acc: 0.8194, Grad norm: 0.10392894514641993\n",
      "Iteration 5419, BCE loss: 57.75744553764662, Acc: 0.8195, Grad norm: 0.08597835813594652\n",
      "Iteration 5420, BCE loss: 57.75748454781096, Acc: 0.8194, Grad norm: 0.09431033480059083\n",
      "Iteration 5421, BCE loss: 57.757424240163886, Acc: 0.8195, Grad norm: 0.07762658904166886\n",
      "Iteration 5422, BCE loss: 57.75737450987286, Acc: 0.8195, Grad norm: 0.06308036525657942\n",
      "Iteration 5423, BCE loss: 57.75740724116652, Acc: 0.8195, Grad norm: 0.07038916006306493\n",
      "Iteration 5424, BCE loss: 57.757414975390844, Acc: 0.8195, Grad norm: 0.07532255032827126\n",
      "Iteration 5425, BCE loss: 57.757433043591305, Acc: 0.8195, Grad norm: 0.0769532964270708\n",
      "Iteration 5426, BCE loss: 57.75741067181215, Acc: 0.8195, Grad norm: 0.07396390781789382\n",
      "Iteration 5427, BCE loss: 57.757502780314645, Acc: 0.8195, Grad norm: 0.09770425320281251\n",
      "Iteration 5428, BCE loss: 57.75744204528566, Acc: 0.8195, Grad norm: 0.08380187260023575\n",
      "Iteration 5429, BCE loss: 57.757526871463654, Acc: 0.8195, Grad norm: 0.10426218046849652\n",
      "Iteration 5430, BCE loss: 57.75742555796481, Acc: 0.8195, Grad norm: 0.08256306604611661\n",
      "Iteration 5431, BCE loss: 57.75745772176039, Acc: 0.8196, Grad norm: 0.09183522813547362\n",
      "Iteration 5432, BCE loss: 57.75741388200257, Acc: 0.8196, Grad norm: 0.0812950291856342\n",
      "Iteration 5433, BCE loss: 57.75742626655872, Acc: 0.8195, Grad norm: 0.08235095845456772\n",
      "Iteration 5434, BCE loss: 57.75740011242064, Acc: 0.8195, Grad norm: 0.07523222202974214\n",
      "Iteration 5435, BCE loss: 57.75735077639587, Acc: 0.8195, Grad norm: 0.0603000118094576\n",
      "Iteration 5436, BCE loss: 57.75743820506096, Acc: 0.8195, Grad norm: 0.08413425489894243\n",
      "Iteration 5437, BCE loss: 57.75745086602778, Acc: 0.8195, Grad norm: 0.08586959944087687\n",
      "Iteration 5438, BCE loss: 57.757510849923804, Acc: 0.8195, Grad norm: 0.10054563794038586\n",
      "Iteration 5439, BCE loss: 57.757637665066355, Acc: 0.8195, Grad norm: 0.12389812194378386\n",
      "Iteration 5440, BCE loss: 57.757737480681214, Acc: 0.8196, Grad norm: 0.14358166604971725\n",
      "Iteration 5441, BCE loss: 57.75761694029215, Acc: 0.8196, Grad norm: 0.12322260443568916\n",
      "Iteration 5442, BCE loss: 57.757473023268986, Acc: 0.8196, Grad norm: 0.09415087824490723\n",
      "Iteration 5443, BCE loss: 57.75735707162407, Acc: 0.8196, Grad norm: 0.061100668050132194\n",
      "Iteration 5444, BCE loss: 57.75735936945999, Acc: 0.8196, Grad norm: 0.06524235102331634\n",
      "Iteration 5445, BCE loss: 57.75735749704559, Acc: 0.8196, Grad norm: 0.06452814843920213\n",
      "Iteration 5446, BCE loss: 57.75729813047017, Acc: 0.8196, Grad norm: 0.04199497977193245\n",
      "Iteration 5447, BCE loss: 57.7573125234678, Acc: 0.8195, Grad norm: 0.048940730205264966\n",
      "Iteration 5448, BCE loss: 57.75730036784958, Acc: 0.8195, Grad norm: 0.040711657950465246\n",
      "Iteration 5449, BCE loss: 57.75735149898955, Acc: 0.8195, Grad norm: 0.06349507408520519\n",
      "Iteration 5450, BCE loss: 57.75736994815371, Acc: 0.8195, Grad norm: 0.06949657931650113\n",
      "Iteration 5451, BCE loss: 57.7574248725589, Acc: 0.8195, Grad norm: 0.08497814484434982\n",
      "Iteration 5452, BCE loss: 57.7573390534977, Acc: 0.8195, Grad norm: 0.05688978386718543\n",
      "Iteration 5453, BCE loss: 57.75733819870572, Acc: 0.8195, Grad norm: 0.055727446363343716\n",
      "Iteration 5454, BCE loss: 57.757308406853255, Acc: 0.8195, Grad norm: 0.04466627179702877\n",
      "Iteration 5455, BCE loss: 57.75733194758964, Acc: 0.8195, Grad norm: 0.05752667917292904\n",
      "Iteration 5456, BCE loss: 57.757281331212454, Acc: 0.8195, Grad norm: 0.030437368062240875\n",
      "Iteration 5457, BCE loss: 57.757299786557844, Acc: 0.8196, Grad norm: 0.03718711491517369\n",
      "Iteration 5458, BCE loss: 57.75731255777458, Acc: 0.8196, Grad norm: 0.047045479489513875\n",
      "Iteration 5459, BCE loss: 57.75734568958767, Acc: 0.8196, Grad norm: 0.06031400898019765\n",
      "Iteration 5460, BCE loss: 57.75734860474586, Acc: 0.8196, Grad norm: 0.060118364255685904\n",
      "Iteration 5461, BCE loss: 57.7575041397067, Acc: 0.8196, Grad norm: 0.10245735948516037\n",
      "Iteration 5462, BCE loss: 57.75749029505526, Acc: 0.8196, Grad norm: 0.1007922705382027\n",
      "Iteration 5463, BCE loss: 57.75739783622488, Acc: 0.8196, Grad norm: 0.07720018838239638\n",
      "Iteration 5464, BCE loss: 57.75743896617868, Acc: 0.8196, Grad norm: 0.09031399203440307\n",
      "Iteration 5465, BCE loss: 57.757426763249846, Acc: 0.8196, Grad norm: 0.0848577907319294\n",
      "Iteration 5466, BCE loss: 57.75735557164447, Acc: 0.8196, Grad norm: 0.0628851808963472\n",
      "Iteration 5467, BCE loss: 57.757338898828266, Acc: 0.8195, Grad norm: 0.05469142285881211\n",
      "Iteration 5468, BCE loss: 57.75731757687241, Acc: 0.8196, Grad norm: 0.04638524262668995\n",
      "Iteration 5469, BCE loss: 57.757328236562124, Acc: 0.8195, Grad norm: 0.05461379498731552\n",
      "Iteration 5470, BCE loss: 57.75731904562362, Acc: 0.8195, Grad norm: 0.051096953403524104\n",
      "Iteration 5471, BCE loss: 57.75733241567451, Acc: 0.8195, Grad norm: 0.056915641773025105\n",
      "Iteration 5472, BCE loss: 57.75734654783746, Acc: 0.8196, Grad norm: 0.06325113333524479\n",
      "Iteration 5473, BCE loss: 57.75731088993702, Acc: 0.8196, Grad norm: 0.048277411184638644\n",
      "Iteration 5474, BCE loss: 57.757383905988306, Acc: 0.8196, Grad norm: 0.0759508183520728\n",
      "Iteration 5475, BCE loss: 57.75747375459064, Acc: 0.8196, Grad norm: 0.09917390310878399\n",
      "Iteration 5476, BCE loss: 57.75739875129078, Acc: 0.8196, Grad norm: 0.07912952161831116\n",
      "Iteration 5477, BCE loss: 57.75738326425163, Acc: 0.8196, Grad norm: 0.07196785085216757\n",
      "Iteration 5478, BCE loss: 57.75743408146504, Acc: 0.8196, Grad norm: 0.08542989985202065\n",
      "Iteration 5479, BCE loss: 57.757417283997725, Acc: 0.8196, Grad norm: 0.0784249306156401\n",
      "Iteration 5480, BCE loss: 57.75736695272061, Acc: 0.8196, Grad norm: 0.0656615526586384\n",
      "Iteration 5481, BCE loss: 57.757414662803036, Acc: 0.8196, Grad norm: 0.08140847865464292\n",
      "Iteration 5482, BCE loss: 57.75742978310049, Acc: 0.8196, Grad norm: 0.0827956841588559\n",
      "Iteration 5483, BCE loss: 57.7574406303947, Acc: 0.8196, Grad norm: 0.08722320726174314\n",
      "Iteration 5484, BCE loss: 57.7574295952886, Acc: 0.8196, Grad norm: 0.08419211974617818\n",
      "Iteration 5485, BCE loss: 57.757457295783055, Acc: 0.8196, Grad norm: 0.09081407698332733\n",
      "Iteration 5486, BCE loss: 57.75737439435123, Acc: 0.8196, Grad norm: 0.06805590141016951\n",
      "Iteration 5487, BCE loss: 57.7573906803892, Acc: 0.8196, Grad norm: 0.0729163098247118\n",
      "Iteration 5488, BCE loss: 57.757437177865256, Acc: 0.8196, Grad norm: 0.08386437374547484\n",
      "Iteration 5489, BCE loss: 57.75737519028378, Acc: 0.8196, Grad norm: 0.06824993280514327\n",
      "Iteration 5490, BCE loss: 57.75734510582463, Acc: 0.8196, Grad norm: 0.059541879906226496\n",
      "Iteration 5491, BCE loss: 57.75737263402246, Acc: 0.8196, Grad norm: 0.06872391339060785\n",
      "Iteration 5492, BCE loss: 57.757448446660675, Acc: 0.8196, Grad norm: 0.09028114779418918\n",
      "Iteration 5493, BCE loss: 57.7573824956785, Acc: 0.8196, Grad norm: 0.06940928590312159\n",
      "Iteration 5494, BCE loss: 57.75735765246101, Acc: 0.8196, Grad norm: 0.06022969121901289\n",
      "Iteration 5495, BCE loss: 57.7573119754067, Acc: 0.8195, Grad norm: 0.04121633306894478\n",
      "Iteration 5496, BCE loss: 57.757328309105844, Acc: 0.8196, Grad norm: 0.048541647375942444\n",
      "Iteration 5497, BCE loss: 57.75728075016471, Acc: 0.8195, Grad norm: 0.02659127637609899\n",
      "Iteration 5498, BCE loss: 57.757331377727134, Acc: 0.8195, Grad norm: 0.05467912256859548\n",
      "Iteration 5499, BCE loss: 57.75728464232667, Acc: 0.8196, Grad norm: 0.028111614252027343\n",
      "Iteration 5500, BCE loss: 57.75735085195065, Acc: 0.8195, Grad norm: 0.05867062236070936\n",
      "Iteration 5501, BCE loss: 57.75741655224931, Acc: 0.8196, Grad norm: 0.08304932050264383\n",
      "Iteration 5502, BCE loss: 57.75743453217574, Acc: 0.8195, Grad norm: 0.08779513869501206\n",
      "Iteration 5503, BCE loss: 57.757497828942235, Acc: 0.8195, Grad norm: 0.1024952436429661\n",
      "Iteration 5504, BCE loss: 57.75750891791347, Acc: 0.8195, Grad norm: 0.10463103333869597\n",
      "Iteration 5505, BCE loss: 57.75750995408944, Acc: 0.8196, Grad norm: 0.10340935729415494\n",
      "Iteration 5506, BCE loss: 57.75744050213949, Acc: 0.8195, Grad norm: 0.08817547867549523\n",
      "Iteration 5507, BCE loss: 57.75738081459954, Acc: 0.8195, Grad norm: 0.072358655898957\n",
      "Iteration 5508, BCE loss: 57.75736492610285, Acc: 0.8195, Grad norm: 0.06692347616369754\n",
      "Iteration 5509, BCE loss: 57.75740605836757, Acc: 0.8195, Grad norm: 0.0793824318418171\n",
      "Iteration 5510, BCE loss: 57.75735017303788, Acc: 0.8196, Grad norm: 0.05960964168618839\n",
      "Iteration 5511, BCE loss: 57.757329250689395, Acc: 0.8196, Grad norm: 0.05123276856539856\n",
      "Iteration 5512, BCE loss: 57.757317035531386, Acc: 0.8196, Grad norm: 0.043871018043177834\n",
      "Iteration 5513, BCE loss: 57.757316883411704, Acc: 0.8196, Grad norm: 0.04445378474308429\n",
      "Iteration 5514, BCE loss: 57.75730826553999, Acc: 0.8195, Grad norm: 0.042499884430362164\n",
      "Iteration 5515, BCE loss: 57.75729235463426, Acc: 0.8195, Grad norm: 0.036091252269137314\n",
      "Iteration 5516, BCE loss: 57.75736395422874, Acc: 0.8195, Grad norm: 0.06629469423956064\n",
      "Iteration 5517, BCE loss: 57.7573504086328, Acc: 0.8195, Grad norm: 0.0618576405083428\n",
      "Iteration 5518, BCE loss: 57.757321221842204, Acc: 0.8196, Grad norm: 0.05265402782694079\n",
      "Iteration 5519, BCE loss: 57.75736298633926, Acc: 0.8196, Grad norm: 0.06727316507861095\n",
      "Iteration 5520, BCE loss: 57.75732419981432, Acc: 0.8196, Grad norm: 0.052140606883120735\n",
      "Iteration 5521, BCE loss: 57.757391461462554, Acc: 0.8195, Grad norm: 0.07666695789094795\n",
      "Iteration 5522, BCE loss: 57.75733824136307, Acc: 0.8195, Grad norm: 0.05801387128646767\n",
      "Iteration 5523, BCE loss: 57.7573731576573, Acc: 0.8195, Grad norm: 0.06764840987409246\n",
      "Iteration 5524, BCE loss: 57.75735171439513, Acc: 0.8195, Grad norm: 0.06147920613904725\n",
      "Iteration 5525, BCE loss: 57.757368965990906, Acc: 0.8195, Grad norm: 0.06336296418104141\n",
      "Iteration 5526, BCE loss: 57.75738582635661, Acc: 0.8195, Grad norm: 0.06813920516139887\n",
      "Iteration 5527, BCE loss: 57.75740781353414, Acc: 0.8195, Grad norm: 0.07554536247209657\n",
      "Iteration 5528, BCE loss: 57.75743466471266, Acc: 0.8196, Grad norm: 0.08407956988358309\n",
      "Iteration 5529, BCE loss: 57.75740291051625, Acc: 0.8196, Grad norm: 0.07495334769487218\n",
      "Iteration 5530, BCE loss: 57.757445582148065, Acc: 0.8196, Grad norm: 0.08745061303852333\n",
      "Iteration 5531, BCE loss: 57.757445766399385, Acc: 0.8195, Grad norm: 0.08620002670405318\n",
      "Iteration 5532, BCE loss: 57.75753002925877, Acc: 0.8195, Grad norm: 0.10434246860783987\n",
      "Iteration 5533, BCE loss: 57.7574573178761, Acc: 0.8195, Grad norm: 0.08692626673985082\n",
      "Iteration 5534, BCE loss: 57.75743067774734, Acc: 0.8196, Grad norm: 0.08381611448995571\n",
      "Iteration 5535, BCE loss: 57.75741375095287, Acc: 0.8196, Grad norm: 0.07942876068640185\n",
      "Iteration 5536, BCE loss: 57.757355100820014, Acc: 0.8196, Grad norm: 0.061764767875727856\n",
      "Iteration 5537, BCE loss: 57.75734912920004, Acc: 0.8196, Grad norm: 0.06038088445616823\n",
      "Iteration 5538, BCE loss: 57.75732067414832, Acc: 0.8196, Grad norm: 0.04793932702638669\n",
      "Iteration 5539, BCE loss: 57.75731720302544, Acc: 0.8195, Grad norm: 0.047610465970138675\n",
      "Iteration 5540, BCE loss: 57.75734921703072, Acc: 0.8195, Grad norm: 0.06052534542536043\n",
      "Iteration 5541, BCE loss: 57.757352115880934, Acc: 0.8195, Grad norm: 0.060004122295467074\n",
      "Iteration 5542, BCE loss: 57.75736237728992, Acc: 0.8196, Grad norm: 0.06232157198485989\n",
      "Iteration 5543, BCE loss: 57.75737575938655, Acc: 0.8195, Grad norm: 0.06786104810323938\n",
      "Iteration 5544, BCE loss: 57.75734159106632, Acc: 0.8195, Grad norm: 0.05677634102749319\n",
      "Iteration 5545, BCE loss: 57.75733593211531, Acc: 0.8196, Grad norm: 0.053309308461080715\n",
      "Iteration 5546, BCE loss: 57.75732532669765, Acc: 0.8195, Grad norm: 0.04915437185706137\n",
      "Iteration 5547, BCE loss: 57.75734765571926, Acc: 0.8195, Grad norm: 0.05694127758204138\n",
      "Iteration 5548, BCE loss: 57.75733853640757, Acc: 0.8195, Grad norm: 0.05400759140968121\n",
      "Iteration 5549, BCE loss: 57.75736753784241, Acc: 0.8195, Grad norm: 0.06478628737132995\n",
      "Iteration 5550, BCE loss: 57.75735170000531, Acc: 0.8195, Grad norm: 0.057852296466440564\n",
      "Iteration 5551, BCE loss: 57.75736574370161, Acc: 0.8195, Grad norm: 0.06168253311783291\n",
      "Iteration 5552, BCE loss: 57.757418424091995, Acc: 0.8195, Grad norm: 0.07842095759621624\n",
      "Iteration 5553, BCE loss: 57.757407051180756, Acc: 0.8195, Grad norm: 0.07397771488970205\n",
      "Iteration 5554, BCE loss: 57.75740439467832, Acc: 0.8195, Grad norm: 0.07349348983402798\n",
      "Iteration 5555, BCE loss: 57.75743766392149, Acc: 0.8195, Grad norm: 0.08095193556797446\n",
      "Iteration 5556, BCE loss: 57.757398098341326, Acc: 0.8195, Grad norm: 0.07372300656422551\n",
      "Iteration 5557, BCE loss: 57.75740663855467, Acc: 0.8195, Grad norm: 0.0745422705421462\n",
      "Iteration 5558, BCE loss: 57.75745874538343, Acc: 0.8195, Grad norm: 0.08758294315931187\n",
      "Iteration 5559, BCE loss: 57.757480123247035, Acc: 0.8195, Grad norm: 0.09053179601410256\n",
      "Iteration 5560, BCE loss: 57.75748183936585, Acc: 0.8196, Grad norm: 0.09232831357133199\n",
      "Iteration 5561, BCE loss: 57.75740560033479, Acc: 0.8195, Grad norm: 0.07381548372604252\n",
      "Iteration 5562, BCE loss: 57.757394120901196, Acc: 0.8195, Grad norm: 0.0700661726883504\n",
      "Iteration 5563, BCE loss: 57.75735513591803, Acc: 0.8196, Grad norm: 0.058408394325965306\n",
      "Iteration 5564, BCE loss: 57.75743287890634, Acc: 0.8196, Grad norm: 0.08112068096720307\n",
      "Iteration 5565, BCE loss: 57.757454565858396, Acc: 0.8196, Grad norm: 0.0845946634660338\n",
      "Iteration 5566, BCE loss: 57.75752208290267, Acc: 0.8196, Grad norm: 0.10134622975911457\n",
      "Iteration 5567, BCE loss: 57.75748497772106, Acc: 0.8196, Grad norm: 0.0939083019388128\n",
      "Iteration 5568, BCE loss: 57.75744280511971, Acc: 0.8196, Grad norm: 0.08149266470577451\n",
      "Iteration 5569, BCE loss: 57.757443645096885, Acc: 0.8195, Grad norm: 0.07766237600079569\n",
      "Iteration 5570, BCE loss: 57.75746862453606, Acc: 0.8195, Grad norm: 0.086069868441856\n",
      "Iteration 5571, BCE loss: 57.75744557708681, Acc: 0.8195, Grad norm: 0.08180120004496545\n",
      "Iteration 5572, BCE loss: 57.75749121115, Acc: 0.8195, Grad norm: 0.09067841824513093\n",
      "Iteration 5573, BCE loss: 57.757501850017206, Acc: 0.8195, Grad norm: 0.09262378052870957\n",
      "Iteration 5574, BCE loss: 57.75750680088052, Acc: 0.8195, Grad norm: 0.0943030199316326\n",
      "Iteration 5575, BCE loss: 57.75753198660831, Acc: 0.8195, Grad norm: 0.1003312694810879\n",
      "Iteration 5576, BCE loss: 57.75747915388066, Acc: 0.8195, Grad norm: 0.09053153180609917\n",
      "Iteration 5577, BCE loss: 57.75745588041839, Acc: 0.8195, Grad norm: 0.08731863701278288\n",
      "Iteration 5578, BCE loss: 57.75753471243877, Acc: 0.8195, Grad norm: 0.10322627466090814\n",
      "Iteration 5579, BCE loss: 57.75744393556112, Acc: 0.8195, Grad norm: 0.0842527280192543\n",
      "Iteration 5580, BCE loss: 57.75740258268521, Acc: 0.8195, Grad norm: 0.0740611664803827\n",
      "Iteration 5581, BCE loss: 57.75744832117337, Acc: 0.8195, Grad norm: 0.08443379737189877\n",
      "Iteration 5582, BCE loss: 57.757472678607655, Acc: 0.8195, Grad norm: 0.08953100947751975\n",
      "Iteration 5583, BCE loss: 57.75746686809741, Acc: 0.8195, Grad norm: 0.0878337013730729\n",
      "Iteration 5584, BCE loss: 57.75755238504217, Acc: 0.8195, Grad norm: 0.10881567143003786\n",
      "Iteration 5585, BCE loss: 57.757481835615735, Acc: 0.8195, Grad norm: 0.09631707481149471\n",
      "Iteration 5586, BCE loss: 57.757592051346634, Acc: 0.8195, Grad norm: 0.11926905661817888\n",
      "Iteration 5587, BCE loss: 57.757493672001786, Acc: 0.8195, Grad norm: 0.09836575059821978\n",
      "Iteration 5588, BCE loss: 57.75738745751494, Acc: 0.8195, Grad norm: 0.06965978140897906\n",
      "Iteration 5589, BCE loss: 57.757390600897736, Acc: 0.8195, Grad norm: 0.07096193523614387\n",
      "Iteration 5590, BCE loss: 57.75744110098188, Acc: 0.8195, Grad norm: 0.08663684222967218\n",
      "Iteration 5591, BCE loss: 57.75740585141906, Acc: 0.8195, Grad norm: 0.07936098231740656\n",
      "Iteration 5592, BCE loss: 57.75736263454431, Acc: 0.8195, Grad norm: 0.06543574491918278\n",
      "Iteration 5593, BCE loss: 57.757348020752744, Acc: 0.8195, Grad norm: 0.05846720587232851\n",
      "Iteration 5594, BCE loss: 57.75742695004402, Acc: 0.8195, Grad norm: 0.08609716431772042\n",
      "Iteration 5595, BCE loss: 57.757568255441285, Acc: 0.8195, Grad norm: 0.1199880446379548\n",
      "Iteration 5596, BCE loss: 57.75749538071337, Acc: 0.8195, Grad norm: 0.10502738717475259\n",
      "Iteration 5597, BCE loss: 57.75742545400894, Acc: 0.8195, Grad norm: 0.08663732574224252\n",
      "Iteration 5598, BCE loss: 57.75739767765855, Acc: 0.8195, Grad norm: 0.07636124757096734\n",
      "Iteration 5599, BCE loss: 57.75744538829278, Acc: 0.8195, Grad norm: 0.09011028766247307\n",
      "Iteration 5600, BCE loss: 57.75740788480981, Acc: 0.8195, Grad norm: 0.0805903538699737\n",
      "Iteration 5601, BCE loss: 57.7573784390441, Acc: 0.8195, Grad norm: 0.07010888638687116\n",
      "Iteration 5602, BCE loss: 57.75738557755051, Acc: 0.8195, Grad norm: 0.07106482057769518\n",
      "Iteration 5603, BCE loss: 57.75739767878819, Acc: 0.8195, Grad norm: 0.0749972461567928\n",
      "Iteration 5604, BCE loss: 57.75741310881592, Acc: 0.8195, Grad norm: 0.07823071320946244\n",
      "Iteration 5605, BCE loss: 57.75741725642395, Acc: 0.8195, Grad norm: 0.07886225670141216\n",
      "Iteration 5606, BCE loss: 57.75745675839678, Acc: 0.8195, Grad norm: 0.08747836562453619\n",
      "Iteration 5607, BCE loss: 57.757424552232735, Acc: 0.8195, Grad norm: 0.07785319305974582\n",
      "Iteration 5608, BCE loss: 57.757473362175354, Acc: 0.8195, Grad norm: 0.09241965793005283\n",
      "Iteration 5609, BCE loss: 57.75737828401028, Acc: 0.8195, Grad norm: 0.06353305766012363\n",
      "Iteration 5610, BCE loss: 57.757408450770924, Acc: 0.8195, Grad norm: 0.0734113044196815\n",
      "Iteration 5611, BCE loss: 57.75742773402726, Acc: 0.8195, Grad norm: 0.08304165073215541\n",
      "Iteration 5612, BCE loss: 57.75749376355138, Acc: 0.8196, Grad norm: 0.09954513692297187\n",
      "Iteration 5613, BCE loss: 57.75754495796602, Acc: 0.8196, Grad norm: 0.11082887820882101\n",
      "Iteration 5614, BCE loss: 57.75756824133066, Acc: 0.8196, Grad norm: 0.11470976777311082\n",
      "Iteration 5615, BCE loss: 57.75744210387961, Acc: 0.8196, Grad norm: 0.0831915110871126\n",
      "Iteration 5616, BCE loss: 57.75745361703878, Acc: 0.8196, Grad norm: 0.08399248757951105\n",
      "Iteration 5617, BCE loss: 57.757442577008476, Acc: 0.8196, Grad norm: 0.08145607071631379\n",
      "Iteration 5618, BCE loss: 57.757444122703156, Acc: 0.8196, Grad norm: 0.08144851541564038\n",
      "Iteration 5619, BCE loss: 57.75751759660251, Acc: 0.8196, Grad norm: 0.1010476897793585\n",
      "Iteration 5620, BCE loss: 57.757528922341024, Acc: 0.8196, Grad norm: 0.10234595354919139\n",
      "Iteration 5621, BCE loss: 57.757556825808464, Acc: 0.8195, Grad norm: 0.10618515749946489\n",
      "Iteration 5622, BCE loss: 57.757500118579756, Acc: 0.8195, Grad norm: 0.09204646450167932\n",
      "Iteration 5623, BCE loss: 57.757449437479345, Acc: 0.8195, Grad norm: 0.08197601533671\n",
      "Iteration 5624, BCE loss: 57.75748213336268, Acc: 0.8195, Grad norm: 0.09202096644669742\n",
      "Iteration 5625, BCE loss: 57.757549734506355, Acc: 0.8196, Grad norm: 0.10728156467486191\n",
      "Iteration 5626, BCE loss: 57.75752815693687, Acc: 0.8196, Grad norm: 0.1020358099343158\n",
      "Iteration 5627, BCE loss: 57.75748177734507, Acc: 0.8196, Grad norm: 0.0914437127277523\n",
      "Iteration 5628, BCE loss: 57.75750221695097, Acc: 0.8196, Grad norm: 0.0986105357360749\n",
      "Iteration 5629, BCE loss: 57.7574730323075, Acc: 0.8196, Grad norm: 0.09233348633166821\n",
      "Iteration 5630, BCE loss: 57.75738686322488, Acc: 0.8196, Grad norm: 0.06980725374457347\n",
      "Iteration 5631, BCE loss: 57.757420828409366, Acc: 0.8196, Grad norm: 0.0804011960599039\n",
      "Iteration 5632, BCE loss: 57.75747501887351, Acc: 0.8196, Grad norm: 0.09424887909322686\n",
      "Iteration 5633, BCE loss: 57.757498641230804, Acc: 0.8197, Grad norm: 0.0952842658353851\n",
      "Iteration 5634, BCE loss: 57.75757589227322, Acc: 0.8197, Grad norm: 0.1142595293907835\n",
      "Iteration 5635, BCE loss: 57.757686609742606, Acc: 0.8197, Grad norm: 0.13341176266523502\n",
      "Iteration 5636, BCE loss: 57.757540407763784, Acc: 0.8197, Grad norm: 0.10818308653978435\n",
      "Iteration 5637, BCE loss: 57.757513360483614, Acc: 0.8196, Grad norm: 0.10112969387643236\n",
      "Iteration 5638, BCE loss: 57.75747360666888, Acc: 0.8196, Grad norm: 0.09004920733786421\n",
      "Iteration 5639, BCE loss: 57.75747375430764, Acc: 0.8196, Grad norm: 0.09164025646743959\n",
      "Iteration 5640, BCE loss: 57.75743614406418, Acc: 0.8196, Grad norm: 0.08276895045487283\n",
      "Iteration 5641, BCE loss: 57.75740569262116, Acc: 0.8196, Grad norm: 0.07700193593582817\n",
      "Iteration 5642, BCE loss: 57.75739935165167, Acc: 0.8196, Grad norm: 0.07634822896958253\n",
      "Iteration 5643, BCE loss: 57.757346267382914, Acc: 0.8196, Grad norm: 0.058717558035103645\n",
      "Iteration 5644, BCE loss: 57.757386076695134, Acc: 0.8196, Grad norm: 0.07116961188978062\n",
      "Iteration 5645, BCE loss: 57.75736943647688, Acc: 0.8196, Grad norm: 0.06743942156575111\n",
      "Iteration 5646, BCE loss: 57.75743374723342, Acc: 0.8196, Grad norm: 0.08224380872585187\n",
      "Iteration 5647, BCE loss: 57.757404484546946, Acc: 0.8196, Grad norm: 0.07299099193156751\n",
      "Iteration 5648, BCE loss: 57.75742402800316, Acc: 0.8196, Grad norm: 0.07700265865467343\n",
      "Iteration 5649, BCE loss: 57.757422015936456, Acc: 0.8196, Grad norm: 0.07833291253329884\n",
      "Iteration 5650, BCE loss: 57.75737347855194, Acc: 0.8196, Grad norm: 0.06514960722241721\n",
      "Iteration 5651, BCE loss: 57.75742057559148, Acc: 0.8196, Grad norm: 0.07709493312709453\n",
      "Iteration 5652, BCE loss: 57.75739604540688, Acc: 0.8195, Grad norm: 0.06877447692897469\n",
      "Iteration 5653, BCE loss: 57.75737352348255, Acc: 0.8195, Grad norm: 0.06332279512456401\n",
      "Iteration 5654, BCE loss: 57.757428666353306, Acc: 0.8196, Grad norm: 0.07846913952511543\n",
      "Iteration 5655, BCE loss: 57.757509197820724, Acc: 0.8195, Grad norm: 0.09822524820137972\n",
      "Iteration 5656, BCE loss: 57.75750605816573, Acc: 0.8196, Grad norm: 0.0982649970959764\n",
      "Iteration 5657, BCE loss: 57.757613227000576, Acc: 0.8196, Grad norm: 0.1191302157887936\n",
      "Iteration 5658, BCE loss: 57.757561091931194, Acc: 0.8196, Grad norm: 0.10958150456900954\n",
      "Iteration 5659, BCE loss: 57.75754319498419, Acc: 0.8196, Grad norm: 0.10573793945435375\n",
      "Iteration 5660, BCE loss: 57.75752540897582, Acc: 0.8196, Grad norm: 0.10482085690813081\n",
      "Iteration 5661, BCE loss: 57.75756215426581, Acc: 0.8196, Grad norm: 0.1127328247881506\n",
      "Iteration 5662, BCE loss: 57.75747792954098, Acc: 0.8196, Grad norm: 0.09591195887421788\n",
      "Iteration 5663, BCE loss: 57.757431865279344, Acc: 0.8196, Grad norm: 0.08501915275222827\n",
      "Iteration 5664, BCE loss: 57.75742221120815, Acc: 0.8196, Grad norm: 0.08223201939642238\n",
      "Iteration 5665, BCE loss: 57.75743441760237, Acc: 0.8196, Grad norm: 0.08352245194368776\n",
      "Iteration 5666, BCE loss: 57.7573947774737, Acc: 0.8196, Grad norm: 0.07362629361706881\n",
      "Iteration 5667, BCE loss: 57.757379583352495, Acc: 0.8196, Grad norm: 0.07017375006925777\n",
      "Iteration 5668, BCE loss: 57.75746776639569, Acc: 0.8196, Grad norm: 0.09713572300570973\n",
      "Iteration 5669, BCE loss: 57.75749617834124, Acc: 0.8196, Grad norm: 0.10349472209216216\n",
      "Iteration 5670, BCE loss: 57.75750476968786, Acc: 0.8196, Grad norm: 0.10266084549976572\n",
      "Iteration 5671, BCE loss: 57.75737408199661, Acc: 0.8196, Grad norm: 0.06918443347070864\n",
      "Iteration 5672, BCE loss: 57.75736450604592, Acc: 0.8196, Grad norm: 0.06389330918394737\n",
      "Iteration 5673, BCE loss: 57.757349732197106, Acc: 0.8196, Grad norm: 0.058626933457968024\n",
      "Iteration 5674, BCE loss: 57.757334480401084, Acc: 0.8196, Grad norm: 0.052533622228683845\n",
      "Iteration 5675, BCE loss: 57.75738058248085, Acc: 0.8196, Grad norm: 0.06923224423680151\n",
      "Iteration 5676, BCE loss: 57.75742782428052, Acc: 0.8196, Grad norm: 0.0818289217154868\n",
      "Iteration 5677, BCE loss: 57.75736912000812, Acc: 0.8196, Grad norm: 0.06684505718044478\n",
      "Iteration 5678, BCE loss: 57.75734359075051, Acc: 0.8196, Grad norm: 0.05625564612943015\n",
      "Iteration 5679, BCE loss: 57.757454819127005, Acc: 0.8196, Grad norm: 0.09221605962131157\n",
      "Iteration 5680, BCE loss: 57.75739674921404, Acc: 0.8196, Grad norm: 0.07589668749693274\n",
      "Iteration 5681, BCE loss: 57.75739279434178, Acc: 0.8196, Grad norm: 0.07377304899309618\n",
      "Iteration 5682, BCE loss: 57.75744718061804, Acc: 0.8196, Grad norm: 0.08949806810736612\n",
      "Iteration 5683, BCE loss: 57.75740163070506, Acc: 0.8196, Grad norm: 0.07443618748749016\n",
      "Iteration 5684, BCE loss: 57.75731188875972, Acc: 0.8196, Grad norm: 0.04184258194239866\n",
      "Iteration 5685, BCE loss: 57.75730675544896, Acc: 0.8196, Grad norm: 0.04049985248255231\n",
      "Iteration 5686, BCE loss: 57.757354171826286, Acc: 0.8196, Grad norm: 0.05956680599064073\n",
      "Iteration 5687, BCE loss: 57.75737698274034, Acc: 0.8196, Grad norm: 0.06883267846048356\n",
      "Iteration 5688, BCE loss: 57.75739309191357, Acc: 0.8196, Grad norm: 0.07440722113872048\n",
      "Iteration 5689, BCE loss: 57.75735600077233, Acc: 0.8196, Grad norm: 0.06151139666029016\n",
      "Iteration 5690, BCE loss: 57.757348979598646, Acc: 0.8195, Grad norm: 0.06041279547877506\n",
      "Iteration 5691, BCE loss: 57.757359675347956, Acc: 0.8195, Grad norm: 0.06384053623052051\n",
      "Iteration 5692, BCE loss: 57.75739325265541, Acc: 0.8195, Grad norm: 0.07311470179474687\n",
      "Iteration 5693, BCE loss: 57.75730183778626, Acc: 0.8196, Grad norm: 0.038921338962781576\n",
      "Iteration 5694, BCE loss: 57.757326773434656, Acc: 0.8196, Grad norm: 0.05006318740243292\n",
      "Iteration 5695, BCE loss: 57.757346595268864, Acc: 0.8196, Grad norm: 0.060218092310434644\n",
      "Iteration 5696, BCE loss: 57.75732360302364, Acc: 0.8196, Grad norm: 0.052454924667377105\n",
      "Iteration 5697, BCE loss: 57.75731897492431, Acc: 0.8195, Grad norm: 0.04800324819178119\n",
      "Iteration 5698, BCE loss: 57.75738354860052, Acc: 0.8195, Grad norm: 0.07051023280611399\n",
      "Iteration 5699, BCE loss: 57.75736710287443, Acc: 0.8196, Grad norm: 0.0659891988264727\n",
      "Iteration 5700, BCE loss: 57.75730685782639, Acc: 0.8196, Grad norm: 0.04245026419083854\n",
      "Iteration 5701, BCE loss: 57.75734721358831, Acc: 0.8196, Grad norm: 0.06101269417893962\n",
      "Iteration 5702, BCE loss: 57.75735027450017, Acc: 0.8196, Grad norm: 0.0642090591441381\n",
      "Iteration 5703, BCE loss: 57.75730421809938, Acc: 0.8196, Grad norm: 0.044969849619889576\n",
      "Iteration 5704, BCE loss: 57.75730858248629, Acc: 0.8196, Grad norm: 0.04347088898530156\n",
      "Iteration 5705, BCE loss: 57.75733814043648, Acc: 0.8196, Grad norm: 0.05938928915816401\n",
      "Iteration 5706, BCE loss: 57.75737594303634, Acc: 0.8196, Grad norm: 0.07144818038713245\n",
      "Iteration 5707, BCE loss: 57.757356545267996, Acc: 0.8196, Grad norm: 0.06351555440263928\n",
      "Iteration 5708, BCE loss: 57.757391064827814, Acc: 0.8196, Grad norm: 0.07639754789924427\n",
      "Iteration 5709, BCE loss: 57.75746173284523, Acc: 0.8196, Grad norm: 0.0934822153769612\n",
      "Iteration 5710, BCE loss: 57.75743046304282, Acc: 0.8196, Grad norm: 0.08588630534780671\n",
      "Iteration 5711, BCE loss: 57.75737447146595, Acc: 0.8196, Grad norm: 0.06753748201385454\n",
      "Iteration 5712, BCE loss: 57.7574301794281, Acc: 0.8196, Grad norm: 0.08441829649299652\n",
      "Iteration 5713, BCE loss: 57.757497832020576, Acc: 0.8196, Grad norm: 0.09963660330197605\n",
      "Iteration 5714, BCE loss: 57.75732724486279, Acc: 0.8196, Grad norm: 0.050334798402306195\n",
      "Iteration 5715, BCE loss: 57.75733355019662, Acc: 0.8195, Grad norm: 0.05120713385861025\n",
      "Iteration 5716, BCE loss: 57.757419545550604, Acc: 0.8195, Grad norm: 0.08102804269059442\n",
      "Iteration 5717, BCE loss: 57.757408593729124, Acc: 0.8195, Grad norm: 0.07982069505302909\n",
      "Iteration 5718, BCE loss: 57.75743718587664, Acc: 0.8195, Grad norm: 0.09009306185598867\n",
      "Iteration 5719, BCE loss: 57.757330270764164, Acc: 0.8195, Grad norm: 0.05682614558486328\n",
      "Iteration 5720, BCE loss: 57.75735803557389, Acc: 0.8195, Grad norm: 0.06815074800874157\n",
      "Iteration 5721, BCE loss: 57.75730357764482, Acc: 0.8195, Grad norm: 0.04304249995750369\n",
      "Iteration 5722, BCE loss: 57.7573035267786, Acc: 0.8195, Grad norm: 0.04300892072883276\n",
      "Iteration 5723, BCE loss: 57.75731774141059, Acc: 0.8195, Grad norm: 0.0449809297495386\n",
      "Iteration 5724, BCE loss: 57.757320951643734, Acc: 0.8195, Grad norm: 0.04616034515067978\n",
      "Iteration 5725, BCE loss: 57.75735714531716, Acc: 0.8195, Grad norm: 0.0594728485536239\n",
      "Iteration 5726, BCE loss: 57.75731709766207, Acc: 0.8195, Grad norm: 0.04603886820488032\n",
      "Iteration 5727, BCE loss: 57.75733662272275, Acc: 0.8195, Grad norm: 0.05369737966166192\n",
      "Iteration 5728, BCE loss: 57.757404453298996, Acc: 0.8195, Grad norm: 0.07659627703959676\n",
      "Iteration 5729, BCE loss: 57.757448476954, Acc: 0.8196, Grad norm: 0.08561843964610895\n",
      "Iteration 5730, BCE loss: 57.75751775769176, Acc: 0.8196, Grad norm: 0.10234601900580632\n",
      "Iteration 5731, BCE loss: 57.75755518902186, Acc: 0.8196, Grad norm: 0.10859223745919484\n",
      "Iteration 5732, BCE loss: 57.75765966804582, Acc: 0.8196, Grad norm: 0.1303734867164654\n",
      "Iteration 5733, BCE loss: 57.75761737928811, Acc: 0.8195, Grad norm: 0.12488475628492712\n",
      "Iteration 5734, BCE loss: 57.757594985617914, Acc: 0.8195, Grad norm: 0.121142864597696\n",
      "Iteration 5735, BCE loss: 57.75755552854507, Acc: 0.8195, Grad norm: 0.11543965175216087\n",
      "Iteration 5736, BCE loss: 57.757538128010076, Acc: 0.8195, Grad norm: 0.1101883880201008\n",
      "Iteration 5737, BCE loss: 57.75745214623545, Acc: 0.8195, Grad norm: 0.09123231872538963\n",
      "Iteration 5738, BCE loss: 57.75740167649471, Acc: 0.8195, Grad norm: 0.07686715291022105\n",
      "Iteration 5739, BCE loss: 57.75746106796568, Acc: 0.8195, Grad norm: 0.09284024418929698\n",
      "Iteration 5740, BCE loss: 57.75744223775625, Acc: 0.8195, Grad norm: 0.08851434990290134\n",
      "Iteration 5741, BCE loss: 57.757537889534944, Acc: 0.8195, Grad norm: 0.11086018106826526\n",
      "Iteration 5742, BCE loss: 57.75739753813605, Acc: 0.8195, Grad norm: 0.07386731146516362\n",
      "Iteration 5743, BCE loss: 57.75738381886735, Acc: 0.8195, Grad norm: 0.0723946505909692\n",
      "Iteration 5744, BCE loss: 57.757397022281346, Acc: 0.8196, Grad norm: 0.07609857582244728\n",
      "Iteration 5745, BCE loss: 57.75736637850609, Acc: 0.8196, Grad norm: 0.06669066795469866\n",
      "Iteration 5746, BCE loss: 57.7573739902939, Acc: 0.8196, Grad norm: 0.06910953199100793\n",
      "Iteration 5747, BCE loss: 57.757366092598474, Acc: 0.8196, Grad norm: 0.06644345413581466\n",
      "Iteration 5748, BCE loss: 57.75737514676521, Acc: 0.8195, Grad norm: 0.06763202417744231\n",
      "Iteration 5749, BCE loss: 57.75737239390021, Acc: 0.8196, Grad norm: 0.0671067505070153\n",
      "Iteration 5750, BCE loss: 57.757363833710784, Acc: 0.8195, Grad norm: 0.06297473290761972\n",
      "Iteration 5751, BCE loss: 57.757352819893214, Acc: 0.8195, Grad norm: 0.05901340808420603\n",
      "Iteration 5752, BCE loss: 57.75739004962599, Acc: 0.8195, Grad norm: 0.07271045843549871\n",
      "Iteration 5753, BCE loss: 57.75739098654485, Acc: 0.8196, Grad norm: 0.07189982491351449\n",
      "Iteration 5754, BCE loss: 57.75738165278012, Acc: 0.8196, Grad norm: 0.07193955578433403\n",
      "Iteration 5755, BCE loss: 57.75740009974071, Acc: 0.8196, Grad norm: 0.07777765739853262\n",
      "Iteration 5756, BCE loss: 57.75739898325301, Acc: 0.8196, Grad norm: 0.07695564297854902\n",
      "Iteration 5757, BCE loss: 57.75738630197682, Acc: 0.8195, Grad norm: 0.0711309006069753\n",
      "Iteration 5758, BCE loss: 57.75740013327464, Acc: 0.8196, Grad norm: 0.07854729945530052\n",
      "Iteration 5759, BCE loss: 57.75733574410445, Acc: 0.8195, Grad norm: 0.05551957112129386\n",
      "Iteration 5760, BCE loss: 57.75742475295991, Acc: 0.8195, Grad norm: 0.08440707512189359\n",
      "Iteration 5761, BCE loss: 57.757335119652524, Acc: 0.8195, Grad norm: 0.05615984552955848\n",
      "Iteration 5762, BCE loss: 57.75731436291546, Acc: 0.8195, Grad norm: 0.046188577771714395\n",
      "Iteration 5763, BCE loss: 57.757338935990106, Acc: 0.8196, Grad norm: 0.05914566956728472\n",
      "Iteration 5764, BCE loss: 57.75732604773385, Acc: 0.8196, Grad norm: 0.05311608811451907\n",
      "Iteration 5765, BCE loss: 57.75737155375558, Acc: 0.8196, Grad norm: 0.07074196193398205\n",
      "Iteration 5766, BCE loss: 57.75740313642483, Acc: 0.8196, Grad norm: 0.08066656868423241\n",
      "Iteration 5767, BCE loss: 57.75741235908572, Acc: 0.8196, Grad norm: 0.08127031560113313\n",
      "Iteration 5768, BCE loss: 57.757366408449464, Acc: 0.8196, Grad norm: 0.06638990177645385\n",
      "Iteration 5769, BCE loss: 57.757403506593874, Acc: 0.8196, Grad norm: 0.07808275770664384\n",
      "Iteration 5770, BCE loss: 57.75744135959056, Acc: 0.8196, Grad norm: 0.08802056298998269\n",
      "Iteration 5771, BCE loss: 57.757367473203594, Acc: 0.8196, Grad norm: 0.06585239350303822\n",
      "Iteration 5772, BCE loss: 57.75735247658224, Acc: 0.8196, Grad norm: 0.0601616120954687\n",
      "Iteration 5773, BCE loss: 57.75731592992697, Acc: 0.8195, Grad norm: 0.04462523712164224\n",
      "Iteration 5774, BCE loss: 57.75734364942525, Acc: 0.8196, Grad norm: 0.05541401663565013\n",
      "Iteration 5775, BCE loss: 57.75731960633279, Acc: 0.8196, Grad norm: 0.045187842859994894\n",
      "Iteration 5776, BCE loss: 57.75731622570265, Acc: 0.8196, Grad norm: 0.04296096885217096\n",
      "Iteration 5777, BCE loss: 57.75735044479546, Acc: 0.8196, Grad norm: 0.05620678129849509\n",
      "Iteration 5778, BCE loss: 57.75741926456575, Acc: 0.8195, Grad norm: 0.08034178155584885\n",
      "Iteration 5779, BCE loss: 57.75743634061762, Acc: 0.8195, Grad norm: 0.08324714365948162\n",
      "Iteration 5780, BCE loss: 57.75739999367696, Acc: 0.8195, Grad norm: 0.07667464497252782\n",
      "Iteration 5781, BCE loss: 57.75738210716493, Acc: 0.8195, Grad norm: 0.07122117334143434\n",
      "Iteration 5782, BCE loss: 57.75736031557247, Acc: 0.8196, Grad norm: 0.06232435408233342\n",
      "Iteration 5783, BCE loss: 57.75735389650058, Acc: 0.8196, Grad norm: 0.05831406666716534\n",
      "Iteration 5784, BCE loss: 57.7573793552593, Acc: 0.8196, Grad norm: 0.06565864041629617\n",
      "Iteration 5785, BCE loss: 57.75737265507466, Acc: 0.8196, Grad norm: 0.0670959532919553\n",
      "Iteration 5786, BCE loss: 57.75736957777458, Acc: 0.8196, Grad norm: 0.06547227905584876\n",
      "Iteration 5787, BCE loss: 57.757424725506866, Acc: 0.8196, Grad norm: 0.07971889131751815\n",
      "Iteration 5788, BCE loss: 57.7573653212083, Acc: 0.8196, Grad norm: 0.0620484638436856\n",
      "Iteration 5789, BCE loss: 57.757370460808914, Acc: 0.8196, Grad norm: 0.06488031232712081\n",
      "Iteration 5790, BCE loss: 57.75733526412614, Acc: 0.8196, Grad norm: 0.05244931090799846\n",
      "Iteration 5791, BCE loss: 57.75735980872706, Acc: 0.8196, Grad norm: 0.06180545834940298\n",
      "Iteration 5792, BCE loss: 57.75733307419215, Acc: 0.8196, Grad norm: 0.053466420485702264\n",
      "Iteration 5793, BCE loss: 57.75735939093592, Acc: 0.8196, Grad norm: 0.06620050886464261\n",
      "Iteration 5794, BCE loss: 57.757319855260484, Acc: 0.8196, Grad norm: 0.04905217489637253\n",
      "Iteration 5795, BCE loss: 57.75735268920084, Acc: 0.8196, Grad norm: 0.061622268269389595\n",
      "Iteration 5796, BCE loss: 57.75731609304062, Acc: 0.8196, Grad norm: 0.04717797050853547\n",
      "Iteration 5797, BCE loss: 57.757353735811265, Acc: 0.8196, Grad norm: 0.061562318995603174\n",
      "Iteration 5798, BCE loss: 57.75744527982641, Acc: 0.8196, Grad norm: 0.08729795975897572\n",
      "Iteration 5799, BCE loss: 57.75747926444018, Acc: 0.8196, Grad norm: 0.09479779547624963\n",
      "Iteration 5800, BCE loss: 57.75736771114718, Acc: 0.8196, Grad norm: 0.0663905934053737\n",
      "Iteration 5801, BCE loss: 57.75741312253318, Acc: 0.8196, Grad norm: 0.08025989327399183\n",
      "Iteration 5802, BCE loss: 57.757433216720116, Acc: 0.8196, Grad norm: 0.08558614594813363\n",
      "Iteration 5803, BCE loss: 57.75750335380879, Acc: 0.8195, Grad norm: 0.10187080421827815\n",
      "Iteration 5804, BCE loss: 57.75750571867425, Acc: 0.8195, Grad norm: 0.10185080075012817\n",
      "Iteration 5805, BCE loss: 57.757420701369256, Acc: 0.8195, Grad norm: 0.08238024226211879\n",
      "Iteration 5806, BCE loss: 57.75745159841048, Acc: 0.8195, Grad norm: 0.09048017091211041\n",
      "Iteration 5807, BCE loss: 57.75743751051277, Acc: 0.8196, Grad norm: 0.08651617177397294\n",
      "Iteration 5808, BCE loss: 57.757422536219835, Acc: 0.8196, Grad norm: 0.08340608532561293\n",
      "Iteration 5809, BCE loss: 57.757430769106975, Acc: 0.8196, Grad norm: 0.08466955918202047\n",
      "Iteration 5810, BCE loss: 57.75740510421602, Acc: 0.8196, Grad norm: 0.07697149038576519\n",
      "Iteration 5811, BCE loss: 57.75731438654062, Acc: 0.8196, Grad norm: 0.04562195885353116\n",
      "Iteration 5812, BCE loss: 57.75731552848211, Acc: 0.8196, Grad norm: 0.04397730672542287\n",
      "Iteration 5813, BCE loss: 57.75732395745639, Acc: 0.8196, Grad norm: 0.04744225727987353\n",
      "Iteration 5814, BCE loss: 57.75736787682678, Acc: 0.8196, Grad norm: 0.06259808125952067\n",
      "Iteration 5815, BCE loss: 57.75737375703758, Acc: 0.8196, Grad norm: 0.06426085329452848\n",
      "Iteration 5816, BCE loss: 57.75736514772258, Acc: 0.8196, Grad norm: 0.06037898525933001\n",
      "Iteration 5817, BCE loss: 57.757443173776345, Acc: 0.8196, Grad norm: 0.08530201492902298\n",
      "Iteration 5818, BCE loss: 57.757413322708516, Acc: 0.8196, Grad norm: 0.08063244326156625\n",
      "Iteration 5819, BCE loss: 57.75739897130309, Acc: 0.8195, Grad norm: 0.07625644358369252\n",
      "Iteration 5820, BCE loss: 57.75741970966048, Acc: 0.8195, Grad norm: 0.08369435155787194\n",
      "Iteration 5821, BCE loss: 57.75740453061543, Acc: 0.8195, Grad norm: 0.0809995008235628\n",
      "Iteration 5822, BCE loss: 57.75743921418798, Acc: 0.8195, Grad norm: 0.08803138493976964\n",
      "Iteration 5823, BCE loss: 57.75731595600578, Acc: 0.8195, Grad norm: 0.04597753852915089\n",
      "Iteration 5824, BCE loss: 57.757365464381756, Acc: 0.8195, Grad norm: 0.06717104195201061\n",
      "Iteration 5825, BCE loss: 57.757412045552584, Acc: 0.8195, Grad norm: 0.0822451957555641\n",
      "Iteration 5826, BCE loss: 57.75735353089967, Acc: 0.8195, Grad norm: 0.06306836118426218\n",
      "Iteration 5827, BCE loss: 57.75736107253228, Acc: 0.8195, Grad norm: 0.06617949270266996\n",
      "Iteration 5828, BCE loss: 57.75736530415845, Acc: 0.8195, Grad norm: 0.0696314502520855\n",
      "Iteration 5829, BCE loss: 57.75737190964704, Acc: 0.8195, Grad norm: 0.07005313319760212\n",
      "Iteration 5830, BCE loss: 57.75738004355091, Acc: 0.8195, Grad norm: 0.07100766184147249\n",
      "Iteration 5831, BCE loss: 57.75734815330761, Acc: 0.8195, Grad norm: 0.05805110928081796\n",
      "Iteration 5832, BCE loss: 57.75741127479376, Acc: 0.8195, Grad norm: 0.0793066915563627\n",
      "Iteration 5833, BCE loss: 57.75734964919235, Acc: 0.8195, Grad norm: 0.059769572218627116\n",
      "Iteration 5834, BCE loss: 57.75741352986481, Acc: 0.8195, Grad norm: 0.07782845043747999\n",
      "Iteration 5835, BCE loss: 57.75736276236438, Acc: 0.8195, Grad norm: 0.0611627096208756\n",
      "Iteration 5836, BCE loss: 57.75735495649903, Acc: 0.8195, Grad norm: 0.05755764586621215\n",
      "Iteration 5837, BCE loss: 57.757333047677584, Acc: 0.8195, Grad norm: 0.05019818717374201\n",
      "Iteration 5838, BCE loss: 57.75730614352457, Acc: 0.8195, Grad norm: 0.04013673604391333\n",
      "Iteration 5839, BCE loss: 57.75732103382586, Acc: 0.8195, Grad norm: 0.0461951703695722\n",
      "Iteration 5840, BCE loss: 57.757286606937626, Acc: 0.8195, Grad norm: 0.032628481399949304\n",
      "Iteration 5841, BCE loss: 57.75729898674956, Acc: 0.8195, Grad norm: 0.03891425535698593\n",
      "Iteration 5842, BCE loss: 57.75730909963292, Acc: 0.8195, Grad norm: 0.04426216363366004\n",
      "Iteration 5843, BCE loss: 57.75729590689765, Acc: 0.8195, Grad norm: 0.03486895152845328\n",
      "Iteration 5844, BCE loss: 57.757321603946224, Acc: 0.8195, Grad norm: 0.04824783345744047\n",
      "Iteration 5845, BCE loss: 57.75736792181852, Acc: 0.8195, Grad norm: 0.06620220337478107\n",
      "Iteration 5846, BCE loss: 57.75739305954161, Acc: 0.8195, Grad norm: 0.07486234261934475\n",
      "Iteration 5847, BCE loss: 57.757323094092584, Acc: 0.8195, Grad norm: 0.04998385545836796\n",
      "Iteration 5848, BCE loss: 57.757330618458596, Acc: 0.8195, Grad norm: 0.050614019664264216\n",
      "Iteration 5849, BCE loss: 57.757315993627486, Acc: 0.8195, Grad norm: 0.043887555811692916\n",
      "Iteration 5850, BCE loss: 57.757299488183264, Acc: 0.8195, Grad norm: 0.037140184256270845\n",
      "Iteration 5851, BCE loss: 57.75734836601372, Acc: 0.8195, Grad norm: 0.05726224259280872\n",
      "Iteration 5852, BCE loss: 57.75737493472201, Acc: 0.8195, Grad norm: 0.06495796113779193\n",
      "Iteration 5853, BCE loss: 57.7574310344171, Acc: 0.8195, Grad norm: 0.08009159347378261\n",
      "Iteration 5854, BCE loss: 57.75740352550113, Acc: 0.8195, Grad norm: 0.0701044209196855\n",
      "Iteration 5855, BCE loss: 57.75736666293963, Acc: 0.8195, Grad norm: 0.0621712730083595\n",
      "Iteration 5856, BCE loss: 57.757337551038795, Acc: 0.8195, Grad norm: 0.05279403645060065\n",
      "Iteration 5857, BCE loss: 57.75738236335072, Acc: 0.8195, Grad norm: 0.0661287848847717\n",
      "Iteration 5858, BCE loss: 57.75738266263886, Acc: 0.8195, Grad norm: 0.06674882200884363\n",
      "Iteration 5859, BCE loss: 57.75744012805448, Acc: 0.8195, Grad norm: 0.0843896085248939\n",
      "Iteration 5860, BCE loss: 57.75741416721917, Acc: 0.8195, Grad norm: 0.07727804161249882\n",
      "Iteration 5861, BCE loss: 57.757486886803775, Acc: 0.8195, Grad norm: 0.0948913012225978\n",
      "Iteration 5862, BCE loss: 57.75748790849276, Acc: 0.8195, Grad norm: 0.09584426462110356\n",
      "Iteration 5863, BCE loss: 57.757399629790214, Acc: 0.8195, Grad norm: 0.07500869471208428\n",
      "Iteration 5864, BCE loss: 57.75734841319121, Acc: 0.8195, Grad norm: 0.059008076895750126\n",
      "Iteration 5865, BCE loss: 57.75737918725206, Acc: 0.8195, Grad norm: 0.07108272118001907\n",
      "Iteration 5866, BCE loss: 57.75737051694102, Acc: 0.8195, Grad norm: 0.06547436892723167\n",
      "Iteration 5867, BCE loss: 57.75736249203082, Acc: 0.8195, Grad norm: 0.06192970418789859\n",
      "Iteration 5868, BCE loss: 57.757375026068246, Acc: 0.8195, Grad norm: 0.06810487477628693\n",
      "Iteration 5869, BCE loss: 57.757342114378545, Acc: 0.8196, Grad norm: 0.0556096152734776\n",
      "Iteration 5870, BCE loss: 57.75736662112247, Acc: 0.8196, Grad norm: 0.06341506761699905\n",
      "Iteration 5871, BCE loss: 57.757337256331496, Acc: 0.8195, Grad norm: 0.051621481503979584\n",
      "Iteration 5872, BCE loss: 57.75735323981555, Acc: 0.8195, Grad norm: 0.05730008808274394\n",
      "Iteration 5873, BCE loss: 57.757384913893645, Acc: 0.8195, Grad norm: 0.06922199045981803\n",
      "Iteration 5874, BCE loss: 57.757420490905176, Acc: 0.8196, Grad norm: 0.08171806436744461\n",
      "Iteration 5875, BCE loss: 57.757488995555704, Acc: 0.8196, Grad norm: 0.10118579732981707\n",
      "Iteration 5876, BCE loss: 57.75743576751517, Acc: 0.8195, Grad norm: 0.0868227165864811\n",
      "Iteration 5877, BCE loss: 57.757431201564714, Acc: 0.8195, Grad norm: 0.0856222927630923\n",
      "Iteration 5878, BCE loss: 57.75748481597118, Acc: 0.8195, Grad norm: 0.10003704691553433\n",
      "Iteration 5879, BCE loss: 57.75747811671509, Acc: 0.8195, Grad norm: 0.09778911169825182\n",
      "Iteration 5880, BCE loss: 57.757423332278506, Acc: 0.8195, Grad norm: 0.08137941427570418\n",
      "Iteration 5881, BCE loss: 57.75740286967175, Acc: 0.8195, Grad norm: 0.07572596731156309\n",
      "Iteration 5882, BCE loss: 57.75744192424373, Acc: 0.8195, Grad norm: 0.08824618494457584\n",
      "Iteration 5883, BCE loss: 57.75743344639886, Acc: 0.8195, Grad norm: 0.0873255520131123\n",
      "Iteration 5884, BCE loss: 57.7574399607568, Acc: 0.8195, Grad norm: 0.08950145689529268\n",
      "Iteration 5885, BCE loss: 57.75747840773979, Acc: 0.8195, Grad norm: 0.09819060774311128\n",
      "Iteration 5886, BCE loss: 57.75739752505149, Acc: 0.8195, Grad norm: 0.07445971642466338\n",
      "Iteration 5887, BCE loss: 57.75743377438238, Acc: 0.8196, Grad norm: 0.08347280247655019\n",
      "Iteration 5888, BCE loss: 57.757393618188786, Acc: 0.8196, Grad norm: 0.07470964006475617\n",
      "Iteration 5889, BCE loss: 57.7574047407889, Acc: 0.8196, Grad norm: 0.07450147553003118\n",
      "Iteration 5890, BCE loss: 57.757429836254474, Acc: 0.8196, Grad norm: 0.08230572604105696\n",
      "Iteration 5891, BCE loss: 57.75750590559511, Acc: 0.8196, Grad norm: 0.09989834554438622\n",
      "Iteration 5892, BCE loss: 57.75749042409133, Acc: 0.8196, Grad norm: 0.09606343224715383\n",
      "Iteration 5893, BCE loss: 57.7574959862755, Acc: 0.8196, Grad norm: 0.09477659346167865\n",
      "Iteration 5894, BCE loss: 57.75747326475558, Acc: 0.8196, Grad norm: 0.09306983084781449\n",
      "Iteration 5895, BCE loss: 57.75745818629514, Acc: 0.8196, Grad norm: 0.08983385000336037\n",
      "Iteration 5896, BCE loss: 57.75748058054155, Acc: 0.8196, Grad norm: 0.09635781451566791\n",
      "Iteration 5897, BCE loss: 57.757416125880866, Acc: 0.8196, Grad norm: 0.07832125610950037\n",
      "Iteration 5898, BCE loss: 57.75742547630052, Acc: 0.8196, Grad norm: 0.08018130073072034\n",
      "Iteration 5899, BCE loss: 57.757429958683026, Acc: 0.8196, Grad norm: 0.07897706675072749\n",
      "Iteration 5900, BCE loss: 57.757458563480924, Acc: 0.8196, Grad norm: 0.08653036989657166\n",
      "Iteration 5901, BCE loss: 57.75742866674613, Acc: 0.8196, Grad norm: 0.07992279712304684\n",
      "Iteration 5902, BCE loss: 57.75739690441675, Acc: 0.8196, Grad norm: 0.06939656034068385\n",
      "Iteration 5903, BCE loss: 57.757436493500684, Acc: 0.8196, Grad norm: 0.07841717763837654\n",
      "Iteration 5904, BCE loss: 57.75742009447233, Acc: 0.8196, Grad norm: 0.07644195476121061\n",
      "Iteration 5905, BCE loss: 57.7573413948075, Acc: 0.8196, Grad norm: 0.056296531962423436\n",
      "Iteration 5906, BCE loss: 57.75733849389606, Acc: 0.8196, Grad norm: 0.05502681878628745\n",
      "Iteration 5907, BCE loss: 57.757371383837786, Acc: 0.8196, Grad norm: 0.06613245429669165\n",
      "Iteration 5908, BCE loss: 57.75733947357292, Acc: 0.8196, Grad norm: 0.054919469183563524\n",
      "Iteration 5909, BCE loss: 57.75733612056148, Acc: 0.8196, Grad norm: 0.051413152170104115\n",
      "Iteration 5910, BCE loss: 57.75733947152446, Acc: 0.8196, Grad norm: 0.05482110914976404\n",
      "Iteration 5911, BCE loss: 57.75734609954229, Acc: 0.8196, Grad norm: 0.05946847645215275\n",
      "Iteration 5912, BCE loss: 57.75733335489261, Acc: 0.8196, Grad norm: 0.05339858241155092\n",
      "Iteration 5913, BCE loss: 57.757342598710295, Acc: 0.8196, Grad norm: 0.055693297099035095\n",
      "Iteration 5914, BCE loss: 57.75737269701547, Acc: 0.8195, Grad norm: 0.06993241625533302\n",
      "Iteration 5915, BCE loss: 57.757333585675056, Acc: 0.8195, Grad norm: 0.05585466240920438\n",
      "Iteration 5916, BCE loss: 57.757332791658115, Acc: 0.8195, Grad norm: 0.055810257606927656\n",
      "Iteration 5917, BCE loss: 57.75732625864025, Acc: 0.8195, Grad norm: 0.051085724601760696\n",
      "Iteration 5918, BCE loss: 57.757338678019494, Acc: 0.8196, Grad norm: 0.05463281578935302\n",
      "Iteration 5919, BCE loss: 57.757369793703646, Acc: 0.8196, Grad norm: 0.06748594416782187\n",
      "Iteration 5920, BCE loss: 57.75738183143517, Acc: 0.8196, Grad norm: 0.07173002729796914\n",
      "Iteration 5921, BCE loss: 57.75741144950023, Acc: 0.8196, Grad norm: 0.08158268742734329\n",
      "Iteration 5922, BCE loss: 57.757323823561805, Acc: 0.8196, Grad norm: 0.052015914182606995\n",
      "Iteration 5923, BCE loss: 57.75730606942383, Acc: 0.8196, Grad norm: 0.04092971916074348\n",
      "Iteration 5924, BCE loss: 57.75731366444237, Acc: 0.8196, Grad norm: 0.046123965710614244\n",
      "Iteration 5925, BCE loss: 57.757309304631725, Acc: 0.8196, Grad norm: 0.047052557194459235\n",
      "Iteration 5926, BCE loss: 57.75730780824239, Acc: 0.8196, Grad norm: 0.045235373856070286\n",
      "Iteration 5927, BCE loss: 57.757293105939205, Acc: 0.8196, Grad norm: 0.0354558145270962\n",
      "Iteration 5928, BCE loss: 57.75731416286056, Acc: 0.8196, Grad norm: 0.04473801110508294\n",
      "Iteration 5929, BCE loss: 57.757350110241504, Acc: 0.8196, Grad norm: 0.05805748706513209\n",
      "Iteration 5930, BCE loss: 57.75733673792423, Acc: 0.8195, Grad norm: 0.05528867441480863\n",
      "Iteration 5931, BCE loss: 57.75730491466955, Acc: 0.8195, Grad norm: 0.04241246295862478\n",
      "Iteration 5932, BCE loss: 57.75734378561349, Acc: 0.8195, Grad norm: 0.05912180210347604\n",
      "Iteration 5933, BCE loss: 57.75733968722917, Acc: 0.8195, Grad norm: 0.05601558165187549\n",
      "Iteration 5934, BCE loss: 57.75733304787086, Acc: 0.8195, Grad norm: 0.05287797825159155\n",
      "Iteration 5935, BCE loss: 57.75738261949722, Acc: 0.8195, Grad norm: 0.07167367864355162\n",
      "Iteration 5936, BCE loss: 57.75733404642099, Acc: 0.8195, Grad norm: 0.05488664185001129\n",
      "Iteration 5937, BCE loss: 57.757327848054956, Acc: 0.8195, Grad norm: 0.05310720528785427\n",
      "Iteration 5938, BCE loss: 57.75732654803969, Acc: 0.8195, Grad norm: 0.05263966604620198\n",
      "Iteration 5939, BCE loss: 57.75738893611346, Acc: 0.8195, Grad norm: 0.07113062727546605\n",
      "Iteration 5940, BCE loss: 57.757342492757644, Acc: 0.8195, Grad norm: 0.05574893483642974\n",
      "Iteration 5941, BCE loss: 57.7573292190532, Acc: 0.8195, Grad norm: 0.047627761900926886\n",
      "Iteration 5942, BCE loss: 57.757316426912254, Acc: 0.8195, Grad norm: 0.04525001599845912\n",
      "Iteration 5943, BCE loss: 57.75734681250266, Acc: 0.8195, Grad norm: 0.060111700795540284\n",
      "Iteration 5944, BCE loss: 57.75743081665161, Acc: 0.8195, Grad norm: 0.08556621756148573\n",
      "Iteration 5945, BCE loss: 57.757310592237374, Acc: 0.8195, Grad norm: 0.04345716359634677\n",
      "Iteration 5946, BCE loss: 57.75732042060952, Acc: 0.8195, Grad norm: 0.0496657960475027\n",
      "Iteration 5947, BCE loss: 57.757300891096875, Acc: 0.8195, Grad norm: 0.04062100634724668\n",
      "Iteration 5948, BCE loss: 57.75733282161495, Acc: 0.8195, Grad norm: 0.0576995324544725\n",
      "Iteration 5949, BCE loss: 57.75734902303762, Acc: 0.8196, Grad norm: 0.06369297235863325\n",
      "Iteration 5950, BCE loss: 57.757323920871, Acc: 0.8195, Grad norm: 0.05196119226079609\n",
      "Iteration 5951, BCE loss: 57.75729793486977, Acc: 0.8196, Grad norm: 0.04042668773362656\n",
      "Iteration 5952, BCE loss: 57.75734616012856, Acc: 0.8196, Grad norm: 0.06189524367699697\n",
      "Iteration 5953, BCE loss: 57.7573466681527, Acc: 0.8196, Grad norm: 0.0628148414089328\n",
      "Iteration 5954, BCE loss: 57.757310631084955, Acc: 0.8196, Grad norm: 0.048135121530774684\n",
      "Iteration 5955, BCE loss: 57.7572800029574, Acc: 0.8196, Grad norm: 0.030197398126759537\n",
      "Iteration 5956, BCE loss: 57.757274876994465, Acc: 0.8196, Grad norm: 0.0256748997479937\n",
      "Iteration 5957, BCE loss: 57.75729170914564, Acc: 0.8195, Grad norm: 0.038717747849496956\n",
      "Iteration 5958, BCE loss: 57.75731429566217, Acc: 0.8195, Grad norm: 0.04787218399573334\n",
      "Iteration 5959, BCE loss: 57.757355781584664, Acc: 0.8196, Grad norm: 0.061464861547852016\n",
      "Iteration 5960, BCE loss: 57.757353011136445, Acc: 0.8196, Grad norm: 0.06163453408938282\n",
      "Iteration 5961, BCE loss: 57.75739979576552, Acc: 0.8196, Grad norm: 0.07362063679996915\n",
      "Iteration 5962, BCE loss: 57.757454177581245, Acc: 0.8196, Grad norm: 0.08638167376559323\n",
      "Iteration 5963, BCE loss: 57.7574121837692, Acc: 0.8196, Grad norm: 0.0765519341797772\n",
      "Iteration 5964, BCE loss: 57.75739269886466, Acc: 0.8196, Grad norm: 0.06944465501619425\n",
      "Iteration 5965, BCE loss: 57.757355759573684, Acc: 0.8196, Grad norm: 0.05784949172306522\n",
      "Iteration 5966, BCE loss: 57.75736866796166, Acc: 0.8196, Grad norm: 0.06433388536154877\n",
      "Iteration 5967, BCE loss: 57.757433444214215, Acc: 0.8196, Grad norm: 0.0838278287928333\n",
      "Iteration 5968, BCE loss: 57.757450480135034, Acc: 0.8196, Grad norm: 0.08738132114469842\n",
      "Iteration 5969, BCE loss: 57.75745789209432, Acc: 0.8196, Grad norm: 0.08986306822690433\n",
      "Iteration 5970, BCE loss: 57.757395127861216, Acc: 0.8196, Grad norm: 0.07219848738359703\n",
      "Iteration 5971, BCE loss: 57.75739001619019, Acc: 0.8196, Grad norm: 0.06976363679933728\n",
      "Iteration 5972, BCE loss: 57.757435174670896, Acc: 0.8196, Grad norm: 0.08080480894503506\n",
      "Iteration 5973, BCE loss: 57.75738659809214, Acc: 0.8196, Grad norm: 0.06580140698226777\n",
      "Iteration 5974, BCE loss: 57.75738099216976, Acc: 0.8196, Grad norm: 0.06404326522167438\n",
      "Iteration 5975, BCE loss: 57.75740336025211, Acc: 0.8196, Grad norm: 0.07190982564829394\n",
      "Iteration 5976, BCE loss: 57.757445799033874, Acc: 0.8196, Grad norm: 0.08424043046601294\n",
      "Iteration 5977, BCE loss: 57.75742260943717, Acc: 0.8196, Grad norm: 0.07884794196118691\n",
      "Iteration 5978, BCE loss: 57.757410654527206, Acc: 0.8196, Grad norm: 0.07592729107168227\n",
      "Iteration 5979, BCE loss: 57.757381982832904, Acc: 0.8196, Grad norm: 0.06990896507344255\n",
      "Iteration 5980, BCE loss: 57.757368423093055, Acc: 0.8196, Grad norm: 0.06553560930228892\n",
      "Iteration 5981, BCE loss: 57.75739247642625, Acc: 0.8196, Grad norm: 0.07340539944607115\n",
      "Iteration 5982, BCE loss: 57.7573930315664, Acc: 0.8196, Grad norm: 0.0722551109857297\n",
      "Iteration 5983, BCE loss: 57.75748808601307, Acc: 0.8196, Grad norm: 0.09951844627289064\n",
      "Iteration 5984, BCE loss: 57.757403966651594, Acc: 0.8196, Grad norm: 0.07954927058205888\n",
      "Iteration 5985, BCE loss: 57.757393341878895, Acc: 0.8196, Grad norm: 0.07415981504715319\n",
      "Iteration 5986, BCE loss: 57.75740629079053, Acc: 0.8196, Grad norm: 0.07867150883595271\n",
      "Iteration 5987, BCE loss: 57.7573343526245, Acc: 0.8196, Grad norm: 0.05479036008347655\n",
      "Iteration 5988, BCE loss: 57.75739696495505, Acc: 0.8196, Grad norm: 0.07706279982358308\n",
      "Iteration 5989, BCE loss: 57.75746443211811, Acc: 0.8196, Grad norm: 0.09449491362530353\n",
      "Iteration 5990, BCE loss: 57.75741970951002, Acc: 0.8196, Grad norm: 0.08401640766041418\n",
      "Iteration 5991, BCE loss: 57.75737281304988, Acc: 0.8196, Grad norm: 0.07244601932658803\n",
      "Iteration 5992, BCE loss: 57.7574172841332, Acc: 0.8196, Grad norm: 0.0827920875487053\n",
      "Iteration 5993, BCE loss: 57.757417848024645, Acc: 0.8196, Grad norm: 0.08250073856740775\n",
      "Iteration 5994, BCE loss: 57.757387303565075, Acc: 0.8196, Grad norm: 0.07262153989444319\n",
      "Iteration 5995, BCE loss: 57.75741294630164, Acc: 0.8196, Grad norm: 0.08174490111303098\n",
      "Iteration 5996, BCE loss: 57.757465859692054, Acc: 0.8196, Grad norm: 0.0955844936172061\n",
      "Iteration 5997, BCE loss: 57.757380064754976, Acc: 0.8196, Grad norm: 0.07003293315565037\n",
      "Iteration 5998, BCE loss: 57.75739253056885, Acc: 0.8196, Grad norm: 0.06970001813576683\n",
      "Iteration 5999, BCE loss: 57.757358107226004, Acc: 0.8196, Grad norm: 0.06208764436831514\n",
      "Iteration 6000, BCE loss: 57.75731473290727, Acc: 0.8196, Grad norm: 0.04226144891938895\n",
      "Iteration 6001, BCE loss: 57.7573270115706, Acc: 0.8196, Grad norm: 0.0504095408439424\n",
      "Iteration 6002, BCE loss: 57.757402808386374, Acc: 0.8196, Grad norm: 0.07757857409361034\n",
      "Iteration 6003, BCE loss: 57.757469006430256, Acc: 0.8196, Grad norm: 0.09464803668743343\n",
      "Iteration 6004, BCE loss: 57.75743828519073, Acc: 0.8196, Grad norm: 0.08836257062271326\n",
      "Iteration 6005, BCE loss: 57.75742950689043, Acc: 0.8196, Grad norm: 0.08690165727860574\n",
      "Iteration 6006, BCE loss: 57.75746677463144, Acc: 0.8196, Grad norm: 0.09508859684751492\n",
      "Iteration 6007, BCE loss: 57.75741495550463, Acc: 0.8196, Grad norm: 0.08135856983475243\n",
      "Iteration 6008, BCE loss: 57.757398362004714, Acc: 0.8196, Grad norm: 0.07794953925029544\n",
      "Iteration 6009, BCE loss: 57.757358161202724, Acc: 0.8196, Grad norm: 0.06526938730622348\n",
      "Iteration 6010, BCE loss: 57.75729197724125, Acc: 0.8196, Grad norm: 0.035243986024705826\n",
      "Iteration 6011, BCE loss: 57.75729512024418, Acc: 0.8196, Grad norm: 0.03851730491883559\n",
      "Iteration 6012, BCE loss: 57.75730131050813, Acc: 0.8196, Grad norm: 0.040852398646414954\n",
      "Iteration 6013, BCE loss: 57.7573477176811, Acc: 0.8196, Grad norm: 0.06129344520423154\n",
      "Iteration 6014, BCE loss: 57.75736985413233, Acc: 0.8195, Grad norm: 0.07054956236142704\n",
      "Iteration 6015, BCE loss: 57.757354030717906, Acc: 0.8195, Grad norm: 0.06487201679066028\n",
      "Iteration 6016, BCE loss: 57.75738026699102, Acc: 0.8195, Grad norm: 0.07511188443291315\n",
      "Iteration 6017, BCE loss: 57.757328976038565, Acc: 0.8195, Grad norm: 0.05537511316574751\n",
      "Iteration 6018, BCE loss: 57.7573571932612, Acc: 0.8195, Grad norm: 0.06428274269046194\n",
      "Iteration 6019, BCE loss: 57.75736012071114, Acc: 0.8195, Grad norm: 0.06142826528944611\n",
      "Iteration 6020, BCE loss: 57.75738531451461, Acc: 0.8195, Grad norm: 0.06846246667968399\n",
      "Iteration 6021, BCE loss: 57.75737360626681, Acc: 0.8195, Grad norm: 0.06589766107774594\n",
      "Iteration 6022, BCE loss: 57.757438634837655, Acc: 0.8195, Grad norm: 0.08263331763296924\n",
      "Iteration 6023, BCE loss: 57.757490493561136, Acc: 0.8195, Grad norm: 0.09759691852048621\n",
      "Iteration 6024, BCE loss: 57.75742840109816, Acc: 0.8196, Grad norm: 0.08146371326599317\n",
      "Iteration 6025, BCE loss: 57.75739840716838, Acc: 0.8196, Grad norm: 0.07215682487983127\n",
      "Iteration 6026, BCE loss: 57.75745171869108, Acc: 0.8195, Grad norm: 0.08798555489351345\n",
      "Iteration 6027, BCE loss: 57.757497145448184, Acc: 0.8196, Grad norm: 0.09873371415198925\n",
      "Iteration 6028, BCE loss: 57.75747455944389, Acc: 0.8196, Grad norm: 0.08980409636217007\n",
      "Iteration 6029, BCE loss: 57.757535351491825, Acc: 0.8196, Grad norm: 0.10061351002184078\n",
      "Iteration 6030, BCE loss: 57.75745384169951, Acc: 0.8196, Grad norm: 0.082396362010861\n",
      "Iteration 6031, BCE loss: 57.757463195795594, Acc: 0.8195, Grad norm: 0.08359589881771812\n",
      "Iteration 6032, BCE loss: 57.75739886711326, Acc: 0.8196, Grad norm: 0.07107748118433545\n",
      "Iteration 6033, BCE loss: 57.75742918233762, Acc: 0.8196, Grad norm: 0.0857852505934387\n",
      "Iteration 6034, BCE loss: 57.75744658834799, Acc: 0.8196, Grad norm: 0.09104670561024124\n",
      "Iteration 6035, BCE loss: 57.75739828710921, Acc: 0.8196, Grad norm: 0.07842945692354954\n",
      "Iteration 6036, BCE loss: 57.757467673399205, Acc: 0.8196, Grad norm: 0.09711902001161778\n",
      "Iteration 6037, BCE loss: 57.75739965012196, Acc: 0.8196, Grad norm: 0.07903816478680104\n",
      "Iteration 6038, BCE loss: 57.757413693498776, Acc: 0.8196, Grad norm: 0.08013691354778858\n",
      "Iteration 6039, BCE loss: 57.757411605497964, Acc: 0.8196, Grad norm: 0.08079104399015352\n",
      "Iteration 6040, BCE loss: 57.75745760565532, Acc: 0.8196, Grad norm: 0.09211889232591325\n",
      "Iteration 6041, BCE loss: 57.75750754585239, Acc: 0.8196, Grad norm: 0.10446768488995525\n",
      "Iteration 6042, BCE loss: 57.75749477538746, Acc: 0.8196, Grad norm: 0.10282595755184856\n",
      "Iteration 6043, BCE loss: 57.75749742413042, Acc: 0.8196, Grad norm: 0.10166522270601346\n",
      "Iteration 6044, BCE loss: 57.75742268659356, Acc: 0.8196, Grad norm: 0.07840185909286501\n",
      "Iteration 6045, BCE loss: 57.75738443309116, Acc: 0.8196, Grad norm: 0.06984035039551378\n",
      "Iteration 6046, BCE loss: 57.75743570303767, Acc: 0.8196, Grad norm: 0.08490609877033739\n",
      "Iteration 6047, BCE loss: 57.75753906953429, Acc: 0.8196, Grad norm: 0.11255939093500786\n",
      "Iteration 6048, BCE loss: 57.75747443919304, Acc: 0.8196, Grad norm: 0.09627146560998873\n",
      "Iteration 6049, BCE loss: 57.757423464070236, Acc: 0.8196, Grad norm: 0.08305400383770284\n",
      "Iteration 6050, BCE loss: 57.757397461831836, Acc: 0.8196, Grad norm: 0.07547597345190785\n",
      "Iteration 6051, BCE loss: 57.757341324997334, Acc: 0.8196, Grad norm: 0.05744421011471959\n",
      "Iteration 6052, BCE loss: 57.757347526605486, Acc: 0.8195, Grad norm: 0.056148523107088406\n",
      "Iteration 6053, BCE loss: 57.757355692643515, Acc: 0.8195, Grad norm: 0.05926242174641774\n",
      "Iteration 6054, BCE loss: 57.75734679091431, Acc: 0.8196, Grad norm: 0.053139019490975055\n",
      "Iteration 6055, BCE loss: 57.75736693503572, Acc: 0.8195, Grad norm: 0.06052413973929902\n",
      "Iteration 6056, BCE loss: 57.757318535624606, Acc: 0.8196, Grad norm: 0.04185858491576899\n",
      "Iteration 6057, BCE loss: 57.7573357617319, Acc: 0.8196, Grad norm: 0.04990165772971575\n",
      "Iteration 6058, BCE loss: 57.75734583055778, Acc: 0.8196, Grad norm: 0.054460460436072194\n",
      "Iteration 6059, BCE loss: 57.75738672106337, Acc: 0.8196, Grad norm: 0.06912857868262873\n",
      "Iteration 6060, BCE loss: 57.757353358613734, Acc: 0.8196, Grad norm: 0.057683462863774286\n",
      "Iteration 6061, BCE loss: 57.75735005956262, Acc: 0.8196, Grad norm: 0.05713092170945448\n",
      "Iteration 6062, BCE loss: 57.75733347517606, Acc: 0.8195, Grad norm: 0.048699188173138695\n",
      "Iteration 6063, BCE loss: 57.757337060799856, Acc: 0.8195, Grad norm: 0.05088796108354709\n",
      "Iteration 6064, BCE loss: 57.75734002066721, Acc: 0.8196, Grad norm: 0.054381672748649496\n",
      "Iteration 6065, BCE loss: 57.75739521833906, Acc: 0.8196, Grad norm: 0.07248606589912537\n",
      "Iteration 6066, BCE loss: 57.75744405299041, Acc: 0.8196, Grad norm: 0.08969321107140772\n",
      "Iteration 6067, BCE loss: 57.75744490762598, Acc: 0.8196, Grad norm: 0.08896239843276542\n",
      "Iteration 6068, BCE loss: 57.75738864636148, Acc: 0.8196, Grad norm: 0.072067370352594\n",
      "Iteration 6069, BCE loss: 57.757374537301, Acc: 0.8196, Grad norm: 0.06551012499908056\n",
      "Iteration 6070, BCE loss: 57.757394859597454, Acc: 0.8196, Grad norm: 0.0712474537343669\n",
      "Iteration 6071, BCE loss: 57.75748272536612, Acc: 0.8196, Grad norm: 0.09520506355517117\n",
      "Iteration 6072, BCE loss: 57.7573763989242, Acc: 0.8196, Grad norm: 0.06691777452031941\n",
      "Iteration 6073, BCE loss: 57.75740529331661, Acc: 0.8196, Grad norm: 0.07626821142599148\n",
      "Iteration 6074, BCE loss: 57.757383572447694, Acc: 0.8196, Grad norm: 0.0693662181316702\n",
      "Iteration 6075, BCE loss: 57.75737760004756, Acc: 0.8196, Grad norm: 0.07150382216719245\n",
      "Iteration 6076, BCE loss: 57.75737845650084, Acc: 0.8196, Grad norm: 0.07149584265967864\n",
      "Iteration 6077, BCE loss: 57.75737534852946, Acc: 0.8195, Grad norm: 0.06868386546044043\n",
      "Iteration 6078, BCE loss: 57.757374505586725, Acc: 0.8195, Grad norm: 0.0661143661927074\n",
      "Iteration 6079, BCE loss: 57.75739312627249, Acc: 0.8196, Grad norm: 0.07661476883807766\n",
      "Iteration 6080, BCE loss: 57.757391294188075, Acc: 0.8196, Grad norm: 0.07371768031624451\n",
      "Iteration 6081, BCE loss: 57.757405662664965, Acc: 0.8196, Grad norm: 0.07621548395117686\n",
      "Iteration 6082, BCE loss: 57.75742234270024, Acc: 0.8196, Grad norm: 0.08373918930322978\n",
      "Iteration 6083, BCE loss: 57.757428950983154, Acc: 0.8196, Grad norm: 0.08531148124560557\n",
      "Iteration 6084, BCE loss: 57.75754441923278, Acc: 0.8196, Grad norm: 0.11306465619524729\n",
      "Iteration 6085, BCE loss: 57.757430703549794, Acc: 0.8196, Grad norm: 0.08529005937690028\n",
      "Iteration 6086, BCE loss: 57.757503032572075, Acc: 0.8196, Grad norm: 0.10259560998660155\n",
      "Iteration 6087, BCE loss: 57.75743335028317, Acc: 0.8196, Grad norm: 0.08671675802166959\n",
      "Iteration 6088, BCE loss: 57.757378150272416, Acc: 0.8195, Grad norm: 0.06977958605572931\n",
      "Iteration 6089, BCE loss: 57.75736895215316, Acc: 0.8196, Grad norm: 0.06876721939415524\n",
      "Iteration 6090, BCE loss: 57.75736540451719, Acc: 0.8195, Grad norm: 0.0648311214695311\n",
      "Iteration 6091, BCE loss: 57.75737927905203, Acc: 0.8195, Grad norm: 0.07042531795694507\n",
      "Iteration 6092, BCE loss: 57.7574317429943, Acc: 0.8195, Grad norm: 0.0854243875370988\n",
      "Iteration 6093, BCE loss: 57.757426101128075, Acc: 0.8195, Grad norm: 0.08172113082112939\n",
      "Iteration 6094, BCE loss: 57.757418294804864, Acc: 0.8195, Grad norm: 0.08225294258773554\n",
      "Iteration 6095, BCE loss: 57.75743473648745, Acc: 0.8195, Grad norm: 0.0857641231748925\n",
      "Iteration 6096, BCE loss: 57.75749457564551, Acc: 0.8196, Grad norm: 0.10215580138002313\n",
      "Iteration 6097, BCE loss: 57.75746440444436, Acc: 0.8195, Grad norm: 0.0934706743165193\n",
      "Iteration 6098, BCE loss: 57.75754054553041, Acc: 0.8195, Grad norm: 0.11191880394136962\n",
      "Iteration 6099, BCE loss: 57.75751321279377, Acc: 0.8196, Grad norm: 0.10466690660240102\n",
      "Iteration 6100, BCE loss: 57.75755885961407, Acc: 0.8196, Grad norm: 0.11564809653742002\n",
      "Iteration 6101, BCE loss: 57.75748958602887, Acc: 0.8196, Grad norm: 0.10089723064919603\n",
      "Iteration 6102, BCE loss: 57.75745324309856, Acc: 0.8196, Grad norm: 0.09315390610833452\n",
      "Iteration 6103, BCE loss: 57.75738914785886, Acc: 0.8196, Grad norm: 0.0755537674140359\n",
      "Iteration 6104, BCE loss: 57.7573948452093, Acc: 0.8195, Grad norm: 0.07669353943560574\n",
      "Iteration 6105, BCE loss: 57.75734267458333, Acc: 0.8195, Grad norm: 0.05931583327587984\n",
      "Iteration 6106, BCE loss: 57.75734629654521, Acc: 0.8196, Grad norm: 0.0631935360321827\n",
      "Iteration 6107, BCE loss: 57.757300208440505, Acc: 0.8195, Grad norm: 0.0430997588036209\n",
      "Iteration 6108, BCE loss: 57.75728047354855, Acc: 0.8196, Grad norm: 0.027956146120841487\n",
      "Iteration 6109, BCE loss: 57.75730401733124, Acc: 0.8196, Grad norm: 0.043789138467112396\n",
      "Iteration 6110, BCE loss: 57.75728631760771, Acc: 0.8195, Grad norm: 0.032335082326969966\n",
      "Iteration 6111, BCE loss: 57.75729958403919, Acc: 0.8195, Grad norm: 0.04114032613430051\n",
      "Iteration 6112, BCE loss: 57.757339027330175, Acc: 0.8195, Grad norm: 0.060095655986374646\n",
      "Iteration 6113, BCE loss: 57.757390063979514, Acc: 0.8195, Grad norm: 0.07639450241830217\n",
      "Iteration 6114, BCE loss: 57.75736573125293, Acc: 0.8195, Grad norm: 0.06850222422715486\n",
      "Iteration 6115, BCE loss: 57.75732116479648, Acc: 0.8195, Grad norm: 0.05123222669939587\n",
      "Iteration 6116, BCE loss: 57.75729002035284, Acc: 0.8195, Grad norm: 0.0364334065789106\n",
      "Iteration 6117, BCE loss: 57.75731071832121, Acc: 0.8195, Grad norm: 0.042791969263385125\n",
      "Iteration 6118, BCE loss: 57.75734373579289, Acc: 0.8195, Grad norm: 0.056584232304730614\n",
      "Iteration 6119, BCE loss: 57.75738011743181, Acc: 0.8196, Grad norm: 0.06559119031967972\n",
      "Iteration 6120, BCE loss: 57.757392787176954, Acc: 0.8196, Grad norm: 0.06933321497329546\n",
      "Iteration 6121, BCE loss: 57.757375933891424, Acc: 0.8196, Grad norm: 0.06567218445868543\n",
      "Iteration 6122, BCE loss: 57.75738764885101, Acc: 0.8196, Grad norm: 0.0723981096019273\n",
      "Iteration 6123, BCE loss: 57.75734849226454, Acc: 0.8196, Grad norm: 0.059576278401220296\n",
      "Iteration 6124, BCE loss: 57.75736599184302, Acc: 0.8196, Grad norm: 0.06494792948388345\n",
      "Iteration 6125, BCE loss: 57.757468087536026, Acc: 0.8196, Grad norm: 0.09526866096128807\n",
      "Iteration 6126, BCE loss: 57.757452973895695, Acc: 0.8196, Grad norm: 0.0924656456293552\n",
      "Iteration 6127, BCE loss: 57.757512577626414, Acc: 0.8196, Grad norm: 0.10687282823548157\n",
      "Iteration 6128, BCE loss: 57.75745961986108, Acc: 0.8196, Grad norm: 0.09416818974845363\n",
      "Iteration 6129, BCE loss: 57.7574932571289, Acc: 0.8196, Grad norm: 0.10222331517379568\n",
      "Iteration 6130, BCE loss: 57.75740891197161, Acc: 0.8196, Grad norm: 0.07876094142902718\n",
      "Iteration 6131, BCE loss: 57.7573889639029, Acc: 0.8196, Grad norm: 0.0719946933292827\n",
      "Iteration 6132, BCE loss: 57.75741291547179, Acc: 0.8196, Grad norm: 0.08201297161630142\n",
      "Iteration 6133, BCE loss: 57.75748345283128, Acc: 0.8196, Grad norm: 0.09795160395445864\n",
      "Iteration 6134, BCE loss: 57.75755409922277, Acc: 0.8196, Grad norm: 0.11359343762844504\n",
      "Iteration 6135, BCE loss: 57.75749997680937, Acc: 0.8196, Grad norm: 0.10321693095278602\n",
      "Iteration 6136, BCE loss: 57.75746390459088, Acc: 0.8196, Grad norm: 0.09638111609439019\n",
      "Iteration 6137, BCE loss: 57.75745185603141, Acc: 0.8196, Grad norm: 0.09295180768914511\n",
      "Iteration 6138, BCE loss: 57.757384485175265, Acc: 0.8196, Grad norm: 0.07570139355444629\n",
      "Iteration 6139, BCE loss: 57.75734064167888, Acc: 0.8195, Grad norm: 0.059787362618145976\n",
      "Iteration 6140, BCE loss: 57.75727378721567, Acc: 0.8195, Grad norm: 0.02375935322553692\n",
      "Iteration 6141, BCE loss: 57.757286774560455, Acc: 0.8195, Grad norm: 0.03386674984289752\n",
      "Iteration 6142, BCE loss: 57.75728544786691, Acc: 0.8195, Grad norm: 0.034345343075905733\n",
      "Iteration 6143, BCE loss: 57.757341713784186, Acc: 0.8196, Grad norm: 0.06100099053791399\n",
      "Iteration 6144, BCE loss: 57.75735309539202, Acc: 0.8196, Grad norm: 0.06575646292760853\n",
      "Iteration 6145, BCE loss: 57.75735208732424, Acc: 0.8196, Grad norm: 0.061535717426277725\n",
      "Iteration 6146, BCE loss: 57.75734195011714, Acc: 0.8196, Grad norm: 0.05839422579469932\n",
      "Iteration 6147, BCE loss: 57.757356935410854, Acc: 0.8196, Grad norm: 0.062294396823926255\n",
      "Iteration 6148, BCE loss: 57.75734976347101, Acc: 0.8196, Grad norm: 0.059062208692610724\n",
      "Iteration 6149, BCE loss: 57.757377683615914, Acc: 0.8196, Grad norm: 0.06610061797152289\n",
      "Iteration 6150, BCE loss: 57.75737634043058, Acc: 0.8196, Grad norm: 0.06474991093205996\n",
      "Iteration 6151, BCE loss: 57.75731571894477, Acc: 0.8196, Grad norm: 0.04545675770569524\n",
      "Iteration 6152, BCE loss: 57.75729150789256, Acc: 0.8196, Grad norm: 0.032366045907402406\n",
      "Iteration 6153, BCE loss: 57.75730939680818, Acc: 0.8196, Grad norm: 0.040463610310073526\n",
      "Iteration 6154, BCE loss: 57.75731284819713, Acc: 0.8196, Grad norm: 0.04308213713550469\n",
      "Iteration 6155, BCE loss: 57.75729900686119, Acc: 0.8196, Grad norm: 0.03723361594571286\n",
      "Iteration 6156, BCE loss: 57.75733447338628, Acc: 0.8196, Grad norm: 0.053030417039968254\n",
      "Iteration 6157, BCE loss: 57.75731412558229, Acc: 0.8195, Grad norm: 0.0445409557183943\n",
      "Iteration 6158, BCE loss: 57.75732130226034, Acc: 0.8196, Grad norm: 0.050492694081608405\n",
      "Iteration 6159, BCE loss: 57.75732324951396, Acc: 0.8196, Grad norm: 0.05154686639708419\n",
      "Iteration 6160, BCE loss: 57.75731482869159, Acc: 0.8196, Grad norm: 0.04705732444711055\n",
      "Iteration 6161, BCE loss: 57.757301919683364, Acc: 0.8196, Grad norm: 0.04208155109378203\n",
      "Iteration 6162, BCE loss: 57.757323733231416, Acc: 0.8196, Grad norm: 0.05148777293240769\n",
      "Iteration 6163, BCE loss: 57.757317931452675, Acc: 0.8196, Grad norm: 0.046905902255779244\n",
      "Iteration 6164, BCE loss: 57.757403097148355, Acc: 0.8196, Grad norm: 0.08032060929637104\n",
      "Iteration 6165, BCE loss: 57.75737848999461, Acc: 0.8196, Grad norm: 0.07312801750248563\n",
      "Iteration 6166, BCE loss: 57.75731293301788, Acc: 0.8196, Grad norm: 0.04714573101956768\n",
      "Iteration 6167, BCE loss: 57.75731927579095, Acc: 0.8196, Grad norm: 0.047847041002357156\n",
      "Iteration 6168, BCE loss: 57.75731450003707, Acc: 0.8196, Grad norm: 0.04283989480084705\n",
      "Iteration 6169, BCE loss: 57.75732048030366, Acc: 0.8196, Grad norm: 0.04726213866454193\n",
      "Iteration 6170, BCE loss: 57.757316239748015, Acc: 0.8196, Grad norm: 0.04583775362315744\n",
      "Iteration 6171, BCE loss: 57.75730888323835, Acc: 0.8196, Grad norm: 0.04060013010478224\n",
      "Iteration 6172, BCE loss: 57.75733516340842, Acc: 0.8196, Grad norm: 0.04934153596115731\n",
      "Iteration 6173, BCE loss: 57.75731273773013, Acc: 0.8196, Grad norm: 0.0438347344007004\n",
      "Iteration 6174, BCE loss: 57.7573444201704, Acc: 0.8196, Grad norm: 0.0567796073292087\n",
      "Iteration 6175, BCE loss: 57.75732126923649, Acc: 0.8196, Grad norm: 0.04738403891093715\n",
      "Iteration 6176, BCE loss: 57.75733036516238, Acc: 0.8196, Grad norm: 0.052632524932731066\n",
      "Iteration 6177, BCE loss: 57.75737252233503, Acc: 0.8196, Grad norm: 0.06661583489561954\n",
      "Iteration 6178, BCE loss: 57.75737793278921, Acc: 0.8196, Grad norm: 0.06911673302329938\n",
      "Iteration 6179, BCE loss: 57.757328905834086, Acc: 0.8196, Grad norm: 0.05084420077712613\n",
      "Iteration 6180, BCE loss: 57.75736807883078, Acc: 0.8196, Grad norm: 0.06672430120635844\n",
      "Iteration 6181, BCE loss: 57.75735928515533, Acc: 0.8196, Grad norm: 0.06425971161458639\n",
      "Iteration 6182, BCE loss: 57.75730425355937, Acc: 0.8196, Grad norm: 0.040420483308656135\n",
      "Iteration 6183, BCE loss: 57.75732380060928, Acc: 0.8196, Grad norm: 0.04827394488527934\n",
      "Iteration 6184, BCE loss: 57.75742004965102, Acc: 0.8195, Grad norm: 0.08207770132429504\n",
      "Iteration 6185, BCE loss: 57.75741711204999, Acc: 0.8195, Grad norm: 0.07905341053454724\n",
      "Iteration 6186, BCE loss: 57.7574144965605, Acc: 0.8195, Grad norm: 0.0817360904726632\n",
      "Iteration 6187, BCE loss: 57.75736994141896, Acc: 0.8195, Grad norm: 0.06851602951487719\n",
      "Iteration 6188, BCE loss: 57.757318018813834, Acc: 0.8195, Grad norm: 0.046450576214667735\n",
      "Iteration 6189, BCE loss: 57.75731017514559, Acc: 0.8195, Grad norm: 0.04430130776754447\n",
      "Iteration 6190, BCE loss: 57.75729953501181, Acc: 0.8195, Grad norm: 0.03700780973810489\n",
      "Iteration 6191, BCE loss: 57.75732026658267, Acc: 0.8195, Grad norm: 0.04585734140583956\n",
      "Iteration 6192, BCE loss: 57.75734219711432, Acc: 0.8195, Grad norm: 0.053842294557901405\n",
      "Iteration 6193, BCE loss: 57.75733641118113, Acc: 0.8195, Grad norm: 0.049968829388751704\n",
      "Iteration 6194, BCE loss: 57.75735015690009, Acc: 0.8195, Grad norm: 0.054312149779525934\n",
      "Iteration 6195, BCE loss: 57.75731531033604, Acc: 0.8195, Grad norm: 0.04073230863342417\n",
      "Iteration 6196, BCE loss: 57.757336817973055, Acc: 0.8195, Grad norm: 0.049064262666994075\n",
      "Iteration 6197, BCE loss: 57.75738772580981, Acc: 0.8195, Grad norm: 0.06860643951464926\n",
      "Iteration 6198, BCE loss: 57.7573908013363, Acc: 0.8195, Grad norm: 0.06822793810001884\n",
      "Iteration 6199, BCE loss: 57.75734017660514, Acc: 0.8195, Grad norm: 0.05012456826895382\n",
      "Iteration 6200, BCE loss: 57.7573440774739, Acc: 0.8195, Grad norm: 0.054647317155252124\n",
      "Iteration 6201, BCE loss: 57.757409970268995, Acc: 0.8196, Grad norm: 0.07791741636404884\n",
      "Iteration 6202, BCE loss: 57.75748488186511, Acc: 0.8196, Grad norm: 0.09661285361163599\n",
      "Iteration 6203, BCE loss: 57.75742407817032, Acc: 0.8196, Grad norm: 0.08173149764109505\n",
      "Iteration 6204, BCE loss: 57.75741219600032, Acc: 0.8196, Grad norm: 0.0772094265442153\n",
      "Iteration 6205, BCE loss: 57.75743293134708, Acc: 0.8196, Grad norm: 0.08431448259334556\n",
      "Iteration 6206, BCE loss: 57.75741301005597, Acc: 0.8196, Grad norm: 0.0799057291426867\n",
      "Iteration 6207, BCE loss: 57.75746574904136, Acc: 0.8196, Grad norm: 0.09565220597819794\n",
      "Iteration 6208, BCE loss: 57.75746614541579, Acc: 0.8196, Grad norm: 0.0961773547237555\n",
      "Iteration 6209, BCE loss: 57.757543172901805, Acc: 0.8196, Grad norm: 0.11417777335479833\n",
      "Iteration 6210, BCE loss: 57.75756262708238, Acc: 0.8196, Grad norm: 0.11810599544141229\n",
      "Iteration 6211, BCE loss: 57.75743673452103, Acc: 0.8196, Grad norm: 0.08845115581136048\n",
      "Iteration 6212, BCE loss: 57.75747532452524, Acc: 0.8196, Grad norm: 0.09925203384138911\n",
      "Iteration 6213, BCE loss: 57.757439241097316, Acc: 0.8196, Grad norm: 0.0920919639958358\n",
      "Iteration 6214, BCE loss: 57.757419645000766, Acc: 0.8196, Grad norm: 0.08543011256999675\n",
      "Iteration 6215, BCE loss: 57.757406831989, Acc: 0.8196, Grad norm: 0.08204652406298259\n",
      "Iteration 6216, BCE loss: 57.75750136626783, Acc: 0.8196, Grad norm: 0.10646202249422666\n",
      "Iteration 6217, BCE loss: 57.75745571364196, Acc: 0.8196, Grad norm: 0.09502922058572065\n",
      "Iteration 6218, BCE loss: 57.7574391001409, Acc: 0.8196, Grad norm: 0.09016338039560305\n",
      "Iteration 6219, BCE loss: 57.757450553049694, Acc: 0.8196, Grad norm: 0.0919849595280523\n",
      "Iteration 6220, BCE loss: 57.75743846363958, Acc: 0.8196, Grad norm: 0.08869494463623738\n",
      "Iteration 6221, BCE loss: 57.757430732354685, Acc: 0.8196, Grad norm: 0.0877312918379293\n",
      "Iteration 6222, BCE loss: 57.75740479726345, Acc: 0.8196, Grad norm: 0.08022257546074024\n",
      "Iteration 6223, BCE loss: 57.75740555667072, Acc: 0.8196, Grad norm: 0.08013015740327435\n",
      "Iteration 6224, BCE loss: 57.75745815437893, Acc: 0.8196, Grad norm: 0.09376350076397931\n",
      "Iteration 6225, BCE loss: 57.75742001698475, Acc: 0.8196, Grad norm: 0.0839421102308517\n",
      "Iteration 6226, BCE loss: 57.75736215693249, Acc: 0.8195, Grad norm: 0.06671377316008441\n",
      "Iteration 6227, BCE loss: 57.757396900022485, Acc: 0.8195, Grad norm: 0.07413694016527077\n",
      "Iteration 6228, BCE loss: 57.757407792980224, Acc: 0.8195, Grad norm: 0.07662852049876494\n",
      "Iteration 6229, BCE loss: 57.75747649875248, Acc: 0.8195, Grad norm: 0.09420579017575431\n",
      "Iteration 6230, BCE loss: 57.7574347790722, Acc: 0.8195, Grad norm: 0.08368678549602569\n",
      "Iteration 6231, BCE loss: 57.75738480884759, Acc: 0.8195, Grad norm: 0.06854443191857833\n",
      "Iteration 6232, BCE loss: 57.75737201092774, Acc: 0.8195, Grad norm: 0.06552121437596052\n",
      "Iteration 6233, BCE loss: 57.75735303129489, Acc: 0.8195, Grad norm: 0.061512831908824984\n",
      "Iteration 6234, BCE loss: 57.75740125161377, Acc: 0.8195, Grad norm: 0.07526183765944357\n",
      "Iteration 6235, BCE loss: 57.75749231151691, Acc: 0.8195, Grad norm: 0.09767111558747035\n",
      "Iteration 6236, BCE loss: 57.75754985226413, Acc: 0.8195, Grad norm: 0.11009381421040365\n",
      "Iteration 6237, BCE loss: 57.7574955600945, Acc: 0.8195, Grad norm: 0.10074270839432972\n",
      "Iteration 6238, BCE loss: 57.75745851893565, Acc: 0.8195, Grad norm: 0.09086434313429456\n",
      "Iteration 6239, BCE loss: 57.75740847253966, Acc: 0.8195, Grad norm: 0.07864931252128184\n",
      "Iteration 6240, BCE loss: 57.75742845141167, Acc: 0.8195, Grad norm: 0.08332412380877778\n",
      "Iteration 6241, BCE loss: 57.757458301053234, Acc: 0.8195, Grad norm: 0.09044026180231607\n",
      "Iteration 6242, BCE loss: 57.75739154060085, Acc: 0.8195, Grad norm: 0.07297826898784121\n",
      "Iteration 6243, BCE loss: 57.75739838937756, Acc: 0.8195, Grad norm: 0.07545825510173956\n",
      "Iteration 6244, BCE loss: 57.757340359490996, Acc: 0.8195, Grad norm: 0.05682182638010646\n",
      "Iteration 6245, BCE loss: 57.75734944382249, Acc: 0.8195, Grad norm: 0.06000065731152291\n",
      "Iteration 6246, BCE loss: 57.75733790971148, Acc: 0.8195, Grad norm: 0.05723193957357148\n",
      "Iteration 6247, BCE loss: 57.75732693642047, Acc: 0.8196, Grad norm: 0.052284021627569156\n",
      "Iteration 6248, BCE loss: 57.75736355914997, Acc: 0.8196, Grad norm: 0.0658850416872121\n",
      "Iteration 6249, BCE loss: 57.75734637641611, Acc: 0.8196, Grad norm: 0.059355526981032726\n",
      "Iteration 6250, BCE loss: 57.75732381923551, Acc: 0.8195, Grad norm: 0.0500165957323799\n",
      "Iteration 6251, BCE loss: 57.75734727562762, Acc: 0.8196, Grad norm: 0.05869262475967648\n",
      "Iteration 6252, BCE loss: 57.75734611347062, Acc: 0.8195, Grad norm: 0.056722300555373764\n",
      "Iteration 6253, BCE loss: 57.75736610991896, Acc: 0.8196, Grad norm: 0.06058140934419707\n",
      "Iteration 6254, BCE loss: 57.757369609859175, Acc: 0.8196, Grad norm: 0.062255841849396955\n",
      "Iteration 6255, BCE loss: 57.7573597105511, Acc: 0.8195, Grad norm: 0.060321984576416965\n",
      "Iteration 6256, BCE loss: 57.75734195454554, Acc: 0.8196, Grad norm: 0.05460669387382315\n",
      "Iteration 6257, BCE loss: 57.75730650858912, Acc: 0.8195, Grad norm: 0.03955797163531058\n",
      "Iteration 6258, BCE loss: 57.75734027935993, Acc: 0.8195, Grad norm: 0.0536833929715358\n",
      "Iteration 6259, BCE loss: 57.757338799890064, Acc: 0.8195, Grad norm: 0.054677970037003476\n",
      "Iteration 6260, BCE loss: 57.75736295689228, Acc: 0.8195, Grad norm: 0.06621718089294432\n",
      "Iteration 6261, BCE loss: 57.75734264870749, Acc: 0.8195, Grad norm: 0.05604243523740839\n",
      "Iteration 6262, BCE loss: 57.75735490295031, Acc: 0.8195, Grad norm: 0.06010730232156471\n",
      "Iteration 6263, BCE loss: 57.75740659245757, Acc: 0.8195, Grad norm: 0.07461195624049696\n",
      "Iteration 6264, BCE loss: 57.75740147734067, Acc: 0.8195, Grad norm: 0.07172953054411894\n",
      "Iteration 6265, BCE loss: 57.75739707380645, Acc: 0.8196, Grad norm: 0.06994621057109701\n",
      "Iteration 6266, BCE loss: 57.757332447447354, Acc: 0.8196, Grad norm: 0.05045722248048445\n",
      "Iteration 6267, BCE loss: 57.75734256594411, Acc: 0.8195, Grad norm: 0.055310397924359794\n",
      "Iteration 6268, BCE loss: 57.757362653701364, Acc: 0.8195, Grad norm: 0.0639415722539603\n",
      "Iteration 6269, BCE loss: 57.757317782469876, Acc: 0.8195, Grad norm: 0.04865124517074955\n",
      "Iteration 6270, BCE loss: 57.757384980003025, Acc: 0.8195, Grad norm: 0.07294382122128677\n",
      "Iteration 6271, BCE loss: 57.75740302160232, Acc: 0.8195, Grad norm: 0.07835317120080641\n",
      "Iteration 6272, BCE loss: 57.757401569944314, Acc: 0.8195, Grad norm: 0.07697989445404904\n",
      "Iteration 6273, BCE loss: 57.7574703032207, Acc: 0.8195, Grad norm: 0.09648261698066958\n",
      "Iteration 6274, BCE loss: 57.75747731474355, Acc: 0.8196, Grad norm: 0.0954908343868497\n",
      "Iteration 6275, BCE loss: 57.757415038296834, Acc: 0.8196, Grad norm: 0.07967881331316452\n",
      "Iteration 6276, BCE loss: 57.75741809544443, Acc: 0.8196, Grad norm: 0.08189824857197392\n",
      "Iteration 6277, BCE loss: 57.75740149393747, Acc: 0.8196, Grad norm: 0.07486606149855646\n",
      "Iteration 6278, BCE loss: 57.7573744483273, Acc: 0.8196, Grad norm: 0.06369312482972331\n",
      "Iteration 6279, BCE loss: 57.757339378517756, Acc: 0.8196, Grad norm: 0.05151608314917231\n",
      "Iteration 6280, BCE loss: 57.757352631982336, Acc: 0.8196, Grad norm: 0.05604211376898378\n",
      "Iteration 6281, BCE loss: 57.7573914607801, Acc: 0.8196, Grad norm: 0.06711723040559966\n",
      "Iteration 6282, BCE loss: 57.75740649913614, Acc: 0.8196, Grad norm: 0.07119404853990756\n",
      "Iteration 6283, BCE loss: 57.75737679901568, Acc: 0.8196, Grad norm: 0.06620225492707123\n",
      "Iteration 6284, BCE loss: 57.75736542486795, Acc: 0.8196, Grad norm: 0.06094071254439294\n",
      "Iteration 6285, BCE loss: 57.757353862095485, Acc: 0.8195, Grad norm: 0.05507880320281206\n",
      "Iteration 6286, BCE loss: 57.7573250550063, Acc: 0.8196, Grad norm: 0.04470310131973098\n",
      "Iteration 6287, BCE loss: 57.757323314042516, Acc: 0.8196, Grad norm: 0.04741872755651483\n",
      "Iteration 6288, BCE loss: 57.75729614582045, Acc: 0.8196, Grad norm: 0.03652010142879514\n",
      "Iteration 6289, BCE loss: 57.75728358066519, Acc: 0.8195, Grad norm: 0.03000499143341285\n",
      "Iteration 6290, BCE loss: 57.75728981718156, Acc: 0.8196, Grad norm: 0.03273583131574121\n",
      "Iteration 6291, BCE loss: 57.75729942946877, Acc: 0.8195, Grad norm: 0.037095035476513945\n",
      "Iteration 6292, BCE loss: 57.757330682594144, Acc: 0.8195, Grad norm: 0.05504139822163775\n",
      "Iteration 6293, BCE loss: 57.75737326391281, Acc: 0.8195, Grad norm: 0.0722780919453867\n",
      "Iteration 6294, BCE loss: 57.757334934029316, Acc: 0.8195, Grad norm: 0.057005693305012026\n",
      "Iteration 6295, BCE loss: 57.75736416190244, Acc: 0.8195, Grad norm: 0.06645386962004522\n",
      "Iteration 6296, BCE loss: 57.75734972348984, Acc: 0.8195, Grad norm: 0.061977009085546785\n",
      "Iteration 6297, BCE loss: 57.75742062022103, Acc: 0.8195, Grad norm: 0.08425399957924332\n",
      "Iteration 6298, BCE loss: 57.75742198038863, Acc: 0.8195, Grad norm: 0.08282469194644021\n",
      "Iteration 6299, BCE loss: 57.75746418921132, Acc: 0.8195, Grad norm: 0.09274216309592281\n",
      "Iteration 6300, BCE loss: 57.75743940676135, Acc: 0.8195, Grad norm: 0.08468119331096632\n",
      "Iteration 6301, BCE loss: 57.757461402507985, Acc: 0.8195, Grad norm: 0.08972038143803762\n",
      "Iteration 6302, BCE loss: 57.75745445170244, Acc: 0.8195, Grad norm: 0.09066812155844746\n",
      "Iteration 6303, BCE loss: 57.75740290821578, Acc: 0.8195, Grad norm: 0.07724371576290072\n",
      "Iteration 6304, BCE loss: 57.757407642163585, Acc: 0.8196, Grad norm: 0.07657924551710339\n",
      "Iteration 6305, BCE loss: 57.757374384248735, Acc: 0.8195, Grad norm: 0.06542584255601178\n",
      "Iteration 6306, BCE loss: 57.757354929720336, Acc: 0.8195, Grad norm: 0.05885676756636126\n",
      "Iteration 6307, BCE loss: 57.75733697478137, Acc: 0.8195, Grad norm: 0.0526821505327075\n",
      "Iteration 6308, BCE loss: 57.75737084877607, Acc: 0.8195, Grad norm: 0.06456509996123992\n",
      "Iteration 6309, BCE loss: 57.75741371545628, Acc: 0.8195, Grad norm: 0.07927979048226794\n",
      "Iteration 6310, BCE loss: 57.757420696696265, Acc: 0.8195, Grad norm: 0.08290119200841317\n",
      "Iteration 6311, BCE loss: 57.75743747651764, Acc: 0.8196, Grad norm: 0.08772772618277679\n",
      "Iteration 6312, BCE loss: 57.75740953143949, Acc: 0.8195, Grad norm: 0.08108230284690826\n",
      "Iteration 6313, BCE loss: 57.75742889957712, Acc: 0.8196, Grad norm: 0.08502681773280266\n",
      "Iteration 6314, BCE loss: 57.757409275026944, Acc: 0.8195, Grad norm: 0.0800099310762645\n",
      "Iteration 6315, BCE loss: 57.757383624865376, Acc: 0.8195, Grad norm: 0.07393586709948428\n",
      "Iteration 6316, BCE loss: 57.75732619682134, Acc: 0.8195, Grad norm: 0.05208040292821198\n",
      "Iteration 6317, BCE loss: 57.75733813813522, Acc: 0.8195, Grad norm: 0.054498472050894156\n",
      "Iteration 6318, BCE loss: 57.75734417813058, Acc: 0.8195, Grad norm: 0.054837708967544536\n",
      "Iteration 6319, BCE loss: 57.757321528142, Acc: 0.8195, Grad norm: 0.04618680253632583\n",
      "Iteration 6320, BCE loss: 57.75732875204699, Acc: 0.8196, Grad norm: 0.050685193393661974\n",
      "Iteration 6321, BCE loss: 57.757343620131195, Acc: 0.8196, Grad norm: 0.05547167395598861\n",
      "Iteration 6322, BCE loss: 57.757361397087344, Acc: 0.8196, Grad norm: 0.0626304091724504\n",
      "Iteration 6323, BCE loss: 57.757386276989465, Acc: 0.8196, Grad norm: 0.07009536074920224\n",
      "Iteration 6324, BCE loss: 57.75741646250317, Acc: 0.8196, Grad norm: 0.07760164339402444\n",
      "Iteration 6325, BCE loss: 57.75740281228047, Acc: 0.8196, Grad norm: 0.07492701158742299\n",
      "Iteration 6326, BCE loss: 57.757378080207886, Acc: 0.8196, Grad norm: 0.06891817710357453\n",
      "Iteration 6327, BCE loss: 57.75737249990796, Acc: 0.8196, Grad norm: 0.06773746929333992\n",
      "Iteration 6328, BCE loss: 57.75746133187816, Acc: 0.8196, Grad norm: 0.09249935020076938\n",
      "Iteration 6329, BCE loss: 57.75749216340476, Acc: 0.8196, Grad norm: 0.09766645505058769\n",
      "Iteration 6330, BCE loss: 57.75746948908844, Acc: 0.8196, Grad norm: 0.09183342483453011\n",
      "Iteration 6331, BCE loss: 57.7574539815365, Acc: 0.8196, Grad norm: 0.0877623920106556\n",
      "Iteration 6332, BCE loss: 57.75746250680726, Acc: 0.8196, Grad norm: 0.09017819251637707\n",
      "Iteration 6333, BCE loss: 57.757428322446444, Acc: 0.8196, Grad norm: 0.08182839896903615\n",
      "Iteration 6334, BCE loss: 57.75744827760677, Acc: 0.8196, Grad norm: 0.08834974347320754\n",
      "Iteration 6335, BCE loss: 57.757454345356365, Acc: 0.8196, Grad norm: 0.09177649299948616\n",
      "Iteration 6336, BCE loss: 57.75750452952708, Acc: 0.8196, Grad norm: 0.10547292279287952\n",
      "Iteration 6337, BCE loss: 57.75749475617542, Acc: 0.8196, Grad norm: 0.1039343625574617\n",
      "Iteration 6338, BCE loss: 57.75750903877003, Acc: 0.8196, Grad norm: 0.10534920498296071\n",
      "Iteration 6339, BCE loss: 57.75756088754923, Acc: 0.8196, Grad norm: 0.11531523297570473\n",
      "Iteration 6340, BCE loss: 57.757476979888025, Acc: 0.8196, Grad norm: 0.09736381720655961\n",
      "Iteration 6341, BCE loss: 57.75736100589184, Acc: 0.8196, Grad norm: 0.06582481499974233\n",
      "Iteration 6342, BCE loss: 57.75742867686448, Acc: 0.8196, Grad norm: 0.08703783391344597\n",
      "Iteration 6343, BCE loss: 57.75744226688006, Acc: 0.8196, Grad norm: 0.09019102383805719\n",
      "Iteration 6344, BCE loss: 57.75743696732353, Acc: 0.8196, Grad norm: 0.08688431218166853\n",
      "Iteration 6345, BCE loss: 57.75740718162579, Acc: 0.8196, Grad norm: 0.07690260461957645\n",
      "Iteration 6346, BCE loss: 57.75748244691846, Acc: 0.8196, Grad norm: 0.09683991930733013\n",
      "Iteration 6347, BCE loss: 57.757379100517205, Acc: 0.8195, Grad norm: 0.0694627471992886\n",
      "Iteration 6348, BCE loss: 57.7574284750035, Acc: 0.8195, Grad norm: 0.08330375527751009\n",
      "Iteration 6349, BCE loss: 57.757483183898614, Acc: 0.8196, Grad norm: 0.09988983549174826\n",
      "Iteration 6350, BCE loss: 57.7573804204895, Acc: 0.8196, Grad norm: 0.07178885604915963\n",
      "Iteration 6351, BCE loss: 57.75733027040994, Acc: 0.8195, Grad norm: 0.04989388233481294\n",
      "Iteration 6352, BCE loss: 57.75738556741696, Acc: 0.8195, Grad norm: 0.071148737749269\n",
      "Iteration 6353, BCE loss: 57.75735297304905, Acc: 0.8195, Grad norm: 0.05897135580665228\n",
      "Iteration 6354, BCE loss: 57.757352231474385, Acc: 0.8195, Grad norm: 0.057343965635400514\n",
      "Iteration 6355, BCE loss: 57.757329862491176, Acc: 0.8195, Grad norm: 0.05065771220944004\n",
      "Iteration 6356, BCE loss: 57.75733266143382, Acc: 0.8195, Grad norm: 0.05122731694343654\n",
      "Iteration 6357, BCE loss: 57.757320523902465, Acc: 0.8195, Grad norm: 0.04586714645209717\n",
      "Iteration 6358, BCE loss: 57.75736670209116, Acc: 0.8195, Grad norm: 0.06028510745624344\n",
      "Iteration 6359, BCE loss: 57.75736098922457, Acc: 0.8195, Grad norm: 0.060153290051282754\n",
      "Iteration 6360, BCE loss: 57.75736163375163, Acc: 0.8195, Grad norm: 0.059689441857117956\n",
      "Iteration 6361, BCE loss: 57.757408454904265, Acc: 0.8195, Grad norm: 0.07035209491715776\n",
      "Iteration 6362, BCE loss: 57.75742921904104, Acc: 0.8195, Grad norm: 0.07883443046578423\n",
      "Iteration 6363, BCE loss: 57.75748133216868, Acc: 0.8195, Grad norm: 0.09221514359818886\n",
      "Iteration 6364, BCE loss: 57.75746124044559, Acc: 0.8195, Grad norm: 0.08903034676138374\n",
      "Iteration 6365, BCE loss: 57.75742655300647, Acc: 0.8195, Grad norm: 0.08023398139506512\n",
      "Iteration 6366, BCE loss: 57.757392056369696, Acc: 0.8195, Grad norm: 0.07232651512511196\n",
      "Iteration 6367, BCE loss: 57.757410310615526, Acc: 0.8195, Grad norm: 0.07428138694946375\n",
      "Iteration 6368, BCE loss: 57.75744437540518, Acc: 0.8195, Grad norm: 0.08107087567015638\n",
      "Iteration 6369, BCE loss: 57.757442527271806, Acc: 0.8196, Grad norm: 0.08371976401624545\n",
      "Iteration 6370, BCE loss: 57.75739826117829, Acc: 0.8196, Grad norm: 0.07151816193763794\n",
      "Iteration 6371, BCE loss: 57.75740416736272, Acc: 0.8196, Grad norm: 0.07280699208228221\n",
      "Iteration 6372, BCE loss: 57.757372266401404, Acc: 0.8196, Grad norm: 0.06326569939023456\n",
      "Iteration 6373, BCE loss: 57.75732779621721, Acc: 0.8196, Grad norm: 0.046225063707953595\n",
      "Iteration 6374, BCE loss: 57.757356613645534, Acc: 0.8196, Grad norm: 0.05960723107244379\n",
      "Iteration 6375, BCE loss: 57.75741723520166, Acc: 0.8196, Grad norm: 0.07678576628268462\n",
      "Iteration 6376, BCE loss: 57.757483708501056, Acc: 0.8196, Grad norm: 0.09838186701512779\n",
      "Iteration 6377, BCE loss: 57.75742943167606, Acc: 0.8196, Grad norm: 0.08271863684497058\n",
      "Iteration 6378, BCE loss: 57.75746904839774, Acc: 0.8196, Grad norm: 0.09446561789954847\n",
      "Iteration 6379, BCE loss: 57.757431581496725, Acc: 0.8196, Grad norm: 0.08503564501518757\n",
      "Iteration 6380, BCE loss: 57.757526154456556, Acc: 0.8196, Grad norm: 0.10837030308582782\n",
      "Iteration 6381, BCE loss: 57.75749128525016, Acc: 0.8196, Grad norm: 0.10229304124785805\n",
      "Iteration 6382, BCE loss: 57.757563434198104, Acc: 0.8196, Grad norm: 0.11887259291254723\n",
      "Iteration 6383, BCE loss: 57.757499402812854, Acc: 0.8196, Grad norm: 0.10644229509277417\n",
      "Iteration 6384, BCE loss: 57.757538167128786, Acc: 0.8196, Grad norm: 0.11483835555722882\n",
      "Iteration 6385, BCE loss: 57.75738658456606, Acc: 0.8196, Grad norm: 0.07701942275104028\n",
      "Iteration 6386, BCE loss: 57.757380488659976, Acc: 0.8196, Grad norm: 0.07230320898300141\n",
      "Iteration 6387, BCE loss: 57.757336015080284, Acc: 0.8196, Grad norm: 0.05636147089750659\n",
      "Iteration 6388, BCE loss: 57.75743433005391, Acc: 0.8196, Grad norm: 0.0883680101084884\n",
      "Iteration 6389, BCE loss: 57.75746236959041, Acc: 0.8196, Grad norm: 0.09555173027042038\n",
      "Iteration 6390, BCE loss: 57.7575270304166, Acc: 0.8196, Grad norm: 0.10918913683472574\n",
      "Iteration 6391, BCE loss: 57.75749215925132, Acc: 0.8196, Grad norm: 0.10163167605068547\n",
      "Iteration 6392, BCE loss: 57.75737391519213, Acc: 0.8196, Grad norm: 0.06955671587481933\n",
      "Iteration 6393, BCE loss: 57.75735648658343, Acc: 0.8196, Grad norm: 0.06270630896293063\n",
      "Iteration 6394, BCE loss: 57.757366955179776, Acc: 0.8196, Grad norm: 0.06730693621196852\n",
      "Iteration 6395, BCE loss: 57.75733753458904, Acc: 0.8196, Grad norm: 0.056592850916014036\n",
      "Iteration 6396, BCE loss: 57.75734217949651, Acc: 0.8196, Grad norm: 0.05607400274681451\n",
      "Iteration 6397, BCE loss: 57.75735324831204, Acc: 0.8196, Grad norm: 0.059358306858507334\n",
      "Iteration 6398, BCE loss: 57.757389987273214, Acc: 0.8196, Grad norm: 0.0716383078936986\n",
      "Iteration 6399, BCE loss: 57.75736395144954, Acc: 0.8196, Grad norm: 0.0632036936616275\n",
      "Iteration 6400, BCE loss: 57.75741458461411, Acc: 0.8196, Grad norm: 0.07937123526644681\n",
      "Iteration 6401, BCE loss: 57.75737596209462, Acc: 0.8196, Grad norm: 0.06778335768646049\n",
      "Iteration 6402, BCE loss: 57.75740934713159, Acc: 0.8195, Grad norm: 0.07672089216971836\n",
      "Iteration 6403, BCE loss: 57.75741877423485, Acc: 0.8195, Grad norm: 0.07818405588417011\n",
      "Iteration 6404, BCE loss: 57.75738614419392, Acc: 0.8195, Grad norm: 0.06908973252519594\n",
      "Iteration 6405, BCE loss: 57.75741734305909, Acc: 0.8195, Grad norm: 0.07975593818654882\n",
      "Iteration 6406, BCE loss: 57.75734081763191, Acc: 0.8195, Grad norm: 0.05440118172345986\n",
      "Iteration 6407, BCE loss: 57.757348509858, Acc: 0.8195, Grad norm: 0.05879401101857999\n",
      "Iteration 6408, BCE loss: 57.757345057805765, Acc: 0.8195, Grad norm: 0.05439388505718148\n",
      "Iteration 6409, BCE loss: 57.757337629591305, Acc: 0.8195, Grad norm: 0.05083827618286859\n",
      "Iteration 6410, BCE loss: 57.75733884039076, Acc: 0.8195, Grad norm: 0.0509341721846102\n",
      "Iteration 6411, BCE loss: 57.75734124841013, Acc: 0.8196, Grad norm: 0.05310685072561604\n",
      "Iteration 6412, BCE loss: 57.75735053105272, Acc: 0.8195, Grad norm: 0.055785233512804004\n",
      "Iteration 6413, BCE loss: 57.75736502255936, Acc: 0.8196, Grad norm: 0.05865751579086014\n",
      "Iteration 6414, BCE loss: 57.7573845591473, Acc: 0.8196, Grad norm: 0.06569536231148891\n",
      "Iteration 6415, BCE loss: 57.75740464836047, Acc: 0.8195, Grad norm: 0.07127577649420992\n",
      "Iteration 6416, BCE loss: 57.75737107022087, Acc: 0.8195, Grad norm: 0.0631925118026163\n",
      "Iteration 6417, BCE loss: 57.757353853298056, Acc: 0.8196, Grad norm: 0.05619982858821437\n",
      "Iteration 6418, BCE loss: 57.75733232147686, Acc: 0.8196, Grad norm: 0.048993181033448824\n",
      "Iteration 6419, BCE loss: 57.7574456540561, Acc: 0.8196, Grad norm: 0.08407568408785468\n",
      "Iteration 6420, BCE loss: 57.757391248351766, Acc: 0.8196, Grad norm: 0.06907136863154777\n",
      "Iteration 6421, BCE loss: 57.75738313803453, Acc: 0.8196, Grad norm: 0.06773848506682023\n",
      "Iteration 6422, BCE loss: 57.75734305909228, Acc: 0.8196, Grad norm: 0.05429793328294792\n",
      "Iteration 6423, BCE loss: 57.75734013140098, Acc: 0.8196, Grad norm: 0.05298020647619569\n",
      "Iteration 6424, BCE loss: 57.75734197370711, Acc: 0.8196, Grad norm: 0.054611670393303946\n",
      "Iteration 6425, BCE loss: 57.757329405117304, Acc: 0.8196, Grad norm: 0.04977945230630196\n",
      "Iteration 6426, BCE loss: 57.75735426207643, Acc: 0.8196, Grad norm: 0.05894671170174174\n",
      "Iteration 6427, BCE loss: 57.75735076500156, Acc: 0.8196, Grad norm: 0.06097432111641236\n",
      "Iteration 6428, BCE loss: 57.7574220721845, Acc: 0.8196, Grad norm: 0.08279862494767685\n",
      "Iteration 6429, BCE loss: 57.757437610389445, Acc: 0.8196, Grad norm: 0.08796211198981285\n",
      "Iteration 6430, BCE loss: 57.75747421879391, Acc: 0.8196, Grad norm: 0.09779563847195377\n",
      "Iteration 6431, BCE loss: 57.7573767458552, Acc: 0.8196, Grad norm: 0.06835957020269842\n",
      "Iteration 6432, BCE loss: 57.757347857906076, Acc: 0.8196, Grad norm: 0.058000437997704206\n",
      "Iteration 6433, BCE loss: 57.75735845912614, Acc: 0.8196, Grad norm: 0.062436802338156516\n",
      "Iteration 6434, BCE loss: 57.75729840036092, Acc: 0.8196, Grad norm: 0.039751042964683125\n",
      "Iteration 6435, BCE loss: 57.757285970666224, Acc: 0.8196, Grad norm: 0.032613720856659194\n",
      "Iteration 6436, BCE loss: 57.757313221081624, Acc: 0.8195, Grad norm: 0.047596079498073554\n",
      "Iteration 6437, BCE loss: 57.75728748975854, Acc: 0.8196, Grad norm: 0.032994324003322986\n",
      "Iteration 6438, BCE loss: 57.75730158010048, Acc: 0.8195, Grad norm: 0.04304058716566191\n",
      "Iteration 6439, BCE loss: 57.75728666770706, Acc: 0.8195, Grad norm: 0.03470466916928699\n",
      "Iteration 6440, BCE loss: 57.75727738668547, Acc: 0.8195, Grad norm: 0.02687193120633341\n",
      "Iteration 6441, BCE loss: 57.75726921436238, Acc: 0.8195, Grad norm: 0.02039618341513697\n",
      "Iteration 6442, BCE loss: 57.75727237121099, Acc: 0.8195, Grad norm: 0.021849362611912993\n",
      "Iteration 6443, BCE loss: 57.757295847672644, Acc: 0.8195, Grad norm: 0.03903865306398945\n",
      "Iteration 6444, BCE loss: 57.75732265654872, Acc: 0.8195, Grad norm: 0.05101555287834224\n",
      "Iteration 6445, BCE loss: 57.75738283054739, Acc: 0.8195, Grad norm: 0.07138183967422257\n",
      "Iteration 6446, BCE loss: 57.75734149570616, Acc: 0.8196, Grad norm: 0.060808942473218316\n",
      "Iteration 6447, BCE loss: 57.757368886215346, Acc: 0.8196, Grad norm: 0.06826877243407693\n",
      "Iteration 6448, BCE loss: 57.75738407843945, Acc: 0.8196, Grad norm: 0.07515337747122447\n",
      "Iteration 6449, BCE loss: 57.75736265167433, Acc: 0.8196, Grad norm: 0.06632219831706095\n",
      "Iteration 6450, BCE loss: 57.75734006529889, Acc: 0.8196, Grad norm: 0.05749215915660726\n",
      "Iteration 6451, BCE loss: 57.757338724051024, Acc: 0.8195, Grad norm: 0.05829047808383004\n",
      "Iteration 6452, BCE loss: 57.757317001888076, Acc: 0.8195, Grad norm: 0.047121702535200725\n",
      "Iteration 6453, BCE loss: 57.757354957887316, Acc: 0.8195, Grad norm: 0.059831496369255034\n",
      "Iteration 6454, BCE loss: 57.75735475679441, Acc: 0.8195, Grad norm: 0.060625300909729674\n",
      "Iteration 6455, BCE loss: 57.75735490573327, Acc: 0.8195, Grad norm: 0.060078762076060235\n",
      "Iteration 6456, BCE loss: 57.7573903026468, Acc: 0.8195, Grad norm: 0.07084918124739463\n",
      "Iteration 6457, BCE loss: 57.75733321471883, Acc: 0.8195, Grad norm: 0.053724929188158875\n",
      "Iteration 6458, BCE loss: 57.75730515245448, Acc: 0.8195, Grad norm: 0.043950540123156454\n",
      "Iteration 6459, BCE loss: 57.757293940565575, Acc: 0.8195, Grad norm: 0.0383168602044315\n",
      "Iteration 6460, BCE loss: 57.757310490036616, Acc: 0.8195, Grad norm: 0.046149170369982256\n",
      "Iteration 6461, BCE loss: 57.75728789454324, Acc: 0.8195, Grad norm: 0.03350644714484686\n",
      "Iteration 6462, BCE loss: 57.75732721099585, Acc: 0.8196, Grad norm: 0.053352658618356835\n",
      "Iteration 6463, BCE loss: 57.75737323437997, Acc: 0.8196, Grad norm: 0.06981595315861057\n",
      "Iteration 6464, BCE loss: 57.757326673053484, Acc: 0.8196, Grad norm: 0.050058325966271876\n",
      "Iteration 6465, BCE loss: 57.75735674186653, Acc: 0.8196, Grad norm: 0.06118067371561364\n",
      "Iteration 6466, BCE loss: 57.757407692166225, Acc: 0.8196, Grad norm: 0.0800258750766675\n",
      "Iteration 6467, BCE loss: 57.75737283744445, Acc: 0.8196, Grad norm: 0.06953120380396018\n",
      "Iteration 6468, BCE loss: 57.75734789869524, Acc: 0.8196, Grad norm: 0.06091851818278757\n",
      "Iteration 6469, BCE loss: 57.757318555558626, Acc: 0.8196, Grad norm: 0.048913099721618156\n",
      "Iteration 6470, BCE loss: 57.75730979575004, Acc: 0.8196, Grad norm: 0.04645287883482231\n",
      "Iteration 6471, BCE loss: 57.75736352274012, Acc: 0.8196, Grad norm: 0.06742774104013671\n",
      "Iteration 6472, BCE loss: 57.75743290081161, Acc: 0.8196, Grad norm: 0.08833732533882985\n",
      "Iteration 6473, BCE loss: 57.757397358896874, Acc: 0.8196, Grad norm: 0.07957162393356855\n",
      "Iteration 6474, BCE loss: 57.75735178857834, Acc: 0.8196, Grad norm: 0.06286053394021449\n",
      "Iteration 6475, BCE loss: 57.757360260225745, Acc: 0.8196, Grad norm: 0.0658304426106537\n",
      "Iteration 6476, BCE loss: 57.757359723703985, Acc: 0.8196, Grad norm: 0.06402698552294246\n",
      "Iteration 6477, BCE loss: 57.757368680771734, Acc: 0.8196, Grad norm: 0.06712422909643341\n",
      "Iteration 6478, BCE loss: 57.75735964097058, Acc: 0.8196, Grad norm: 0.06443190900349617\n",
      "Iteration 6479, BCE loss: 57.75734583435466, Acc: 0.8196, Grad norm: 0.058616829058906826\n",
      "Iteration 6480, BCE loss: 57.75736005356643, Acc: 0.8196, Grad norm: 0.06431072864024069\n",
      "Iteration 6481, BCE loss: 57.75733984475701, Acc: 0.8196, Grad norm: 0.057328051153695686\n",
      "Iteration 6482, BCE loss: 57.75736544271898, Acc: 0.8196, Grad norm: 0.06649724047891642\n",
      "Iteration 6483, BCE loss: 57.75738370970862, Acc: 0.8196, Grad norm: 0.07192746644862458\n",
      "Iteration 6484, BCE loss: 57.75736621813245, Acc: 0.8196, Grad norm: 0.06615920424663281\n",
      "Iteration 6485, BCE loss: 57.75736606784477, Acc: 0.8196, Grad norm: 0.06537740810443363\n",
      "Iteration 6486, BCE loss: 57.75733593621139, Acc: 0.8196, Grad norm: 0.05343466190175075\n",
      "Iteration 6487, BCE loss: 57.75735788029648, Acc: 0.8196, Grad norm: 0.05864988596238647\n",
      "Iteration 6488, BCE loss: 57.75735203673026, Acc: 0.8196, Grad norm: 0.05606161706947517\n",
      "Iteration 6489, BCE loss: 57.75739997176362, Acc: 0.8196, Grad norm: 0.07011481587288138\n",
      "Iteration 6490, BCE loss: 57.75737877406608, Acc: 0.8196, Grad norm: 0.06623205130643413\n",
      "Iteration 6491, BCE loss: 57.75740677050699, Acc: 0.8196, Grad norm: 0.07345362869520139\n",
      "Iteration 6492, BCE loss: 57.75741876910567, Acc: 0.8196, Grad norm: 0.07835278164417894\n",
      "Iteration 6493, BCE loss: 57.75739297271451, Acc: 0.8196, Grad norm: 0.07134504808606448\n",
      "Iteration 6494, BCE loss: 57.757391297778895, Acc: 0.8196, Grad norm: 0.07230625720482897\n",
      "Iteration 6495, BCE loss: 57.757417509435825, Acc: 0.8196, Grad norm: 0.07995308393514826\n",
      "Iteration 6496, BCE loss: 57.75738777585177, Acc: 0.8196, Grad norm: 0.07155954601666974\n",
      "Iteration 6497, BCE loss: 57.75738282195162, Acc: 0.8196, Grad norm: 0.07076357141580303\n",
      "Iteration 6498, BCE loss: 57.757372890956766, Acc: 0.8196, Grad norm: 0.06635761979341563\n",
      "Iteration 6499, BCE loss: 57.757337938639246, Acc: 0.8196, Grad norm: 0.055594652958275995\n",
      "Iteration 6500, BCE loss: 57.757315013050146, Acc: 0.8196, Grad norm: 0.04649632502441526\n",
      "Iteration 6501, BCE loss: 57.7572856207831, Acc: 0.8196, Grad norm: 0.03247121711749863\n",
      "Iteration 6502, BCE loss: 57.757323463285516, Acc: 0.8196, Grad norm: 0.05150010690063641\n",
      "Iteration 6503, BCE loss: 57.75736862468176, Acc: 0.8196, Grad norm: 0.06770828126746388\n",
      "Iteration 6504, BCE loss: 57.75737422587822, Acc: 0.8196, Grad norm: 0.0706914663084181\n",
      "Iteration 6505, BCE loss: 57.75729287527949, Acc: 0.8196, Grad norm: 0.03615497320674643\n",
      "Iteration 6506, BCE loss: 57.75729027297571, Acc: 0.8196, Grad norm: 0.03487370536638152\n",
      "Iteration 6507, BCE loss: 57.75732904227823, Acc: 0.8195, Grad norm: 0.049224407100424604\n",
      "Iteration 6508, BCE loss: 57.75734931793013, Acc: 0.8195, Grad norm: 0.058278953010729\n",
      "Iteration 6509, BCE loss: 57.75734374745417, Acc: 0.8195, Grad norm: 0.056302282791228536\n",
      "Iteration 6510, BCE loss: 57.75731189211576, Acc: 0.8195, Grad norm: 0.04418753807552946\n",
      "Iteration 6511, BCE loss: 57.7573016851413, Acc: 0.8196, Grad norm: 0.04124017855450018\n",
      "Iteration 6512, BCE loss: 57.75731432687816, Acc: 0.8196, Grad norm: 0.049500524593985976\n",
      "Iteration 6513, BCE loss: 57.757322235333206, Acc: 0.8196, Grad norm: 0.05163564627305612\n",
      "Iteration 6514, BCE loss: 57.75732112697715, Acc: 0.8196, Grad norm: 0.04802656999737043\n",
      "Iteration 6515, BCE loss: 57.757286576306484, Acc: 0.8196, Grad norm: 0.03004780345146508\n",
      "Iteration 6516, BCE loss: 57.75731434694259, Acc: 0.8196, Grad norm: 0.04628256582010267\n",
      "Iteration 6517, BCE loss: 57.75736502412839, Acc: 0.8196, Grad norm: 0.06974435128046032\n",
      "Iteration 6518, BCE loss: 57.75733293050831, Acc: 0.8196, Grad norm: 0.0569718248681773\n",
      "Iteration 6519, BCE loss: 57.75744315441026, Acc: 0.8196, Grad norm: 0.09082071366365416\n",
      "Iteration 6520, BCE loss: 57.75735597554288, Acc: 0.8196, Grad norm: 0.06505436160267233\n",
      "Iteration 6521, BCE loss: 57.75735891162924, Acc: 0.8196, Grad norm: 0.06589154076128652\n",
      "Iteration 6522, BCE loss: 57.75751643420281, Acc: 0.8196, Grad norm: 0.10649017284930665\n",
      "Iteration 6523, BCE loss: 57.757419034142, Acc: 0.8196, Grad norm: 0.08413014278429057\n",
      "Iteration 6524, BCE loss: 57.75739734669641, Acc: 0.8196, Grad norm: 0.07651677616305577\n",
      "Iteration 6525, BCE loss: 57.75742499874666, Acc: 0.8196, Grad norm: 0.08493437808976967\n",
      "Iteration 6526, BCE loss: 57.75740526923141, Acc: 0.8196, Grad norm: 0.07832969076264136\n",
      "Iteration 6527, BCE loss: 57.75746305764279, Acc: 0.8196, Grad norm: 0.09410639495654621\n",
      "Iteration 6528, BCE loss: 57.75742843145031, Acc: 0.8196, Grad norm: 0.08312547488454396\n",
      "Iteration 6529, BCE loss: 57.757366228188545, Acc: 0.8196, Grad norm: 0.06472104161188805\n",
      "Iteration 6530, BCE loss: 57.757332296086204, Acc: 0.8195, Grad norm: 0.052323301556570015\n",
      "Iteration 6531, BCE loss: 57.75735792882796, Acc: 0.8196, Grad norm: 0.06549734958916545\n",
      "Iteration 6532, BCE loss: 57.75736188668825, Acc: 0.8196, Grad norm: 0.06692072973742776\n",
      "Iteration 6533, BCE loss: 57.757350047261404, Acc: 0.8196, Grad norm: 0.06363674401934713\n",
      "Iteration 6534, BCE loss: 57.75730893924404, Acc: 0.8195, Grad norm: 0.04564938989820858\n",
      "Iteration 6535, BCE loss: 57.75730990858591, Acc: 0.8195, Grad norm: 0.04846950144633222\n",
      "Iteration 6536, BCE loss: 57.757382420238926, Acc: 0.8196, Grad norm: 0.07698363808529723\n",
      "Iteration 6537, BCE loss: 57.757386821509975, Acc: 0.8195, Grad norm: 0.07755278839782825\n",
      "Iteration 6538, BCE loss: 57.757351756286056, Acc: 0.8195, Grad norm: 0.06470232575316871\n",
      "Iteration 6539, BCE loss: 57.757407736976205, Acc: 0.8195, Grad norm: 0.08185947325920899\n",
      "Iteration 6540, BCE loss: 57.75737871447873, Acc: 0.8195, Grad norm: 0.07415399845066678\n",
      "Iteration 6541, BCE loss: 57.75740133509697, Acc: 0.8195, Grad norm: 0.07962313647321818\n",
      "Iteration 6542, BCE loss: 57.75737211027793, Acc: 0.8196, Grad norm: 0.0705259017301086\n",
      "Iteration 6543, BCE loss: 57.757396358660145, Acc: 0.8196, Grad norm: 0.0774806588053494\n",
      "Iteration 6544, BCE loss: 57.7574293968449, Acc: 0.8196, Grad norm: 0.08817908253249847\n",
      "Iteration 6545, BCE loss: 57.75738600435919, Acc: 0.8196, Grad norm: 0.0756036290642659\n",
      "Iteration 6546, BCE loss: 57.75744386530111, Acc: 0.8196, Grad norm: 0.09300686887003946\n",
      "Iteration 6547, BCE loss: 57.7573986807522, Acc: 0.8196, Grad norm: 0.07741348383845514\n",
      "Iteration 6548, BCE loss: 57.7573937337186, Acc: 0.8196, Grad norm: 0.07311432336423909\n",
      "Iteration 6549, BCE loss: 57.75731982486724, Acc: 0.8195, Grad norm: 0.04535509865419222\n",
      "Iteration 6550, BCE loss: 57.7573052458351, Acc: 0.8195, Grad norm: 0.039547897813463066\n",
      "Iteration 6551, BCE loss: 57.75729310300047, Acc: 0.8195, Grad norm: 0.03382929105483448\n",
      "Iteration 6552, BCE loss: 57.75729792007232, Acc: 0.8195, Grad norm: 0.038512775720287495\n",
      "Iteration 6553, BCE loss: 57.757354494709304, Acc: 0.8195, Grad norm: 0.06400993389580102\n",
      "Iteration 6554, BCE loss: 57.75741107749032, Acc: 0.8195, Grad norm: 0.07940042641749538\n",
      "Iteration 6555, BCE loss: 57.75742721065107, Acc: 0.8195, Grad norm: 0.08445477319654711\n",
      "Iteration 6556, BCE loss: 57.757376858325856, Acc: 0.8195, Grad norm: 0.06701569293523892\n",
      "Iteration 6557, BCE loss: 57.75742450061213, Acc: 0.8195, Grad norm: 0.07697520606811716\n",
      "Iteration 6558, BCE loss: 57.7574492266526, Acc: 0.8195, Grad norm: 0.08362163334776423\n",
      "Iteration 6559, BCE loss: 57.7574776686412, Acc: 0.8195, Grad norm: 0.09197976630114504\n",
      "Iteration 6560, BCE loss: 57.75748430539497, Acc: 0.8195, Grad norm: 0.09265156855112679\n",
      "Iteration 6561, BCE loss: 57.75747577650947, Acc: 0.8195, Grad norm: 0.09189334499074071\n",
      "Iteration 6562, BCE loss: 57.757508663402774, Acc: 0.8195, Grad norm: 0.10037435745499645\n",
      "Iteration 6563, BCE loss: 57.75748375068134, Acc: 0.8195, Grad norm: 0.09622654906646298\n",
      "Iteration 6564, BCE loss: 57.75742642122593, Acc: 0.8195, Grad norm: 0.08227039213534298\n",
      "Iteration 6565, BCE loss: 57.75742866945696, Acc: 0.8195, Grad norm: 0.08058179741321311\n",
      "Iteration 6566, BCE loss: 57.75744549191623, Acc: 0.8195, Grad norm: 0.08270890765040409\n",
      "Iteration 6567, BCE loss: 57.75746984755489, Acc: 0.8195, Grad norm: 0.08705800365700028\n",
      "Iteration 6568, BCE loss: 57.75743127254046, Acc: 0.8195, Grad norm: 0.08140210261945664\n",
      "Iteration 6569, BCE loss: 57.75750389875514, Acc: 0.8195, Grad norm: 0.0984588751614554\n",
      "Iteration 6570, BCE loss: 57.75759891301884, Acc: 0.8195, Grad norm: 0.11656105373648327\n",
      "Iteration 6571, BCE loss: 57.75760755244772, Acc: 0.8195, Grad norm: 0.12058209582045835\n",
      "Iteration 6572, BCE loss: 57.7576260425279, Acc: 0.8195, Grad norm: 0.12277646824648894\n",
      "Iteration 6573, BCE loss: 57.75755201565288, Acc: 0.8195, Grad norm: 0.10978548502566103\n",
      "Iteration 6574, BCE loss: 57.757505912322756, Acc: 0.8195, Grad norm: 0.10033289347813461\n",
      "Iteration 6575, BCE loss: 57.757550151463256, Acc: 0.8195, Grad norm: 0.10724115695732281\n",
      "Iteration 6576, BCE loss: 57.75758752264117, Acc: 0.8195, Grad norm: 0.11353447105413125\n",
      "Iteration 6577, BCE loss: 57.75760334376537, Acc: 0.8195, Grad norm: 0.11562580534036741\n",
      "Iteration 6578, BCE loss: 57.757631201722205, Acc: 0.8195, Grad norm: 0.1235434390260694\n",
      "Iteration 6579, BCE loss: 57.75759051329537, Acc: 0.8195, Grad norm: 0.11617628533878668\n",
      "Iteration 6580, BCE loss: 57.75755162342825, Acc: 0.8195, Grad norm: 0.10766703216883439\n",
      "Iteration 6581, BCE loss: 57.757544740812456, Acc: 0.8195, Grad norm: 0.10979539157285596\n",
      "Iteration 6582, BCE loss: 57.75747736264884, Acc: 0.8195, Grad norm: 0.09537331816656293\n",
      "Iteration 6583, BCE loss: 57.7574727972318, Acc: 0.8195, Grad norm: 0.09182485531314473\n",
      "Iteration 6584, BCE loss: 57.75745453430111, Acc: 0.8195, Grad norm: 0.0882833023256785\n",
      "Iteration 6585, BCE loss: 57.757443297237906, Acc: 0.8195, Grad norm: 0.08586274147428578\n",
      "Iteration 6586, BCE loss: 57.75740746786768, Acc: 0.8195, Grad norm: 0.0743539110968106\n",
      "Iteration 6587, BCE loss: 57.75744973600625, Acc: 0.8195, Grad norm: 0.08299449683449374\n",
      "Iteration 6588, BCE loss: 57.75741495463711, Acc: 0.8195, Grad norm: 0.07460206704224846\n",
      "Iteration 6589, BCE loss: 57.75743073664076, Acc: 0.8196, Grad norm: 0.08116902931679583\n",
      "Iteration 6590, BCE loss: 57.7575864978043, Acc: 0.8196, Grad norm: 0.11635791584129374\n",
      "Iteration 6591, BCE loss: 57.757730212505365, Acc: 0.8196, Grad norm: 0.143705017103176\n",
      "Iteration 6592, BCE loss: 57.75768288382831, Acc: 0.8196, Grad norm: 0.13488901846391402\n",
      "Iteration 6593, BCE loss: 57.75762263004857, Acc: 0.8196, Grad norm: 0.12403818211693819\n",
      "Iteration 6594, BCE loss: 57.75752961949071, Acc: 0.8196, Grad norm: 0.10438862456357507\n",
      "Iteration 6595, BCE loss: 57.75755901877109, Acc: 0.8196, Grad norm: 0.1092539367911982\n",
      "Iteration 6596, BCE loss: 57.757444695183935, Acc: 0.8196, Grad norm: 0.0844481975678164\n",
      "Iteration 6597, BCE loss: 57.757426716185535, Acc: 0.8196, Grad norm: 0.08108056647530953\n",
      "Iteration 6598, BCE loss: 57.75742707364155, Acc: 0.8195, Grad norm: 0.08366295948256502\n",
      "Iteration 6599, BCE loss: 57.757474300681885, Acc: 0.8195, Grad norm: 0.09444089503616679\n",
      "Iteration 6600, BCE loss: 57.75751601100846, Acc: 0.8196, Grad norm: 0.10407838086779453\n",
      "Iteration 6601, BCE loss: 57.75750427792303, Acc: 0.8196, Grad norm: 0.10389086550723603\n",
      "Iteration 6602, BCE loss: 57.7575031894172, Acc: 0.8196, Grad norm: 0.1034977435214492\n",
      "Iteration 6603, BCE loss: 57.75739354390429, Acc: 0.8196, Grad norm: 0.07565727290543485\n",
      "Iteration 6604, BCE loss: 57.75742485815001, Acc: 0.8196, Grad norm: 0.08624728373114084\n",
      "Iteration 6605, BCE loss: 57.75749296758082, Acc: 0.8195, Grad norm: 0.10208792800014452\n",
      "Iteration 6606, BCE loss: 57.757449455449496, Acc: 0.8195, Grad norm: 0.09129519140632258\n",
      "Iteration 6607, BCE loss: 57.75741937034509, Acc: 0.8195, Grad norm: 0.08339193041017047\n",
      "Iteration 6608, BCE loss: 57.75741286924732, Acc: 0.8196, Grad norm: 0.08247564434736085\n",
      "Iteration 6609, BCE loss: 57.757423693532886, Acc: 0.8196, Grad norm: 0.08605870226507145\n",
      "Iteration 6610, BCE loss: 57.757398761526915, Acc: 0.8196, Grad norm: 0.07952003664080592\n",
      "Iteration 6611, BCE loss: 57.75736786057167, Acc: 0.8196, Grad norm: 0.07036748358683945\n",
      "Iteration 6612, BCE loss: 57.75741929779407, Acc: 0.8195, Grad norm: 0.08680585344506483\n",
      "Iteration 6613, BCE loss: 57.75733407802828, Acc: 0.8195, Grad norm: 0.058492344939607165\n",
      "Iteration 6614, BCE loss: 57.75731687183659, Acc: 0.8195, Grad norm: 0.04917549980214593\n",
      "Iteration 6615, BCE loss: 57.75735675708771, Acc: 0.8195, Grad norm: 0.06388939124484519\n",
      "Iteration 6616, BCE loss: 57.7573599857061, Acc: 0.8195, Grad norm: 0.06466196224607913\n",
      "Iteration 6617, BCE loss: 57.757414979298375, Acc: 0.8195, Grad norm: 0.08043592254475108\n",
      "Iteration 6618, BCE loss: 57.75748462614331, Acc: 0.8194, Grad norm: 0.09818937406031344\n",
      "Iteration 6619, BCE loss: 57.75742395073151, Acc: 0.8195, Grad norm: 0.08254094055008493\n",
      "Iteration 6620, BCE loss: 57.75735215189566, Acc: 0.8195, Grad norm: 0.06116226804858792\n",
      "Iteration 6621, BCE loss: 57.757321640163966, Acc: 0.8195, Grad norm: 0.05114105290897136\n",
      "Iteration 6622, BCE loss: 57.75737202846185, Acc: 0.8195, Grad norm: 0.06822694391211175\n",
      "Iteration 6623, BCE loss: 57.75732273616147, Acc: 0.8195, Grad norm: 0.05116475700635225\n",
      "Iteration 6624, BCE loss: 57.75736359148235, Acc: 0.8195, Grad norm: 0.06420163535718423\n",
      "Iteration 6625, BCE loss: 57.75738971378919, Acc: 0.8195, Grad norm: 0.07289138993726813\n",
      "Iteration 6626, BCE loss: 57.75734471214426, Acc: 0.8195, Grad norm: 0.05796344214805384\n",
      "Iteration 6627, BCE loss: 57.75734620459765, Acc: 0.8195, Grad norm: 0.05634433296856158\n",
      "Iteration 6628, BCE loss: 57.75734532320741, Acc: 0.8195, Grad norm: 0.05485562991313032\n",
      "Iteration 6629, BCE loss: 57.757366047564034, Acc: 0.8195, Grad norm: 0.06325933472415847\n",
      "Iteration 6630, BCE loss: 57.757344446466895, Acc: 0.8195, Grad norm: 0.05760104883488077\n",
      "Iteration 6631, BCE loss: 57.75738891876197, Acc: 0.8195, Grad norm: 0.07327703489343551\n",
      "Iteration 6632, BCE loss: 57.75737733963269, Acc: 0.8195, Grad norm: 0.06846720564500519\n",
      "Iteration 6633, BCE loss: 57.757404713480256, Acc: 0.8195, Grad norm: 0.07674307213438902\n",
      "Iteration 6634, BCE loss: 57.7574150318293, Acc: 0.8195, Grad norm: 0.07877564170574422\n",
      "Iteration 6635, BCE loss: 57.75736457190476, Acc: 0.8195, Grad norm: 0.06304134524649664\n",
      "Iteration 6636, BCE loss: 57.75731412181014, Acc: 0.8195, Grad norm: 0.04718472485898169\n",
      "Iteration 6637, BCE loss: 57.757344832459545, Acc: 0.8195, Grad norm: 0.057961344532907154\n",
      "Iteration 6638, BCE loss: 57.757351366672665, Acc: 0.8195, Grad norm: 0.0577595443771639\n",
      "Iteration 6639, BCE loss: 57.757340795980014, Acc: 0.8195, Grad norm: 0.057720419974678805\n",
      "Iteration 6640, BCE loss: 57.7573357338033, Acc: 0.8195, Grad norm: 0.05415853830527026\n",
      "Iteration 6641, BCE loss: 57.75740218450919, Acc: 0.8196, Grad norm: 0.07569913573707547\n",
      "Iteration 6642, BCE loss: 57.757324667957946, Acc: 0.8195, Grad norm: 0.04974939235133548\n",
      "Iteration 6643, BCE loss: 57.757319636717014, Acc: 0.8195, Grad norm: 0.04663031142096708\n",
      "Iteration 6644, BCE loss: 57.75736861319034, Acc: 0.8196, Grad norm: 0.06739373063592967\n",
      "Iteration 6645, BCE loss: 57.75741359177232, Acc: 0.8196, Grad norm: 0.08004132016114712\n",
      "Iteration 6646, BCE loss: 57.757447496135455, Acc: 0.8196, Grad norm: 0.08887993943757734\n",
      "Iteration 6647, BCE loss: 57.75742186227335, Acc: 0.8196, Grad norm: 0.08360971110946297\n",
      "Iteration 6648, BCE loss: 57.75739213124777, Acc: 0.8196, Grad norm: 0.07469986579145677\n",
      "Iteration 6649, BCE loss: 57.757401769078726, Acc: 0.8196, Grad norm: 0.07805441139491974\n",
      "Iteration 6650, BCE loss: 57.75745031671669, Acc: 0.8196, Grad norm: 0.09180707660627005\n",
      "Iteration 6651, BCE loss: 57.75752507876534, Acc: 0.8196, Grad norm: 0.11004296457985911\n",
      "Iteration 6652, BCE loss: 57.75764807410891, Acc: 0.8196, Grad norm: 0.1332720973935435\n",
      "Iteration 6653, BCE loss: 57.75767688075646, Acc: 0.8196, Grad norm: 0.13901432010730197\n",
      "Iteration 6654, BCE loss: 57.75750907194285, Acc: 0.8196, Grad norm: 0.1035476812928689\n",
      "Iteration 6655, BCE loss: 57.75745607733066, Acc: 0.8196, Grad norm: 0.09327864889253845\n",
      "Iteration 6656, BCE loss: 57.75746436643992, Acc: 0.8196, Grad norm: 0.09625542868669808\n",
      "Iteration 6657, BCE loss: 57.75744939294543, Acc: 0.8196, Grad norm: 0.09160769787033442\n",
      "Iteration 6658, BCE loss: 57.75747793173532, Acc: 0.8196, Grad norm: 0.09874603133035599\n",
      "Iteration 6659, BCE loss: 57.757384832693205, Acc: 0.8196, Grad norm: 0.0729761199152333\n",
      "Iteration 6660, BCE loss: 57.75734841957832, Acc: 0.8196, Grad norm: 0.06140714331413275\n",
      "Iteration 6661, BCE loss: 57.75732261264387, Acc: 0.8196, Grad norm: 0.05020340417611211\n",
      "Iteration 6662, BCE loss: 57.75735171125443, Acc: 0.8196, Grad norm: 0.05929511276734575\n",
      "Iteration 6663, BCE loss: 57.75733408732843, Acc: 0.8196, Grad norm: 0.05127872155706818\n",
      "Iteration 6664, BCE loss: 57.75736877555304, Acc: 0.8196, Grad norm: 0.06203419135608426\n",
      "Iteration 6665, BCE loss: 57.757362882954986, Acc: 0.8196, Grad norm: 0.0615071497867791\n",
      "Iteration 6666, BCE loss: 57.757381818203214, Acc: 0.8196, Grad norm: 0.06761785322656415\n",
      "Iteration 6667, BCE loss: 57.75741036702226, Acc: 0.8196, Grad norm: 0.07675294413952542\n",
      "Iteration 6668, BCE loss: 57.757464974508935, Acc: 0.8196, Grad norm: 0.08928416848390532\n",
      "Iteration 6669, BCE loss: 57.75745571674647, Acc: 0.8196, Grad norm: 0.08636182854726827\n",
      "Iteration 6670, BCE loss: 57.75749462658594, Acc: 0.8196, Grad norm: 0.09647242205920839\n",
      "Iteration 6671, BCE loss: 57.75744955174035, Acc: 0.8196, Grad norm: 0.08421623854976867\n",
      "Iteration 6672, BCE loss: 57.75745009288768, Acc: 0.8196, Grad norm: 0.08187699745259275\n",
      "Iteration 6673, BCE loss: 57.75740401737082, Acc: 0.8196, Grad norm: 0.0718438317586275\n",
      "Iteration 6674, BCE loss: 57.757437509008184, Acc: 0.8196, Grad norm: 0.08012341261967017\n",
      "Iteration 6675, BCE loss: 57.75750246850019, Acc: 0.8196, Grad norm: 0.0995930229882955\n",
      "Iteration 6676, BCE loss: 57.757471466681864, Acc: 0.8196, Grad norm: 0.09165621389444636\n",
      "Iteration 6677, BCE loss: 57.75752370122038, Acc: 0.8196, Grad norm: 0.10077567293590198\n",
      "Iteration 6678, BCE loss: 57.75751592094396, Acc: 0.8196, Grad norm: 0.10152609739331135\n",
      "Iteration 6679, BCE loss: 57.75754569005127, Acc: 0.8196, Grad norm: 0.10716202278958187\n",
      "Iteration 6680, BCE loss: 57.75750567923375, Acc: 0.8196, Grad norm: 0.10150441739453268\n",
      "Iteration 6681, BCE loss: 57.757514552836064, Acc: 0.8196, Grad norm: 0.10519283440271963\n",
      "Iteration 6682, BCE loss: 57.757412358872564, Acc: 0.8196, Grad norm: 0.08156346437458949\n",
      "Iteration 6683, BCE loss: 57.75740774099518, Acc: 0.8196, Grad norm: 0.08068297745236132\n",
      "Iteration 6684, BCE loss: 57.75741623949738, Acc: 0.8196, Grad norm: 0.08432291708235494\n",
      "Iteration 6685, BCE loss: 57.75743736155033, Acc: 0.8196, Grad norm: 0.08994451835378245\n",
      "Iteration 6686, BCE loss: 57.7573773811626, Acc: 0.8196, Grad norm: 0.07217356534485692\n",
      "Iteration 6687, BCE loss: 57.75734387953306, Acc: 0.8196, Grad norm: 0.05767163901552028\n",
      "Iteration 6688, BCE loss: 57.7572924908907, Acc: 0.8196, Grad norm: 0.036095234211766206\n",
      "Iteration 6689, BCE loss: 57.7573022260062, Acc: 0.8195, Grad norm: 0.0404311327120409\n",
      "Iteration 6690, BCE loss: 57.757297475413104, Acc: 0.8195, Grad norm: 0.036560796853814054\n",
      "Iteration 6691, BCE loss: 57.75729719099164, Acc: 0.8195, Grad norm: 0.03845660779649823\n",
      "Iteration 6692, BCE loss: 57.75731169780495, Acc: 0.8195, Grad norm: 0.04503085670876629\n",
      "Iteration 6693, BCE loss: 57.75738128450874, Acc: 0.8196, Grad norm: 0.07050522205374297\n",
      "Iteration 6694, BCE loss: 57.75736990248089, Acc: 0.8196, Grad norm: 0.06554897200862062\n",
      "Iteration 6695, BCE loss: 57.757366142636876, Acc: 0.8196, Grad norm: 0.06337956373860502\n",
      "Iteration 6696, BCE loss: 57.75737657223928, Acc: 0.8196, Grad norm: 0.06662698893682036\n",
      "Iteration 6697, BCE loss: 57.757417709364795, Acc: 0.8196, Grad norm: 0.07802713851627642\n",
      "Iteration 6698, BCE loss: 57.757428945058905, Acc: 0.8195, Grad norm: 0.0817685995751702\n",
      "Iteration 6699, BCE loss: 57.75743821529905, Acc: 0.8195, Grad norm: 0.08436302321382218\n",
      "Iteration 6700, BCE loss: 57.75741657615052, Acc: 0.8196, Grad norm: 0.07812516440738189\n",
      "Iteration 6701, BCE loss: 57.75735734892878, Acc: 0.8195, Grad norm: 0.05900863042571785\n",
      "Iteration 6702, BCE loss: 57.75733381801817, Acc: 0.8195, Grad norm: 0.05103573410076183\n",
      "Iteration 6703, BCE loss: 57.757314446951256, Acc: 0.8196, Grad norm: 0.042795024485163057\n",
      "Iteration 6704, BCE loss: 57.75732779574467, Acc: 0.8196, Grad norm: 0.052043398809800905\n",
      "Iteration 6705, BCE loss: 57.75732136062205, Acc: 0.8196, Grad norm: 0.046642729943940316\n",
      "Iteration 6706, BCE loss: 57.75734717526857, Acc: 0.8196, Grad norm: 0.05846497590563021\n",
      "Iteration 6707, BCE loss: 57.757381794235, Acc: 0.8196, Grad norm: 0.06939875188766431\n",
      "Iteration 6708, BCE loss: 57.757399147644115, Acc: 0.8196, Grad norm: 0.07396524561709716\n",
      "Iteration 6709, BCE loss: 57.7574706174098, Acc: 0.8196, Grad norm: 0.09326909342190468\n",
      "Iteration 6710, BCE loss: 57.75739906453989, Acc: 0.8196, Grad norm: 0.07506945783444545\n",
      "Iteration 6711, BCE loss: 57.75738290571936, Acc: 0.8196, Grad norm: 0.0701124845001873\n",
      "Iteration 6712, BCE loss: 57.757387197218165, Acc: 0.8196, Grad norm: 0.07257627603346511\n",
      "Iteration 6713, BCE loss: 57.75739580687081, Acc: 0.8196, Grad norm: 0.07354039746355019\n",
      "Iteration 6714, BCE loss: 57.75738585074332, Acc: 0.8196, Grad norm: 0.06994958890966224\n",
      "Iteration 6715, BCE loss: 57.757314104990385, Acc: 0.8196, Grad norm: 0.0454567414533549\n",
      "Iteration 6716, BCE loss: 57.75731962281368, Acc: 0.8196, Grad norm: 0.047867235996714115\n",
      "Iteration 6717, BCE loss: 57.75733564930492, Acc: 0.8196, Grad norm: 0.05421753434040931\n",
      "Iteration 6718, BCE loss: 57.75733661342261, Acc: 0.8196, Grad norm: 0.054926706361341865\n",
      "Iteration 6719, BCE loss: 57.75736284668333, Acc: 0.8195, Grad norm: 0.061699711615270734\n",
      "Iteration 6720, BCE loss: 57.75734436871609, Acc: 0.8196, Grad norm: 0.05666154061685742\n",
      "Iteration 6721, BCE loss: 57.757347644894764, Acc: 0.8196, Grad norm: 0.05742038257055134\n",
      "Iteration 6722, BCE loss: 57.757345493980864, Acc: 0.8195, Grad norm: 0.055520734068904704\n",
      "Iteration 6723, BCE loss: 57.75739500221037, Acc: 0.8195, Grad norm: 0.07174870557261598\n",
      "Iteration 6724, BCE loss: 57.75735505906425, Acc: 0.8196, Grad norm: 0.060089205232871666\n",
      "Iteration 6725, BCE loss: 57.7573930770719, Acc: 0.8195, Grad norm: 0.07085298829853316\n",
      "Iteration 6726, BCE loss: 57.757361377461194, Acc: 0.8195, Grad norm: 0.06251958538535665\n",
      "Iteration 6727, BCE loss: 57.757333343749686, Acc: 0.8195, Grad norm: 0.05491647415226466\n",
      "Iteration 6728, BCE loss: 57.75734804735611, Acc: 0.8195, Grad norm: 0.06297133223685623\n",
      "Iteration 6729, BCE loss: 57.75735296196936, Acc: 0.8195, Grad norm: 0.06286834374914156\n",
      "Iteration 6730, BCE loss: 57.75736143409074, Acc: 0.8195, Grad norm: 0.06428198704370512\n",
      "Iteration 6731, BCE loss: 57.75733782203564, Acc: 0.8195, Grad norm: 0.05535264676827295\n",
      "Iteration 6732, BCE loss: 57.75733897483055, Acc: 0.8195, Grad norm: 0.05543929926547891\n",
      "Iteration 6733, BCE loss: 57.757331417827324, Acc: 0.8195, Grad norm: 0.05386507629641314\n",
      "Iteration 6734, BCE loss: 57.7573763594837, Acc: 0.8195, Grad norm: 0.06945681380426047\n",
      "Iteration 6735, BCE loss: 57.75737510025177, Acc: 0.8195, Grad norm: 0.06787727217928467\n",
      "Iteration 6736, BCE loss: 57.75731983320602, Acc: 0.8195, Grad norm: 0.04755952089085879\n",
      "Iteration 6737, BCE loss: 57.7573244770017, Acc: 0.8196, Grad norm: 0.04975071129127758\n",
      "Iteration 6738, BCE loss: 57.75738892699919, Acc: 0.8196, Grad norm: 0.07339414306223621\n",
      "Iteration 6739, BCE loss: 57.75742545255403, Acc: 0.8196, Grad norm: 0.08336246208528145\n",
      "Iteration 6740, BCE loss: 57.75747277392158, Acc: 0.8196, Grad norm: 0.09787521372066306\n",
      "Iteration 6741, BCE loss: 57.75751199519143, Acc: 0.8196, Grad norm: 0.10795867608797494\n",
      "Iteration 6742, BCE loss: 57.75759020508985, Acc: 0.8196, Grad norm: 0.1229309899804744\n",
      "Iteration 6743, BCE loss: 57.75755360038534, Acc: 0.8196, Grad norm: 0.11314510541644983\n",
      "Iteration 6744, BCE loss: 57.75750554688491, Acc: 0.8196, Grad norm: 0.1025643546360241\n",
      "Iteration 6745, BCE loss: 57.7574565597845, Acc: 0.8196, Grad norm: 0.09084068685392829\n",
      "Iteration 6746, BCE loss: 57.757391991428975, Acc: 0.8196, Grad norm: 0.07504520453838882\n",
      "Iteration 6747, BCE loss: 57.75737121823289, Acc: 0.8196, Grad norm: 0.06792859498761981\n",
      "Iteration 6748, BCE loss: 57.75741626939789, Acc: 0.8196, Grad norm: 0.08373182030624897\n",
      "Iteration 6749, BCE loss: 57.75743699878724, Acc: 0.8196, Grad norm: 0.08872411477284468\n",
      "Iteration 6750, BCE loss: 57.757475985759896, Acc: 0.8196, Grad norm: 0.09691510732624882\n",
      "Iteration 6751, BCE loss: 57.75741710609954, Acc: 0.8196, Grad norm: 0.08117702495809802\n",
      "Iteration 6752, BCE loss: 57.75734756195523, Acc: 0.8195, Grad norm: 0.057639445068729894\n",
      "Iteration 6753, BCE loss: 57.75737420067615, Acc: 0.8195, Grad norm: 0.06731004686366518\n",
      "Iteration 6754, BCE loss: 57.7573983010141, Acc: 0.8195, Grad norm: 0.07459240268112956\n",
      "Iteration 6755, BCE loss: 57.757414704325825, Acc: 0.8195, Grad norm: 0.07714012635617767\n",
      "Iteration 6756, BCE loss: 57.757378733558625, Acc: 0.8195, Grad norm: 0.06600073795076135\n",
      "Iteration 6757, BCE loss: 57.75733988897383, Acc: 0.8195, Grad norm: 0.05393871290824736\n",
      "Iteration 6758, BCE loss: 57.75732904632335, Acc: 0.8195, Grad norm: 0.04910148396837145\n",
      "Iteration 6759, BCE loss: 57.75732779941718, Acc: 0.8195, Grad norm: 0.04769673758499307\n",
      "Iteration 6760, BCE loss: 57.75730492971027, Acc: 0.8195, Grad norm: 0.03859972473568078\n",
      "Iteration 6761, BCE loss: 57.757322598087676, Acc: 0.8195, Grad norm: 0.046588768946144846\n",
      "Iteration 6762, BCE loss: 57.75732129246127, Acc: 0.8195, Grad norm: 0.04726248795098008\n",
      "Iteration 6763, BCE loss: 57.75739166509315, Acc: 0.8195, Grad norm: 0.07185229862483884\n",
      "Iteration 6764, BCE loss: 57.75735673359115, Acc: 0.8195, Grad norm: 0.06338709923232179\n",
      "Iteration 6765, BCE loss: 57.75734104338923, Acc: 0.8195, Grad norm: 0.05470925675408596\n",
      "Iteration 6766, BCE loss: 57.757379225438726, Acc: 0.8195, Grad norm: 0.07122713876611106\n",
      "Iteration 6767, BCE loss: 57.75743104787599, Acc: 0.8195, Grad norm: 0.08652826922532614\n",
      "Iteration 6768, BCE loss: 57.75737603733555, Acc: 0.8195, Grad norm: 0.07081560850806562\n",
      "Iteration 6769, BCE loss: 57.75738294305581, Acc: 0.8195, Grad norm: 0.0748093129280745\n",
      "Iteration 6770, BCE loss: 57.75744123610136, Acc: 0.8196, Grad norm: 0.09003381730534735\n",
      "Iteration 6771, BCE loss: 57.757424729678235, Acc: 0.8196, Grad norm: 0.08509928424148884\n",
      "Iteration 6772, BCE loss: 57.75742403870636, Acc: 0.8196, Grad norm: 0.08414371452522634\n",
      "Iteration 6773, BCE loss: 57.7574414136388, Acc: 0.8196, Grad norm: 0.08959211702337248\n",
      "Iteration 6774, BCE loss: 57.757539426731654, Acc: 0.8196, Grad norm: 0.11091373180601073\n",
      "Iteration 6775, BCE loss: 57.75753421088438, Acc: 0.8196, Grad norm: 0.10854219125459706\n",
      "Iteration 6776, BCE loss: 57.75755927882662, Acc: 0.8196, Grad norm: 0.11332158227000005\n",
      "Iteration 6777, BCE loss: 57.75748952652543, Acc: 0.8196, Grad norm: 0.10054873987730402\n",
      "Iteration 6778, BCE loss: 57.75735871238025, Acc: 0.8196, Grad norm: 0.061107445588437115\n",
      "Iteration 6779, BCE loss: 57.75730922217552, Acc: 0.8196, Grad norm: 0.03976261774933281\n",
      "Iteration 6780, BCE loss: 57.75734354216635, Acc: 0.8196, Grad norm: 0.05618709425480179\n",
      "Iteration 6781, BCE loss: 57.75741074128551, Acc: 0.8195, Grad norm: 0.08068720975486873\n",
      "Iteration 6782, BCE loss: 57.75743924629922, Acc: 0.8195, Grad norm: 0.09010859911318414\n",
      "Iteration 6783, BCE loss: 57.757329538883404, Acc: 0.8195, Grad norm: 0.052566066593774226\n",
      "Iteration 6784, BCE loss: 57.75732687185564, Acc: 0.8195, Grad norm: 0.046424078967114715\n",
      "Iteration 6785, BCE loss: 57.757325560547834, Acc: 0.8196, Grad norm: 0.04659368246461124\n",
      "Iteration 6786, BCE loss: 57.75732497895284, Acc: 0.8195, Grad norm: 0.047061195444041165\n",
      "Iteration 6787, BCE loss: 57.7573165641608, Acc: 0.8196, Grad norm: 0.04218483335886274\n",
      "Iteration 6788, BCE loss: 57.75733856628986, Acc: 0.8196, Grad norm: 0.05220014172380545\n",
      "Iteration 6789, BCE loss: 57.757380544604985, Acc: 0.8196, Grad norm: 0.06964075431896052\n",
      "Iteration 6790, BCE loss: 57.75736456186921, Acc: 0.8196, Grad norm: 0.0626493137405008\n",
      "Iteration 6791, BCE loss: 57.7573511826986, Acc: 0.8195, Grad norm: 0.057656452686277775\n",
      "Iteration 6792, BCE loss: 57.757367299696625, Acc: 0.8196, Grad norm: 0.06392793447530788\n",
      "Iteration 6793, BCE loss: 57.75732006839651, Acc: 0.8196, Grad norm: 0.04653946136791032\n",
      "Iteration 6794, BCE loss: 57.75734721146803, Acc: 0.8196, Grad norm: 0.05721750052504428\n",
      "Iteration 6795, BCE loss: 57.75736487376618, Acc: 0.8196, Grad norm: 0.06329854192370928\n",
      "Iteration 6796, BCE loss: 57.757381006785835, Acc: 0.8196, Grad norm: 0.06910897379724307\n",
      "Iteration 6797, BCE loss: 57.757370817230736, Acc: 0.8196, Grad norm: 0.06572613630372787\n",
      "Iteration 6798, BCE loss: 57.7573549604073, Acc: 0.8196, Grad norm: 0.05961032176923501\n",
      "Iteration 6799, BCE loss: 57.75731256703959, Acc: 0.8196, Grad norm: 0.04303768234235635\n",
      "Iteration 6800, BCE loss: 57.75732923163956, Acc: 0.8196, Grad norm: 0.052922368477142094\n",
      "Iteration 6801, BCE loss: 57.757342346064355, Acc: 0.8195, Grad norm: 0.06027517884625031\n",
      "Iteration 6802, BCE loss: 57.75730852285031, Acc: 0.8196, Grad norm: 0.04567232322663758\n",
      "Iteration 6803, BCE loss: 57.757371229256066, Acc: 0.8196, Grad norm: 0.07033167709757447\n",
      "Iteration 6804, BCE loss: 57.75733782729151, Acc: 0.8196, Grad norm: 0.0573309981767128\n",
      "Iteration 6805, BCE loss: 57.757316470224566, Acc: 0.8196, Grad norm: 0.050824092809521715\n",
      "Iteration 6806, BCE loss: 57.757299866049614, Acc: 0.8195, Grad norm: 0.043763492695449754\n",
      "Iteration 6807, BCE loss: 57.75731815051966, Acc: 0.8195, Grad norm: 0.05158581411583406\n",
      "Iteration 6808, BCE loss: 57.757331317646006, Acc: 0.8196, Grad norm: 0.05369053281663342\n",
      "Iteration 6809, BCE loss: 57.75730402569429, Acc: 0.8196, Grad norm: 0.039570563960681066\n",
      "Iteration 6810, BCE loss: 57.757323020793834, Acc: 0.8196, Grad norm: 0.04846623320685805\n",
      "Iteration 6811, BCE loss: 57.75734657490479, Acc: 0.8196, Grad norm: 0.05746072857764577\n",
      "Iteration 6812, BCE loss: 57.7573808019653, Acc: 0.8196, Grad norm: 0.06589549493280436\n",
      "Iteration 6813, BCE loss: 57.75740153306032, Acc: 0.8196, Grad norm: 0.07254615467825362\n",
      "Iteration 6814, BCE loss: 57.757396641881925, Acc: 0.8196, Grad norm: 0.06980292944237262\n",
      "Iteration 6815, BCE loss: 57.757397248586045, Acc: 0.8196, Grad norm: 0.07234630471546874\n",
      "Iteration 6816, BCE loss: 57.757388115429414, Acc: 0.8196, Grad norm: 0.06897984491583935\n",
      "Iteration 6817, BCE loss: 57.75739912776203, Acc: 0.8196, Grad norm: 0.07415062115039732\n",
      "Iteration 6818, BCE loss: 57.75741113162711, Acc: 0.8196, Grad norm: 0.08001017148738328\n",
      "Iteration 6819, BCE loss: 57.75736042375225, Acc: 0.8196, Grad norm: 0.06065875697364907\n",
      "Iteration 6820, BCE loss: 57.757363643035895, Acc: 0.8196, Grad norm: 0.06062282491475247\n",
      "Iteration 6821, BCE loss: 57.75741809900462, Acc: 0.8196, Grad norm: 0.07800671688686062\n",
      "Iteration 6822, BCE loss: 57.75749342058636, Acc: 0.8196, Grad norm: 0.09888553308030851\n",
      "Iteration 6823, BCE loss: 57.75749409589702, Acc: 0.8196, Grad norm: 0.09789179002421018\n",
      "Iteration 6824, BCE loss: 57.757460492498055, Acc: 0.8196, Grad norm: 0.09146039465471992\n",
      "Iteration 6825, BCE loss: 57.75745374203803, Acc: 0.8196, Grad norm: 0.09121650599790482\n",
      "Iteration 6826, BCE loss: 57.75738519533529, Acc: 0.8196, Grad norm: 0.07154143524080285\n",
      "Iteration 6827, BCE loss: 57.757346238362906, Acc: 0.8195, Grad norm: 0.05529613936440275\n",
      "Iteration 6828, BCE loss: 57.75743058438016, Acc: 0.8195, Grad norm: 0.08187482636586385\n",
      "Iteration 6829, BCE loss: 57.757426670420244, Acc: 0.8195, Grad norm: 0.08489605491148966\n",
      "Iteration 6830, BCE loss: 57.75737063633923, Acc: 0.8195, Grad norm: 0.06771956194398754\n",
      "Iteration 6831, BCE loss: 57.75743693124441, Acc: 0.8196, Grad norm: 0.08603883079136593\n",
      "Iteration 6832, BCE loss: 57.757504525456724, Acc: 0.8196, Grad norm: 0.10235285588409117\n",
      "Iteration 6833, BCE loss: 57.75748586308049, Acc: 0.8196, Grad norm: 0.09870523013983229\n",
      "Iteration 6834, BCE loss: 57.757455299214115, Acc: 0.8196, Grad norm: 0.09220803356573797\n",
      "Iteration 6835, BCE loss: 57.75741601873841, Acc: 0.8196, Grad norm: 0.0827306155282766\n",
      "Iteration 6836, BCE loss: 57.757438221607984, Acc: 0.8196, Grad norm: 0.0892464358041497\n",
      "Iteration 6837, BCE loss: 57.757357557298405, Acc: 0.8196, Grad norm: 0.06476905755278008\n",
      "Iteration 6838, BCE loss: 57.757298689510726, Acc: 0.8196, Grad norm: 0.03875641876355225\n",
      "Iteration 6839, BCE loss: 57.75732647559248, Acc: 0.8196, Grad norm: 0.05177264605261271\n",
      "Iteration 6840, BCE loss: 57.75729248352846, Acc: 0.8196, Grad norm: 0.03387635779701223\n",
      "Iteration 6841, BCE loss: 57.75729031664092, Acc: 0.8196, Grad norm: 0.03369800072972749\n",
      "Iteration 6842, BCE loss: 57.75731825986173, Acc: 0.8196, Grad norm: 0.049323613098530644\n",
      "Iteration 6843, BCE loss: 57.75732946998731, Acc: 0.8196, Grad norm: 0.05520384478519512\n",
      "Iteration 6844, BCE loss: 57.75735060072117, Acc: 0.8195, Grad norm: 0.06300681970860543\n",
      "Iteration 6845, BCE loss: 57.7573681916348, Acc: 0.8196, Grad norm: 0.06690672924397942\n",
      "Iteration 6846, BCE loss: 57.75733252682331, Acc: 0.8196, Grad norm: 0.05628879622764685\n",
      "Iteration 6847, BCE loss: 57.757317061663116, Acc: 0.8195, Grad norm: 0.0508276426810038\n",
      "Iteration 6848, BCE loss: 57.757305477945984, Acc: 0.8195, Grad norm: 0.04530727418674447\n",
      "Iteration 6849, BCE loss: 57.7572969008039, Acc: 0.8196, Grad norm: 0.03854811044714667\n",
      "Iteration 6850, BCE loss: 57.75731670910906, Acc: 0.8195, Grad norm: 0.04802399662777444\n",
      "Iteration 6851, BCE loss: 57.75734292911032, Acc: 0.8195, Grad norm: 0.058917331322195794\n",
      "Iteration 6852, BCE loss: 57.757372481986906, Acc: 0.8195, Grad norm: 0.07164659150529765\n",
      "Iteration 6853, BCE loss: 57.75742694844969, Acc: 0.8195, Grad norm: 0.08543501898944734\n",
      "Iteration 6854, BCE loss: 57.75745079994785, Acc: 0.8195, Grad norm: 0.09149745316533435\n",
      "Iteration 6855, BCE loss: 57.75752064800376, Acc: 0.8195, Grad norm: 0.10690911609649013\n",
      "Iteration 6856, BCE loss: 57.757476332850956, Acc: 0.8195, Grad norm: 0.09796085530020847\n",
      "Iteration 6857, BCE loss: 57.75752325560381, Acc: 0.8195, Grad norm: 0.10951921659898635\n",
      "Iteration 6858, BCE loss: 57.75744348147191, Acc: 0.8196, Grad norm: 0.08991374674927599\n",
      "Iteration 6859, BCE loss: 57.75735042805744, Acc: 0.8196, Grad norm: 0.06335802980829802\n",
      "Iteration 6860, BCE loss: 57.75733995477114, Acc: 0.8196, Grad norm: 0.05946695410034074\n",
      "Iteration 6861, BCE loss: 57.75732006091108, Acc: 0.8196, Grad norm: 0.04908717837524228\n",
      "Iteration 6862, BCE loss: 57.757319147914906, Acc: 0.8196, Grad norm: 0.04640649152766014\n",
      "Iteration 6863, BCE loss: 57.75732671271169, Acc: 0.8196, Grad norm: 0.05086345663635557\n",
      "Iteration 6864, BCE loss: 57.75737322577278, Acc: 0.8196, Grad norm: 0.06964523597695821\n",
      "Iteration 6865, BCE loss: 57.757417710795494, Acc: 0.8196, Grad norm: 0.08340176622142202\n",
      "Iteration 6866, BCE loss: 57.75738539282132, Acc: 0.8196, Grad norm: 0.07521672854102894\n",
      "Iteration 6867, BCE loss: 57.75737776833278, Acc: 0.8196, Grad norm: 0.0720634726031593\n",
      "Iteration 6868, BCE loss: 57.757323077231774, Acc: 0.8196, Grad norm: 0.051857722731454024\n",
      "Iteration 6869, BCE loss: 57.75731385696652, Acc: 0.8196, Grad norm: 0.04754575132064272\n",
      "Iteration 6870, BCE loss: 57.75733180027237, Acc: 0.8196, Grad norm: 0.057046750070671025\n",
      "Iteration 6871, BCE loss: 57.757304687457676, Acc: 0.8196, Grad norm: 0.04485917468326121\n",
      "Iteration 6872, BCE loss: 57.75729019139959, Acc: 0.8196, Grad norm: 0.03611250798448218\n",
      "Iteration 6873, BCE loss: 57.75729963961028, Acc: 0.8195, Grad norm: 0.04101575299466306\n",
      "Iteration 6874, BCE loss: 57.7573220174868, Acc: 0.8195, Grad norm: 0.04997420994888479\n",
      "Iteration 6875, BCE loss: 57.757367194923106, Acc: 0.8195, Grad norm: 0.06655674336726695\n",
      "Iteration 6876, BCE loss: 57.75739699527087, Acc: 0.8195, Grad norm: 0.0766022220498324\n",
      "Iteration 6877, BCE loss: 57.75739152067085, Acc: 0.8195, Grad norm: 0.07475088384107874\n",
      "Iteration 6878, BCE loss: 57.75733248305177, Acc: 0.8195, Grad norm: 0.05225961549694105\n",
      "Iteration 6879, BCE loss: 57.75730993198623, Acc: 0.8196, Grad norm: 0.04422195654972545\n",
      "Iteration 6880, BCE loss: 57.7573250880024, Acc: 0.8196, Grad norm: 0.051503426722371265\n",
      "Iteration 6881, BCE loss: 57.75735268661677, Acc: 0.8196, Grad norm: 0.063610164601884\n",
      "Iteration 6882, BCE loss: 57.75742439085574, Acc: 0.8195, Grad norm: 0.08551236178682832\n",
      "Iteration 6883, BCE loss: 57.75740740708896, Acc: 0.8195, Grad norm: 0.08220132343378042\n",
      "Iteration 6884, BCE loss: 57.75744808229824, Acc: 0.8195, Grad norm: 0.09286742506267448\n",
      "Iteration 6885, BCE loss: 57.75742038546699, Acc: 0.8195, Grad norm: 0.08511650446813711\n",
      "Iteration 6886, BCE loss: 57.7573544013497, Acc: 0.8196, Grad norm: 0.0623112988961686\n",
      "Iteration 6887, BCE loss: 57.7573457356217, Acc: 0.8196, Grad norm: 0.0566743148272014\n",
      "Iteration 6888, BCE loss: 57.75733446165883, Acc: 0.8196, Grad norm: 0.05048403924025741\n",
      "Iteration 6889, BCE loss: 57.757359582257976, Acc: 0.8196, Grad norm: 0.05958429620875497\n",
      "Iteration 6890, BCE loss: 57.75740290057935, Acc: 0.8196, Grad norm: 0.07418538833174909\n",
      "Iteration 6891, BCE loss: 57.75743416877269, Acc: 0.8196, Grad norm: 0.08192161111477547\n",
      "Iteration 6892, BCE loss: 57.757440219268375, Acc: 0.8196, Grad norm: 0.08367332398557005\n",
      "Iteration 6893, BCE loss: 57.75739604135325, Acc: 0.8196, Grad norm: 0.07240984996550165\n",
      "Iteration 6894, BCE loss: 57.757369688220905, Acc: 0.8196, Grad norm: 0.06493885906322192\n",
      "Iteration 6895, BCE loss: 57.75739680335374, Acc: 0.8196, Grad norm: 0.07088631893862307\n",
      "Iteration 6896, BCE loss: 57.757452612438556, Acc: 0.8196, Grad norm: 0.08206173086020606\n",
      "Iteration 6897, BCE loss: 57.757500915260216, Acc: 0.8196, Grad norm: 0.09169075669461968\n",
      "Iteration 6898, BCE loss: 57.75749349935995, Acc: 0.8196, Grad norm: 0.09084044723953483\n",
      "Iteration 6899, BCE loss: 57.75744938544241, Acc: 0.8196, Grad norm: 0.08066401303590755\n",
      "Iteration 6900, BCE loss: 57.7574699461634, Acc: 0.8196, Grad norm: 0.08542787548262368\n",
      "Iteration 6901, BCE loss: 57.7574437633416, Acc: 0.8196, Grad norm: 0.0810763722319434\n",
      "Iteration 6902, BCE loss: 57.757402450921916, Acc: 0.8196, Grad norm: 0.0728119377554848\n",
      "Iteration 6903, BCE loss: 57.757433561548794, Acc: 0.8196, Grad norm: 0.08305346422975582\n",
      "Iteration 6904, BCE loss: 57.7574149418062, Acc: 0.8196, Grad norm: 0.07910332375274738\n",
      "Iteration 6905, BCE loss: 57.7574076898422, Acc: 0.8196, Grad norm: 0.0770877564244015\n",
      "Iteration 6906, BCE loss: 57.75739061508713, Acc: 0.8196, Grad norm: 0.07395046866161592\n",
      "Iteration 6907, BCE loss: 57.75742725849028, Acc: 0.8196, Grad norm: 0.08491073123902836\n",
      "Iteration 6908, BCE loss: 57.75735752671563, Acc: 0.8196, Grad norm: 0.06394219367727381\n",
      "Iteration 6909, BCE loss: 57.75737848849016, Acc: 0.8196, Grad norm: 0.07192570492011566\n",
      "Iteration 6910, BCE loss: 57.75740246293741, Acc: 0.8196, Grad norm: 0.081506988402494\n",
      "Iteration 6911, BCE loss: 57.757428506093234, Acc: 0.8196, Grad norm: 0.08783709187098598\n",
      "Iteration 6912, BCE loss: 57.75747369590894, Acc: 0.8196, Grad norm: 0.09986253182662884\n",
      "Iteration 6913, BCE loss: 57.75752035270975, Acc: 0.8196, Grad norm: 0.11083624652939705\n",
      "Iteration 6914, BCE loss: 57.75742596921877, Acc: 0.8196, Grad norm: 0.08910717628161363\n",
      "Iteration 6915, BCE loss: 57.757457503136685, Acc: 0.8196, Grad norm: 0.09720306094618365\n",
      "Iteration 6916, BCE loss: 57.75747941853807, Acc: 0.8196, Grad norm: 0.10215246892933984\n",
      "Iteration 6917, BCE loss: 57.7574692439628, Acc: 0.8196, Grad norm: 0.09887578160041552\n",
      "Iteration 6918, BCE loss: 57.75745623221887, Acc: 0.8196, Grad norm: 0.09493942109420313\n",
      "Iteration 6919, BCE loss: 57.75737650600544, Acc: 0.8196, Grad norm: 0.07261760377794682\n",
      "Iteration 6920, BCE loss: 57.75740235844222, Acc: 0.8196, Grad norm: 0.08148927357563944\n",
      "Iteration 6921, BCE loss: 57.75735141150855, Acc: 0.8196, Grad norm: 0.06480287296163005\n",
      "Iteration 6922, BCE loss: 57.75735276039663, Acc: 0.8196, Grad norm: 0.06338266865282362\n",
      "Iteration 6923, BCE loss: 57.757347146307765, Acc: 0.8195, Grad norm: 0.05928784986653922\n",
      "Iteration 6924, BCE loss: 57.75735088530682, Acc: 0.8196, Grad norm: 0.060177428607686004\n",
      "Iteration 6925, BCE loss: 57.75732856054991, Acc: 0.8196, Grad norm: 0.04988292548960277\n",
      "Iteration 6926, BCE loss: 57.7574125523758, Acc: 0.8196, Grad norm: 0.07939103205250657\n",
      "Iteration 6927, BCE loss: 57.75743013046656, Acc: 0.8196, Grad norm: 0.0849691070272726\n",
      "Iteration 6928, BCE loss: 57.757510570288076, Acc: 0.8196, Grad norm: 0.10542311601649927\n",
      "Iteration 6929, BCE loss: 57.757494253723266, Acc: 0.8196, Grad norm: 0.10057757503510144\n",
      "Iteration 6930, BCE loss: 57.757477727039664, Acc: 0.8196, Grad norm: 0.09713392772197986\n",
      "Iteration 6931, BCE loss: 57.75743332692667, Acc: 0.8196, Grad norm: 0.08709556075398502\n",
      "Iteration 6932, BCE loss: 57.75739707536949, Acc: 0.8196, Grad norm: 0.07595461423147183\n",
      "Iteration 6933, BCE loss: 57.757336791157655, Acc: 0.8196, Grad norm: 0.05572164330742978\n",
      "Iteration 6934, BCE loss: 57.757327220971106, Acc: 0.8196, Grad norm: 0.04941004947295036\n",
      "Iteration 6935, BCE loss: 57.7573462746586, Acc: 0.8196, Grad norm: 0.05321848644069612\n",
      "Iteration 6936, BCE loss: 57.75735962418513, Acc: 0.8196, Grad norm: 0.059190923915488004\n",
      "Iteration 6937, BCE loss: 57.75736000289963, Acc: 0.8196, Grad norm: 0.05827592381329418\n",
      "Iteration 6938, BCE loss: 57.7573740761254, Acc: 0.8196, Grad norm: 0.06509020131391544\n",
      "Iteration 6939, BCE loss: 57.75743139437111, Acc: 0.8196, Grad norm: 0.08385642817424843\n",
      "Iteration 6940, BCE loss: 57.75744440233555, Acc: 0.8196, Grad norm: 0.08811678040285785\n",
      "Iteration 6941, BCE loss: 57.75739274288478, Acc: 0.8196, Grad norm: 0.0717946679449804\n",
      "Iteration 6942, BCE loss: 57.75742199474733, Acc: 0.8196, Grad norm: 0.07864794489357699\n",
      "Iteration 6943, BCE loss: 57.75739551019712, Acc: 0.8196, Grad norm: 0.07062523453923007\n",
      "Iteration 6944, BCE loss: 57.75741469596646, Acc: 0.8196, Grad norm: 0.0741221326585467\n",
      "Iteration 6945, BCE loss: 57.75737418401149, Acc: 0.8196, Grad norm: 0.06298569482584043\n",
      "Iteration 6946, BCE loss: 57.75736973499808, Acc: 0.8196, Grad norm: 0.06231314496710212\n",
      "Iteration 6947, BCE loss: 57.75735538114742, Acc: 0.8196, Grad norm: 0.05625122029069626\n",
      "Iteration 6948, BCE loss: 57.75736243454406, Acc: 0.8195, Grad norm: 0.06033187928787871\n",
      "Iteration 6949, BCE loss: 57.757356626266215, Acc: 0.8196, Grad norm: 0.05782809318141677\n",
      "Iteration 6950, BCE loss: 57.75737809477738, Acc: 0.8196, Grad norm: 0.06391824688754522\n",
      "Iteration 6951, BCE loss: 57.75738581954239, Acc: 0.8196, Grad norm: 0.07074061701062224\n",
      "Iteration 6952, BCE loss: 57.75740875130387, Acc: 0.8195, Grad norm: 0.07605991497907891\n",
      "Iteration 6953, BCE loss: 57.757350504921206, Acc: 0.8196, Grad norm: 0.05980133126517589\n",
      "Iteration 6954, BCE loss: 57.75734184321409, Acc: 0.8195, Grad norm: 0.05398324598476589\n",
      "Iteration 6955, BCE loss: 57.75728905409824, Acc: 0.8195, Grad norm: 0.03295894398590158\n",
      "Iteration 6956, BCE loss: 57.75729025798826, Acc: 0.8195, Grad norm: 0.034215046092289154\n",
      "Iteration 6957, BCE loss: 57.75729196028569, Acc: 0.8195, Grad norm: 0.034485420889868834\n",
      "Iteration 6958, BCE loss: 57.75730220448419, Acc: 0.8195, Grad norm: 0.04090505816469691\n",
      "Iteration 6959, BCE loss: 57.75729746801143, Acc: 0.8195, Grad norm: 0.038407742303225606\n",
      "Iteration 6960, BCE loss: 57.75730773849635, Acc: 0.8195, Grad norm: 0.04535062561243983\n",
      "Iteration 6961, BCE loss: 57.757348455549575, Acc: 0.8195, Grad norm: 0.0632760591498457\n",
      "Iteration 6962, BCE loss: 57.75733860745356, Acc: 0.8196, Grad norm: 0.05878756710676934\n",
      "Iteration 6963, BCE loss: 57.7573529599354, Acc: 0.8196, Grad norm: 0.0651147985306246\n",
      "Iteration 6964, BCE loss: 57.75734153300601, Acc: 0.8196, Grad norm: 0.059041460118811534\n",
      "Iteration 6965, BCE loss: 57.75734257972185, Acc: 0.8196, Grad norm: 0.057662705884353085\n",
      "Iteration 6966, BCE loss: 57.75733649343973, Acc: 0.8196, Grad norm: 0.0554302898588383\n",
      "Iteration 6967, BCE loss: 57.75732321757005, Acc: 0.8196, Grad norm: 0.0522158058771479\n",
      "Iteration 6968, BCE loss: 57.75733096572826, Acc: 0.8196, Grad norm: 0.055015365358917566\n",
      "Iteration 6969, BCE loss: 57.75737191052697, Acc: 0.8196, Grad norm: 0.0707305046544908\n",
      "Iteration 6970, BCE loss: 57.75738966943326, Acc: 0.8196, Grad norm: 0.07820223568442569\n",
      "Iteration 6971, BCE loss: 57.75736770056652, Acc: 0.8196, Grad norm: 0.07101535809001001\n",
      "Iteration 6972, BCE loss: 57.75734552388388, Acc: 0.8196, Grad norm: 0.06206214987512065\n",
      "Iteration 6973, BCE loss: 57.757329627679766, Acc: 0.8196, Grad norm: 0.05414291909392633\n",
      "Iteration 6974, BCE loss: 57.75732078033177, Acc: 0.8196, Grad norm: 0.05025254161826966\n",
      "Iteration 6975, BCE loss: 57.75738141511643, Acc: 0.8196, Grad norm: 0.07150983356750326\n",
      "Iteration 6976, BCE loss: 57.757320476764995, Acc: 0.8196, Grad norm: 0.0496218095489695\n",
      "Iteration 6977, BCE loss: 57.75732620974672, Acc: 0.8195, Grad norm: 0.0510027479983218\n",
      "Iteration 6978, BCE loss: 57.7573023279128, Acc: 0.8196, Grad norm: 0.04028901182546821\n",
      "Iteration 6979, BCE loss: 57.757293190964376, Acc: 0.8195, Grad norm: 0.0388650265338357\n",
      "Iteration 6980, BCE loss: 57.757283456973965, Acc: 0.8195, Grad norm: 0.03133199693511476\n",
      "Iteration 6981, BCE loss: 57.757309342310165, Acc: 0.8195, Grad norm: 0.04717912084347989\n",
      "Iteration 6982, BCE loss: 57.75729643537027, Acc: 0.8195, Grad norm: 0.04115303243943365\n",
      "Iteration 6983, BCE loss: 57.75729974479715, Acc: 0.8195, Grad norm: 0.043045211483969614\n",
      "Iteration 6984, BCE loss: 57.757305767520364, Acc: 0.8195, Grad norm: 0.04609179645274675\n",
      "Iteration 6985, BCE loss: 57.75729097200281, Acc: 0.8195, Grad norm: 0.03714374928391477\n",
      "Iteration 6986, BCE loss: 57.75730785503468, Acc: 0.8195, Grad norm: 0.04578438814816952\n",
      "Iteration 6987, BCE loss: 57.75730377505403, Acc: 0.8195, Grad norm: 0.041922051966474666\n",
      "Iteration 6988, BCE loss: 57.75728692469198, Acc: 0.8195, Grad norm: 0.031910794740319555\n",
      "Iteration 6989, BCE loss: 57.75729248978806, Acc: 0.8195, Grad norm: 0.03498260986264934\n",
      "Iteration 6990, BCE loss: 57.75727864248333, Acc: 0.8195, Grad norm: 0.027226450856016323\n",
      "Iteration 6991, BCE loss: 57.75729712892701, Acc: 0.8195, Grad norm: 0.03908363605498597\n",
      "Iteration 6992, BCE loss: 57.757313063938554, Acc: 0.8195, Grad norm: 0.048192301240232355\n",
      "Iteration 6993, BCE loss: 57.757328536315, Acc: 0.8195, Grad norm: 0.05470228813867336\n",
      "Iteration 6994, BCE loss: 57.75731817777014, Acc: 0.8195, Grad norm: 0.048553775674066546\n",
      "Iteration 6995, BCE loss: 57.757285708158825, Acc: 0.8196, Grad norm: 0.03169291065624572\n",
      "Iteration 6996, BCE loss: 57.757316499339964, Acc: 0.8195, Grad norm: 0.049012117229196815\n",
      "Iteration 6997, BCE loss: 57.7573057348488, Acc: 0.8195, Grad norm: 0.043440638565917275\n",
      "Iteration 6998, BCE loss: 57.757344119365285, Acc: 0.8195, Grad norm: 0.05841376374554088\n",
      "Iteration 6999, BCE loss: 57.7573347613423, Acc: 0.8195, Grad norm: 0.053883132595521355\n",
      "Iteration 7000, BCE loss: 57.7573807687931, Acc: 0.8196, Grad norm: 0.0720634355919323\n",
      "Iteration 7001, BCE loss: 57.757336944338974, Acc: 0.8195, Grad norm: 0.0561411465614365\n",
      "Iteration 7002, BCE loss: 57.7573557556252, Acc: 0.8195, Grad norm: 0.059461068800709574\n",
      "Iteration 7003, BCE loss: 57.7573722667221, Acc: 0.8195, Grad norm: 0.06538122626627409\n",
      "Iteration 7004, BCE loss: 57.757326189905505, Acc: 0.8195, Grad norm: 0.048918112426915905\n",
      "Iteration 7005, BCE loss: 57.75735028848204, Acc: 0.8195, Grad norm: 0.061337700871669246\n",
      "Iteration 7006, BCE loss: 57.75728996344604, Acc: 0.8195, Grad norm: 0.033038851681833924\n",
      "Iteration 7007, BCE loss: 57.75730804690639, Acc: 0.8195, Grad norm: 0.042316048500049906\n",
      "Iteration 7008, BCE loss: 57.757328919507444, Acc: 0.8195, Grad norm: 0.05234606320005028\n",
      "Iteration 7009, BCE loss: 57.757367849202694, Acc: 0.8195, Grad norm: 0.0652427826064155\n",
      "Iteration 7010, BCE loss: 57.7573518159507, Acc: 0.8195, Grad norm: 0.058212586285081275\n",
      "Iteration 7011, BCE loss: 57.75737936626627, Acc: 0.8195, Grad norm: 0.06698569608354607\n",
      "Iteration 7012, BCE loss: 57.75739372155728, Acc: 0.8195, Grad norm: 0.07125948238015534\n",
      "Iteration 7013, BCE loss: 57.75738594825349, Acc: 0.8195, Grad norm: 0.06989741425891122\n",
      "Iteration 7014, BCE loss: 57.75743438311394, Acc: 0.8195, Grad norm: 0.08333824753877005\n",
      "Iteration 7015, BCE loss: 57.7574765933207, Acc: 0.8194, Grad norm: 0.09543834243469254\n",
      "Iteration 7016, BCE loss: 57.757454224762924, Acc: 0.8195, Grad norm: 0.0913998034189394\n",
      "Iteration 7017, BCE loss: 57.757399101396175, Acc: 0.8195, Grad norm: 0.07466430736924766\n",
      "Iteration 7018, BCE loss: 57.75737570720766, Acc: 0.8195, Grad norm: 0.06781056484553341\n",
      "Iteration 7019, BCE loss: 57.757360429746626, Acc: 0.8195, Grad norm: 0.06166471426324872\n",
      "Iteration 7020, BCE loss: 57.757409952979486, Acc: 0.8195, Grad norm: 0.07640424361818425\n",
      "Iteration 7021, BCE loss: 57.757403658471084, Acc: 0.8195, Grad norm: 0.07523238906098757\n",
      "Iteration 7022, BCE loss: 57.75735762041623, Acc: 0.8195, Grad norm: 0.0626909608667204\n",
      "Iteration 7023, BCE loss: 57.75738341220648, Acc: 0.8195, Grad norm: 0.07031785258392989\n",
      "Iteration 7024, BCE loss: 57.75737678772828, Acc: 0.8195, Grad norm: 0.06476584044089223\n",
      "Iteration 7025, BCE loss: 57.757352010845594, Acc: 0.8195, Grad norm: 0.05873612932565359\n",
      "Iteration 7026, BCE loss: 57.75736058900164, Acc: 0.8195, Grad norm: 0.061576787296488904\n",
      "Iteration 7027, BCE loss: 57.75739321969688, Acc: 0.8195, Grad norm: 0.07444365649152122\n",
      "Iteration 7028, BCE loss: 57.75744158244596, Acc: 0.8195, Grad norm: 0.08650558663883046\n",
      "Iteration 7029, BCE loss: 57.75739597435658, Acc: 0.8195, Grad norm: 0.07297050845609841\n",
      "Iteration 7030, BCE loss: 57.75739566790541, Acc: 0.8195, Grad norm: 0.07214933430464732\n",
      "Iteration 7031, BCE loss: 57.757369670397495, Acc: 0.8195, Grad norm: 0.06304135338458314\n",
      "Iteration 7032, BCE loss: 57.75733496812438, Acc: 0.8195, Grad norm: 0.050648533010603354\n",
      "Iteration 7033, BCE loss: 57.75733415024628, Acc: 0.8195, Grad norm: 0.05159790428839655\n",
      "Iteration 7034, BCE loss: 57.75733605454904, Acc: 0.8195, Grad norm: 0.054046736207614965\n",
      "Iteration 7035, BCE loss: 57.757399093586514, Acc: 0.8195, Grad norm: 0.07389096364893022\n",
      "Iteration 7036, BCE loss: 57.75736691294054, Acc: 0.8195, Grad norm: 0.06769315976975768\n",
      "Iteration 7037, BCE loss: 57.7573463559615, Acc: 0.8195, Grad norm: 0.06138127294660414\n",
      "Iteration 7038, BCE loss: 57.75739573641271, Acc: 0.8195, Grad norm: 0.07642835365926416\n",
      "Iteration 7039, BCE loss: 57.7573506574851, Acc: 0.8195, Grad norm: 0.06297229852149477\n",
      "Iteration 7040, BCE loss: 57.75736708524099, Acc: 0.8195, Grad norm: 0.06817762188094965\n",
      "Iteration 7041, BCE loss: 57.75737864708175, Acc: 0.8195, Grad norm: 0.0718263384381971\n",
      "Iteration 7042, BCE loss: 57.75740441952276, Acc: 0.8195, Grad norm: 0.08084290117626343\n",
      "Iteration 7043, BCE loss: 57.757332137941134, Acc: 0.8195, Grad norm: 0.05689139846901235\n",
      "Iteration 7044, BCE loss: 57.75731499980202, Acc: 0.8195, Grad norm: 0.04917286531509123\n",
      "Iteration 7045, BCE loss: 57.75731236702721, Acc: 0.8195, Grad norm: 0.04518081157848729\n",
      "Iteration 7046, BCE loss: 57.75731114787704, Acc: 0.8196, Grad norm: 0.04508995223433408\n",
      "Iteration 7047, BCE loss: 57.757363849910874, Acc: 0.8195, Grad norm: 0.0674183925220417\n",
      "Iteration 7048, BCE loss: 57.75741862517365, Acc: 0.8195, Grad norm: 0.08433328618864838\n",
      "Iteration 7049, BCE loss: 57.757439298454436, Acc: 0.8195, Grad norm: 0.08987933622411964\n",
      "Iteration 7050, BCE loss: 57.757468462559586, Acc: 0.8195, Grad norm: 0.09803108761111685\n",
      "Iteration 7051, BCE loss: 57.75743990772137, Acc: 0.8195, Grad norm: 0.09067421674434015\n",
      "Iteration 7052, BCE loss: 57.75743827776981, Acc: 0.8195, Grad norm: 0.0908259054124039\n",
      "Iteration 7053, BCE loss: 57.75740072437715, Acc: 0.8195, Grad norm: 0.08001911614148624\n",
      "Iteration 7054, BCE loss: 57.7573830761724, Acc: 0.8195, Grad norm: 0.07247821844202756\n",
      "Iteration 7055, BCE loss: 57.757331367155345, Acc: 0.8195, Grad norm: 0.054048190529085105\n",
      "Iteration 7056, BCE loss: 57.75736385848519, Acc: 0.8195, Grad norm: 0.06758158718358702\n",
      "Iteration 7057, BCE loss: 57.757325159413284, Acc: 0.8195, Grad norm: 0.05371222504107335\n",
      "Iteration 7058, BCE loss: 57.757298513155156, Acc: 0.8195, Grad norm: 0.04134249204843377\n",
      "Iteration 7059, BCE loss: 57.75729551062953, Acc: 0.8196, Grad norm: 0.0375216983055953\n",
      "Iteration 7060, BCE loss: 57.757324229355746, Acc: 0.8196, Grad norm: 0.052016433448537264\n",
      "Iteration 7061, BCE loss: 57.75730059590526, Acc: 0.8196, Grad norm: 0.04051912668212974\n",
      "Iteration 7062, BCE loss: 57.75732848742808, Acc: 0.8196, Grad norm: 0.05264628208259678\n",
      "Iteration 7063, BCE loss: 57.75730859683903, Acc: 0.8196, Grad norm: 0.04515000260403947\n",
      "Iteration 7064, BCE loss: 57.757327115095976, Acc: 0.8196, Grad norm: 0.053467077717389136\n",
      "Iteration 7065, BCE loss: 57.75730630124474, Acc: 0.8196, Grad norm: 0.044886568362411096\n",
      "Iteration 7066, BCE loss: 57.757357473046326, Acc: 0.8195, Grad norm: 0.06863414477073805\n",
      "Iteration 7067, BCE loss: 57.75730865641991, Acc: 0.8195, Grad norm: 0.04739353202937903\n",
      "Iteration 7068, BCE loss: 57.757303425358614, Acc: 0.8196, Grad norm: 0.04487023118911111\n",
      "Iteration 7069, BCE loss: 57.75730312627635, Acc: 0.8196, Grad norm: 0.044156953130952255\n",
      "Iteration 7070, BCE loss: 57.757295138143064, Acc: 0.8195, Grad norm: 0.03861380873630322\n",
      "Iteration 7071, BCE loss: 57.75729978342068, Acc: 0.8195, Grad norm: 0.04042602890445803\n",
      "Iteration 7072, BCE loss: 57.75730454715148, Acc: 0.8195, Grad norm: 0.043219356552692115\n",
      "Iteration 7073, BCE loss: 57.75731307305666, Acc: 0.8195, Grad norm: 0.04686637828035578\n",
      "Iteration 7074, BCE loss: 57.75730959962728, Acc: 0.8195, Grad norm: 0.04595464213472423\n",
      "Iteration 7075, BCE loss: 57.75736242046716, Acc: 0.8196, Grad norm: 0.06887592847264926\n",
      "Iteration 7076, BCE loss: 57.757327779054094, Acc: 0.8195, Grad norm: 0.05403035332115198\n",
      "Iteration 7077, BCE loss: 57.757318496851596, Acc: 0.8195, Grad norm: 0.05050647284277575\n",
      "Iteration 7078, BCE loss: 57.757286313005224, Acc: 0.8196, Grad norm: 0.035516523549185386\n",
      "Iteration 7079, BCE loss: 57.75728939427991, Acc: 0.8195, Grad norm: 0.03415455812252445\n",
      "Iteration 7080, BCE loss: 57.7572816063598, Acc: 0.8195, Grad norm: 0.02788402464245264\n",
      "Iteration 7081, BCE loss: 57.757305752767145, Acc: 0.8195, Grad norm: 0.04152234305154654\n",
      "Iteration 7082, BCE loss: 57.75732647198599, Acc: 0.8195, Grad norm: 0.05025297865846277\n",
      "Iteration 7083, BCE loss: 57.75733035691671, Acc: 0.8195, Grad norm: 0.052943794552757474\n",
      "Iteration 7084, BCE loss: 57.75739583164072, Acc: 0.8195, Grad norm: 0.07605515447550039\n",
      "Iteration 7085, BCE loss: 57.75735326603515, Acc: 0.8195, Grad norm: 0.06272872190973251\n",
      "Iteration 7086, BCE loss: 57.757294278115864, Acc: 0.8195, Grad norm: 0.03786077483444901\n",
      "Iteration 7087, BCE loss: 57.75729401133328, Acc: 0.8195, Grad norm: 0.03748015017743096\n",
      "Iteration 7088, BCE loss: 57.75729325878052, Acc: 0.8195, Grad norm: 0.03658388152079138\n",
      "Iteration 7089, BCE loss: 57.75731023081131, Acc: 0.8195, Grad norm: 0.044439673673915506\n",
      "Iteration 7090, BCE loss: 57.75730403950103, Acc: 0.8195, Grad norm: 0.04155930281045341\n",
      "Iteration 7091, BCE loss: 57.757312720909106, Acc: 0.8195, Grad norm: 0.04597873515626473\n",
      "Iteration 7092, BCE loss: 57.757378289963725, Acc: 0.8196, Grad norm: 0.06901590312319272\n",
      "Iteration 7093, BCE loss: 57.757380114227345, Acc: 0.8196, Grad norm: 0.07010083209161024\n",
      "Iteration 7094, BCE loss: 57.757336584997915, Acc: 0.8196, Grad norm: 0.055089305902966026\n",
      "Iteration 7095, BCE loss: 57.757341197588964, Acc: 0.8196, Grad norm: 0.056491797149911434\n",
      "Iteration 7096, BCE loss: 57.757322790215696, Acc: 0.8196, Grad norm: 0.04882831212640311\n",
      "Iteration 7097, BCE loss: 57.75736792180071, Acc: 0.8196, Grad norm: 0.06382232017007627\n",
      "Iteration 7098, BCE loss: 57.757355140254234, Acc: 0.8196, Grad norm: 0.060769038592143824\n",
      "Iteration 7099, BCE loss: 57.757336723210614, Acc: 0.8196, Grad norm: 0.05428820363690933\n",
      "Iteration 7100, BCE loss: 57.75730033311379, Acc: 0.8196, Grad norm: 0.038175426390859406\n",
      "Iteration 7101, BCE loss: 57.757342916136835, Acc: 0.8196, Grad norm: 0.05738112010707325\n",
      "Iteration 7102, BCE loss: 57.75738770795156, Acc: 0.8196, Grad norm: 0.07224002258957926\n",
      "Iteration 7103, BCE loss: 57.75738444479388, Acc: 0.8196, Grad norm: 0.07264243675144395\n",
      "Iteration 7104, BCE loss: 57.75736791216512, Acc: 0.8196, Grad norm: 0.06771691311748645\n",
      "Iteration 7105, BCE loss: 57.75740295074999, Acc: 0.8196, Grad norm: 0.07790738154754269\n",
      "Iteration 7106, BCE loss: 57.75736457868247, Acc: 0.8196, Grad norm: 0.0654626192719685\n",
      "Iteration 7107, BCE loss: 57.75732757116586, Acc: 0.8195, Grad norm: 0.05414920547154264\n",
      "Iteration 7108, BCE loss: 57.757366789803925, Acc: 0.8195, Grad norm: 0.06759897654083466\n",
      "Iteration 7109, BCE loss: 57.75741005249373, Acc: 0.8195, Grad norm: 0.07972610756851733\n",
      "Iteration 7110, BCE loss: 57.75747132428481, Acc: 0.8195, Grad norm: 0.09755962199833777\n",
      "Iteration 7111, BCE loss: 57.75742183574965, Acc: 0.8195, Grad norm: 0.08553789498494879\n",
      "Iteration 7112, BCE loss: 57.7573960107386, Acc: 0.8195, Grad norm: 0.0784894235332777\n",
      "Iteration 7113, BCE loss: 57.75738554159122, Acc: 0.8195, Grad norm: 0.07526472075163368\n",
      "Iteration 7114, BCE loss: 57.75734743944005, Acc: 0.8195, Grad norm: 0.06111260859235864\n",
      "Iteration 7115, BCE loss: 57.757398146424194, Acc: 0.8195, Grad norm: 0.07688399341913302\n",
      "Iteration 7116, BCE loss: 57.75735703294375, Acc: 0.8195, Grad norm: 0.0646556050527559\n",
      "Iteration 7117, BCE loss: 57.757344019523885, Acc: 0.8195, Grad norm: 0.056806505544648576\n",
      "Iteration 7118, BCE loss: 57.757355627597576, Acc: 0.8195, Grad norm: 0.06220338346605833\n",
      "Iteration 7119, BCE loss: 57.75739909165284, Acc: 0.8195, Grad norm: 0.07568661130864511\n",
      "Iteration 7120, BCE loss: 57.75734006577244, Acc: 0.8195, Grad norm: 0.05466077171990716\n",
      "Iteration 7121, BCE loss: 57.75737682592736, Acc: 0.8195, Grad norm: 0.06647172011659536\n",
      "Iteration 7122, BCE loss: 57.75737500674913, Acc: 0.8195, Grad norm: 0.06760268238071265\n",
      "Iteration 7123, BCE loss: 57.75738914201366, Acc: 0.8195, Grad norm: 0.07469332500360111\n",
      "Iteration 7124, BCE loss: 57.75743121392286, Acc: 0.8195, Grad norm: 0.08863231798554658\n",
      "Iteration 7125, BCE loss: 57.75740012041953, Acc: 0.8195, Grad norm: 0.0752578706227808\n",
      "Iteration 7126, BCE loss: 57.75747751527308, Acc: 0.8194, Grad norm: 0.09819094187041971\n",
      "Iteration 7127, BCE loss: 57.75746938735044, Acc: 0.8194, Grad norm: 0.09774268305721275\n",
      "Iteration 7128, BCE loss: 57.757399880038705, Acc: 0.8195, Grad norm: 0.08016004760556775\n",
      "Iteration 7129, BCE loss: 57.75735829860062, Acc: 0.8195, Grad norm: 0.061534260281073117\n",
      "Iteration 7130, BCE loss: 57.757348108802695, Acc: 0.8195, Grad norm: 0.0577681808823599\n",
      "Iteration 7131, BCE loss: 57.75734658982871, Acc: 0.8195, Grad norm: 0.05895665661833115\n",
      "Iteration 7132, BCE loss: 57.75742321917546, Acc: 0.8195, Grad norm: 0.08686323883260466\n",
      "Iteration 7133, BCE loss: 57.757451661838346, Acc: 0.8195, Grad norm: 0.09466945887040734\n",
      "Iteration 7134, BCE loss: 57.757384265062655, Acc: 0.8195, Grad norm: 0.07670066520651088\n",
      "Iteration 7135, BCE loss: 57.75730168617667, Acc: 0.8195, Grad norm: 0.0434434368715642\n",
      "Iteration 7136, BCE loss: 57.75731873468926, Acc: 0.8195, Grad norm: 0.049597711950521144\n",
      "Iteration 7137, BCE loss: 57.7573330541439, Acc: 0.8195, Grad norm: 0.05526745261253387\n",
      "Iteration 7138, BCE loss: 57.7573050382795, Acc: 0.8195, Grad norm: 0.04219962374387262\n",
      "Iteration 7139, BCE loss: 57.757335857228014, Acc: 0.8195, Grad norm: 0.052923381696764485\n",
      "Iteration 7140, BCE loss: 57.757344444324545, Acc: 0.8195, Grad norm: 0.05490126232169214\n",
      "Iteration 7141, BCE loss: 57.75733416717378, Acc: 0.8195, Grad norm: 0.05241541548349137\n",
      "Iteration 7142, BCE loss: 57.75732097549542, Acc: 0.8195, Grad norm: 0.04762372012122196\n",
      "Iteration 7143, BCE loss: 57.75732608188176, Acc: 0.8195, Grad norm: 0.05248173964478271\n",
      "Iteration 7144, BCE loss: 57.757329871645645, Acc: 0.8195, Grad norm: 0.0536395288148212\n",
      "Iteration 7145, BCE loss: 57.75729754280074, Acc: 0.8196, Grad norm: 0.03672425426651703\n",
      "Iteration 7146, BCE loss: 57.75730319263392, Acc: 0.8196, Grad norm: 0.041508383181216066\n",
      "Iteration 7147, BCE loss: 57.757304646382806, Acc: 0.8196, Grad norm: 0.044260675784287114\n",
      "Iteration 7148, BCE loss: 57.75733234011217, Acc: 0.8196, Grad norm: 0.05594797570439649\n",
      "Iteration 7149, BCE loss: 57.75734508026837, Acc: 0.8196, Grad norm: 0.060771872440244706\n",
      "Iteration 7150, BCE loss: 57.75740667006451, Acc: 0.8196, Grad norm: 0.081924606802507\n",
      "Iteration 7151, BCE loss: 57.75742482744201, Acc: 0.8196, Grad norm: 0.08818070420149889\n",
      "Iteration 7152, BCE loss: 57.75739613253275, Acc: 0.8196, Grad norm: 0.07959969067652348\n",
      "Iteration 7153, BCE loss: 57.7573805573749, Acc: 0.8196, Grad norm: 0.074478686663165\n",
      "Iteration 7154, BCE loss: 57.757446370267864, Acc: 0.8196, Grad norm: 0.09244226121032496\n",
      "Iteration 7155, BCE loss: 57.75741084245233, Acc: 0.8196, Grad norm: 0.0837430729584191\n",
      "Iteration 7156, BCE loss: 57.75744435238818, Acc: 0.8196, Grad norm: 0.09167423431274863\n",
      "Iteration 7157, BCE loss: 57.75756795650154, Acc: 0.8196, Grad norm: 0.11841803607368552\n",
      "Iteration 7158, BCE loss: 57.75753808652931, Acc: 0.8196, Grad norm: 0.11159229927778491\n",
      "Iteration 7159, BCE loss: 57.757481140449926, Acc: 0.8196, Grad norm: 0.09898794787549094\n",
      "Iteration 7160, BCE loss: 57.75744465081904, Acc: 0.8196, Grad norm: 0.08937061255172958\n",
      "Iteration 7161, BCE loss: 57.75744806679863, Acc: 0.8196, Grad norm: 0.08997027163144536\n",
      "Iteration 7162, BCE loss: 57.75741142643721, Acc: 0.8196, Grad norm: 0.08006848993011514\n",
      "Iteration 7163, BCE loss: 57.75735221491551, Acc: 0.8196, Grad norm: 0.06149073320512046\n",
      "Iteration 7164, BCE loss: 57.757348468322604, Acc: 0.8196, Grad norm: 0.06028020364499471\n",
      "Iteration 7165, BCE loss: 57.75741022842534, Acc: 0.8196, Grad norm: 0.07913853065421089\n",
      "Iteration 7166, BCE loss: 57.7573358615432, Acc: 0.8196, Grad norm: 0.05543542542686931\n",
      "Iteration 7167, BCE loss: 57.75734585977426, Acc: 0.8196, Grad norm: 0.05899535005759327\n",
      "Iteration 7168, BCE loss: 57.75738187289998, Acc: 0.8196, Grad norm: 0.07425923520908234\n",
      "Iteration 7169, BCE loss: 57.75734844712518, Acc: 0.8196, Grad norm: 0.06531977220483305\n",
      "Iteration 7170, BCE loss: 57.757326012418815, Acc: 0.8196, Grad norm: 0.0562895348787698\n",
      "Iteration 7171, BCE loss: 57.757372754793835, Acc: 0.8196, Grad norm: 0.07302393035443884\n",
      "Iteration 7172, BCE loss: 57.7573530766327, Acc: 0.8196, Grad norm: 0.06626747815927918\n",
      "Iteration 7173, BCE loss: 57.757412086335734, Acc: 0.8196, Grad norm: 0.08547213736143117\n",
      "Iteration 7174, BCE loss: 57.75739441315404, Acc: 0.8196, Grad norm: 0.08009514183166475\n",
      "Iteration 7175, BCE loss: 57.75736458999364, Acc: 0.8196, Grad norm: 0.06766880312628186\n",
      "Iteration 7176, BCE loss: 57.75735888023312, Acc: 0.8196, Grad norm: 0.06475701988388025\n",
      "Iteration 7177, BCE loss: 57.75741103055216, Acc: 0.8196, Grad norm: 0.0826868393733816\n",
      "Iteration 7178, BCE loss: 57.75740923970443, Acc: 0.8196, Grad norm: 0.0831246382533575\n",
      "Iteration 7179, BCE loss: 57.75737133291205, Acc: 0.8196, Grad norm: 0.06896064548470815\n",
      "Iteration 7180, BCE loss: 57.75736975990244, Acc: 0.8196, Grad norm: 0.06966991514943227\n",
      "Iteration 7181, BCE loss: 57.75732279945696, Acc: 0.8196, Grad norm: 0.052339362124190325\n",
      "Iteration 7182, BCE loss: 57.75731799848178, Acc: 0.8195, Grad norm: 0.05009804605555626\n",
      "Iteration 7183, BCE loss: 57.757287601350896, Acc: 0.8196, Grad norm: 0.03199703405191242\n",
      "Iteration 7184, BCE loss: 57.75729507526052, Acc: 0.8196, Grad norm: 0.034992808631407656\n",
      "Iteration 7185, BCE loss: 57.7573052506474, Acc: 0.8196, Grad norm: 0.04292980490436409\n",
      "Iteration 7186, BCE loss: 57.75731185674044, Acc: 0.8196, Grad norm: 0.04641949691208752\n",
      "Iteration 7187, BCE loss: 57.75730237130519, Acc: 0.8196, Grad norm: 0.040621739872479114\n",
      "Iteration 7188, BCE loss: 57.75731783347696, Acc: 0.8196, Grad norm: 0.046792750927587\n",
      "Iteration 7189, BCE loss: 57.75733022729056, Acc: 0.8196, Grad norm: 0.05249464285906115\n",
      "Iteration 7190, BCE loss: 57.75737491382145, Acc: 0.8196, Grad norm: 0.07256286417310258\n",
      "Iteration 7191, BCE loss: 57.75739971509774, Acc: 0.8196, Grad norm: 0.08014180764304923\n",
      "Iteration 7192, BCE loss: 57.757470492758735, Acc: 0.8196, Grad norm: 0.10002389351028496\n",
      "Iteration 7193, BCE loss: 57.75737201390864, Acc: 0.8196, Grad norm: 0.07089080324969188\n",
      "Iteration 7194, BCE loss: 57.7573710800232, Acc: 0.8196, Grad norm: 0.0694076509566536\n",
      "Iteration 7195, BCE loss: 57.75740021677875, Acc: 0.8196, Grad norm: 0.07936295328997679\n",
      "Iteration 7196, BCE loss: 57.7574187815108, Acc: 0.8196, Grad norm: 0.08342207172429143\n",
      "Iteration 7197, BCE loss: 57.757401231432276, Acc: 0.8196, Grad norm: 0.07926591842373226\n",
      "Iteration 7198, BCE loss: 57.75740518031114, Acc: 0.8196, Grad norm: 0.07982686349736978\n",
      "Iteration 7199, BCE loss: 57.757383854630724, Acc: 0.8196, Grad norm: 0.07348261085685558\n",
      "Iteration 7200, BCE loss: 57.75736377294261, Acc: 0.8196, Grad norm: 0.06615789138735907\n",
      "Iteration 7201, BCE loss: 57.757352076608, Acc: 0.8196, Grad norm: 0.061184840631307885\n",
      "Iteration 7202, BCE loss: 57.757361111116225, Acc: 0.8196, Grad norm: 0.06351072866052525\n",
      "Iteration 7203, BCE loss: 57.757307772373494, Acc: 0.8196, Grad norm: 0.04292446068818647\n",
      "Iteration 7204, BCE loss: 57.757326369583296, Acc: 0.8196, Grad norm: 0.05053768244283022\n",
      "Iteration 7205, BCE loss: 57.75730466383385, Acc: 0.8196, Grad norm: 0.043314761958802885\n",
      "Iteration 7206, BCE loss: 57.75730952687776, Acc: 0.8196, Grad norm: 0.04632186516142903\n",
      "Iteration 7207, BCE loss: 57.75730825403883, Acc: 0.8196, Grad norm: 0.04318057678843554\n",
      "Iteration 7208, BCE loss: 57.757296534743176, Acc: 0.8196, Grad norm: 0.03888171466304652\n",
      "Iteration 7209, BCE loss: 57.75729870636365, Acc: 0.8196, Grad norm: 0.04077660345398156\n",
      "Iteration 7210, BCE loss: 57.757305027027506, Acc: 0.8196, Grad norm: 0.044301557618653786\n",
      "Iteration 7211, BCE loss: 57.75732522480115, Acc: 0.8196, Grad norm: 0.05136756352069081\n",
      "Iteration 7212, BCE loss: 57.75735922404421, Acc: 0.8196, Grad norm: 0.06545288571742253\n",
      "Iteration 7213, BCE loss: 57.757313936071455, Acc: 0.8196, Grad norm: 0.048698830255768266\n",
      "Iteration 7214, BCE loss: 57.75735141694253, Acc: 0.8196, Grad norm: 0.06379395372888783\n",
      "Iteration 7215, BCE loss: 57.75736045010575, Acc: 0.8196, Grad norm: 0.06655919486032738\n",
      "Iteration 7216, BCE loss: 57.757350010152535, Acc: 0.8196, Grad norm: 0.06341991336083533\n",
      "Iteration 7217, BCE loss: 57.757315045727445, Acc: 0.8196, Grad norm: 0.04507989583528983\n",
      "Iteration 7218, BCE loss: 57.75729606536527, Acc: 0.8195, Grad norm: 0.03481131666329615\n",
      "Iteration 7219, BCE loss: 57.75730812045656, Acc: 0.8196, Grad norm: 0.041077546339598335\n",
      "Iteration 7220, BCE loss: 57.757300248307985, Acc: 0.8195, Grad norm: 0.038996429444960005\n",
      "Iteration 7221, BCE loss: 57.75730387768286, Acc: 0.8196, Grad norm: 0.039403481213452156\n",
      "Iteration 7222, BCE loss: 57.75729563936841, Acc: 0.8195, Grad norm: 0.03722923976484856\n",
      "Iteration 7223, BCE loss: 57.75731665033281, Acc: 0.8195, Grad norm: 0.04712786418323192\n",
      "Iteration 7224, BCE loss: 57.757374989867955, Acc: 0.8196, Grad norm: 0.07122261004273145\n",
      "Iteration 7225, BCE loss: 57.75738424228685, Acc: 0.8196, Grad norm: 0.07281927027134688\n",
      "Iteration 7226, BCE loss: 57.757401894084886, Acc: 0.8196, Grad norm: 0.07688932794552707\n",
      "Iteration 7227, BCE loss: 57.757412816812675, Acc: 0.8196, Grad norm: 0.08169621907195018\n",
      "Iteration 7228, BCE loss: 57.75740737586207, Acc: 0.8196, Grad norm: 0.0806062172040029\n",
      "Iteration 7229, BCE loss: 57.757398919222354, Acc: 0.8196, Grad norm: 0.07845814361171652\n",
      "Iteration 7230, BCE loss: 57.75738858122611, Acc: 0.8196, Grad norm: 0.0745052162035647\n",
      "Iteration 7231, BCE loss: 57.75742048970524, Acc: 0.8195, Grad norm: 0.08316079463571115\n",
      "Iteration 7232, BCE loss: 57.757369538856054, Acc: 0.8195, Grad norm: 0.06605422710955006\n",
      "Iteration 7233, BCE loss: 57.75739632804485, Acc: 0.8195, Grad norm: 0.07522218845404524\n",
      "Iteration 7234, BCE loss: 57.75739268328465, Acc: 0.8195, Grad norm: 0.07471390538331038\n",
      "Iteration 7235, BCE loss: 57.75742128241551, Acc: 0.8195, Grad norm: 0.08163499246141942\n",
      "Iteration 7236, BCE loss: 57.75739195399984, Acc: 0.8195, Grad norm: 0.0728687820077898\n",
      "Iteration 7237, BCE loss: 57.75736730817744, Acc: 0.8195, Grad norm: 0.06505899700959207\n",
      "Iteration 7238, BCE loss: 57.75736539809659, Acc: 0.8195, Grad norm: 0.06555815754719409\n",
      "Iteration 7239, BCE loss: 57.75739704687024, Acc: 0.8195, Grad norm: 0.0757844123148201\n",
      "Iteration 7240, BCE loss: 57.75740974052267, Acc: 0.8195, Grad norm: 0.07938219259309057\n",
      "Iteration 7241, BCE loss: 57.75744843993572, Acc: 0.8195, Grad norm: 0.09130959198381283\n",
      "Iteration 7242, BCE loss: 57.75738881531629, Acc: 0.8195, Grad norm: 0.07450573068443544\n",
      "Iteration 7243, BCE loss: 57.75736222402889, Acc: 0.8195, Grad norm: 0.06658920591128432\n",
      "Iteration 7244, BCE loss: 57.75735189727941, Acc: 0.8195, Grad norm: 0.06092150261203667\n",
      "Iteration 7245, BCE loss: 57.757336817881054, Acc: 0.8195, Grad norm: 0.05613047914077412\n",
      "Iteration 7246, BCE loss: 57.7573200376037, Acc: 0.8195, Grad norm: 0.04899326451444203\n",
      "Iteration 7247, BCE loss: 57.7573108931246, Acc: 0.8195, Grad norm: 0.041515339611780125\n",
      "Iteration 7248, BCE loss: 57.75730673872425, Acc: 0.8195, Grad norm: 0.040518987987392834\n",
      "Iteration 7249, BCE loss: 57.75730296600372, Acc: 0.8195, Grad norm: 0.037493608285469666\n",
      "Iteration 7250, BCE loss: 57.75732225401359, Acc: 0.8195, Grad norm: 0.04808299455411551\n",
      "Iteration 7251, BCE loss: 57.757316827560885, Acc: 0.8195, Grad norm: 0.04538532747236954\n",
      "Iteration 7252, BCE loss: 57.757312806681625, Acc: 0.8195, Grad norm: 0.04445892767463952\n",
      "Iteration 7253, BCE loss: 57.7573242782911, Acc: 0.8195, Grad norm: 0.04859744719199033\n",
      "Iteration 7254, BCE loss: 57.75731868596013, Acc: 0.8195, Grad norm: 0.04463841716971634\n",
      "Iteration 7255, BCE loss: 57.75733393006225, Acc: 0.8195, Grad norm: 0.04995341816818144\n",
      "Iteration 7256, BCE loss: 57.75730114764076, Acc: 0.8195, Grad norm: 0.03808054412761959\n",
      "Iteration 7257, BCE loss: 57.757300290226695, Acc: 0.8195, Grad norm: 0.03817606388549133\n",
      "Iteration 7258, BCE loss: 57.75731343721982, Acc: 0.8195, Grad norm: 0.043100284806729346\n",
      "Iteration 7259, BCE loss: 57.75735656323516, Acc: 0.8196, Grad norm: 0.059036402049503514\n",
      "Iteration 7260, BCE loss: 57.75737152732301, Acc: 0.8196, Grad norm: 0.06524996366353372\n",
      "Iteration 7261, BCE loss: 57.75739363979618, Acc: 0.8196, Grad norm: 0.07227396153254165\n",
      "Iteration 7262, BCE loss: 57.7574141627349, Acc: 0.8196, Grad norm: 0.07783002001322462\n",
      "Iteration 7263, BCE loss: 57.75749145106505, Acc: 0.8196, Grad norm: 0.09720201344870492\n",
      "Iteration 7264, BCE loss: 57.757525600358264, Acc: 0.8196, Grad norm: 0.10361609652658417\n",
      "Iteration 7265, BCE loss: 57.75743065935479, Acc: 0.8196, Grad norm: 0.07809173011906108\n",
      "Iteration 7266, BCE loss: 57.75742806800352, Acc: 0.8196, Grad norm: 0.0778874835556245\n",
      "Iteration 7267, BCE loss: 57.757413014436246, Acc: 0.8196, Grad norm: 0.07186616155284663\n",
      "Iteration 7268, BCE loss: 57.757413321450045, Acc: 0.8196, Grad norm: 0.07154484729635804\n",
      "Iteration 7269, BCE loss: 57.75737970135269, Acc: 0.8196, Grad norm: 0.0633622962897665\n",
      "Iteration 7270, BCE loss: 57.75734988871514, Acc: 0.8196, Grad norm: 0.05618320334016699\n",
      "Iteration 7271, BCE loss: 57.75733308531505, Acc: 0.8196, Grad norm: 0.05227328783774015\n",
      "Iteration 7272, BCE loss: 57.75731161495622, Acc: 0.8196, Grad norm: 0.042391122980871075\n",
      "Iteration 7273, BCE loss: 57.75730960258194, Acc: 0.8196, Grad norm: 0.0422375601714357\n",
      "Iteration 7274, BCE loss: 57.75732985988368, Acc: 0.8196, Grad norm: 0.05322087013132602\n",
      "Iteration 7275, BCE loss: 57.75731950192389, Acc: 0.8196, Grad norm: 0.046330611064304035\n",
      "Iteration 7276, BCE loss: 57.75732353168002, Acc: 0.8196, Grad norm: 0.04755201088349458\n",
      "Iteration 7277, BCE loss: 57.75736136433608, Acc: 0.8196, Grad norm: 0.060307848390436423\n",
      "Iteration 7278, BCE loss: 57.75737489795321, Acc: 0.8196, Grad norm: 0.06543122277121285\n",
      "Iteration 7279, BCE loss: 57.75734937517346, Acc: 0.8196, Grad norm: 0.057386618265639844\n",
      "Iteration 7280, BCE loss: 57.75735493985631, Acc: 0.8196, Grad norm: 0.06191568840588263\n",
      "Iteration 7281, BCE loss: 57.75738643418657, Acc: 0.8196, Grad norm: 0.07051468951824191\n",
      "Iteration 7282, BCE loss: 57.75743980984207, Acc: 0.8196, Grad norm: 0.08677909235250823\n",
      "Iteration 7283, BCE loss: 57.75742629388196, Acc: 0.8196, Grad norm: 0.08350522265840706\n",
      "Iteration 7284, BCE loss: 57.75743025467408, Acc: 0.8196, Grad norm: 0.08358550681286286\n",
      "Iteration 7285, BCE loss: 57.757427269618006, Acc: 0.8196, Grad norm: 0.08549531965538319\n",
      "Iteration 7286, BCE loss: 57.757476265575335, Acc: 0.8196, Grad norm: 0.09902209044238135\n",
      "Iteration 7287, BCE loss: 57.75754819962994, Acc: 0.8196, Grad norm: 0.11488755948670797\n",
      "Iteration 7288, BCE loss: 57.75758920271597, Acc: 0.8196, Grad norm: 0.12100739448854639\n",
      "Iteration 7289, BCE loss: 57.757563769035286, Acc: 0.8196, Grad norm: 0.1151069287799858\n",
      "Iteration 7290, BCE loss: 57.75759987751296, Acc: 0.8196, Grad norm: 0.1197063678872761\n",
      "Iteration 7291, BCE loss: 57.75761607307651, Acc: 0.8196, Grad norm: 0.12280312730454786\n",
      "Iteration 7292, BCE loss: 57.75749755745316, Acc: 0.8196, Grad norm: 0.10014290697113634\n",
      "Iteration 7293, BCE loss: 57.75744907965748, Acc: 0.8196, Grad norm: 0.08753265031279667\n",
      "Iteration 7294, BCE loss: 57.7574043698139, Acc: 0.8196, Grad norm: 0.07601142792356401\n",
      "Iteration 7295, BCE loss: 57.75739181943224, Acc: 0.8196, Grad norm: 0.07220806147910121\n",
      "Iteration 7296, BCE loss: 57.75742699892393, Acc: 0.8196, Grad norm: 0.08337215063160738\n",
      "Iteration 7297, BCE loss: 57.757445089583896, Acc: 0.8196, Grad norm: 0.08734106415121319\n",
      "Iteration 7298, BCE loss: 57.75741778184287, Acc: 0.8196, Grad norm: 0.08150799253855476\n",
      "Iteration 7299, BCE loss: 57.757496326783865, Acc: 0.8196, Grad norm: 0.10211522460979275\n",
      "Iteration 7300, BCE loss: 57.75740041703444, Acc: 0.8196, Grad norm: 0.07986316848485812\n",
      "Iteration 7301, BCE loss: 57.75743078161453, Acc: 0.8196, Grad norm: 0.08854246351987938\n",
      "Iteration 7302, BCE loss: 57.757382800819116, Acc: 0.8196, Grad norm: 0.07372879138035494\n",
      "Iteration 7303, BCE loss: 57.75745275295712, Acc: 0.8196, Grad norm: 0.09328719014384691\n",
      "Iteration 7304, BCE loss: 57.757433548545336, Acc: 0.8196, Grad norm: 0.08895832264297625\n",
      "Iteration 7305, BCE loss: 57.75747705878857, Acc: 0.8196, Grad norm: 0.09967518863235737\n",
      "Iteration 7306, BCE loss: 57.757438918225276, Acc: 0.8196, Grad norm: 0.09078974059289946\n",
      "Iteration 7307, BCE loss: 57.757405947974775, Acc: 0.8196, Grad norm: 0.08208503091441571\n",
      "Iteration 7308, BCE loss: 57.7574845742365, Acc: 0.8196, Grad norm: 0.10202525201981583\n",
      "Iteration 7309, BCE loss: 57.75750933255615, Acc: 0.8196, Grad norm: 0.1062341316234737\n",
      "Iteration 7310, BCE loss: 57.75749278146468, Acc: 0.8196, Grad norm: 0.10091802853395997\n",
      "Iteration 7311, BCE loss: 57.75747550805022, Acc: 0.8196, Grad norm: 0.09700213063321303\n",
      "Iteration 7312, BCE loss: 57.7574736784374, Acc: 0.8196, Grad norm: 0.09759983136979516\n",
      "Iteration 7313, BCE loss: 57.757506043042994, Acc: 0.8196, Grad norm: 0.10510333297941325\n",
      "Iteration 7314, BCE loss: 57.75745836261011, Acc: 0.8196, Grad norm: 0.09487343494096666\n",
      "Iteration 7315, BCE loss: 57.75738148341294, Acc: 0.8196, Grad norm: 0.07389082402918043\n",
      "Iteration 7316, BCE loss: 57.757454795249785, Acc: 0.8196, Grad norm: 0.0953024971850272\n",
      "Iteration 7317, BCE loss: 57.75735927825836, Acc: 0.8195, Grad norm: 0.06729684213402713\n",
      "Iteration 7318, BCE loss: 57.75733843112897, Acc: 0.8196, Grad norm: 0.0599260811145583\n",
      "Iteration 7319, BCE loss: 57.7573251964191, Acc: 0.8196, Grad norm: 0.05451276855386367\n",
      "Iteration 7320, BCE loss: 57.75733355705646, Acc: 0.8196, Grad norm: 0.057224237029197704\n",
      "Iteration 7321, BCE loss: 57.75736809411194, Acc: 0.8196, Grad norm: 0.069415421562417\n",
      "Iteration 7322, BCE loss: 57.75740263497039, Acc: 0.8196, Grad norm: 0.08059124962118923\n",
      "Iteration 7323, BCE loss: 57.75741029466242, Acc: 0.8196, Grad norm: 0.0821246705138016\n",
      "Iteration 7324, BCE loss: 57.757382692732705, Acc: 0.8196, Grad norm: 0.0714898971441893\n",
      "Iteration 7325, BCE loss: 57.75736351647723, Acc: 0.8196, Grad norm: 0.06492788173597139\n",
      "Iteration 7326, BCE loss: 57.75734299323388, Acc: 0.8196, Grad norm: 0.05743826739923169\n",
      "Iteration 7327, BCE loss: 57.75734075595361, Acc: 0.8196, Grad norm: 0.055974448925721905\n",
      "Iteration 7328, BCE loss: 57.75734474106575, Acc: 0.8196, Grad norm: 0.057120118144702456\n",
      "Iteration 7329, BCE loss: 57.75739776188294, Acc: 0.8196, Grad norm: 0.07281752011821173\n",
      "Iteration 7330, BCE loss: 57.757363016054114, Acc: 0.8196, Grad norm: 0.061676689188141565\n",
      "Iteration 7331, BCE loss: 57.75735602235132, Acc: 0.8196, Grad norm: 0.05996270483741639\n",
      "Iteration 7332, BCE loss: 57.75734202424077, Acc: 0.8196, Grad norm: 0.056188802752656614\n",
      "Iteration 7333, BCE loss: 57.75740346891714, Acc: 0.8196, Grad norm: 0.07390658323613819\n",
      "Iteration 7334, BCE loss: 57.757357171302445, Acc: 0.8196, Grad norm: 0.06214465505647188\n",
      "Iteration 7335, BCE loss: 57.75736431095446, Acc: 0.8195, Grad norm: 0.06743559456203205\n",
      "Iteration 7336, BCE loss: 57.75736071957454, Acc: 0.8195, Grad norm: 0.06522029691633238\n",
      "Iteration 7337, BCE loss: 57.757325237194664, Acc: 0.8195, Grad norm: 0.050348864664059564\n",
      "Iteration 7338, BCE loss: 57.75735167122542, Acc: 0.8195, Grad norm: 0.05973565594402023\n",
      "Iteration 7339, BCE loss: 57.75732927725653, Acc: 0.8195, Grad norm: 0.0512223650734669\n",
      "Iteration 7340, BCE loss: 57.75732942581742, Acc: 0.8195, Grad norm: 0.05119336839171517\n",
      "Iteration 7341, BCE loss: 57.757301382198975, Acc: 0.8195, Grad norm: 0.03901762134248239\n",
      "Iteration 7342, BCE loss: 57.757325804804815, Acc: 0.8195, Grad norm: 0.04994325577422372\n",
      "Iteration 7343, BCE loss: 57.75733001231759, Acc: 0.8195, Grad norm: 0.05055508903072757\n",
      "Iteration 7344, BCE loss: 57.75731929810584, Acc: 0.8195, Grad norm: 0.04629134639392774\n",
      "Iteration 7345, BCE loss: 57.757311811718175, Acc: 0.8195, Grad norm: 0.04563229593377665\n",
      "Iteration 7346, BCE loss: 57.75729718392941, Acc: 0.8195, Grad norm: 0.037520008973222246\n",
      "Iteration 7347, BCE loss: 57.75731590947325, Acc: 0.8195, Grad norm: 0.04821734663982003\n",
      "Iteration 7348, BCE loss: 57.757284613149295, Acc: 0.8195, Grad norm: 0.03158662124049676\n",
      "Iteration 7349, BCE loss: 57.757275152164894, Acc: 0.8195, Grad norm: 0.028505412733945322\n",
      "Iteration 7350, BCE loss: 57.75727518822096, Acc: 0.8195, Grad norm: 0.02670446942258759\n",
      "Iteration 7351, BCE loss: 57.75727593727018, Acc: 0.8195, Grad norm: 0.02635194676150517\n",
      "Iteration 7352, BCE loss: 57.7572840270672, Acc: 0.8195, Grad norm: 0.031767400232034435\n",
      "Iteration 7353, BCE loss: 57.757297923175486, Acc: 0.8196, Grad norm: 0.039384733615884626\n",
      "Iteration 7354, BCE loss: 57.757313236728926, Acc: 0.8196, Grad norm: 0.04747571181201923\n",
      "Iteration 7355, BCE loss: 57.75728560452621, Acc: 0.8196, Grad norm: 0.030732202955667542\n",
      "Iteration 7356, BCE loss: 57.75729952156311, Acc: 0.8196, Grad norm: 0.03856792882593336\n",
      "Iteration 7357, BCE loss: 57.75730126978854, Acc: 0.8196, Grad norm: 0.03782095162950152\n",
      "Iteration 7358, BCE loss: 57.75733255094757, Acc: 0.8196, Grad norm: 0.0522638127882192\n",
      "Iteration 7359, BCE loss: 57.75731385272887, Acc: 0.8195, Grad norm: 0.04364782075757609\n",
      "Iteration 7360, BCE loss: 57.75735709452163, Acc: 0.8195, Grad norm: 0.06430857384785506\n",
      "Iteration 7361, BCE loss: 57.75737289198398, Acc: 0.8195, Grad norm: 0.07026765974865351\n",
      "Iteration 7362, BCE loss: 57.75738298361806, Acc: 0.8195, Grad norm: 0.07246177370770418\n",
      "Iteration 7363, BCE loss: 57.75735655536826, Acc: 0.8195, Grad norm: 0.06429694013080428\n",
      "Iteration 7364, BCE loss: 57.75733867017506, Acc: 0.8196, Grad norm: 0.05543966239922659\n",
      "Iteration 7365, BCE loss: 57.757338583539706, Acc: 0.8195, Grad norm: 0.05590748660577766\n",
      "Iteration 7366, BCE loss: 57.75732498784162, Acc: 0.8196, Grad norm: 0.05280148628548897\n",
      "Iteration 7367, BCE loss: 57.757346431851474, Acc: 0.8196, Grad norm: 0.0619682116775522\n",
      "Iteration 7368, BCE loss: 57.757317226598175, Acc: 0.8195, Grad norm: 0.04722767852119974\n",
      "Iteration 7369, BCE loss: 57.75737008574857, Acc: 0.8195, Grad norm: 0.06568422425473941\n",
      "Iteration 7370, BCE loss: 57.75735886501049, Acc: 0.8195, Grad norm: 0.05948749834946668\n",
      "Iteration 7371, BCE loss: 57.75735459087423, Acc: 0.8195, Grad norm: 0.05909773974430781\n",
      "Iteration 7372, BCE loss: 57.75736596484193, Acc: 0.8195, Grad norm: 0.06585849341112653\n",
      "Iteration 7373, BCE loss: 57.75732844015042, Acc: 0.8195, Grad norm: 0.05338368847056113\n",
      "Iteration 7374, BCE loss: 57.75733683039219, Acc: 0.8196, Grad norm: 0.05934669927285486\n",
      "Iteration 7375, BCE loss: 57.757331863442346, Acc: 0.8196, Grad norm: 0.05457809206658464\n",
      "Iteration 7376, BCE loss: 57.7573227530941, Acc: 0.8195, Grad norm: 0.04840244742513813\n",
      "Iteration 7377, BCE loss: 57.75735915278615, Acc: 0.8195, Grad norm: 0.06187588393865567\n",
      "Iteration 7378, BCE loss: 57.7573663725976, Acc: 0.8195, Grad norm: 0.06583940188871437\n",
      "Iteration 7379, BCE loss: 57.75741084040113, Acc: 0.8196, Grad norm: 0.08132115316489066\n",
      "Iteration 7380, BCE loss: 57.757345442917014, Acc: 0.8195, Grad norm: 0.05858973091562965\n",
      "Iteration 7381, BCE loss: 57.75735465856259, Acc: 0.8196, Grad norm: 0.06176112488653588\n",
      "Iteration 7382, BCE loss: 57.75741465548076, Acc: 0.8196, Grad norm: 0.0805929659020695\n",
      "Iteration 7383, BCE loss: 57.75741517103374, Acc: 0.8196, Grad norm: 0.0818957779197834\n",
      "Iteration 7384, BCE loss: 57.75735099474461, Acc: 0.8195, Grad norm: 0.06207633359976999\n",
      "Iteration 7385, BCE loss: 57.757376593642704, Acc: 0.8196, Grad norm: 0.0710441077100012\n",
      "Iteration 7386, BCE loss: 57.75737453869988, Acc: 0.8196, Grad norm: 0.07200410247556382\n",
      "Iteration 7387, BCE loss: 57.757407485829276, Acc: 0.8196, Grad norm: 0.08313117491784708\n",
      "Iteration 7388, BCE loss: 57.75733314151489, Acc: 0.8196, Grad norm: 0.05608198667029038\n",
      "Iteration 7389, BCE loss: 57.757305567530565, Acc: 0.8196, Grad norm: 0.042354051606341654\n",
      "Iteration 7390, BCE loss: 57.75728058710895, Acc: 0.8196, Grad norm: 0.028452180536321512\n",
      "Iteration 7391, BCE loss: 57.7572806888385, Acc: 0.8196, Grad norm: 0.030090204063885614\n",
      "Iteration 7392, BCE loss: 57.757299173007254, Acc: 0.8196, Grad norm: 0.04182106915898205\n",
      "Iteration 7393, BCE loss: 57.757335378229286, Acc: 0.8196, Grad norm: 0.059441440862647445\n",
      "Iteration 7394, BCE loss: 57.75742632407757, Acc: 0.8196, Grad norm: 0.08827857613060162\n",
      "Iteration 7395, BCE loss: 57.75736558700835, Acc: 0.8196, Grad norm: 0.07028790268896311\n",
      "Iteration 7396, BCE loss: 57.75735560588261, Acc: 0.8196, Grad norm: 0.06575645989885184\n",
      "Iteration 7397, BCE loss: 57.757418050795906, Acc: 0.8196, Grad norm: 0.08339290521864286\n",
      "Iteration 7398, BCE loss: 57.75736532669916, Acc: 0.8196, Grad norm: 0.06827673482457589\n",
      "Iteration 7399, BCE loss: 57.75739814375052, Acc: 0.8196, Grad norm: 0.07866724656113072\n",
      "Iteration 7400, BCE loss: 57.75741602478285, Acc: 0.8196, Grad norm: 0.08438803408006211\n",
      "Iteration 7401, BCE loss: 57.75742008385993, Acc: 0.8196, Grad norm: 0.08580063934337198\n",
      "Iteration 7402, BCE loss: 57.75741123399093, Acc: 0.8196, Grad norm: 0.08202101409370904\n",
      "Iteration 7403, BCE loss: 57.75749642098151, Acc: 0.8196, Grad norm: 0.10353856213167563\n",
      "Iteration 7404, BCE loss: 57.75752469941963, Acc: 0.8196, Grad norm: 0.11060563885257342\n",
      "Iteration 7405, BCE loss: 57.7574515021207, Acc: 0.8196, Grad norm: 0.0940970469013249\n",
      "Iteration 7406, BCE loss: 57.75737422905065, Acc: 0.8196, Grad norm: 0.07238428740611591\n",
      "Iteration 7407, BCE loss: 57.75739280696631, Acc: 0.8196, Grad norm: 0.07850571772753502\n",
      "Iteration 7408, BCE loss: 57.75737069727955, Acc: 0.8196, Grad norm: 0.07087635890480531\n",
      "Iteration 7409, BCE loss: 57.75735631063428, Acc: 0.8196, Grad norm: 0.06291286190091072\n",
      "Iteration 7410, BCE loss: 57.7574057872915, Acc: 0.8195, Grad norm: 0.07868972269152445\n",
      "Iteration 7411, BCE loss: 57.757351793674914, Acc: 0.8195, Grad norm: 0.060027686500426045\n",
      "Iteration 7412, BCE loss: 57.75734951991738, Acc: 0.8195, Grad norm: 0.060870397450787414\n",
      "Iteration 7413, BCE loss: 57.75736277160632, Acc: 0.8195, Grad norm: 0.06438685806549868\n",
      "Iteration 7414, BCE loss: 57.75733680692362, Acc: 0.8195, Grad norm: 0.05692838453900705\n",
      "Iteration 7415, BCE loss: 57.757382115604656, Acc: 0.8195, Grad norm: 0.07252796805624953\n",
      "Iteration 7416, BCE loss: 57.7573927451928, Acc: 0.8196, Grad norm: 0.07704208836243746\n",
      "Iteration 7417, BCE loss: 57.75741998073008, Acc: 0.8196, Grad norm: 0.08488415877293064\n",
      "Iteration 7418, BCE loss: 57.75739759675895, Acc: 0.8196, Grad norm: 0.07942835432450869\n",
      "Iteration 7419, BCE loss: 57.75739860617716, Acc: 0.8196, Grad norm: 0.0808294608430078\n",
      "Iteration 7420, BCE loss: 57.757365301094936, Acc: 0.8196, Grad norm: 0.07107769641825194\n",
      "Iteration 7421, BCE loss: 57.757365506701426, Acc: 0.8196, Grad norm: 0.07017295038500486\n",
      "Iteration 7422, BCE loss: 57.75739312707596, Acc: 0.8196, Grad norm: 0.08044577665844685\n",
      "Iteration 7423, BCE loss: 57.75740971326674, Acc: 0.8196, Grad norm: 0.08492952992332893\n",
      "Iteration 7424, BCE loss: 57.75747403103878, Acc: 0.8196, Grad norm: 0.10130552892457988\n",
      "Iteration 7425, BCE loss: 57.75740638173512, Acc: 0.8196, Grad norm: 0.0841928164374688\n",
      "Iteration 7426, BCE loss: 57.75744880959867, Acc: 0.8196, Grad norm: 0.09486030414486676\n",
      "Iteration 7427, BCE loss: 57.75737493776239, Acc: 0.8196, Grad norm: 0.07254009444030153\n",
      "Iteration 7428, BCE loss: 57.757399257248494, Acc: 0.8196, Grad norm: 0.07993724089582707\n",
      "Iteration 7429, BCE loss: 57.757356972627356, Acc: 0.8196, Grad norm: 0.06694029417037373\n",
      "Iteration 7430, BCE loss: 57.757328572438354, Acc: 0.8196, Grad norm: 0.05626788341589313\n",
      "Iteration 7431, BCE loss: 57.757363808136475, Acc: 0.8196, Grad norm: 0.0671751596698285\n",
      "Iteration 7432, BCE loss: 57.75735482987338, Acc: 0.8196, Grad norm: 0.06592485333167998\n",
      "Iteration 7433, BCE loss: 57.75737798904494, Acc: 0.8196, Grad norm: 0.07335281048239142\n",
      "Iteration 7434, BCE loss: 57.75732489448869, Acc: 0.8196, Grad norm: 0.05243770702540614\n",
      "Iteration 7435, BCE loss: 57.75729688532114, Acc: 0.8196, Grad norm: 0.03912908543536963\n",
      "Iteration 7436, BCE loss: 57.75731187940201, Acc: 0.8195, Grad norm: 0.04599494254187669\n",
      "Iteration 7437, BCE loss: 57.75729587314086, Acc: 0.8195, Grad norm: 0.03836248797927688\n",
      "Iteration 7438, BCE loss: 57.757304074278366, Acc: 0.8195, Grad norm: 0.04306481234117289\n",
      "Iteration 7439, BCE loss: 57.75731934332285, Acc: 0.8195, Grad norm: 0.0506343979977269\n",
      "Iteration 7440, BCE loss: 57.75728567978916, Acc: 0.8195, Grad norm: 0.030131168769825224\n",
      "Iteration 7441, BCE loss: 57.75730194813351, Acc: 0.8195, Grad norm: 0.037212913400927944\n",
      "Iteration 7442, BCE loss: 57.75730323066087, Acc: 0.8195, Grad norm: 0.040093053228984564\n",
      "Iteration 7443, BCE loss: 57.757329539377594, Acc: 0.8195, Grad norm: 0.05037512090869901\n",
      "Iteration 7444, BCE loss: 57.75737935550447, Acc: 0.8195, Grad norm: 0.06709712165766502\n",
      "Iteration 7445, BCE loss: 57.75740306911598, Acc: 0.8195, Grad norm: 0.07565845247663538\n",
      "Iteration 7446, BCE loss: 57.7573366208495, Acc: 0.8195, Grad norm: 0.05386713673666458\n",
      "Iteration 7447, BCE loss: 57.75735375696509, Acc: 0.8195, Grad norm: 0.05757164423987535\n",
      "Iteration 7448, BCE loss: 57.75735921618572, Acc: 0.8195, Grad norm: 0.05967634103033364\n",
      "Iteration 7449, BCE loss: 57.75734561001485, Acc: 0.8195, Grad norm: 0.056326204406255444\n",
      "Iteration 7450, BCE loss: 57.7573209599146, Acc: 0.8195, Grad norm: 0.046638549035925854\n",
      "Iteration 7451, BCE loss: 57.75731603969241, Acc: 0.8195, Grad norm: 0.04758719623904499\n",
      "Iteration 7452, BCE loss: 57.757356506839415, Acc: 0.8195, Grad norm: 0.06500150976548724\n",
      "Iteration 7453, BCE loss: 57.75730812325094, Acc: 0.8195, Grad norm: 0.04303870624432371\n",
      "Iteration 7454, BCE loss: 57.75729176287615, Acc: 0.8195, Grad norm: 0.033346998959904825\n",
      "Iteration 7455, BCE loss: 57.757294777998936, Acc: 0.8195, Grad norm: 0.036742877662393314\n",
      "Iteration 7456, BCE loss: 57.75731861926997, Acc: 0.8195, Grad norm: 0.04697969011898918\n",
      "Iteration 7457, BCE loss: 57.75734678546267, Acc: 0.8195, Grad norm: 0.058172990032654484\n",
      "Iteration 7458, BCE loss: 57.75734467576945, Acc: 0.8195, Grad norm: 0.06046144799597196\n",
      "Iteration 7459, BCE loss: 57.75735404792863, Acc: 0.8195, Grad norm: 0.061360364066569\n",
      "Iteration 7460, BCE loss: 57.75741205180601, Acc: 0.8195, Grad norm: 0.07889069847451742\n",
      "Iteration 7461, BCE loss: 57.75737856439669, Acc: 0.8195, Grad norm: 0.07111273022759068\n",
      "Iteration 7462, BCE loss: 57.75738615313895, Acc: 0.8195, Grad norm: 0.07298911078711028\n",
      "Iteration 7463, BCE loss: 57.75738626957913, Acc: 0.8195, Grad norm: 0.07013167137981612\n",
      "Iteration 7464, BCE loss: 57.75743238534284, Acc: 0.8195, Grad norm: 0.08337012742322424\n",
      "Iteration 7465, BCE loss: 57.757441125902474, Acc: 0.8195, Grad norm: 0.08634505517096673\n",
      "Iteration 7466, BCE loss: 57.75741564797647, Acc: 0.8195, Grad norm: 0.08018616274395922\n",
      "Iteration 7467, BCE loss: 57.757441064628935, Acc: 0.8195, Grad norm: 0.08707079437561631\n",
      "Iteration 7468, BCE loss: 57.75738487991674, Acc: 0.8195, Grad norm: 0.07039937800063958\n",
      "Iteration 7469, BCE loss: 57.75738386364446, Acc: 0.8195, Grad norm: 0.06956516275160282\n",
      "Iteration 7470, BCE loss: 57.75739074878123, Acc: 0.8195, Grad norm: 0.07130908164733694\n",
      "Iteration 7471, BCE loss: 57.757371177456946, Acc: 0.8195, Grad norm: 0.0635316209229596\n",
      "Iteration 7472, BCE loss: 57.75732098973227, Acc: 0.8195, Grad norm: 0.04693283654188768\n",
      "Iteration 7473, BCE loss: 57.75734850629534, Acc: 0.8195, Grad norm: 0.05833841043743666\n",
      "Iteration 7474, BCE loss: 57.75735231906026, Acc: 0.8195, Grad norm: 0.05903647366852584\n",
      "Iteration 7475, BCE loss: 57.75734397954297, Acc: 0.8195, Grad norm: 0.055923327283206585\n",
      "Iteration 7476, BCE loss: 57.75732674535701, Acc: 0.8195, Grad norm: 0.04873362843975067\n",
      "Iteration 7477, BCE loss: 57.75730341002172, Acc: 0.8195, Grad norm: 0.040638199877235284\n",
      "Iteration 7478, BCE loss: 57.7573252133543, Acc: 0.8195, Grad norm: 0.047353892210676436\n",
      "Iteration 7479, BCE loss: 57.757320929765626, Acc: 0.8195, Grad norm: 0.04407197928165977\n",
      "Iteration 7480, BCE loss: 57.75733367453812, Acc: 0.8195, Grad norm: 0.05166552634381608\n",
      "Iteration 7481, BCE loss: 57.75732984999938, Acc: 0.8195, Grad norm: 0.049190631938989975\n",
      "Iteration 7482, BCE loss: 57.75737808900577, Acc: 0.8196, Grad norm: 0.0663576043059559\n",
      "Iteration 7483, BCE loss: 57.75733962096513, Acc: 0.8196, Grad norm: 0.052999630077347024\n",
      "Iteration 7484, BCE loss: 57.75732441621096, Acc: 0.8195, Grad norm: 0.04870564126563813\n",
      "Iteration 7485, BCE loss: 57.75732913805478, Acc: 0.8195, Grad norm: 0.049233965957733376\n",
      "Iteration 7486, BCE loss: 57.75734991543632, Acc: 0.8195, Grad norm: 0.05869680928164179\n",
      "Iteration 7487, BCE loss: 57.75734379410571, Acc: 0.8195, Grad norm: 0.05626416716438173\n",
      "Iteration 7488, BCE loss: 57.75734918774492, Acc: 0.8196, Grad norm: 0.06001543359246174\n",
      "Iteration 7489, BCE loss: 57.75736619659043, Acc: 0.8196, Grad norm: 0.06829860341832227\n",
      "Iteration 7490, BCE loss: 57.75737638108907, Acc: 0.8196, Grad norm: 0.07182625982835739\n",
      "Iteration 7491, BCE loss: 57.7573829852226, Acc: 0.8196, Grad norm: 0.07311330703754582\n",
      "Iteration 7492, BCE loss: 57.75739386771214, Acc: 0.8196, Grad norm: 0.07509764290265253\n",
      "Iteration 7493, BCE loss: 57.75740272808184, Acc: 0.8196, Grad norm: 0.07790190572806056\n",
      "Iteration 7494, BCE loss: 57.757411545188994, Acc: 0.8196, Grad norm: 0.07943408892712159\n",
      "Iteration 7495, BCE loss: 57.75742318278216, Acc: 0.8196, Grad norm: 0.08400366882758967\n",
      "Iteration 7496, BCE loss: 57.75735676546721, Acc: 0.8196, Grad norm: 0.06290111618117775\n",
      "Iteration 7497, BCE loss: 57.75733114519156, Acc: 0.8196, Grad norm: 0.05404742412314532\n",
      "Iteration 7498, BCE loss: 57.75734569221933, Acc: 0.8196, Grad norm: 0.05965820099434243\n",
      "Iteration 7499, BCE loss: 57.75732885457205, Acc: 0.8196, Grad norm: 0.05195373932414267\n",
      "Iteration 7500, BCE loss: 57.75732056230936, Acc: 0.8196, Grad norm: 0.04797350564296344\n",
      "Iteration 7501, BCE loss: 57.75732400924372, Acc: 0.8196, Grad norm: 0.04928526958536092\n",
      "Iteration 7502, BCE loss: 57.75730140206823, Acc: 0.8196, Grad norm: 0.03988901502993716\n",
      "Iteration 7503, BCE loss: 57.757313813764895, Acc: 0.8196, Grad norm: 0.046872584339232456\n",
      "Iteration 7504, BCE loss: 57.75734443751559, Acc: 0.8195, Grad norm: 0.05995772806348347\n",
      "Iteration 7505, BCE loss: 57.75732811114909, Acc: 0.8196, Grad norm: 0.0532263507037389\n",
      "Iteration 7506, BCE loss: 57.757354815395324, Acc: 0.8195, Grad norm: 0.06390609518583003\n",
      "Iteration 7507, BCE loss: 57.757355292916046, Acc: 0.8195, Grad norm: 0.06434221672540409\n",
      "Iteration 7508, BCE loss: 57.75736555870936, Acc: 0.8196, Grad norm: 0.06545072962811264\n",
      "Iteration 7509, BCE loss: 57.757442114343256, Acc: 0.8196, Grad norm: 0.08890294166404296\n",
      "Iteration 7510, BCE loss: 57.757434925301595, Acc: 0.8196, Grad norm: 0.08634362972829555\n",
      "Iteration 7511, BCE loss: 57.75744494581134, Acc: 0.8196, Grad norm: 0.08907353358404092\n",
      "Iteration 7512, BCE loss: 57.75735277963673, Acc: 0.8196, Grad norm: 0.06174684713778135\n",
      "Iteration 7513, BCE loss: 57.757343631793304, Acc: 0.8196, Grad norm: 0.05747408403976236\n",
      "Iteration 7514, BCE loss: 57.75732471274593, Acc: 0.8195, Grad norm: 0.05052749674697893\n",
      "Iteration 7515, BCE loss: 57.757334164268165, Acc: 0.8196, Grad norm: 0.053702583858066706\n",
      "Iteration 7516, BCE loss: 57.757371787570435, Acc: 0.8196, Grad norm: 0.0666180961748393\n",
      "Iteration 7517, BCE loss: 57.75736340901493, Acc: 0.8196, Grad norm: 0.0644517915438622\n",
      "Iteration 7518, BCE loss: 57.7573590559351, Acc: 0.8196, Grad norm: 0.06364517528878522\n",
      "Iteration 7519, BCE loss: 57.75735668151955, Acc: 0.8196, Grad norm: 0.06267598787064735\n",
      "Iteration 7520, BCE loss: 57.757361447911265, Acc: 0.8196, Grad norm: 0.06427692926616126\n",
      "Iteration 7521, BCE loss: 57.75731381499445, Acc: 0.8196, Grad norm: 0.04469554825820538\n",
      "Iteration 7522, BCE loss: 57.75732581790436, Acc: 0.8196, Grad norm: 0.05170503864632105\n",
      "Iteration 7523, BCE loss: 57.757278747563994, Acc: 0.8196, Grad norm: 0.02755079909099065\n",
      "Iteration 7524, BCE loss: 57.757287054938416, Acc: 0.8196, Grad norm: 0.03357511531668159\n",
      "Iteration 7525, BCE loss: 57.75730697369377, Acc: 0.8196, Grad norm: 0.04379266548272075\n",
      "Iteration 7526, BCE loss: 57.757325230198646, Acc: 0.8196, Grad norm: 0.05194979090563012\n",
      "Iteration 7527, BCE loss: 57.75734530249862, Acc: 0.8196, Grad norm: 0.05971881779689481\n",
      "Iteration 7528, BCE loss: 57.75737099447053, Acc: 0.8196, Grad norm: 0.06829784060802034\n",
      "Iteration 7529, BCE loss: 57.75741930325956, Acc: 0.8196, Grad norm: 0.08302088030249599\n",
      "Iteration 7530, BCE loss: 57.75734789432332, Acc: 0.8196, Grad norm: 0.06049985877708578\n",
      "Iteration 7531, BCE loss: 57.75729609732382, Acc: 0.8196, Grad norm: 0.03822097125230194\n",
      "Iteration 7532, BCE loss: 57.75733378055804, Acc: 0.8196, Grad norm: 0.05604305241100157\n",
      "Iteration 7533, BCE loss: 57.75731223507327, Acc: 0.8196, Grad norm: 0.045069692425586984\n",
      "Iteration 7534, BCE loss: 57.757338402889005, Acc: 0.8196, Grad norm: 0.05311633858737264\n",
      "Iteration 7535, BCE loss: 57.757317470844654, Acc: 0.8195, Grad norm: 0.0439072774074043\n",
      "Iteration 7536, BCE loss: 57.75730047528848, Acc: 0.8195, Grad norm: 0.03850280288040843\n",
      "Iteration 7537, BCE loss: 57.757293992986774, Acc: 0.8195, Grad norm: 0.03526588531147214\n",
      "Iteration 7538, BCE loss: 57.75731156965315, Acc: 0.8196, Grad norm: 0.04166294138735516\n",
      "Iteration 7539, BCE loss: 57.7573380330508, Acc: 0.8196, Grad norm: 0.0518529417667773\n",
      "Iteration 7540, BCE loss: 57.75732615202756, Acc: 0.8195, Grad norm: 0.04814596941606003\n",
      "Iteration 7541, BCE loss: 57.75736632389676, Acc: 0.8195, Grad norm: 0.06275640547300501\n",
      "Iteration 7542, BCE loss: 57.75738313356335, Acc: 0.8195, Grad norm: 0.06730804477916759\n",
      "Iteration 7543, BCE loss: 57.75739199491599, Acc: 0.8196, Grad norm: 0.07191653944686684\n",
      "Iteration 7544, BCE loss: 57.75738538221627, Acc: 0.8195, Grad norm: 0.07184876390145903\n",
      "Iteration 7545, BCE loss: 57.757395408841916, Acc: 0.8195, Grad norm: 0.07485293131635465\n",
      "Iteration 7546, BCE loss: 57.757343097868244, Acc: 0.8195, Grad norm: 0.059151802351726596\n",
      "Iteration 7547, BCE loss: 57.75735512571653, Acc: 0.8195, Grad norm: 0.06376293577512156\n",
      "Iteration 7548, BCE loss: 57.75734422801669, Acc: 0.8195, Grad norm: 0.06291210264585863\n",
      "Iteration 7549, BCE loss: 57.757298370109005, Acc: 0.8195, Grad norm: 0.0422982983640243\n",
      "Iteration 7550, BCE loss: 57.75729605976098, Acc: 0.8195, Grad norm: 0.03943311793467603\n",
      "Iteration 7551, BCE loss: 57.757297506850556, Acc: 0.8195, Grad norm: 0.03885692932287988\n",
      "Iteration 7552, BCE loss: 57.75731748920601, Acc: 0.8195, Grad norm: 0.05077654890970973\n",
      "Iteration 7553, BCE loss: 57.757282331336896, Acc: 0.8195, Grad norm: 0.03306896503042148\n",
      "Iteration 7554, BCE loss: 57.757282171269864, Acc: 0.8195, Grad norm: 0.033655466466483125\n",
      "Iteration 7555, BCE loss: 57.75726510750394, Acc: 0.8195, Grad norm: 0.017659108870327584\n",
      "Iteration 7556, BCE loss: 57.75727514446199, Acc: 0.8196, Grad norm: 0.02837590071978417\n",
      "Iteration 7557, BCE loss: 57.7572819102605, Acc: 0.8195, Grad norm: 0.03216141351778204\n",
      "Iteration 7558, BCE loss: 57.75729723673595, Acc: 0.8195, Grad norm: 0.04190161739641422\n",
      "Iteration 7559, BCE loss: 57.757291529411106, Acc: 0.8195, Grad norm: 0.03941693919948479\n",
      "Iteration 7560, BCE loss: 57.75730615856061, Acc: 0.8196, Grad norm: 0.047101843118181205\n",
      "Iteration 7561, BCE loss: 57.75727944680368, Acc: 0.8195, Grad norm: 0.031222212505896657\n",
      "Iteration 7562, BCE loss: 57.757274134797306, Acc: 0.8196, Grad norm: 0.0263558280443486\n",
      "Iteration 7563, BCE loss: 57.757266862162176, Acc: 0.8195, Grad norm: 0.01777389834193191\n",
      "Iteration 7564, BCE loss: 57.75727461415255, Acc: 0.8195, Grad norm: 0.025663015236623468\n",
      "Iteration 7565, BCE loss: 57.757296252659785, Acc: 0.8195, Grad norm: 0.04048303884536684\n",
      "Iteration 7566, BCE loss: 57.75729024608812, Acc: 0.8195, Grad norm: 0.0375471242001191\n",
      "Iteration 7567, BCE loss: 57.75734355001755, Acc: 0.8195, Grad norm: 0.06250782887145281\n",
      "Iteration 7568, BCE loss: 57.75738628939436, Acc: 0.8195, Grad norm: 0.07793715883400773\n",
      "Iteration 7569, BCE loss: 57.75739984312029, Acc: 0.8195, Grad norm: 0.07939609763597266\n",
      "Iteration 7570, BCE loss: 57.75733927302475, Acc: 0.8195, Grad norm: 0.05817811575084684\n",
      "Iteration 7571, BCE loss: 57.7573533925609, Acc: 0.8195, Grad norm: 0.06427554239671975\n",
      "Iteration 7572, BCE loss: 57.757304703518905, Acc: 0.8195, Grad norm: 0.043889849289624375\n",
      "Iteration 7573, BCE loss: 57.75729175727089, Acc: 0.8195, Grad norm: 0.03792810933629109\n",
      "Iteration 7574, BCE loss: 57.757295349612505, Acc: 0.8195, Grad norm: 0.040271246082617805\n",
      "Iteration 7575, BCE loss: 57.75733767311975, Acc: 0.8195, Grad norm: 0.05912489938188054\n",
      "Iteration 7576, BCE loss: 57.757290902141634, Acc: 0.8195, Grad norm: 0.03815983598830123\n",
      "Iteration 7577, BCE loss: 57.75730883707146, Acc: 0.8195, Grad norm: 0.04763894162543124\n",
      "Iteration 7578, BCE loss: 57.75730076946555, Acc: 0.8195, Grad norm: 0.04274717453039382\n",
      "Iteration 7579, BCE loss: 57.75727678328524, Acc: 0.8195, Grad norm: 0.026999903252782356\n",
      "Iteration 7580, BCE loss: 57.75727115425332, Acc: 0.8195, Grad norm: 0.02322781503579167\n",
      "Iteration 7581, BCE loss: 57.757300792406056, Acc: 0.8195, Grad norm: 0.043084172095806686\n",
      "Iteration 7582, BCE loss: 57.75727554451143, Acc: 0.8195, Grad norm: 0.026635673711760826\n",
      "Iteration 7583, BCE loss: 57.75727010259364, Acc: 0.8195, Grad norm: 0.020012808486417896\n",
      "Iteration 7584, BCE loss: 57.757273443633764, Acc: 0.8195, Grad norm: 0.024743662899379158\n",
      "Iteration 7585, BCE loss: 57.75728657100118, Acc: 0.8195, Grad norm: 0.03461197957900766\n",
      "Iteration 7586, BCE loss: 57.757268363883696, Acc: 0.8195, Grad norm: 0.01930932196090619\n",
      "Iteration 7587, BCE loss: 57.75727783868096, Acc: 0.8195, Grad norm: 0.02872755512850874\n",
      "Iteration 7588, BCE loss: 57.75727754935501, Acc: 0.8196, Grad norm: 0.03045331826455634\n",
      "Iteration 7589, BCE loss: 57.75729587876636, Acc: 0.8196, Grad norm: 0.04106392192109881\n",
      "Iteration 7590, BCE loss: 57.75729098861852, Acc: 0.8196, Grad norm: 0.0391091051668142\n",
      "Iteration 7591, BCE loss: 57.757309853034855, Acc: 0.8196, Grad norm: 0.04912095218132325\n",
      "Iteration 7592, BCE loss: 57.75735119610796, Acc: 0.8196, Grad norm: 0.06501405861414251\n",
      "Iteration 7593, BCE loss: 57.75732846225519, Acc: 0.8196, Grad norm: 0.055922305295721474\n",
      "Iteration 7594, BCE loss: 57.7573166042789, Acc: 0.8196, Grad norm: 0.051558378644225904\n",
      "Iteration 7595, BCE loss: 57.7572870505993, Acc: 0.8196, Grad norm: 0.03480956043405524\n",
      "Iteration 7596, BCE loss: 57.75727088338965, Acc: 0.8196, Grad norm: 0.022174098242483432\n",
      "Iteration 7597, BCE loss: 57.757270291681365, Acc: 0.8196, Grad norm: 0.02145771544955637\n",
      "Iteration 7598, BCE loss: 57.757277640034545, Acc: 0.8195, Grad norm: 0.030525555292102543\n",
      "Iteration 7599, BCE loss: 57.7572863983714, Acc: 0.8195, Grad norm: 0.03448722372524743\n",
      "Iteration 7600, BCE loss: 57.757283348724556, Acc: 0.8195, Grad norm: 0.03287622196714633\n",
      "Iteration 7601, BCE loss: 57.757298263370345, Acc: 0.8196, Grad norm: 0.04225156147340422\n",
      "Iteration 7602, BCE loss: 57.7572965499109, Acc: 0.8196, Grad norm: 0.04166746704004353\n",
      "Iteration 7603, BCE loss: 57.75731218124251, Acc: 0.8196, Grad norm: 0.04705116583521523\n",
      "Iteration 7604, BCE loss: 57.757329073538216, Acc: 0.8196, Grad norm: 0.05523504701885544\n",
      "Iteration 7605, BCE loss: 57.75729823147042, Acc: 0.8196, Grad norm: 0.038756863237210774\n",
      "Iteration 7606, BCE loss: 57.757306909583164, Acc: 0.8195, Grad norm: 0.04351584086612371\n",
      "Iteration 7607, BCE loss: 57.75732297168947, Acc: 0.8195, Grad norm: 0.048763284255110464\n",
      "Iteration 7608, BCE loss: 57.75733732187246, Acc: 0.8195, Grad norm: 0.05333255991181872\n",
      "Iteration 7609, BCE loss: 57.75731140134738, Acc: 0.8195, Grad norm: 0.04373857460100947\n",
      "Iteration 7610, BCE loss: 57.75731249402741, Acc: 0.8195, Grad norm: 0.044085344187225574\n",
      "Iteration 7611, BCE loss: 57.757330977273824, Acc: 0.8195, Grad norm: 0.05176357460943678\n",
      "Iteration 7612, BCE loss: 57.75732007950681, Acc: 0.8195, Grad norm: 0.04856760445158323\n",
      "Iteration 7613, BCE loss: 57.757319927046176, Acc: 0.8195, Grad norm: 0.04958657762691865\n",
      "Iteration 7614, BCE loss: 57.7573127907294, Acc: 0.8195, Grad norm: 0.04519751216725803\n",
      "Iteration 7615, BCE loss: 57.75732211800262, Acc: 0.8195, Grad norm: 0.049485624132692155\n",
      "Iteration 7616, BCE loss: 57.75734372206597, Acc: 0.8195, Grad norm: 0.05583507988182651\n",
      "Iteration 7617, BCE loss: 57.75733968277702, Acc: 0.8195, Grad norm: 0.05270049636599029\n",
      "Iteration 7618, BCE loss: 57.75735592842804, Acc: 0.8195, Grad norm: 0.06077194846503999\n",
      "Iteration 7619, BCE loss: 57.75735737479338, Acc: 0.8195, Grad norm: 0.059983860923494996\n",
      "Iteration 7620, BCE loss: 57.7573612400343, Acc: 0.8195, Grad norm: 0.06194731247236577\n",
      "Iteration 7621, BCE loss: 57.75733267988561, Acc: 0.8195, Grad norm: 0.05151924682885226\n",
      "Iteration 7622, BCE loss: 57.75735678651945, Acc: 0.8196, Grad norm: 0.059537969829668726\n",
      "Iteration 7623, BCE loss: 57.7573695371361, Acc: 0.8196, Grad norm: 0.062475270043015604\n",
      "Iteration 7624, BCE loss: 57.75734886327146, Acc: 0.8196, Grad norm: 0.05703049648679251\n",
      "Iteration 7625, BCE loss: 57.75734932468906, Acc: 0.8196, Grad norm: 0.059476946029359525\n",
      "Iteration 7626, BCE loss: 57.75734077913274, Acc: 0.8195, Grad norm: 0.05825708764625121\n",
      "Iteration 7627, BCE loss: 57.757317176375196, Acc: 0.8195, Grad norm: 0.04972153989023524\n",
      "Iteration 7628, BCE loss: 57.75730311445978, Acc: 0.8195, Grad norm: 0.042027354972238355\n",
      "Iteration 7629, BCE loss: 57.7573157029299, Acc: 0.8195, Grad norm: 0.04751067908737397\n",
      "Iteration 7630, BCE loss: 57.75735397971221, Acc: 0.8195, Grad norm: 0.06181684792900915\n",
      "Iteration 7631, BCE loss: 57.757347296908776, Acc: 0.8195, Grad norm: 0.057745315625900996\n",
      "Iteration 7632, BCE loss: 57.75735503189727, Acc: 0.8195, Grad norm: 0.06019881522116888\n",
      "Iteration 7633, BCE loss: 57.75731811333132, Acc: 0.8195, Grad norm: 0.04663229694824889\n",
      "Iteration 7634, BCE loss: 57.7573057394593, Acc: 0.8195, Grad norm: 0.04141130911055067\n",
      "Iteration 7635, BCE loss: 57.757308167023965, Acc: 0.8195, Grad norm: 0.043651349805431616\n",
      "Iteration 7636, BCE loss: 57.75729127876099, Acc: 0.8195, Grad norm: 0.034039625225755224\n",
      "Iteration 7637, BCE loss: 57.75729815080318, Acc: 0.8195, Grad norm: 0.03797770770821948\n",
      "Iteration 7638, BCE loss: 57.75729969116229, Acc: 0.8195, Grad norm: 0.03912540145060484\n",
      "Iteration 7639, BCE loss: 57.75727352762511, Acc: 0.8195, Grad norm: 0.02451059688051192\n",
      "Iteration 7640, BCE loss: 57.75727853219169, Acc: 0.8195, Grad norm: 0.029479476644704454\n",
      "Iteration 7641, BCE loss: 57.75728036982319, Acc: 0.8195, Grad norm: 0.029127599038826002\n",
      "Iteration 7642, BCE loss: 57.75729743042662, Acc: 0.8195, Grad norm: 0.038837154200321196\n",
      "Iteration 7643, BCE loss: 57.757342298415594, Acc: 0.8195, Grad norm: 0.059713003711000176\n",
      "Iteration 7644, BCE loss: 57.75734180376409, Acc: 0.8195, Grad norm: 0.05953480515876106\n",
      "Iteration 7645, BCE loss: 57.75736448313045, Acc: 0.8196, Grad norm: 0.06786331166610526\n",
      "Iteration 7646, BCE loss: 57.757342980642164, Acc: 0.8196, Grad norm: 0.060654973231047196\n",
      "Iteration 7647, BCE loss: 57.75732126517676, Acc: 0.8196, Grad norm: 0.05121747113171984\n",
      "Iteration 7648, BCE loss: 57.757323503265994, Acc: 0.8196, Grad norm: 0.05132906557172335\n",
      "Iteration 7649, BCE loss: 57.75731258384067, Acc: 0.8196, Grad norm: 0.04599332744855562\n",
      "Iteration 7650, BCE loss: 57.75730489341798, Acc: 0.8196, Grad norm: 0.040877699498656245\n",
      "Iteration 7651, BCE loss: 57.75732039255807, Acc: 0.8196, Grad norm: 0.04909444459652074\n",
      "Iteration 7652, BCE loss: 57.757287678147435, Acc: 0.8195, Grad norm: 0.03342312533285108\n",
      "Iteration 7653, BCE loss: 57.7572933971164, Acc: 0.8195, Grad norm: 0.034710822472366565\n",
      "Iteration 7654, BCE loss: 57.75730826182012, Acc: 0.8196, Grad norm: 0.044619180271487424\n",
      "Iteration 7655, BCE loss: 57.757315406090356, Acc: 0.8196, Grad norm: 0.04928656847701827\n",
      "Iteration 7656, BCE loss: 57.75730793149519, Acc: 0.8196, Grad norm: 0.04442502599125484\n",
      "Iteration 7657, BCE loss: 57.75731391764797, Acc: 0.8196, Grad norm: 0.0467462462560438\n",
      "Iteration 7658, BCE loss: 57.7573346869675, Acc: 0.8196, Grad norm: 0.056750101575425996\n",
      "Iteration 7659, BCE loss: 57.75736782732008, Acc: 0.8196, Grad norm: 0.06830739370242649\n",
      "Iteration 7660, BCE loss: 57.75741597555785, Acc: 0.8196, Grad norm: 0.0822261174077865\n",
      "Iteration 7661, BCE loss: 57.75737214466568, Acc: 0.8196, Grad norm: 0.06646088947888994\n",
      "Iteration 7662, BCE loss: 57.7573948100202, Acc: 0.8196, Grad norm: 0.07143912684417843\n",
      "Iteration 7663, BCE loss: 57.75739251680011, Acc: 0.8196, Grad norm: 0.07006366723491328\n",
      "Iteration 7664, BCE loss: 57.75737240461895, Acc: 0.8196, Grad norm: 0.06356074591505133\n",
      "Iteration 7665, BCE loss: 57.75737709034553, Acc: 0.8196, Grad norm: 0.06473878851858393\n",
      "Iteration 7666, BCE loss: 57.75733298949927, Acc: 0.8196, Grad norm: 0.04919968943227676\n",
      "Iteration 7667, BCE loss: 57.75735691789535, Acc: 0.8195, Grad norm: 0.05876488824885364\n",
      "Iteration 7668, BCE loss: 57.75735268459566, Acc: 0.8195, Grad norm: 0.057728323155015616\n",
      "Iteration 7669, BCE loss: 57.757343950897294, Acc: 0.8195, Grad norm: 0.05489968478396406\n",
      "Iteration 7670, BCE loss: 57.75733508502712, Acc: 0.8195, Grad norm: 0.051213882243876\n",
      "Iteration 7671, BCE loss: 57.7573374199476, Acc: 0.8195, Grad norm: 0.053644345215739846\n",
      "Iteration 7672, BCE loss: 57.75734589500982, Acc: 0.8195, Grad norm: 0.05730643595143657\n",
      "Iteration 7673, BCE loss: 57.75739112747985, Acc: 0.8195, Grad norm: 0.0732824474305013\n",
      "Iteration 7674, BCE loss: 57.7574147675937, Acc: 0.8195, Grad norm: 0.08120565586807864\n",
      "Iteration 7675, BCE loss: 57.75744467091239, Acc: 0.8195, Grad norm: 0.09100844922310501\n",
      "Iteration 7676, BCE loss: 57.757475752976745, Acc: 0.8195, Grad norm: 0.09912082211567746\n",
      "Iteration 7677, BCE loss: 57.75742739296301, Acc: 0.8195, Grad norm: 0.08654001206138896\n",
      "Iteration 7678, BCE loss: 57.7574648115518, Acc: 0.8195, Grad norm: 0.09644071838411568\n",
      "Iteration 7679, BCE loss: 57.7574877354467, Acc: 0.8196, Grad norm: 0.10273135895027768\n",
      "Iteration 7680, BCE loss: 57.757482073821066, Acc: 0.8196, Grad norm: 0.10118359525269528\n",
      "Iteration 7681, BCE loss: 57.75746745336582, Acc: 0.8196, Grad norm: 0.09711758359901139\n",
      "Iteration 7682, BCE loss: 57.757399139900826, Acc: 0.8196, Grad norm: 0.07799626994405726\n",
      "Iteration 7683, BCE loss: 57.757396444783254, Acc: 0.8196, Grad norm: 0.07642373386432749\n",
      "Iteration 7684, BCE loss: 57.75739543691736, Acc: 0.8195, Grad norm: 0.07470639953025972\n",
      "Iteration 7685, BCE loss: 57.757427268734105, Acc: 0.8195, Grad norm: 0.08300524578713075\n",
      "Iteration 7686, BCE loss: 57.75738595026701, Acc: 0.8196, Grad norm: 0.07236566351815904\n",
      "Iteration 7687, BCE loss: 57.75739039095359, Acc: 0.8196, Grad norm: 0.075362427714456\n",
      "Iteration 7688, BCE loss: 57.75736184432745, Acc: 0.8196, Grad norm: 0.06538481051764379\n",
      "Iteration 7689, BCE loss: 57.75738138835388, Acc: 0.8196, Grad norm: 0.07228194283685935\n",
      "Iteration 7690, BCE loss: 57.75732579306475, Acc: 0.8196, Grad norm: 0.05279468067972426\n",
      "Iteration 7691, BCE loss: 57.75731918113127, Acc: 0.8196, Grad norm: 0.05036529322459717\n",
      "Iteration 7692, BCE loss: 57.7573083663784, Acc: 0.8195, Grad norm: 0.0449458109469778\n",
      "Iteration 7693, BCE loss: 57.757320303272, Acc: 0.8195, Grad norm: 0.050750202389688574\n",
      "Iteration 7694, BCE loss: 57.757345846351356, Acc: 0.8195, Grad norm: 0.062467182618035716\n",
      "Iteration 7695, BCE loss: 57.75734115408292, Acc: 0.8195, Grad norm: 0.06067077383787391\n",
      "Iteration 7696, BCE loss: 57.75736770928694, Acc: 0.8195, Grad norm: 0.07078268530313038\n",
      "Iteration 7697, BCE loss: 57.75736507021941, Acc: 0.8195, Grad norm: 0.06988220290619353\n",
      "Iteration 7698, BCE loss: 57.757328986969256, Acc: 0.8195, Grad norm: 0.05695560518722279\n",
      "Iteration 7699, BCE loss: 57.757326090163076, Acc: 0.8195, Grad norm: 0.05545406797775875\n",
      "Iteration 7700, BCE loss: 57.75730179926758, Acc: 0.8195, Grad norm: 0.044463270355115964\n",
      "Iteration 7701, BCE loss: 57.75727597568158, Acc: 0.8195, Grad norm: 0.0240890262556332\n",
      "Iteration 7702, BCE loss: 57.7572942507753, Acc: 0.8195, Grad norm: 0.04013818275883529\n",
      "Iteration 7703, BCE loss: 57.75728314932586, Acc: 0.8195, Grad norm: 0.03133103710226849\n",
      "Iteration 7704, BCE loss: 57.75731965489257, Acc: 0.8195, Grad norm: 0.04990729486749685\n",
      "Iteration 7705, BCE loss: 57.75736306291442, Acc: 0.8196, Grad norm: 0.06376442351713388\n",
      "Iteration 7706, BCE loss: 57.75736842185978, Acc: 0.8196, Grad norm: 0.06494595322720086\n",
      "Iteration 7707, BCE loss: 57.757372521521475, Acc: 0.8195, Grad norm: 0.06617131442917486\n",
      "Iteration 7708, BCE loss: 57.75731490933734, Acc: 0.8196, Grad norm: 0.04399800366169215\n",
      "Iteration 7709, BCE loss: 57.757342017373084, Acc: 0.8196, Grad norm: 0.055211654638962056\n",
      "Iteration 7710, BCE loss: 57.75739310264458, Acc: 0.8196, Grad norm: 0.07117598617261402\n",
      "Iteration 7711, BCE loss: 57.757426923598345, Acc: 0.8196, Grad norm: 0.08047382013735466\n",
      "Iteration 7712, BCE loss: 57.757413958017096, Acc: 0.8196, Grad norm: 0.07936519916381164\n",
      "Iteration 7713, BCE loss: 57.75741645367918, Acc: 0.8196, Grad norm: 0.08043870573534052\n",
      "Iteration 7714, BCE loss: 57.75743969160597, Acc: 0.8196, Grad norm: 0.08506759435505747\n",
      "Iteration 7715, BCE loss: 57.757416829383004, Acc: 0.8196, Grad norm: 0.08063677370375064\n",
      "Iteration 7716, BCE loss: 57.75736590324145, Acc: 0.8196, Grad norm: 0.0666272070113557\n",
      "Iteration 7717, BCE loss: 57.757409793028565, Acc: 0.8196, Grad norm: 0.08161942887522496\n",
      "Iteration 7718, BCE loss: 57.75738382400293, Acc: 0.8196, Grad norm: 0.07443472110647102\n",
      "Iteration 7719, BCE loss: 57.75742853010917, Acc: 0.8196, Grad norm: 0.08497950049991536\n",
      "Iteration 7720, BCE loss: 57.757477072832486, Acc: 0.8196, Grad norm: 0.09845099448973453\n",
      "Iteration 7721, BCE loss: 57.7575066182534, Acc: 0.8196, Grad norm: 0.10450059462432096\n",
      "Iteration 7722, BCE loss: 57.75745185953247, Acc: 0.8196, Grad norm: 0.09230055104681928\n",
      "Iteration 7723, BCE loss: 57.75745953170045, Acc: 0.8196, Grad norm: 0.09410112479733118\n",
      "Iteration 7724, BCE loss: 57.75743741288011, Acc: 0.8196, Grad norm: 0.08708865921280512\n",
      "Iteration 7725, BCE loss: 57.75746920326743, Acc: 0.8196, Grad norm: 0.09265554375789088\n",
      "Iteration 7726, BCE loss: 57.7574085522846, Acc: 0.8196, Grad norm: 0.07800977085785599\n",
      "Iteration 7727, BCE loss: 57.757357812065834, Acc: 0.8196, Grad norm: 0.05871124751135624\n",
      "Iteration 7728, BCE loss: 57.757336990678155, Acc: 0.8196, Grad norm: 0.05068570251914768\n",
      "Iteration 7729, BCE loss: 57.757376013097826, Acc: 0.8195, Grad norm: 0.0664984952703509\n",
      "Iteration 7730, BCE loss: 57.75735354824408, Acc: 0.8195, Grad norm: 0.06104528962300921\n",
      "Iteration 7731, BCE loss: 57.75738291147739, Acc: 0.8195, Grad norm: 0.06956627443277431\n",
      "Iteration 7732, BCE loss: 57.75738749699835, Acc: 0.8195, Grad norm: 0.07152317262516306\n",
      "Iteration 7733, BCE loss: 57.757382356131515, Acc: 0.8196, Grad norm: 0.07036343928619268\n",
      "Iteration 7734, BCE loss: 57.757421194899706, Acc: 0.8196, Grad norm: 0.08198985146255065\n",
      "Iteration 7735, BCE loss: 57.75736501159852, Acc: 0.8195, Grad norm: 0.06550291194927298\n",
      "Iteration 7736, BCE loss: 57.75737674398794, Acc: 0.8196, Grad norm: 0.06909694961304971\n",
      "Iteration 7737, BCE loss: 57.757348942962224, Acc: 0.8196, Grad norm: 0.05895170926678032\n",
      "Iteration 7738, BCE loss: 57.75735801864597, Acc: 0.8196, Grad norm: 0.06300013141773086\n",
      "Iteration 7739, BCE loss: 57.757350524180154, Acc: 0.8196, Grad norm: 0.05953682604799023\n",
      "Iteration 7740, BCE loss: 57.757322668356764, Acc: 0.8196, Grad norm: 0.04860679742876645\n",
      "Iteration 7741, BCE loss: 57.757358302780695, Acc: 0.8196, Grad norm: 0.0614562963572649\n",
      "Iteration 7742, BCE loss: 57.75738560485976, Acc: 0.8196, Grad norm: 0.07042628232690695\n",
      "Iteration 7743, BCE loss: 57.757421674645606, Acc: 0.8196, Grad norm: 0.08207234577572617\n",
      "Iteration 7744, BCE loss: 57.7574120558419, Acc: 0.8196, Grad norm: 0.07892320442029227\n",
      "Iteration 7745, BCE loss: 57.75735322764495, Acc: 0.8196, Grad norm: 0.06139686538750977\n",
      "Iteration 7746, BCE loss: 57.757340971548665, Acc: 0.8196, Grad norm: 0.05696667526766558\n",
      "Iteration 7747, BCE loss: 57.7573353200477, Acc: 0.8196, Grad norm: 0.05530329620818204\n",
      "Iteration 7748, BCE loss: 57.75733138788112, Acc: 0.8196, Grad norm: 0.05558709399896228\n",
      "Iteration 7749, BCE loss: 57.75735499132656, Acc: 0.8196, Grad norm: 0.06413995098681745\n",
      "Iteration 7750, BCE loss: 57.7573029979163, Acc: 0.8196, Grad norm: 0.04212587063836211\n",
      "Iteration 7751, BCE loss: 57.757311098614515, Acc: 0.8196, Grad norm: 0.04538364066855506\n",
      "Iteration 7752, BCE loss: 57.75730854704355, Acc: 0.8196, Grad norm: 0.043569926968250294\n",
      "Iteration 7753, BCE loss: 57.757305007021, Acc: 0.8196, Grad norm: 0.04088872576639792\n",
      "Iteration 7754, BCE loss: 57.75732782976226, Acc: 0.8196, Grad norm: 0.05233758808655158\n",
      "Iteration 7755, BCE loss: 57.757353024002875, Acc: 0.8196, Grad norm: 0.06230409937349359\n",
      "Iteration 7756, BCE loss: 57.75740655283812, Acc: 0.8196, Grad norm: 0.0786748903865035\n",
      "Iteration 7757, BCE loss: 57.757410317427045, Acc: 0.8196, Grad norm: 0.07979308668185447\n",
      "Iteration 7758, BCE loss: 57.75743346345365, Acc: 0.8196, Grad norm: 0.08591418451505721\n",
      "Iteration 7759, BCE loss: 57.75738535007205, Acc: 0.8196, Grad norm: 0.0721170759627586\n",
      "Iteration 7760, BCE loss: 57.757417348711144, Acc: 0.8196, Grad norm: 0.08040667233559592\n",
      "Iteration 7761, BCE loss: 57.75741838521269, Acc: 0.8196, Grad norm: 0.08001102815480518\n",
      "Iteration 7762, BCE loss: 57.75745044172542, Acc: 0.8196, Grad norm: 0.08806190412405018\n",
      "Iteration 7763, BCE loss: 57.75738824884389, Acc: 0.8196, Grad norm: 0.07145021799082489\n",
      "Iteration 7764, BCE loss: 57.757416105211014, Acc: 0.8196, Grad norm: 0.07909211428477145\n",
      "Iteration 7765, BCE loss: 57.75750193676586, Acc: 0.8196, Grad norm: 0.10036295729854323\n",
      "Iteration 7766, BCE loss: 57.757520080949774, Acc: 0.8196, Grad norm: 0.10475293253633473\n",
      "Iteration 7767, BCE loss: 57.757514241918955, Acc: 0.8196, Grad norm: 0.1028357803173454\n",
      "Iteration 7768, BCE loss: 57.75761731115735, Acc: 0.8196, Grad norm: 0.12449443265601653\n",
      "Iteration 7769, BCE loss: 57.757598749585796, Acc: 0.8196, Grad norm: 0.12038695791793451\n",
      "Iteration 7770, BCE loss: 57.75751510291455, Acc: 0.8196, Grad norm: 0.10306249218035941\n",
      "Iteration 7771, BCE loss: 57.75747330374362, Acc: 0.8196, Grad norm: 0.09316182981151133\n",
      "Iteration 7772, BCE loss: 57.757510845431696, Acc: 0.8196, Grad norm: 0.10256189500503092\n",
      "Iteration 7773, BCE loss: 57.757529348995305, Acc: 0.8196, Grad norm: 0.10841515115642823\n",
      "Iteration 7774, BCE loss: 57.757488951134206, Acc: 0.8196, Grad norm: 0.10005178029049215\n",
      "Iteration 7775, BCE loss: 57.757451208938846, Acc: 0.8196, Grad norm: 0.09021397247421252\n",
      "Iteration 7776, BCE loss: 57.75751471199354, Acc: 0.8196, Grad norm: 0.10459698045081246\n",
      "Iteration 7777, BCE loss: 57.75745271495305, Acc: 0.8196, Grad norm: 0.09045841657443222\n",
      "Iteration 7778, BCE loss: 57.757396054226334, Acc: 0.8196, Grad norm: 0.07421946644687585\n",
      "Iteration 7779, BCE loss: 57.7573675652707, Acc: 0.8196, Grad norm: 0.062495208632479315\n",
      "Iteration 7780, BCE loss: 57.757402434493095, Acc: 0.8196, Grad norm: 0.07443502293866121\n",
      "Iteration 7781, BCE loss: 57.75735962742725, Acc: 0.8196, Grad norm: 0.061780521588989286\n",
      "Iteration 7782, BCE loss: 57.757339101054185, Acc: 0.8196, Grad norm: 0.055094823511490365\n",
      "Iteration 7783, BCE loss: 57.75742139527409, Acc: 0.8196, Grad norm: 0.08334219082104184\n",
      "Iteration 7784, BCE loss: 57.75740657760589, Acc: 0.8196, Grad norm: 0.07942014757062862\n",
      "Iteration 7785, BCE loss: 57.757385735357985, Acc: 0.8196, Grad norm: 0.07358158918279707\n",
      "Iteration 7786, BCE loss: 57.75733658228549, Acc: 0.8196, Grad norm: 0.05763581901153668\n",
      "Iteration 7787, BCE loss: 57.757352750774515, Acc: 0.8196, Grad norm: 0.063856830751107\n",
      "Iteration 7788, BCE loss: 57.757351044138666, Acc: 0.8196, Grad norm: 0.062397078418711525\n",
      "Iteration 7789, BCE loss: 57.75738475380588, Acc: 0.8196, Grad norm: 0.0728638030199106\n",
      "Iteration 7790, BCE loss: 57.75734113749745, Acc: 0.8196, Grad norm: 0.05707753366512195\n",
      "Iteration 7791, BCE loss: 57.75734639735696, Acc: 0.8196, Grad norm: 0.05807143939070525\n",
      "Iteration 7792, BCE loss: 57.757352448496064, Acc: 0.8196, Grad norm: 0.06136983915309747\n",
      "Iteration 7793, BCE loss: 57.757345545891795, Acc: 0.8196, Grad norm: 0.05674454684988735\n",
      "Iteration 7794, BCE loss: 57.757323188218194, Acc: 0.8196, Grad norm: 0.04995875117425919\n",
      "Iteration 7795, BCE loss: 57.75735320696795, Acc: 0.8196, Grad norm: 0.06288152777790884\n",
      "Iteration 7796, BCE loss: 57.75735973739735, Acc: 0.8196, Grad norm: 0.06580605171033067\n",
      "Iteration 7797, BCE loss: 57.75736977173644, Acc: 0.8196, Grad norm: 0.0693121962021045\n",
      "Iteration 7798, BCE loss: 57.7573824412961, Acc: 0.8196, Grad norm: 0.07354011242910109\n",
      "Iteration 7799, BCE loss: 57.75744688488648, Acc: 0.8196, Grad norm: 0.08924941085407903\n",
      "Iteration 7800, BCE loss: 57.757421709734075, Acc: 0.8196, Grad norm: 0.08154531770870992\n",
      "Iteration 7801, BCE loss: 57.7573904524738, Acc: 0.8196, Grad norm: 0.07203043502967386\n",
      "Iteration 7802, BCE loss: 57.75735139756201, Acc: 0.8196, Grad norm: 0.05958064802312765\n",
      "Iteration 7803, BCE loss: 57.75735170716088, Acc: 0.8196, Grad norm: 0.05870425835862246\n",
      "Iteration 7804, BCE loss: 57.757333918231, Acc: 0.8196, Grad norm: 0.05349034315417555\n",
      "Iteration 7805, BCE loss: 57.75733828757651, Acc: 0.8196, Grad norm: 0.0552998161655204\n",
      "Iteration 7806, BCE loss: 57.757345825986675, Acc: 0.8196, Grad norm: 0.059602366807858874\n",
      "Iteration 7807, BCE loss: 57.757292602449766, Acc: 0.8196, Grad norm: 0.03503804787706502\n",
      "Iteration 7808, BCE loss: 57.7572958592487, Acc: 0.8196, Grad norm: 0.037035236113670975\n",
      "Iteration 7809, BCE loss: 57.75729633900897, Acc: 0.8196, Grad norm: 0.036574366051424057\n",
      "Iteration 7810, BCE loss: 57.757304344689686, Acc: 0.8195, Grad norm: 0.0418103731344249\n",
      "Iteration 7811, BCE loss: 57.757325796544976, Acc: 0.8195, Grad norm: 0.05229310673935364\n",
      "Iteration 7812, BCE loss: 57.757300421466255, Acc: 0.8195, Grad norm: 0.03872351178454106\n",
      "Iteration 7813, BCE loss: 57.75733844970043, Acc: 0.8195, Grad norm: 0.056491482681114036\n",
      "Iteration 7814, BCE loss: 57.757372892427284, Acc: 0.8195, Grad norm: 0.06908777598947881\n",
      "Iteration 7815, BCE loss: 57.757364814091446, Acc: 0.8195, Grad norm: 0.06531675096593985\n",
      "Iteration 7816, BCE loss: 57.75733429901781, Acc: 0.8196, Grad norm: 0.052226620511887534\n",
      "Iteration 7817, BCE loss: 57.75737322691158, Acc: 0.8195, Grad norm: 0.06856422381402345\n",
      "Iteration 7818, BCE loss: 57.75735965581872, Acc: 0.8195, Grad norm: 0.06510607252198454\n",
      "Iteration 7819, BCE loss: 57.757317996593386, Acc: 0.8195, Grad norm: 0.04805616078408883\n",
      "Iteration 7820, BCE loss: 57.757333541251604, Acc: 0.8195, Grad norm: 0.05615370029572362\n",
      "Iteration 7821, BCE loss: 57.75731383822772, Acc: 0.8196, Grad norm: 0.04716567306619095\n",
      "Iteration 7822, BCE loss: 57.75729537500495, Acc: 0.8196, Grad norm: 0.035229606812559724\n",
      "Iteration 7823, BCE loss: 57.75728128441885, Acc: 0.8196, Grad norm: 0.027813399279535224\n",
      "Iteration 7824, BCE loss: 57.757293383899764, Acc: 0.8196, Grad norm: 0.034317809476549735\n",
      "Iteration 7825, BCE loss: 57.75729572637279, Acc: 0.8195, Grad norm: 0.03673483499962044\n",
      "Iteration 7826, BCE loss: 57.757317727325216, Acc: 0.8195, Grad norm: 0.04889786808741228\n",
      "Iteration 7827, BCE loss: 57.75734227580533, Acc: 0.8195, Grad norm: 0.05838926969648787\n",
      "Iteration 7828, BCE loss: 57.75733786898168, Acc: 0.8195, Grad norm: 0.05505364372742179\n",
      "Iteration 7829, BCE loss: 57.75733841225993, Acc: 0.8195, Grad norm: 0.05375249989632255\n",
      "Iteration 7830, BCE loss: 57.757327784151585, Acc: 0.8195, Grad norm: 0.05027479218097605\n",
      "Iteration 7831, BCE loss: 57.757333299395036, Acc: 0.8195, Grad norm: 0.05267844098974866\n",
      "Iteration 7832, BCE loss: 57.757334836724915, Acc: 0.8195, Grad norm: 0.05300342104312771\n",
      "Iteration 7833, BCE loss: 57.757319486304056, Acc: 0.8195, Grad norm: 0.048137614620218594\n",
      "Iteration 7834, BCE loss: 57.75735150982523, Acc: 0.8195, Grad norm: 0.0605429525368983\n",
      "Iteration 7835, BCE loss: 57.75731540741817, Acc: 0.8195, Grad norm: 0.04534437658854928\n",
      "Iteration 7836, BCE loss: 57.75732618469635, Acc: 0.8195, Grad norm: 0.05065822777729496\n",
      "Iteration 7837, BCE loss: 57.7573344290867, Acc: 0.8195, Grad norm: 0.055931416349111716\n",
      "Iteration 7838, BCE loss: 57.75734184058591, Acc: 0.8195, Grad norm: 0.05859804513030679\n",
      "Iteration 7839, BCE loss: 57.75734065363295, Acc: 0.8195, Grad norm: 0.05791523973424957\n",
      "Iteration 7840, BCE loss: 57.75735023552791, Acc: 0.8195, Grad norm: 0.06206579961082518\n",
      "Iteration 7841, BCE loss: 57.75731322891821, Acc: 0.8195, Grad norm: 0.04642275100085311\n",
      "Iteration 7842, BCE loss: 57.75731965216271, Acc: 0.8196, Grad norm: 0.04902560769181091\n",
      "Iteration 7843, BCE loss: 57.757359016866495, Acc: 0.8196, Grad norm: 0.0638492486909703\n",
      "Iteration 7844, BCE loss: 57.75736450687306, Acc: 0.8196, Grad norm: 0.06393037608423147\n",
      "Iteration 7845, BCE loss: 57.75733505765868, Acc: 0.8196, Grad norm: 0.05295123909692854\n",
      "Iteration 7846, BCE loss: 57.7573842272878, Acc: 0.8196, Grad norm: 0.07094459901315295\n",
      "Iteration 7847, BCE loss: 57.75737847376536, Acc: 0.8196, Grad norm: 0.0680849042828565\n",
      "Iteration 7848, BCE loss: 57.75734857055606, Acc: 0.8196, Grad norm: 0.06117922272443826\n",
      "Iteration 7849, BCE loss: 57.75733872319667, Acc: 0.8196, Grad norm: 0.05844315100196815\n",
      "Iteration 7850, BCE loss: 57.757314857135654, Acc: 0.8196, Grad norm: 0.04817438641432297\n",
      "Iteration 7851, BCE loss: 57.75733740423851, Acc: 0.8196, Grad norm: 0.05826728943713099\n",
      "Iteration 7852, BCE loss: 57.757343975144934, Acc: 0.8195, Grad norm: 0.05819427764721464\n",
      "Iteration 7853, BCE loss: 57.75731476042885, Acc: 0.8196, Grad norm: 0.04743460518036505\n",
      "Iteration 7854, BCE loss: 57.75730483166135, Acc: 0.8196, Grad norm: 0.04252939219387873\n",
      "Iteration 7855, BCE loss: 57.75731517463076, Acc: 0.8196, Grad norm: 0.045968109145724315\n",
      "Iteration 7856, BCE loss: 57.75740380711932, Acc: 0.8196, Grad norm: 0.07576687916766887\n",
      "Iteration 7857, BCE loss: 57.757353563182264, Acc: 0.8196, Grad norm: 0.060975360428785035\n",
      "Iteration 7858, BCE loss: 57.7573243060176, Acc: 0.8196, Grad norm: 0.05018276948121116\n",
      "Iteration 7859, BCE loss: 57.757357804825745, Acc: 0.8196, Grad norm: 0.064013210693598\n",
      "Iteration 7860, BCE loss: 57.7573381862318, Acc: 0.8196, Grad norm: 0.05793888088711969\n",
      "Iteration 7861, BCE loss: 57.757329583643966, Acc: 0.8196, Grad norm: 0.05622419824724074\n",
      "Iteration 7862, BCE loss: 57.75734055094547, Acc: 0.8196, Grad norm: 0.0599536639563986\n",
      "Iteration 7863, BCE loss: 57.75730389634408, Acc: 0.8196, Grad norm: 0.0441307203247287\n",
      "Iteration 7864, BCE loss: 57.75728963542874, Acc: 0.8196, Grad norm: 0.03604602428934381\n",
      "Iteration 7865, BCE loss: 57.75729442016815, Acc: 0.8196, Grad norm: 0.037312800047024954\n",
      "Iteration 7866, BCE loss: 57.757302532405085, Acc: 0.8196, Grad norm: 0.040185544963381746\n",
      "Iteration 7867, BCE loss: 57.7573188103777, Acc: 0.8196, Grad norm: 0.049055839284155585\n",
      "Iteration 7868, BCE loss: 57.757298494903615, Acc: 0.8196, Grad norm: 0.03844383600579801\n",
      "Iteration 7869, BCE loss: 57.75730387171415, Acc: 0.8196, Grad norm: 0.041490470030522704\n",
      "Iteration 7870, BCE loss: 57.75729856993114, Acc: 0.8196, Grad norm: 0.03907384068955964\n",
      "Iteration 7871, BCE loss: 57.757301965429804, Acc: 0.8196, Grad norm: 0.04132682750413276\n",
      "Iteration 7872, BCE loss: 57.7573239614522, Acc: 0.8196, Grad norm: 0.050663403340077594\n",
      "Iteration 7873, BCE loss: 57.75732138966016, Acc: 0.8196, Grad norm: 0.051013754068541375\n",
      "Iteration 7874, BCE loss: 57.75733590384294, Acc: 0.8196, Grad norm: 0.05679631381533636\n",
      "Iteration 7875, BCE loss: 57.75735563424749, Acc: 0.8196, Grad norm: 0.06519865359354858\n",
      "Iteration 7876, BCE loss: 57.757347045168665, Acc: 0.8196, Grad norm: 0.060542554904055815\n",
      "Iteration 7877, BCE loss: 57.75733238172818, Acc: 0.8196, Grad norm: 0.05450348136180687\n",
      "Iteration 7878, BCE loss: 57.757358563500375, Acc: 0.8196, Grad norm: 0.06398493143049985\n",
      "Iteration 7879, BCE loss: 57.75739429861564, Acc: 0.8196, Grad norm: 0.07644019346916837\n",
      "Iteration 7880, BCE loss: 57.75735859545004, Acc: 0.8196, Grad norm: 0.0645141733671364\n",
      "Iteration 7881, BCE loss: 57.75735481746219, Acc: 0.8195, Grad norm: 0.05972801778490914\n",
      "Iteration 7882, BCE loss: 57.75733649188045, Acc: 0.8195, Grad norm: 0.05297986206737146\n",
      "Iteration 7883, BCE loss: 57.75733236086616, Acc: 0.8196, Grad norm: 0.05248896239046777\n",
      "Iteration 7884, BCE loss: 57.75734483050608, Acc: 0.8195, Grad norm: 0.056691682229675036\n",
      "Iteration 7885, BCE loss: 57.75732869374879, Acc: 0.8195, Grad norm: 0.0497221407764579\n",
      "Iteration 7886, BCE loss: 57.757314159420865, Acc: 0.8196, Grad norm: 0.045917845653967275\n",
      "Iteration 7887, BCE loss: 57.75729836805298, Acc: 0.8196, Grad norm: 0.03934149495780809\n",
      "Iteration 7888, BCE loss: 57.75729396637063, Acc: 0.8196, Grad norm: 0.03866476319804027\n",
      "Iteration 7889, BCE loss: 57.757305787683634, Acc: 0.8196, Grad norm: 0.0440954792662286\n",
      "Iteration 7890, BCE loss: 57.75731482059132, Acc: 0.8196, Grad norm: 0.04917860894894243\n",
      "Iteration 7891, BCE loss: 57.75729859038794, Acc: 0.8196, Grad norm: 0.03978250842358489\n",
      "Iteration 7892, BCE loss: 57.75728765104067, Acc: 0.8195, Grad norm: 0.033061020330738745\n",
      "Iteration 7893, BCE loss: 57.75727998205134, Acc: 0.8196, Grad norm: 0.02807723410698639\n",
      "Iteration 7894, BCE loss: 57.757315417747115, Acc: 0.8196, Grad norm: 0.04705581801098789\n",
      "Iteration 7895, BCE loss: 57.75731759168113, Acc: 0.8196, Grad norm: 0.04964005224104856\n",
      "Iteration 7896, BCE loss: 57.75733545350515, Acc: 0.8196, Grad norm: 0.05732004500836502\n",
      "Iteration 7897, BCE loss: 57.757322905010874, Acc: 0.8195, Grad norm: 0.05273147711726886\n",
      "Iteration 7898, BCE loss: 57.75730585704545, Acc: 0.8196, Grad norm: 0.04693221084648191\n",
      "Iteration 7899, BCE loss: 57.75731833223976, Acc: 0.8196, Grad norm: 0.05309879798653055\n",
      "Iteration 7900, BCE loss: 57.757311998976974, Acc: 0.8196, Grad norm: 0.04954270541401165\n",
      "Iteration 7901, BCE loss: 57.7573165043555, Acc: 0.8196, Grad norm: 0.05228287291113176\n",
      "Iteration 7902, BCE loss: 57.75730929638239, Acc: 0.8196, Grad norm: 0.04717604187723678\n",
      "Iteration 7903, BCE loss: 57.757315233265665, Acc: 0.8196, Grad norm: 0.0490727286359383\n",
      "Iteration 7904, BCE loss: 57.75737543944551, Acc: 0.8196, Grad norm: 0.0713631166560424\n",
      "Iteration 7905, BCE loss: 57.75735255079513, Acc: 0.8196, Grad norm: 0.062118618370512885\n",
      "Iteration 7906, BCE loss: 57.757302965429915, Acc: 0.8196, Grad norm: 0.04048957887900712\n",
      "Iteration 7907, BCE loss: 57.75732310979984, Acc: 0.8196, Grad norm: 0.05047603461171373\n",
      "Iteration 7908, BCE loss: 57.757303975461184, Acc: 0.8196, Grad norm: 0.04314970686931293\n",
      "Iteration 7909, BCE loss: 57.7573267032855, Acc: 0.8196, Grad norm: 0.05233806077070024\n",
      "Iteration 7910, BCE loss: 57.75729422873494, Acc: 0.8196, Grad norm: 0.03686750119180402\n",
      "Iteration 7911, BCE loss: 57.757281588846425, Acc: 0.8196, Grad norm: 0.02702574579446946\n",
      "Iteration 7912, BCE loss: 57.7572927725796, Acc: 0.8196, Grad norm: 0.03493631964670964\n",
      "Iteration 7913, BCE loss: 57.75728774423717, Acc: 0.8196, Grad norm: 0.031465446386673195\n",
      "Iteration 7914, BCE loss: 57.75727761970495, Acc: 0.8196, Grad norm: 0.026002388615805817\n",
      "Iteration 7915, BCE loss: 57.75728386531031, Acc: 0.8196, Grad norm: 0.030690066231627542\n",
      "Iteration 7916, BCE loss: 57.75727862322829, Acc: 0.8196, Grad norm: 0.026447010852856955\n",
      "Iteration 7917, BCE loss: 57.75727778878777, Acc: 0.8196, Grad norm: 0.025543775339792196\n",
      "Iteration 7918, BCE loss: 57.757302482353396, Acc: 0.8196, Grad norm: 0.042563865119820195\n",
      "Iteration 7919, BCE loss: 57.7573524102892, Acc: 0.8196, Grad norm: 0.06493534350635845\n",
      "Iteration 7920, BCE loss: 57.75732142880478, Acc: 0.8196, Grad norm: 0.051289620243673374\n",
      "Iteration 7921, BCE loss: 57.757328230181955, Acc: 0.8196, Grad norm: 0.05090261217576974\n",
      "Iteration 7922, BCE loss: 57.757319140368104, Acc: 0.8196, Grad norm: 0.045451805760208686\n",
      "Iteration 7923, BCE loss: 57.75732027873825, Acc: 0.8196, Grad norm: 0.04468633437247468\n",
      "Iteration 7924, BCE loss: 57.75733238198062, Acc: 0.8196, Grad norm: 0.049824906336837047\n",
      "Iteration 7925, BCE loss: 57.75732829922566, Acc: 0.8196, Grad norm: 0.047951153219046284\n",
      "Iteration 7926, BCE loss: 57.757310507395104, Acc: 0.8196, Grad norm: 0.04204284764911934\n",
      "Iteration 7927, BCE loss: 57.75732535272496, Acc: 0.8196, Grad norm: 0.047305503647755\n",
      "Iteration 7928, BCE loss: 57.75735583511528, Acc: 0.8196, Grad norm: 0.06251535937376167\n",
      "Iteration 7929, BCE loss: 57.75733876623963, Acc: 0.8196, Grad norm: 0.055356288100001844\n",
      "Iteration 7930, BCE loss: 57.75738297735485, Acc: 0.8196, Grad norm: 0.07192467383833581\n",
      "Iteration 7931, BCE loss: 57.757343171947895, Acc: 0.8196, Grad norm: 0.05568221867414626\n",
      "Iteration 7932, BCE loss: 57.757336391407264, Acc: 0.8196, Grad norm: 0.05125572025857244\n",
      "Iteration 7933, BCE loss: 57.757335340037436, Acc: 0.8196, Grad norm: 0.05182611465547057\n",
      "Iteration 7934, BCE loss: 57.75734464323024, Acc: 0.8196, Grad norm: 0.054368268930991455\n",
      "Iteration 7935, BCE loss: 57.75734242551789, Acc: 0.8196, Grad norm: 0.05411824501512554\n",
      "Iteration 7936, BCE loss: 57.75735807595038, Acc: 0.8196, Grad norm: 0.058614491092228056\n",
      "Iteration 7937, BCE loss: 57.757387924709604, Acc: 0.8195, Grad norm: 0.06522843981988004\n",
      "Iteration 7938, BCE loss: 57.757378525259824, Acc: 0.8195, Grad norm: 0.06437222963907122\n",
      "Iteration 7939, BCE loss: 57.75739277537362, Acc: 0.8195, Grad norm: 0.06995875286724863\n",
      "Iteration 7940, BCE loss: 57.75741456316976, Acc: 0.8195, Grad norm: 0.080307956627178\n",
      "Iteration 7941, BCE loss: 57.757388357181696, Acc: 0.8195, Grad norm: 0.07539340323710861\n",
      "Iteration 7942, BCE loss: 57.75736958519604, Acc: 0.8195, Grad norm: 0.06951231505380757\n",
      "Iteration 7943, BCE loss: 57.75737406626524, Acc: 0.8195, Grad norm: 0.07095902676170361\n",
      "Iteration 7944, BCE loss: 57.75734011955662, Acc: 0.8195, Grad norm: 0.05896319297312693\n",
      "Iteration 7945, BCE loss: 57.75736487569239, Acc: 0.8195, Grad norm: 0.06862349763779485\n",
      "Iteration 7946, BCE loss: 57.757423242381904, Acc: 0.8195, Grad norm: 0.08587702062438209\n",
      "Iteration 7947, BCE loss: 57.75746806695225, Acc: 0.8195, Grad norm: 0.09673693048277432\n",
      "Iteration 7948, BCE loss: 57.757375021681824, Acc: 0.8195, Grad norm: 0.06941401275798893\n",
      "Iteration 7949, BCE loss: 57.7573689443272, Acc: 0.8195, Grad norm: 0.06842892294240063\n",
      "Iteration 7950, BCE loss: 57.757361220963816, Acc: 0.8195, Grad norm: 0.06421107612752665\n",
      "Iteration 7951, BCE loss: 57.75736382119641, Acc: 0.8195, Grad norm: 0.06346178296981488\n",
      "Iteration 7952, BCE loss: 57.75736206097508, Acc: 0.8195, Grad norm: 0.06439285885655435\n",
      "Iteration 7953, BCE loss: 57.75738336156558, Acc: 0.8195, Grad norm: 0.07005892552583741\n",
      "Iteration 7954, BCE loss: 57.757412234280935, Acc: 0.8195, Grad norm: 0.08046362282516765\n",
      "Iteration 7955, BCE loss: 57.7573950245427, Acc: 0.8195, Grad norm: 0.07415192667315473\n",
      "Iteration 7956, BCE loss: 57.75737671433369, Acc: 0.8195, Grad norm: 0.06668289153598324\n",
      "Iteration 7957, BCE loss: 57.75736813037754, Acc: 0.8195, Grad norm: 0.06334231865102317\n",
      "Iteration 7958, BCE loss: 57.75734003552739, Acc: 0.8195, Grad norm: 0.051140664050807125\n",
      "Iteration 7959, BCE loss: 57.757341252324125, Acc: 0.8196, Grad norm: 0.05271553895081333\n",
      "Iteration 7960, BCE loss: 57.75731003639265, Acc: 0.8196, Grad norm: 0.04158824693041973\n",
      "Iteration 7961, BCE loss: 57.75732966086224, Acc: 0.8196, Grad norm: 0.04875942108159642\n",
      "Iteration 7962, BCE loss: 57.75732183835575, Acc: 0.8196, Grad norm: 0.04934351656511561\n",
      "Iteration 7963, BCE loss: 57.75732182072389, Acc: 0.8196, Grad norm: 0.04866877833126905\n",
      "Iteration 7964, BCE loss: 57.757333265318096, Acc: 0.8196, Grad norm: 0.05141355564950885\n",
      "Iteration 7965, BCE loss: 57.75735137565676, Acc: 0.8196, Grad norm: 0.05826339140806208\n",
      "Iteration 7966, BCE loss: 57.75736396015307, Acc: 0.8196, Grad norm: 0.06169850590752052\n",
      "Iteration 7967, BCE loss: 57.75735147915262, Acc: 0.8196, Grad norm: 0.05735250075945443\n",
      "Iteration 7968, BCE loss: 57.75736049709997, Acc: 0.8196, Grad norm: 0.06294808640179203\n",
      "Iteration 7969, BCE loss: 57.75734656005432, Acc: 0.8196, Grad norm: 0.05710334262178375\n",
      "Iteration 7970, BCE loss: 57.757404142733876, Acc: 0.8196, Grad norm: 0.07528531047790986\n",
      "Iteration 7971, BCE loss: 57.75743062140609, Acc: 0.8196, Grad norm: 0.0804616789885231\n",
      "Iteration 7972, BCE loss: 57.757449944183776, Acc: 0.8196, Grad norm: 0.08754310384531831\n",
      "Iteration 7973, BCE loss: 57.757405886442726, Acc: 0.8196, Grad norm: 0.07590869550179781\n",
      "Iteration 7974, BCE loss: 57.757324403046695, Acc: 0.8196, Grad norm: 0.050855703064942415\n",
      "Iteration 7975, BCE loss: 57.75737071886864, Acc: 0.8196, Grad norm: 0.06598116222489452\n",
      "Iteration 7976, BCE loss: 57.75737122735, Acc: 0.8196, Grad norm: 0.06748782978794143\n",
      "Iteration 7977, BCE loss: 57.75738560969773, Acc: 0.8196, Grad norm: 0.07027335359205343\n",
      "Iteration 7978, BCE loss: 57.7573603979974, Acc: 0.8196, Grad norm: 0.06322999050805542\n",
      "Iteration 7979, BCE loss: 57.7573717917398, Acc: 0.8196, Grad norm: 0.06608060219095206\n",
      "Iteration 7980, BCE loss: 57.75735693223562, Acc: 0.8196, Grad norm: 0.061374045717431165\n",
      "Iteration 7981, BCE loss: 57.75731340463216, Acc: 0.8196, Grad norm: 0.043928233054234954\n",
      "Iteration 7982, BCE loss: 57.75731086881429, Acc: 0.8196, Grad norm: 0.04252290154889952\n",
      "Iteration 7983, BCE loss: 57.757322522778914, Acc: 0.8196, Grad norm: 0.04825912598623305\n",
      "Iteration 7984, BCE loss: 57.75730183194942, Acc: 0.8196, Grad norm: 0.04087213938609667\n",
      "Iteration 7985, BCE loss: 57.757304529912886, Acc: 0.8196, Grad norm: 0.04166225919533468\n",
      "Iteration 7986, BCE loss: 57.757323596806046, Acc: 0.8196, Grad norm: 0.05038631051020989\n",
      "Iteration 7987, BCE loss: 57.75734083421822, Acc: 0.8196, Grad norm: 0.05984872277764676\n",
      "Iteration 7988, BCE loss: 57.75738112539213, Acc: 0.8196, Grad norm: 0.07282127306475836\n",
      "Iteration 7989, BCE loss: 57.75737774783533, Acc: 0.8196, Grad norm: 0.07125161228546746\n",
      "Iteration 7990, BCE loss: 57.757386896376474, Acc: 0.8196, Grad norm: 0.07255440731906539\n",
      "Iteration 7991, BCE loss: 57.75738868717286, Acc: 0.8196, Grad norm: 0.07111545623835115\n",
      "Iteration 7992, BCE loss: 57.757365269086236, Acc: 0.8196, Grad norm: 0.06366113922810411\n",
      "Iteration 7993, BCE loss: 57.75734665054044, Acc: 0.8196, Grad norm: 0.057318025166032516\n",
      "Iteration 7994, BCE loss: 57.75735007030542, Acc: 0.8196, Grad norm: 0.060633212446773437\n",
      "Iteration 7995, BCE loss: 57.75737885649791, Acc: 0.8196, Grad norm: 0.07096060169067235\n",
      "Iteration 7996, BCE loss: 57.75737738021368, Acc: 0.8196, Grad norm: 0.06767143777449501\n",
      "Iteration 7997, BCE loss: 57.75737072387271, Acc: 0.8196, Grad norm: 0.06614992034494226\n",
      "Iteration 7998, BCE loss: 57.75731913203457, Acc: 0.8196, Grad norm: 0.04618812967417941\n",
      "Iteration 7999, BCE loss: 57.75734051185664, Acc: 0.8196, Grad norm: 0.05583156058759729\n",
      "Iteration 8000, BCE loss: 57.75738198800927, Acc: 0.8196, Grad norm: 0.07161289905369514\n",
      "Iteration 8001, BCE loss: 57.75736365315083, Acc: 0.8196, Grad norm: 0.06607656764827761\n",
      "Iteration 8002, BCE loss: 57.757387781125644, Acc: 0.8196, Grad norm: 0.07440919169720461\n",
      "Iteration 8003, BCE loss: 57.757418711308624, Acc: 0.8196, Grad norm: 0.08425333645030889\n",
      "Iteration 8004, BCE loss: 57.757408540593694, Acc: 0.8196, Grad norm: 0.08175495242827838\n",
      "Iteration 8005, BCE loss: 57.75746297001638, Acc: 0.8196, Grad norm: 0.09650485231374838\n",
      "Iteration 8006, BCE loss: 57.75741368645897, Acc: 0.8196, Grad norm: 0.08364912267260331\n",
      "Iteration 8007, BCE loss: 57.75743973146544, Acc: 0.8196, Grad norm: 0.09172293700777219\n",
      "Iteration 8008, BCE loss: 57.75745916287225, Acc: 0.8196, Grad norm: 0.09637449764371538\n",
      "Iteration 8009, BCE loss: 57.7574636835995, Acc: 0.8196, Grad norm: 0.09842758562679015\n",
      "Iteration 8010, BCE loss: 57.75752541079229, Acc: 0.8196, Grad norm: 0.11261511092342083\n",
      "Iteration 8011, BCE loss: 57.75739997466204, Acc: 0.8196, Grad norm: 0.08022094528533825\n",
      "Iteration 8012, BCE loss: 57.75740233163238, Acc: 0.8196, Grad norm: 0.08116166468501007\n",
      "Iteration 8013, BCE loss: 57.757403783714196, Acc: 0.8196, Grad norm: 0.08097984348380372\n",
      "Iteration 8014, BCE loss: 57.75741307260084, Acc: 0.8196, Grad norm: 0.08373464013406196\n",
      "Iteration 8015, BCE loss: 57.75743902855528, Acc: 0.8196, Grad norm: 0.09036900483358513\n",
      "Iteration 8016, BCE loss: 57.757460945129495, Acc: 0.8196, Grad norm: 0.09578918834881019\n",
      "Iteration 8017, BCE loss: 57.75737925775623, Acc: 0.8196, Grad norm: 0.07320054490687179\n",
      "Iteration 8018, BCE loss: 57.75734499366156, Acc: 0.8196, Grad norm: 0.06217341462212635\n",
      "Iteration 8019, BCE loss: 57.75730785389374, Acc: 0.8196, Grad norm: 0.04594898893599278\n",
      "Iteration 8020, BCE loss: 57.75731001225469, Acc: 0.8196, Grad norm: 0.046668564782147814\n",
      "Iteration 8021, BCE loss: 57.757295258118575, Acc: 0.8196, Grad norm: 0.03860884796288423\n",
      "Iteration 8022, BCE loss: 57.75729860900938, Acc: 0.8196, Grad norm: 0.04059022264297324\n",
      "Iteration 8023, BCE loss: 57.757285537386835, Acc: 0.8196, Grad norm: 0.033317967023542776\n",
      "Iteration 8024, BCE loss: 57.75729424483624, Acc: 0.8195, Grad norm: 0.039850859480585386\n",
      "Iteration 8025, BCE loss: 57.75732864236615, Acc: 0.8195, Grad norm: 0.05531108827570181\n",
      "Iteration 8026, BCE loss: 57.75736318658184, Acc: 0.8195, Grad norm: 0.0676902893742588\n",
      "Iteration 8027, BCE loss: 57.75732367060104, Acc: 0.8195, Grad norm: 0.05203517908964681\n",
      "Iteration 8028, BCE loss: 57.75732215944104, Acc: 0.8195, Grad norm: 0.05203101488319485\n",
      "Iteration 8029, BCE loss: 57.75736317533783, Acc: 0.8195, Grad norm: 0.06624031555035145\n",
      "Iteration 8030, BCE loss: 57.757344653558306, Acc: 0.8195, Grad norm: 0.05806845574124611\n",
      "Iteration 8031, BCE loss: 57.757339728550406, Acc: 0.8195, Grad norm: 0.05685337749450306\n",
      "Iteration 8032, BCE loss: 57.75737859694354, Acc: 0.8195, Grad norm: 0.0688209102916101\n",
      "Iteration 8033, BCE loss: 57.757362294176005, Acc: 0.8195, Grad norm: 0.06436099944055591\n",
      "Iteration 8034, BCE loss: 57.75733571111036, Acc: 0.8195, Grad norm: 0.05535267718691262\n",
      "Iteration 8035, BCE loss: 57.75739194838635, Acc: 0.8195, Grad norm: 0.07599255197495318\n",
      "Iteration 8036, BCE loss: 57.757402525221984, Acc: 0.8195, Grad norm: 0.07997890185148204\n",
      "Iteration 8037, BCE loss: 57.75736941826611, Acc: 0.8195, Grad norm: 0.06913787671719167\n",
      "Iteration 8038, BCE loss: 57.757393002901445, Acc: 0.8195, Grad norm: 0.07664876325750669\n",
      "Iteration 8039, BCE loss: 57.757390912513614, Acc: 0.8196, Grad norm: 0.07393540129566294\n",
      "Iteration 8040, BCE loss: 57.757328725808875, Acc: 0.8195, Grad norm: 0.052764575978781474\n",
      "Iteration 8041, BCE loss: 57.75733468946396, Acc: 0.8195, Grad norm: 0.05491249962224372\n",
      "Iteration 8042, BCE loss: 57.757324885386225, Acc: 0.8196, Grad norm: 0.04853183460585346\n",
      "Iteration 8043, BCE loss: 57.75731266154237, Acc: 0.8196, Grad norm: 0.04472831022466758\n",
      "Iteration 8044, BCE loss: 57.75729900183226, Acc: 0.8196, Grad norm: 0.03756164544017263\n",
      "Iteration 8045, BCE loss: 57.7573069609249, Acc: 0.8196, Grad norm: 0.043365239025575965\n",
      "Iteration 8046, BCE loss: 57.757332976512544, Acc: 0.8196, Grad norm: 0.054149962404906785\n",
      "Iteration 8047, BCE loss: 57.75731676899593, Acc: 0.8196, Grad norm: 0.047375827725809457\n",
      "Iteration 8048, BCE loss: 57.757324054828835, Acc: 0.8196, Grad norm: 0.04922774068508214\n",
      "Iteration 8049, BCE loss: 57.757342131599856, Acc: 0.8196, Grad norm: 0.05249033645122857\n",
      "Iteration 8050, BCE loss: 57.757327486277305, Acc: 0.8195, Grad norm: 0.04862884196493766\n",
      "Iteration 8051, BCE loss: 57.757361101755976, Acc: 0.8196, Grad norm: 0.062140711848954304\n",
      "Iteration 8052, BCE loss: 57.75735750367667, Acc: 0.8196, Grad norm: 0.06301217670848663\n",
      "Iteration 8053, BCE loss: 57.75734854968547, Acc: 0.8196, Grad norm: 0.05712853407379449\n",
      "Iteration 8054, BCE loss: 57.75733843349414, Acc: 0.8196, Grad norm: 0.05300130483825889\n",
      "Iteration 8055, BCE loss: 57.75734754960576, Acc: 0.8196, Grad norm: 0.05761451352359134\n",
      "Iteration 8056, BCE loss: 57.7573732767442, Acc: 0.8196, Grad norm: 0.0660964591992586\n",
      "Iteration 8057, BCE loss: 57.757360210759764, Acc: 0.8196, Grad norm: 0.0610384931383323\n",
      "Iteration 8058, BCE loss: 57.757362669257134, Acc: 0.8196, Grad norm: 0.0618995062035345\n",
      "Iteration 8059, BCE loss: 57.75737401413821, Acc: 0.8196, Grad norm: 0.0668292115869158\n",
      "Iteration 8060, BCE loss: 57.757397749196144, Acc: 0.8196, Grad norm: 0.07268027140221264\n",
      "Iteration 8061, BCE loss: 57.757369014870065, Acc: 0.8196, Grad norm: 0.06323565897704221\n",
      "Iteration 8062, BCE loss: 57.75732162022554, Acc: 0.8196, Grad norm: 0.04669158228507258\n",
      "Iteration 8063, BCE loss: 57.75732390943881, Acc: 0.8196, Grad norm: 0.04632882039527134\n",
      "Iteration 8064, BCE loss: 57.7573415820174, Acc: 0.8196, Grad norm: 0.05387749998465028\n",
      "Iteration 8065, BCE loss: 57.75735864898944, Acc: 0.8196, Grad norm: 0.0592538360944154\n",
      "Iteration 8066, BCE loss: 57.757363076294524, Acc: 0.8196, Grad norm: 0.061266030116590556\n",
      "Iteration 8067, BCE loss: 57.7573790079148, Acc: 0.8196, Grad norm: 0.06706933857405091\n",
      "Iteration 8068, BCE loss: 57.75736995253469, Acc: 0.8196, Grad norm: 0.06417759018464558\n",
      "Iteration 8069, BCE loss: 57.7573417483724, Acc: 0.8196, Grad norm: 0.05589512653276464\n",
      "Iteration 8070, BCE loss: 57.75730436057758, Acc: 0.8196, Grad norm: 0.04094790753492452\n",
      "Iteration 8071, BCE loss: 57.757299007646665, Acc: 0.8196, Grad norm: 0.03884061274388169\n",
      "Iteration 8072, BCE loss: 57.75734452273752, Acc: 0.8196, Grad norm: 0.05770378613282672\n",
      "Iteration 8073, BCE loss: 57.757395339119284, Acc: 0.8196, Grad norm: 0.07425406990867048\n",
      "Iteration 8074, BCE loss: 57.75737049060709, Acc: 0.8196, Grad norm: 0.0654686691451343\n",
      "Iteration 8075, BCE loss: 57.75736224900785, Acc: 0.8196, Grad norm: 0.06418984672817152\n",
      "Iteration 8076, BCE loss: 57.757372875666405, Acc: 0.8196, Grad norm: 0.06679937616600615\n",
      "Iteration 8077, BCE loss: 57.75735909469189, Acc: 0.8196, Grad norm: 0.06163451111564795\n",
      "Iteration 8078, BCE loss: 57.75736806219339, Acc: 0.8196, Grad norm: 0.0649111657736376\n",
      "Iteration 8079, BCE loss: 57.757350138815845, Acc: 0.8196, Grad norm: 0.05892897580184255\n",
      "Iteration 8080, BCE loss: 57.75737811227178, Acc: 0.8196, Grad norm: 0.07000518830765373\n",
      "Iteration 8081, BCE loss: 57.757390463016584, Acc: 0.8196, Grad norm: 0.07162837674919033\n",
      "Iteration 8082, BCE loss: 57.75734448690443, Acc: 0.8196, Grad norm: 0.056564466281673734\n",
      "Iteration 8083, BCE loss: 57.75732193183369, Acc: 0.8196, Grad norm: 0.04922795091118173\n",
      "Iteration 8084, BCE loss: 57.75730554849559, Acc: 0.8196, Grad norm: 0.04166713053291041\n",
      "Iteration 8085, BCE loss: 57.75733731077042, Acc: 0.8196, Grad norm: 0.055607131216100135\n",
      "Iteration 8086, BCE loss: 57.75732275948291, Acc: 0.8196, Grad norm: 0.05029745722068349\n",
      "Iteration 8087, BCE loss: 57.75730477024459, Acc: 0.8196, Grad norm: 0.04050488945597047\n",
      "Iteration 8088, BCE loss: 57.757308998473704, Acc: 0.8196, Grad norm: 0.041485998387895705\n",
      "Iteration 8089, BCE loss: 57.75729753325565, Acc: 0.8196, Grad norm: 0.03560799989678243\n",
      "Iteration 8090, BCE loss: 57.75730268925051, Acc: 0.8195, Grad norm: 0.03826490949232151\n",
      "Iteration 8091, BCE loss: 57.75732997136098, Acc: 0.8195, Grad norm: 0.05000601988430421\n",
      "Iteration 8092, BCE loss: 57.75735437185711, Acc: 0.8196, Grad norm: 0.05961740682934766\n",
      "Iteration 8093, BCE loss: 57.75733814127949, Acc: 0.8196, Grad norm: 0.05436866482730862\n",
      "Iteration 8094, BCE loss: 57.75732696549407, Acc: 0.8196, Grad norm: 0.05050787991479169\n",
      "Iteration 8095, BCE loss: 57.757333849790754, Acc: 0.8196, Grad norm: 0.05309375710999314\n",
      "Iteration 8096, BCE loss: 57.75733757116206, Acc: 0.8196, Grad norm: 0.053496455222692474\n",
      "Iteration 8097, BCE loss: 57.75731846811729, Acc: 0.8195, Grad norm: 0.04500030277103305\n",
      "Iteration 8098, BCE loss: 57.757325854078275, Acc: 0.8196, Grad norm: 0.046932883346677674\n",
      "Iteration 8099, BCE loss: 57.757356638477056, Acc: 0.8196, Grad norm: 0.05959763797507915\n",
      "Iteration 8100, BCE loss: 57.75741594806887, Acc: 0.8195, Grad norm: 0.07860040600475385\n",
      "Iteration 8101, BCE loss: 57.75741505660713, Acc: 0.8195, Grad norm: 0.07734899302516829\n",
      "Iteration 8102, BCE loss: 57.75742710011278, Acc: 0.8195, Grad norm: 0.0802354094079282\n",
      "Iteration 8103, BCE loss: 57.75740415216646, Acc: 0.8195, Grad norm: 0.0745409500486012\n",
      "Iteration 8104, BCE loss: 57.75742582736568, Acc: 0.8195, Grad norm: 0.08123287475007668\n",
      "Iteration 8105, BCE loss: 57.75745617414759, Acc: 0.8195, Grad norm: 0.08789061892433472\n",
      "Iteration 8106, BCE loss: 57.75747247282668, Acc: 0.8195, Grad norm: 0.09161311433617898\n",
      "Iteration 8107, BCE loss: 57.757444040293166, Acc: 0.8195, Grad norm: 0.0847754826991685\n",
      "Iteration 8108, BCE loss: 57.75750198125844, Acc: 0.8196, Grad norm: 0.09921053309120065\n",
      "Iteration 8109, BCE loss: 57.7574536588298, Acc: 0.8195, Grad norm: 0.08848907607615732\n",
      "Iteration 8110, BCE loss: 57.75742355952819, Acc: 0.8195, Grad norm: 0.08235986114180385\n",
      "Iteration 8111, BCE loss: 57.75742957151989, Acc: 0.8195, Grad norm: 0.08381982972297997\n",
      "Iteration 8112, BCE loss: 57.75738087384893, Acc: 0.8196, Grad norm: 0.07105285676487219\n",
      "Iteration 8113, BCE loss: 57.757380107126686, Acc: 0.8195, Grad norm: 0.0700701267563015\n",
      "Iteration 8114, BCE loss: 57.75736686466419, Acc: 0.8195, Grad norm: 0.06745366366588375\n",
      "Iteration 8115, BCE loss: 57.75734943372569, Acc: 0.8195, Grad norm: 0.06003594564064977\n",
      "Iteration 8116, BCE loss: 57.75741016726059, Acc: 0.8195, Grad norm: 0.08054814279833694\n",
      "Iteration 8117, BCE loss: 57.757438422134655, Acc: 0.8195, Grad norm: 0.08695038471418372\n",
      "Iteration 8118, BCE loss: 57.75737467370848, Acc: 0.8195, Grad norm: 0.0684625220742223\n",
      "Iteration 8119, BCE loss: 57.75737100098854, Acc: 0.8195, Grad norm: 0.06704030608537125\n",
      "Iteration 8120, BCE loss: 57.757323172148205, Acc: 0.8195, Grad norm: 0.04906804929994528\n",
      "Iteration 8121, BCE loss: 57.75729643354104, Acc: 0.8195, Grad norm: 0.037659656841466875\n",
      "Iteration 8122, BCE loss: 57.75731792397333, Acc: 0.8195, Grad norm: 0.04849261661680725\n",
      "Iteration 8123, BCE loss: 57.75730548457072, Acc: 0.8195, Grad norm: 0.042058503619348715\n",
      "Iteration 8124, BCE loss: 57.757322262236976, Acc: 0.8195, Grad norm: 0.04968595722613618\n",
      "Iteration 8125, BCE loss: 57.757321338163116, Acc: 0.8195, Grad norm: 0.049087254242127086\n",
      "Iteration 8126, BCE loss: 57.75734653503351, Acc: 0.8195, Grad norm: 0.05877958601134188\n",
      "Iteration 8127, BCE loss: 57.7573462969796, Acc: 0.8195, Grad norm: 0.05707806373623219\n",
      "Iteration 8128, BCE loss: 57.7573470522384, Acc: 0.8195, Grad norm: 0.056660341892903164\n",
      "Iteration 8129, BCE loss: 57.757371094065476, Acc: 0.8195, Grad norm: 0.06638572774437787\n",
      "Iteration 8130, BCE loss: 57.75739805645563, Acc: 0.8195, Grad norm: 0.07265733478479618\n",
      "Iteration 8131, BCE loss: 57.757349870353224, Acc: 0.8195, Grad norm: 0.05817012585525943\n",
      "Iteration 8132, BCE loss: 57.75732679121282, Acc: 0.8195, Grad norm: 0.04921461699060426\n",
      "Iteration 8133, BCE loss: 57.75734213253106, Acc: 0.8195, Grad norm: 0.05540380990184317\n",
      "Iteration 8134, BCE loss: 57.75733268998831, Acc: 0.8195, Grad norm: 0.05156379389964439\n",
      "Iteration 8135, BCE loss: 57.757337280456724, Acc: 0.8195, Grad norm: 0.05380564518265231\n",
      "Iteration 8136, BCE loss: 57.757347532679354, Acc: 0.8195, Grad norm: 0.05740247338014205\n",
      "Iteration 8137, BCE loss: 57.7573953302186, Acc: 0.8195, Grad norm: 0.0728747154572577\n",
      "Iteration 8138, BCE loss: 57.75736736922819, Acc: 0.8195, Grad norm: 0.06530788783497699\n",
      "Iteration 8139, BCE loss: 57.75731068907656, Acc: 0.8195, Grad norm: 0.04362677944630463\n",
      "Iteration 8140, BCE loss: 57.75731223778754, Acc: 0.8195, Grad norm: 0.04419658002644061\n",
      "Iteration 8141, BCE loss: 57.757335040359415, Acc: 0.8195, Grad norm: 0.05394081327019441\n",
      "Iteration 8142, BCE loss: 57.75738012183271, Acc: 0.8195, Grad norm: 0.06916603003433566\n",
      "Iteration 8143, BCE loss: 57.75734437907035, Acc: 0.8195, Grad norm: 0.05703815234547223\n",
      "Iteration 8144, BCE loss: 57.757338231089506, Acc: 0.8195, Grad norm: 0.053703985673715746\n",
      "Iteration 8145, BCE loss: 57.75733682668338, Acc: 0.8195, Grad norm: 0.0534925843325201\n",
      "Iteration 8146, BCE loss: 57.75732465620288, Acc: 0.8196, Grad norm: 0.04922152982772678\n",
      "Iteration 8147, BCE loss: 57.75736745213901, Acc: 0.8195, Grad norm: 0.06457977185267327\n",
      "Iteration 8148, BCE loss: 57.7573235476371, Acc: 0.8195, Grad norm: 0.04825726312704908\n",
      "Iteration 8149, BCE loss: 57.757305138996074, Acc: 0.8195, Grad norm: 0.0396120511047704\n",
      "Iteration 8150, BCE loss: 57.7573094314382, Acc: 0.8195, Grad norm: 0.04193601758767236\n",
      "Iteration 8151, BCE loss: 57.75731371975647, Acc: 0.8195, Grad norm: 0.044128157583740495\n",
      "Iteration 8152, BCE loss: 57.7573050812339, Acc: 0.8195, Grad norm: 0.04084302611244365\n",
      "Iteration 8153, BCE loss: 57.75729846690519, Acc: 0.8195, Grad norm: 0.03707695370670389\n",
      "Iteration 8154, BCE loss: 57.75729872777566, Acc: 0.8195, Grad norm: 0.03549751752153371\n",
      "Iteration 8155, BCE loss: 57.7573031685165, Acc: 0.8195, Grad norm: 0.03835363331254599\n",
      "Iteration 8156, BCE loss: 57.75729766459207, Acc: 0.8195, Grad norm: 0.0354428036105308\n",
      "Iteration 8157, BCE loss: 57.757290137446134, Acc: 0.8195, Grad norm: 0.03321379132572824\n",
      "Iteration 8158, BCE loss: 57.75730049406306, Acc: 0.8195, Grad norm: 0.04059305873475419\n",
      "Iteration 8159, BCE loss: 57.75731388418295, Acc: 0.8195, Grad norm: 0.044871639331926824\n",
      "Iteration 8160, BCE loss: 57.75729730101575, Acc: 0.8195, Grad norm: 0.037168801245370975\n",
      "Iteration 8161, BCE loss: 57.75727953461401, Acc: 0.8195, Grad norm: 0.02635721398760464\n",
      "Iteration 8162, BCE loss: 57.75730756006254, Acc: 0.8195, Grad norm: 0.04447370279374488\n",
      "Iteration 8163, BCE loss: 57.7573039102796, Acc: 0.8195, Grad norm: 0.043327174411159866\n",
      "Iteration 8164, BCE loss: 57.75732309647876, Acc: 0.8196, Grad norm: 0.052303912211690874\n",
      "Iteration 8165, BCE loss: 57.75734490590319, Acc: 0.8196, Grad norm: 0.06227619747187232\n",
      "Iteration 8166, BCE loss: 57.757329928912625, Acc: 0.8196, Grad norm: 0.05556837715460027\n",
      "Iteration 8167, BCE loss: 57.757325730909244, Acc: 0.8196, Grad norm: 0.05381505786537011\n",
      "Iteration 8168, BCE loss: 57.757351415647946, Acc: 0.8196, Grad norm: 0.06470417302122297\n",
      "Iteration 8169, BCE loss: 57.75732888905483, Acc: 0.8196, Grad norm: 0.0561931601141996\n",
      "Iteration 8170, BCE loss: 57.75734712676501, Acc: 0.8196, Grad norm: 0.06357427144567829\n",
      "Iteration 8171, BCE loss: 57.75734539283421, Acc: 0.8196, Grad norm: 0.061167350241304506\n",
      "Iteration 8172, BCE loss: 57.757328860190555, Acc: 0.8196, Grad norm: 0.055190953742603244\n",
      "Iteration 8173, BCE loss: 57.75731430517564, Acc: 0.8195, Grad norm: 0.047123241821994594\n",
      "Iteration 8174, BCE loss: 57.75729383336545, Acc: 0.8195, Grad norm: 0.03784209721725189\n",
      "Iteration 8175, BCE loss: 57.757307981737156, Acc: 0.8195, Grad norm: 0.04593649144533333\n",
      "Iteration 8176, BCE loss: 57.75730515308848, Acc: 0.8195, Grad norm: 0.043393974488327894\n",
      "Iteration 8177, BCE loss: 57.757304434605025, Acc: 0.8195, Grad norm: 0.042045187758673455\n",
      "Iteration 8178, BCE loss: 57.757294473161565, Acc: 0.8195, Grad norm: 0.03749744953752218\n",
      "Iteration 8179, BCE loss: 57.75732597876591, Acc: 0.8195, Grad norm: 0.053366794740805405\n",
      "Iteration 8180, BCE loss: 57.75729158524992, Acc: 0.8195, Grad norm: 0.03382214141687189\n",
      "Iteration 8181, BCE loss: 57.75729411405176, Acc: 0.8195, Grad norm: 0.03454366977230445\n",
      "Iteration 8182, BCE loss: 57.75729504952763, Acc: 0.8195, Grad norm: 0.0345258427987074\n",
      "Iteration 8183, BCE loss: 57.75730369704813, Acc: 0.8195, Grad norm: 0.0374946353657584\n",
      "Iteration 8184, BCE loss: 57.75730944270476, Acc: 0.8195, Grad norm: 0.04347094797458357\n",
      "Iteration 8185, BCE loss: 57.75728504939409, Acc: 0.8195, Grad norm: 0.029803522764099605\n",
      "Iteration 8186, BCE loss: 57.757276773231695, Acc: 0.8195, Grad norm: 0.025364909738078922\n",
      "Iteration 8187, BCE loss: 57.75727924725885, Acc: 0.8195, Grad norm: 0.02630007307014604\n",
      "Iteration 8188, BCE loss: 57.75729094253537, Acc: 0.8195, Grad norm: 0.033348210646677964\n",
      "Iteration 8189, BCE loss: 57.75729474085614, Acc: 0.8195, Grad norm: 0.036936681028535825\n",
      "Iteration 8190, BCE loss: 57.75729433985863, Acc: 0.8195, Grad norm: 0.03572500163327869\n",
      "Iteration 8191, BCE loss: 57.75728143569977, Acc: 0.8196, Grad norm: 0.026545484531904622\n",
      "Iteration 8192, BCE loss: 57.75727544499325, Acc: 0.8195, Grad norm: 0.02254657826582085\n",
      "Iteration 8193, BCE loss: 57.75729091317015, Acc: 0.8195, Grad norm: 0.03165170298698759\n",
      "Iteration 8194, BCE loss: 57.75729103140402, Acc: 0.8195, Grad norm: 0.03206675327958909\n",
      "Iteration 8195, BCE loss: 57.757318095349106, Acc: 0.8196, Grad norm: 0.04757792144988696\n",
      "Iteration 8196, BCE loss: 57.75732778379691, Acc: 0.8196, Grad norm: 0.05111263809137643\n",
      "Iteration 8197, BCE loss: 57.75732024470045, Acc: 0.8196, Grad norm: 0.04765166555413673\n",
      "Iteration 8198, BCE loss: 57.75730047039008, Acc: 0.8196, Grad norm: 0.03893472242974681\n",
      "Iteration 8199, BCE loss: 57.757298348543806, Acc: 0.8196, Grad norm: 0.03865990538147297\n",
      "Iteration 8200, BCE loss: 57.757290779374095, Acc: 0.8196, Grad norm: 0.034572539209186445\n",
      "Iteration 8201, BCE loss: 57.75729064839808, Acc: 0.8195, Grad norm: 0.03670501384372951\n",
      "Iteration 8202, BCE loss: 57.757313216112095, Acc: 0.8195, Grad norm: 0.04797346145182438\n",
      "Iteration 8203, BCE loss: 57.75729886181173, Acc: 0.8195, Grad norm: 0.040685585214488965\n",
      "Iteration 8204, BCE loss: 57.757309828759084, Acc: 0.8195, Grad norm: 0.04499186453601037\n",
      "Iteration 8205, BCE loss: 57.75731553792886, Acc: 0.8195, Grad norm: 0.04639774960141976\n",
      "Iteration 8206, BCE loss: 57.757303632429384, Acc: 0.8195, Grad norm: 0.04023136750177897\n",
      "Iteration 8207, BCE loss: 57.75730527604418, Acc: 0.8195, Grad norm: 0.039813279418908325\n",
      "Iteration 8208, BCE loss: 57.7573090420006, Acc: 0.8196, Grad norm: 0.03972240992964181\n",
      "Iteration 8209, BCE loss: 57.75730526909791, Acc: 0.8196, Grad norm: 0.03879440061152042\n",
      "Iteration 8210, BCE loss: 57.75730578122783, Acc: 0.8196, Grad norm: 0.039433287783708554\n",
      "Iteration 8211, BCE loss: 57.757299016020966, Acc: 0.8196, Grad norm: 0.03853978930882815\n",
      "Iteration 8212, BCE loss: 57.75731573634912, Acc: 0.8196, Grad norm: 0.04563699753541328\n",
      "Iteration 8213, BCE loss: 57.75730937762711, Acc: 0.8196, Grad norm: 0.04503604330155215\n",
      "Iteration 8214, BCE loss: 57.75733198510093, Acc: 0.8196, Grad norm: 0.05511416122039974\n",
      "Iteration 8215, BCE loss: 57.75732577923932, Acc: 0.8196, Grad norm: 0.052420961791684366\n",
      "Iteration 8216, BCE loss: 57.7573269433308, Acc: 0.8196, Grad norm: 0.05350958829331785\n",
      "Iteration 8217, BCE loss: 57.75733649725588, Acc: 0.8196, Grad norm: 0.05530870043161842\n",
      "Iteration 8218, BCE loss: 57.75735663949466, Acc: 0.8196, Grad norm: 0.06269480147732327\n",
      "Iteration 8219, BCE loss: 57.75743458504452, Acc: 0.8196, Grad norm: 0.08648890463360648\n",
      "Iteration 8220, BCE loss: 57.75743027730379, Acc: 0.8196, Grad norm: 0.08474257912042311\n",
      "Iteration 8221, BCE loss: 57.75741207404909, Acc: 0.8196, Grad norm: 0.08015072271005702\n",
      "Iteration 8222, BCE loss: 57.75742334885997, Acc: 0.8196, Grad norm: 0.08233185757387053\n",
      "Iteration 8223, BCE loss: 57.75736889743706, Acc: 0.8196, Grad norm: 0.06669225756333207\n",
      "Iteration 8224, BCE loss: 57.757393233852355, Acc: 0.8196, Grad norm: 0.07434275521665523\n",
      "Iteration 8225, BCE loss: 57.75743492311672, Acc: 0.8196, Grad norm: 0.08609108991833721\n",
      "Iteration 8226, BCE loss: 57.75745143462387, Acc: 0.8196, Grad norm: 0.08984156198084234\n",
      "Iteration 8227, BCE loss: 57.75739559248768, Acc: 0.8196, Grad norm: 0.0756078918428008\n",
      "Iteration 8228, BCE loss: 57.757366445585724, Acc: 0.8196, Grad norm: 0.06582908283045251\n",
      "Iteration 8229, BCE loss: 57.75740652474295, Acc: 0.8196, Grad norm: 0.07822687317848466\n",
      "Iteration 8230, BCE loss: 57.7573852463802, Acc: 0.8196, Grad norm: 0.07086836393337877\n",
      "Iteration 8231, BCE loss: 57.75739374692577, Acc: 0.8196, Grad norm: 0.07406407577121635\n",
      "Iteration 8232, BCE loss: 57.75734671506919, Acc: 0.8196, Grad norm: 0.05842761089146308\n",
      "Iteration 8233, BCE loss: 57.757313491820625, Acc: 0.8196, Grad norm: 0.04541912137661426\n",
      "Iteration 8234, BCE loss: 57.75734113931098, Acc: 0.8195, Grad norm: 0.05706312291880169\n",
      "Iteration 8235, BCE loss: 57.75734381239416, Acc: 0.8196, Grad norm: 0.05864238801459439\n",
      "Iteration 8236, BCE loss: 57.75732340970606, Acc: 0.8196, Grad norm: 0.050370818248223634\n",
      "Iteration 8237, BCE loss: 57.75731613732053, Acc: 0.8196, Grad norm: 0.04791367744965027\n",
      "Iteration 8238, BCE loss: 57.75731868292246, Acc: 0.8196, Grad norm: 0.05016533885615202\n",
      "Iteration 8239, BCE loss: 57.75730307411813, Acc: 0.8196, Grad norm: 0.04306387919960488\n",
      "Iteration 8240, BCE loss: 57.75734474674033, Acc: 0.8195, Grad norm: 0.060582093884597385\n",
      "Iteration 8241, BCE loss: 57.757338909965746, Acc: 0.8195, Grad norm: 0.05937645468522043\n",
      "Iteration 8242, BCE loss: 57.75735346233749, Acc: 0.8195, Grad norm: 0.06422828998621005\n",
      "Iteration 8243, BCE loss: 57.75731265172254, Acc: 0.8195, Grad norm: 0.04749589576185032\n",
      "Iteration 8244, BCE loss: 57.75729709557054, Acc: 0.8195, Grad norm: 0.03789952082812316\n",
      "Iteration 8245, BCE loss: 57.75731750699994, Acc: 0.8196, Grad norm: 0.047153463232614184\n",
      "Iteration 8246, BCE loss: 57.75731844951373, Acc: 0.8196, Grad norm: 0.048680848237843244\n",
      "Iteration 8247, BCE loss: 57.757332253383424, Acc: 0.8196, Grad norm: 0.05254384103709743\n",
      "Iteration 8248, BCE loss: 57.75735825589642, Acc: 0.8196, Grad norm: 0.06291432256709598\n",
      "Iteration 8249, BCE loss: 57.75733577165373, Acc: 0.8196, Grad norm: 0.05662408106623055\n",
      "Iteration 8250, BCE loss: 57.757373563964606, Acc: 0.8196, Grad norm: 0.06989781528459563\n",
      "Iteration 8251, BCE loss: 57.757355611164655, Acc: 0.8196, Grad norm: 0.0646559930402124\n",
      "Iteration 8252, BCE loss: 57.75739098638648, Acc: 0.8196, Grad norm: 0.07728331885436493\n",
      "Iteration 8253, BCE loss: 57.75740663807033, Acc: 0.8196, Grad norm: 0.08114506957097345\n",
      "Iteration 8254, BCE loss: 57.75742543077929, Acc: 0.8196, Grad norm: 0.08641534740329336\n",
      "Iteration 8255, BCE loss: 57.75749347317928, Acc: 0.8196, Grad norm: 0.1017950679108131\n",
      "Iteration 8256, BCE loss: 57.757458995516856, Acc: 0.8196, Grad norm: 0.09383116341304712\n",
      "Iteration 8257, BCE loss: 57.75755504948354, Acc: 0.8196, Grad norm: 0.11608637160551795\n",
      "Iteration 8258, BCE loss: 57.75753023553786, Acc: 0.8196, Grad norm: 0.10998582303046568\n",
      "Iteration 8259, BCE loss: 57.75747730517529, Acc: 0.8196, Grad norm: 0.09846855007413194\n",
      "Iteration 8260, BCE loss: 57.757451384962394, Acc: 0.8196, Grad norm: 0.09205100018475253\n",
      "Iteration 8261, BCE loss: 57.75752725981378, Acc: 0.8196, Grad norm: 0.1100964807530994\n",
      "Iteration 8262, BCE loss: 57.75746478098162, Acc: 0.8196, Grad norm: 0.09567902700897854\n",
      "Iteration 8263, BCE loss: 57.757448285455965, Acc: 0.8196, Grad norm: 0.09086112695121237\n",
      "Iteration 8264, BCE loss: 57.75746402100661, Acc: 0.8196, Grad norm: 0.09470221780896321\n",
      "Iteration 8265, BCE loss: 57.75741194124538, Acc: 0.8196, Grad norm: 0.0805828372488741\n",
      "Iteration 8266, BCE loss: 57.75743342249056, Acc: 0.8196, Grad norm: 0.08742908918199542\n",
      "Iteration 8267, BCE loss: 57.757400097219474, Acc: 0.8196, Grad norm: 0.07825175789982244\n",
      "Iteration 8268, BCE loss: 57.757383733432924, Acc: 0.8196, Grad norm: 0.0740395456609708\n",
      "Iteration 8269, BCE loss: 57.757404259598445, Acc: 0.8196, Grad norm: 0.08110565481843218\n",
      "Iteration 8270, BCE loss: 57.75736370293137, Acc: 0.8196, Grad norm: 0.06845440991991704\n",
      "Iteration 8271, BCE loss: 57.757303247220975, Acc: 0.8196, Grad norm: 0.04334355499673338\n",
      "Iteration 8272, BCE loss: 57.7573001304842, Acc: 0.8196, Grad norm: 0.04042718749035759\n",
      "Iteration 8273, BCE loss: 57.75731497370259, Acc: 0.8196, Grad norm: 0.047080886314311544\n",
      "Iteration 8274, BCE loss: 57.75727736168167, Acc: 0.8195, Grad norm: 0.02538544174382498\n",
      "Iteration 8275, BCE loss: 57.75729550325923, Acc: 0.8196, Grad norm: 0.038770769566368095\n",
      "Iteration 8276, BCE loss: 57.75731416222865, Acc: 0.8196, Grad norm: 0.04811076769544228\n",
      "Iteration 8277, BCE loss: 57.75732183341236, Acc: 0.8196, Grad norm: 0.05032853559815088\n",
      "Iteration 8278, BCE loss: 57.75733805142043, Acc: 0.8196, Grad norm: 0.05788776281568112\n",
      "Iteration 8279, BCE loss: 57.75738241997138, Acc: 0.8196, Grad norm: 0.07410985902007974\n",
      "Iteration 8280, BCE loss: 57.75738654535675, Acc: 0.8196, Grad norm: 0.0751508284623454\n",
      "Iteration 8281, BCE loss: 57.757356937232835, Acc: 0.8196, Grad norm: 0.06551531032520463\n",
      "Iteration 8282, BCE loss: 57.757348534950054, Acc: 0.8196, Grad norm: 0.06235188728791908\n",
      "Iteration 8283, BCE loss: 57.75736891561479, Acc: 0.8196, Grad norm: 0.06730638240592747\n",
      "Iteration 8284, BCE loss: 57.757349368929845, Acc: 0.8196, Grad norm: 0.06167207407811796\n",
      "Iteration 8285, BCE loss: 57.75735870616215, Acc: 0.8195, Grad norm: 0.06607246950953032\n",
      "Iteration 8286, BCE loss: 57.75736049357367, Acc: 0.8196, Grad norm: 0.06680252229800167\n",
      "Iteration 8287, BCE loss: 57.75734524562038, Acc: 0.8195, Grad norm: 0.06158180170829102\n",
      "Iteration 8288, BCE loss: 57.75731255809947, Acc: 0.8195, Grad norm: 0.048382667256476254\n",
      "Iteration 8289, BCE loss: 57.75731530755714, Acc: 0.8195, Grad norm: 0.04807577155333198\n",
      "Iteration 8290, BCE loss: 57.757356652804276, Acc: 0.8195, Grad norm: 0.06369578083935876\n",
      "Iteration 8291, BCE loss: 57.757374891039845, Acc: 0.8195, Grad norm: 0.07179423766031827\n",
      "Iteration 8292, BCE loss: 57.75730031178043, Acc: 0.8195, Grad norm: 0.04073955210622509\n",
      "Iteration 8293, BCE loss: 57.757306917554544, Acc: 0.8196, Grad norm: 0.04664215387963252\n",
      "Iteration 8294, BCE loss: 57.75728869065527, Acc: 0.8196, Grad norm: 0.037083801006648376\n",
      "Iteration 8295, BCE loss: 57.757302277366406, Acc: 0.8196, Grad norm: 0.04412030907599664\n",
      "Iteration 8296, BCE loss: 57.75729171656073, Acc: 0.8196, Grad norm: 0.03739022718809123\n",
      "Iteration 8297, BCE loss: 57.757303614436566, Acc: 0.8196, Grad norm: 0.04271833939347677\n",
      "Iteration 8298, BCE loss: 57.75728853226387, Acc: 0.8196, Grad norm: 0.0322137671677053\n",
      "Iteration 8299, BCE loss: 57.757292548074545, Acc: 0.8196, Grad norm: 0.03507033400020101\n",
      "Iteration 8300, BCE loss: 57.75730270423436, Acc: 0.8196, Grad norm: 0.041853541925968844\n",
      "Iteration 8301, BCE loss: 57.75731422527785, Acc: 0.8196, Grad norm: 0.04749068268848675\n",
      "Iteration 8302, BCE loss: 57.75732847229117, Acc: 0.8196, Grad norm: 0.053439822778504266\n",
      "Iteration 8303, BCE loss: 57.75732025283335, Acc: 0.8196, Grad norm: 0.04878984127219022\n",
      "Iteration 8304, BCE loss: 57.75731991491452, Acc: 0.8196, Grad norm: 0.04709453441916345\n",
      "Iteration 8305, BCE loss: 57.75735268182528, Acc: 0.8196, Grad norm: 0.06008197513269207\n",
      "Iteration 8306, BCE loss: 57.75733025413756, Acc: 0.8196, Grad norm: 0.051361153978955784\n",
      "Iteration 8307, BCE loss: 57.75734123897022, Acc: 0.8196, Grad norm: 0.057261474833896195\n",
      "Iteration 8308, BCE loss: 57.75733030273718, Acc: 0.8196, Grad norm: 0.05411853573201556\n",
      "Iteration 8309, BCE loss: 57.75732748103472, Acc: 0.8196, Grad norm: 0.05402518514675424\n",
      "Iteration 8310, BCE loss: 57.757342601114985, Acc: 0.8196, Grad norm: 0.05826761951827949\n",
      "Iteration 8311, BCE loss: 57.75734850546492, Acc: 0.8196, Grad norm: 0.06125309193918713\n",
      "Iteration 8312, BCE loss: 57.75734773510433, Acc: 0.8196, Grad norm: 0.06019936914463923\n",
      "Iteration 8313, BCE loss: 57.75735348772848, Acc: 0.8196, Grad norm: 0.06123598020015823\n",
      "Iteration 8314, BCE loss: 57.757350141257945, Acc: 0.8196, Grad norm: 0.06116611962999158\n",
      "Iteration 8315, BCE loss: 57.75731036292086, Acc: 0.8196, Grad norm: 0.045268661819787254\n",
      "Iteration 8316, BCE loss: 57.75729768736694, Acc: 0.8196, Grad norm: 0.03793127925723922\n",
      "Iteration 8317, BCE loss: 57.75734702554341, Acc: 0.8196, Grad norm: 0.057140928867587905\n",
      "Iteration 8318, BCE loss: 57.75737070842477, Acc: 0.8195, Grad norm: 0.06612698306932997\n",
      "Iteration 8319, BCE loss: 57.757351335753526, Acc: 0.8195, Grad norm: 0.05864158701555086\n",
      "Iteration 8320, BCE loss: 57.757324517688026, Acc: 0.8195, Grad norm: 0.04797313810677912\n",
      "Iteration 8321, BCE loss: 57.757327916091114, Acc: 0.8196, Grad norm: 0.049636317967990624\n",
      "Iteration 8322, BCE loss: 57.75734191157588, Acc: 0.8196, Grad norm: 0.05415939743019783\n",
      "Iteration 8323, BCE loss: 57.75734370563403, Acc: 0.8196, Grad norm: 0.053529208234444264\n",
      "Iteration 8324, BCE loss: 57.7573373554724, Acc: 0.8196, Grad norm: 0.05120696216998214\n",
      "Iteration 8325, BCE loss: 57.757321034793875, Acc: 0.8196, Grad norm: 0.045576356294624094\n",
      "Iteration 8326, BCE loss: 57.7573036824041, Acc: 0.8196, Grad norm: 0.03826899897995022\n",
      "Iteration 8327, BCE loss: 57.7573138656804, Acc: 0.8196, Grad norm: 0.04249805562907576\n",
      "Iteration 8328, BCE loss: 57.75733307498487, Acc: 0.8196, Grad norm: 0.05169229720561743\n",
      "Iteration 8329, BCE loss: 57.75734711023083, Acc: 0.8195, Grad norm: 0.058075100491839636\n",
      "Iteration 8330, BCE loss: 57.75733398629875, Acc: 0.8195, Grad norm: 0.05235635395804256\n",
      "Iteration 8331, BCE loss: 57.757347696931106, Acc: 0.8195, Grad norm: 0.056433364106132484\n",
      "Iteration 8332, BCE loss: 57.757353825689094, Acc: 0.8195, Grad norm: 0.06066511598692118\n",
      "Iteration 8333, BCE loss: 57.75735236413843, Acc: 0.8195, Grad norm: 0.060897600456366364\n",
      "Iteration 8334, BCE loss: 57.75736608487837, Acc: 0.8195, Grad norm: 0.06537155540468635\n",
      "Iteration 8335, BCE loss: 57.75735022734595, Acc: 0.8195, Grad norm: 0.05975994673124474\n",
      "Iteration 8336, BCE loss: 57.75736319080805, Acc: 0.8195, Grad norm: 0.06637326274371415\n",
      "Iteration 8337, BCE loss: 57.75739706291647, Acc: 0.8195, Grad norm: 0.07782544773729509\n",
      "Iteration 8338, BCE loss: 57.75736495824103, Acc: 0.8195, Grad norm: 0.06782208469601844\n",
      "Iteration 8339, BCE loss: 57.75738619594219, Acc: 0.8195, Grad norm: 0.07309387195162438\n",
      "Iteration 8340, BCE loss: 57.75741528270495, Acc: 0.8195, Grad norm: 0.08267930023875852\n",
      "Iteration 8341, BCE loss: 57.757411408827, Acc: 0.8195, Grad norm: 0.08184097046797134\n",
      "Iteration 8342, BCE loss: 57.75744129005588, Acc: 0.8195, Grad norm: 0.08832487866103465\n",
      "Iteration 8343, BCE loss: 57.75738894661395, Acc: 0.8195, Grad norm: 0.07507387972439349\n",
      "Iteration 8344, BCE loss: 57.757371004859095, Acc: 0.8195, Grad norm: 0.07038230401160442\n",
      "Iteration 8345, BCE loss: 57.75739389160722, Acc: 0.8195, Grad norm: 0.07906657536506873\n",
      "Iteration 8346, BCE loss: 57.757420077290924, Acc: 0.8195, Grad norm: 0.08551506684459674\n",
      "Iteration 8347, BCE loss: 57.757441044088466, Acc: 0.8195, Grad norm: 0.08984493338322239\n",
      "Iteration 8348, BCE loss: 57.757375770925975, Acc: 0.8195, Grad norm: 0.07150545747521315\n",
      "Iteration 8349, BCE loss: 57.75735029800035, Acc: 0.8195, Grad norm: 0.0621395181386102\n",
      "Iteration 8350, BCE loss: 57.757373494190276, Acc: 0.8195, Grad norm: 0.06964074223835869\n",
      "Iteration 8351, BCE loss: 57.75736501177032, Acc: 0.8195, Grad norm: 0.065164901022599\n",
      "Iteration 8352, BCE loss: 57.7573338870965, Acc: 0.8195, Grad norm: 0.05069561583069388\n",
      "Iteration 8353, BCE loss: 57.75732957992048, Acc: 0.8195, Grad norm: 0.050159165490132364\n",
      "Iteration 8354, BCE loss: 57.75732332177185, Acc: 0.8195, Grad norm: 0.04859155060909021\n",
      "Iteration 8355, BCE loss: 57.75731854709538, Acc: 0.8195, Grad norm: 0.045412263047120205\n",
      "Iteration 8356, BCE loss: 57.75732919345765, Acc: 0.8196, Grad norm: 0.049029366989920296\n",
      "Iteration 8357, BCE loss: 57.75733003320608, Acc: 0.8196, Grad norm: 0.050943595043340326\n",
      "Iteration 8358, BCE loss: 57.75733112864289, Acc: 0.8196, Grad norm: 0.051363736810849395\n",
      "Iteration 8359, BCE loss: 57.757339915337674, Acc: 0.8196, Grad norm: 0.054362850019720965\n",
      "Iteration 8360, BCE loss: 57.757357679913575, Acc: 0.8196, Grad norm: 0.06357299376944703\n",
      "Iteration 8361, BCE loss: 57.75736116273485, Acc: 0.8196, Grad norm: 0.06439474942237053\n",
      "Iteration 8362, BCE loss: 57.75733306548777, Acc: 0.8196, Grad norm: 0.05149241488679604\n",
      "Iteration 8363, BCE loss: 57.757325026001496, Acc: 0.8195, Grad norm: 0.047367933394350525\n",
      "Iteration 8364, BCE loss: 57.75732539555045, Acc: 0.8196, Grad norm: 0.047367355183710616\n",
      "Iteration 8365, BCE loss: 57.75735214700654, Acc: 0.8195, Grad norm: 0.056397310296154114\n",
      "Iteration 8366, BCE loss: 57.75736642810185, Acc: 0.8195, Grad norm: 0.06228328318125092\n",
      "Iteration 8367, BCE loss: 57.75736277051499, Acc: 0.8195, Grad norm: 0.06320135885588582\n",
      "Iteration 8368, BCE loss: 57.75737547458601, Acc: 0.8195, Grad norm: 0.06903411667105618\n",
      "Iteration 8369, BCE loss: 57.757327788779854, Acc: 0.8195, Grad norm: 0.05074261442097522\n",
      "Iteration 8370, BCE loss: 57.7573672806841, Acc: 0.8195, Grad norm: 0.06736994044825464\n",
      "Iteration 8371, BCE loss: 57.75736135504044, Acc: 0.8195, Grad norm: 0.06697172459939174\n",
      "Iteration 8372, BCE loss: 57.7573436024954, Acc: 0.8196, Grad norm: 0.059725976146265\n",
      "Iteration 8373, BCE loss: 57.757322549598825, Acc: 0.8195, Grad norm: 0.05212787261352809\n",
      "Iteration 8374, BCE loss: 57.757345805048736, Acc: 0.8196, Grad norm: 0.06089390591957882\n",
      "Iteration 8375, BCE loss: 57.757342883878735, Acc: 0.8195, Grad norm: 0.05953855678578351\n",
      "Iteration 8376, BCE loss: 57.75733690170651, Acc: 0.8195, Grad norm: 0.0567821253920562\n",
      "Iteration 8377, BCE loss: 57.75733416203697, Acc: 0.8196, Grad norm: 0.05538420301228996\n",
      "Iteration 8378, BCE loss: 57.75733740265977, Acc: 0.8196, Grad norm: 0.056129657146846756\n",
      "Iteration 8379, BCE loss: 57.75735693651687, Acc: 0.8196, Grad norm: 0.06257620341773638\n",
      "Iteration 8380, BCE loss: 57.75734943985192, Acc: 0.8196, Grad norm: 0.060500494646183185\n",
      "Iteration 8381, BCE loss: 57.75731493805935, Acc: 0.8196, Grad norm: 0.04734525758438003\n",
      "Iteration 8382, BCE loss: 57.75730198747905, Acc: 0.8196, Grad norm: 0.04017168169478389\n",
      "Iteration 8383, BCE loss: 57.757310229111624, Acc: 0.8195, Grad norm: 0.04557856050044293\n",
      "Iteration 8384, BCE loss: 57.75732034119868, Acc: 0.8196, Grad norm: 0.05003243912446734\n",
      "Iteration 8385, BCE loss: 57.757347204403224, Acc: 0.8196, Grad norm: 0.059846437642798185\n",
      "Iteration 8386, BCE loss: 57.75736235231905, Acc: 0.8196, Grad norm: 0.06604490142208819\n",
      "Iteration 8387, BCE loss: 57.757357671766414, Acc: 0.8196, Grad norm: 0.06377706948235053\n",
      "Iteration 8388, BCE loss: 57.75737557773556, Acc: 0.8196, Grad norm: 0.07015902648939386\n",
      "Iteration 8389, BCE loss: 57.75735581183086, Acc: 0.8195, Grad norm: 0.06279100406448183\n",
      "Iteration 8390, BCE loss: 57.75738289472562, Acc: 0.8196, Grad norm: 0.07181995539818294\n",
      "Iteration 8391, BCE loss: 57.75742247521954, Acc: 0.8196, Grad norm: 0.08244590036571922\n",
      "Iteration 8392, BCE loss: 57.7573741622471, Acc: 0.8196, Grad norm: 0.06613332725487206\n",
      "Iteration 8393, BCE loss: 57.757344274957866, Acc: 0.8195, Grad norm: 0.05712940942452218\n",
      "Iteration 8394, BCE loss: 57.75733521129248, Acc: 0.8195, Grad norm: 0.052044957675341855\n",
      "Iteration 8395, BCE loss: 57.75733408735561, Acc: 0.8195, Grad norm: 0.05211831638713756\n",
      "Iteration 8396, BCE loss: 57.75736270229548, Acc: 0.8195, Grad norm: 0.06363248055647061\n",
      "Iteration 8397, BCE loss: 57.75735845668527, Acc: 0.8195, Grad norm: 0.061867359758143836\n",
      "Iteration 8398, BCE loss: 57.75729868334985, Acc: 0.8195, Grad norm: 0.035889773632070035\n",
      "Iteration 8399, BCE loss: 57.757302389987956, Acc: 0.8195, Grad norm: 0.039691540045869604\n",
      "Iteration 8400, BCE loss: 57.75729225280098, Acc: 0.8195, Grad norm: 0.03332801879660492\n",
      "Iteration 8401, BCE loss: 57.75730999224321, Acc: 0.8195, Grad norm: 0.044370385336453246\n",
      "Iteration 8402, BCE loss: 57.75729823732366, Acc: 0.8195, Grad norm: 0.03812724351801989\n",
      "Iteration 8403, BCE loss: 57.757310580745326, Acc: 0.8195, Grad norm: 0.0426200840544272\n",
      "Iteration 8404, BCE loss: 57.7573267296096, Acc: 0.8195, Grad norm: 0.04957061336742579\n",
      "Iteration 8405, BCE loss: 57.75731583521163, Acc: 0.8195, Grad norm: 0.04342541892504535\n",
      "Iteration 8406, BCE loss: 57.75732433030112, Acc: 0.8196, Grad norm: 0.046539442292867224\n",
      "Iteration 8407, BCE loss: 57.757340430588734, Acc: 0.8195, Grad norm: 0.05242719032116859\n",
      "Iteration 8408, BCE loss: 57.757337146450936, Acc: 0.8195, Grad norm: 0.04947003401327535\n",
      "Iteration 8409, BCE loss: 57.757344037958426, Acc: 0.8195, Grad norm: 0.053255758509138856\n",
      "Iteration 8410, BCE loss: 57.75735618422739, Acc: 0.8195, Grad norm: 0.05839046615100437\n",
      "Iteration 8411, BCE loss: 57.757355179854784, Acc: 0.8195, Grad norm: 0.05913399316880365\n",
      "Iteration 8412, BCE loss: 57.75735585326465, Acc: 0.8195, Grad norm: 0.05946907281012453\n",
      "Iteration 8413, BCE loss: 57.757343962207216, Acc: 0.8195, Grad norm: 0.05577584407044059\n",
      "Iteration 8414, BCE loss: 57.757342161870426, Acc: 0.8195, Grad norm: 0.05650980227579903\n",
      "Iteration 8415, BCE loss: 57.757340652066524, Acc: 0.8195, Grad norm: 0.055783929559302434\n",
      "Iteration 8416, BCE loss: 57.75736024689559, Acc: 0.8195, Grad norm: 0.0653081446330657\n",
      "Iteration 8417, BCE loss: 57.75736961739592, Acc: 0.8195, Grad norm: 0.06977726631993639\n",
      "Iteration 8418, BCE loss: 57.75737210076521, Acc: 0.8195, Grad norm: 0.07080578634138783\n",
      "Iteration 8419, BCE loss: 57.7574083241835, Acc: 0.8195, Grad norm: 0.08174062484130486\n",
      "Iteration 8420, BCE loss: 57.75738419996797, Acc: 0.8195, Grad norm: 0.07514001249886836\n",
      "Iteration 8421, BCE loss: 57.7573646802344, Acc: 0.8195, Grad norm: 0.06749318283715762\n",
      "Iteration 8422, BCE loss: 57.75738591171182, Acc: 0.8195, Grad norm: 0.07314095000025765\n",
      "Iteration 8423, BCE loss: 57.757383729010236, Acc: 0.8195, Grad norm: 0.07258543852692598\n",
      "Iteration 8424, BCE loss: 57.75743788776605, Acc: 0.8195, Grad norm: 0.08902275091703477\n",
      "Iteration 8425, BCE loss: 57.757470470724414, Acc: 0.8195, Grad norm: 0.09914882350887118\n",
      "Iteration 8426, BCE loss: 57.75744490911694, Acc: 0.8195, Grad norm: 0.09236345176724416\n",
      "Iteration 8427, BCE loss: 57.757448017115934, Acc: 0.8195, Grad norm: 0.09281038380589977\n",
      "Iteration 8428, BCE loss: 57.757455216617366, Acc: 0.8195, Grad norm: 0.09484318030025098\n",
      "Iteration 8429, BCE loss: 57.757454610116966, Acc: 0.8195, Grad norm: 0.09385477353550062\n",
      "Iteration 8430, BCE loss: 57.757488976754956, Acc: 0.8195, Grad norm: 0.10197119990478756\n",
      "Iteration 8431, BCE loss: 57.7574540409983, Acc: 0.8195, Grad norm: 0.09372762043896668\n",
      "Iteration 8432, BCE loss: 57.757372459085175, Acc: 0.8195, Grad norm: 0.06854805768611412\n",
      "Iteration 8433, BCE loss: 57.757348892583714, Acc: 0.8195, Grad norm: 0.059949750725699046\n",
      "Iteration 8434, BCE loss: 57.75735871136149, Acc: 0.8196, Grad norm: 0.06250190135179533\n",
      "Iteration 8435, BCE loss: 57.75733353416199, Acc: 0.8195, Grad norm: 0.05485601206285045\n",
      "Iteration 8436, BCE loss: 57.757319940382516, Acc: 0.8196, Grad norm: 0.046373198871324475\n",
      "Iteration 8437, BCE loss: 57.75730730240302, Acc: 0.8195, Grad norm: 0.04210293961358031\n",
      "Iteration 8438, BCE loss: 57.75732444715892, Acc: 0.8196, Grad norm: 0.04713880627266438\n",
      "Iteration 8439, BCE loss: 57.757336369250496, Acc: 0.8196, Grad norm: 0.05138610658495636\n",
      "Iteration 8440, BCE loss: 57.7573520037642, Acc: 0.8196, Grad norm: 0.05651677721126429\n",
      "Iteration 8441, BCE loss: 57.75736902100405, Acc: 0.8196, Grad norm: 0.06318185534051038\n",
      "Iteration 8442, BCE loss: 57.757375971645835, Acc: 0.8196, Grad norm: 0.06623480301838473\n",
      "Iteration 8443, BCE loss: 57.75737607229709, Acc: 0.8196, Grad norm: 0.06631870511813637\n",
      "Iteration 8444, BCE loss: 57.75734095757997, Acc: 0.8196, Grad norm: 0.05510851408357124\n",
      "Iteration 8445, BCE loss: 57.757372232561785, Acc: 0.8195, Grad norm: 0.06700961479409079\n",
      "Iteration 8446, BCE loss: 57.757357398712095, Acc: 0.8196, Grad norm: 0.061984441760034134\n",
      "Iteration 8447, BCE loss: 57.757348350730986, Acc: 0.8195, Grad norm: 0.059123967765974374\n",
      "Iteration 8448, BCE loss: 57.75736602324167, Acc: 0.8195, Grad norm: 0.06575511303898482\n",
      "Iteration 8449, BCE loss: 57.7573422871134, Acc: 0.8195, Grad norm: 0.05696611077010088\n",
      "Iteration 8450, BCE loss: 57.75731965880664, Acc: 0.8195, Grad norm: 0.04503552762281701\n",
      "Iteration 8451, BCE loss: 57.75731012676496, Acc: 0.8195, Grad norm: 0.041429036378609334\n",
      "Iteration 8452, BCE loss: 57.757323392424475, Acc: 0.8195, Grad norm: 0.0473525194558404\n",
      "Iteration 8453, BCE loss: 57.75733708906681, Acc: 0.8195, Grad norm: 0.051925626189673016\n",
      "Iteration 8454, BCE loss: 57.75731632064895, Acc: 0.8195, Grad norm: 0.044426562961159675\n",
      "Iteration 8455, BCE loss: 57.75732480700493, Acc: 0.8195, Grad norm: 0.04737211311242175\n",
      "Iteration 8456, BCE loss: 57.757332938913905, Acc: 0.8195, Grad norm: 0.05022796814106485\n",
      "Iteration 8457, BCE loss: 57.75733396935946, Acc: 0.8195, Grad norm: 0.05114712952275444\n",
      "Iteration 8458, BCE loss: 57.75732709234385, Acc: 0.8195, Grad norm: 0.04781085274367343\n",
      "Iteration 8459, BCE loss: 57.757308569905504, Acc: 0.8195, Grad norm: 0.04179217769062975\n",
      "Iteration 8460, BCE loss: 57.75729571000321, Acc: 0.8195, Grad norm: 0.035084602491789324\n",
      "Iteration 8461, BCE loss: 57.757327615895974, Acc: 0.8195, Grad norm: 0.04938913530284064\n",
      "Iteration 8462, BCE loss: 57.757334986233374, Acc: 0.8195, Grad norm: 0.051173605159945595\n",
      "Iteration 8463, BCE loss: 57.75734175799925, Acc: 0.8195, Grad norm: 0.054095002038510996\n",
      "Iteration 8464, BCE loss: 57.757323946684636, Acc: 0.8195, Grad norm: 0.04753368208119413\n",
      "Iteration 8465, BCE loss: 57.757313087658574, Acc: 0.8195, Grad norm: 0.04331021966518667\n",
      "Iteration 8466, BCE loss: 57.757326514409485, Acc: 0.8195, Grad norm: 0.050930004397520066\n",
      "Iteration 8467, BCE loss: 57.75730427728384, Acc: 0.8195, Grad norm: 0.041665341740510904\n",
      "Iteration 8468, BCE loss: 57.75732092374429, Acc: 0.8195, Grad norm: 0.050776517154688276\n",
      "Iteration 8469, BCE loss: 57.757355817454496, Acc: 0.8195, Grad norm: 0.06552644270522699\n",
      "Iteration 8470, BCE loss: 57.75734170843826, Acc: 0.8195, Grad norm: 0.05947650290255587\n",
      "Iteration 8471, BCE loss: 57.75736622265949, Acc: 0.8195, Grad norm: 0.06778234170158874\n",
      "Iteration 8472, BCE loss: 57.757339819971726, Acc: 0.8195, Grad norm: 0.06016915606082671\n",
      "Iteration 8473, BCE loss: 57.757308649548044, Acc: 0.8195, Grad norm: 0.04444320039979914\n",
      "Iteration 8474, BCE loss: 57.75730111253256, Acc: 0.8196, Grad norm: 0.039916511369312686\n",
      "Iteration 8475, BCE loss: 57.75731025235143, Acc: 0.8196, Grad norm: 0.04331755962309246\n",
      "Iteration 8476, BCE loss: 57.7573140373827, Acc: 0.8196, Grad norm: 0.04316387027130757\n",
      "Iteration 8477, BCE loss: 57.75730033262938, Acc: 0.8196, Grad norm: 0.03710318786603841\n",
      "Iteration 8478, BCE loss: 57.75731011583016, Acc: 0.8196, Grad norm: 0.041235086291849764\n",
      "Iteration 8479, BCE loss: 57.7573179634081, Acc: 0.8196, Grad norm: 0.04519246890306033\n",
      "Iteration 8480, BCE loss: 57.75736014590713, Acc: 0.8196, Grad norm: 0.06329920287000698\n",
      "Iteration 8481, BCE loss: 57.757370833829896, Acc: 0.8196, Grad norm: 0.06803763716235263\n",
      "Iteration 8482, BCE loss: 57.757462521335185, Acc: 0.8196, Grad norm: 0.09370046698740218\n",
      "Iteration 8483, BCE loss: 57.75739988707778, Acc: 0.8196, Grad norm: 0.0770059399311211\n",
      "Iteration 8484, BCE loss: 57.75744308306748, Acc: 0.8196, Grad norm: 0.08950778182185286\n",
      "Iteration 8485, BCE loss: 57.75746954462957, Acc: 0.8196, Grad norm: 0.09555111686597556\n",
      "Iteration 8486, BCE loss: 57.757517434620105, Acc: 0.8196, Grad norm: 0.10583539686295043\n",
      "Iteration 8487, BCE loss: 57.75764122525574, Acc: 0.8196, Grad norm: 0.1308263364437283\n",
      "Iteration 8488, BCE loss: 57.7576331314677, Acc: 0.8196, Grad norm: 0.12893260823061098\n",
      "Iteration 8489, BCE loss: 57.75764339405167, Acc: 0.8197, Grad norm: 0.13249432099239977\n",
      "Iteration 8490, BCE loss: 57.75758895612023, Acc: 0.8197, Grad norm: 0.12269828677821283\n",
      "Iteration 8491, BCE loss: 57.757596046424325, Acc: 0.8197, Grad norm: 0.12325692956681546\n",
      "Iteration 8492, BCE loss: 57.75748474804931, Acc: 0.8196, Grad norm: 0.09879630058555945\n",
      "Iteration 8493, BCE loss: 57.75740844823954, Acc: 0.8196, Grad norm: 0.08101307357446082\n",
      "Iteration 8494, BCE loss: 57.7574077128162, Acc: 0.8196, Grad norm: 0.08045782174625908\n",
      "Iteration 8495, BCE loss: 57.7574327958629, Acc: 0.8196, Grad norm: 0.08646052994827091\n",
      "Iteration 8496, BCE loss: 57.757415031898915, Acc: 0.8196, Grad norm: 0.08299659911691583\n",
      "Iteration 8497, BCE loss: 57.7574925254596, Acc: 0.8196, Grad norm: 0.10288368032553599\n",
      "Iteration 8498, BCE loss: 57.75742865684391, Acc: 0.8196, Grad norm: 0.08573209313616909\n",
      "Iteration 8499, BCE loss: 57.75740174694995, Acc: 0.8196, Grad norm: 0.07930864554852715\n",
      "Iteration 8500, BCE loss: 57.757386382234046, Acc: 0.8196, Grad norm: 0.07423630746994761\n",
      "Iteration 8501, BCE loss: 57.757367662452936, Acc: 0.8196, Grad norm: 0.06703146454582329\n",
      "Iteration 8502, BCE loss: 57.75738476464501, Acc: 0.8196, Grad norm: 0.07339695931330718\n",
      "Iteration 8503, BCE loss: 57.75739504628812, Acc: 0.8196, Grad norm: 0.07529282042834223\n",
      "Iteration 8504, BCE loss: 57.75736524051197, Acc: 0.8196, Grad norm: 0.06741961452602917\n",
      "Iteration 8505, BCE loss: 57.75734522456884, Acc: 0.8196, Grad norm: 0.058455141579510594\n",
      "Iteration 8506, BCE loss: 57.757326750362026, Acc: 0.8196, Grad norm: 0.05135330137185315\n",
      "Iteration 8507, BCE loss: 57.7573674867087, Acc: 0.8196, Grad norm: 0.06608818805762619\n",
      "Iteration 8508, BCE loss: 57.757350781251446, Acc: 0.8196, Grad norm: 0.06087935673711847\n",
      "Iteration 8509, BCE loss: 57.75731124845166, Acc: 0.8196, Grad norm: 0.04578006652537044\n",
      "Iteration 8510, BCE loss: 57.75730803042329, Acc: 0.8196, Grad norm: 0.04453316103313845\n",
      "Iteration 8511, BCE loss: 57.75729669415006, Acc: 0.8196, Grad norm: 0.03822693595209178\n",
      "Iteration 8512, BCE loss: 57.757274724064224, Acc: 0.8195, Grad norm: 0.02314072532768246\n",
      "Iteration 8513, BCE loss: 57.75727230055858, Acc: 0.8195, Grad norm: 0.022046906782208678\n",
      "Iteration 8514, BCE loss: 57.75727865590786, Acc: 0.8195, Grad norm: 0.02840406324148443\n",
      "Iteration 8515, BCE loss: 57.75728133455873, Acc: 0.8195, Grad norm: 0.027074620060608103\n",
      "Iteration 8516, BCE loss: 57.75729927663467, Acc: 0.8195, Grad norm: 0.03760094813453199\n",
      "Iteration 8517, BCE loss: 57.757298091110776, Acc: 0.8196, Grad norm: 0.0381373142895145\n",
      "Iteration 8518, BCE loss: 57.7573187660326, Acc: 0.8196, Grad norm: 0.04699979910019315\n",
      "Iteration 8519, BCE loss: 57.75733184642075, Acc: 0.8196, Grad norm: 0.054222818514155176\n",
      "Iteration 8520, BCE loss: 57.75732627289834, Acc: 0.8196, Grad norm: 0.05320222432675472\n",
      "Iteration 8521, BCE loss: 57.75735074150415, Acc: 0.8196, Grad norm: 0.0628099523842937\n",
      "Iteration 8522, BCE loss: 57.757323103525664, Acc: 0.8196, Grad norm: 0.05070161489474933\n",
      "Iteration 8523, BCE loss: 57.7573322468, Acc: 0.8196, Grad norm: 0.054518448663118765\n",
      "Iteration 8524, BCE loss: 57.757335704501884, Acc: 0.8196, Grad norm: 0.05636558766232223\n",
      "Iteration 8525, BCE loss: 57.75730699985182, Acc: 0.8196, Grad norm: 0.044640016557567136\n",
      "Iteration 8526, BCE loss: 57.75731286988259, Acc: 0.8196, Grad norm: 0.04631456619882515\n",
      "Iteration 8527, BCE loss: 57.75731203338108, Acc: 0.8196, Grad norm: 0.04562047830560637\n",
      "Iteration 8528, BCE loss: 57.75728960333896, Acc: 0.8196, Grad norm: 0.03348365958055012\n",
      "Iteration 8529, BCE loss: 57.75730446172246, Acc: 0.8196, Grad norm: 0.04258557900374506\n",
      "Iteration 8530, BCE loss: 57.75727081060314, Acc: 0.8196, Grad norm: 0.020676779897186272\n",
      "Iteration 8531, BCE loss: 57.75728278400535, Acc: 0.8196, Grad norm: 0.03239259892375801\n",
      "Iteration 8532, BCE loss: 57.75728311622946, Acc: 0.8195, Grad norm: 0.032316628254844326\n",
      "Iteration 8533, BCE loss: 57.757296954965156, Acc: 0.8195, Grad norm: 0.03992068861023799\n",
      "Iteration 8534, BCE loss: 57.75729588838698, Acc: 0.8195, Grad norm: 0.04003292300130619\n",
      "Iteration 8535, BCE loss: 57.75731384420687, Acc: 0.8195, Grad norm: 0.04866588350903368\n",
      "Iteration 8536, BCE loss: 57.757348616372795, Acc: 0.8195, Grad norm: 0.06271947881612021\n",
      "Iteration 8537, BCE loss: 57.757310522598544, Acc: 0.8195, Grad norm: 0.04826839530424089\n",
      "Iteration 8538, BCE loss: 57.757309111709915, Acc: 0.8195, Grad norm: 0.04736181927960697\n",
      "Iteration 8539, BCE loss: 57.757306636349384, Acc: 0.8195, Grad norm: 0.04645444569421971\n",
      "Iteration 8540, BCE loss: 57.75728623773766, Acc: 0.8195, Grad norm: 0.03635415445360706\n",
      "Iteration 8541, BCE loss: 57.75730553321798, Acc: 0.8195, Grad norm: 0.04769105058197629\n",
      "Iteration 8542, BCE loss: 57.75728817305899, Acc: 0.8195, Grad norm: 0.038101772999527654\n",
      "Iteration 8543, BCE loss: 57.75728700543682, Acc: 0.8195, Grad norm: 0.037200029351983716\n",
      "Iteration 8544, BCE loss: 57.757318705706126, Acc: 0.8195, Grad norm: 0.05351880343985925\n",
      "Iteration 8545, BCE loss: 57.757291130756315, Acc: 0.8195, Grad norm: 0.0382381685593569\n",
      "Iteration 8546, BCE loss: 57.7573493914246, Acc: 0.8195, Grad norm: 0.06461521497745368\n",
      "Iteration 8547, BCE loss: 57.75738166362197, Acc: 0.8195, Grad norm: 0.0764959699264525\n",
      "Iteration 8548, BCE loss: 57.75739775816662, Acc: 0.8195, Grad norm: 0.07963234945273198\n",
      "Iteration 8549, BCE loss: 57.75742033094925, Acc: 0.8195, Grad norm: 0.0851067723308985\n",
      "Iteration 8550, BCE loss: 57.75746475550244, Acc: 0.8195, Grad norm: 0.09657350580608706\n",
      "Iteration 8551, BCE loss: 57.757474661292726, Acc: 0.8195, Grad norm: 0.10004472514095539\n",
      "Iteration 8552, BCE loss: 57.75735277263916, Acc: 0.8195, Grad norm: 0.06404472883556293\n",
      "Iteration 8553, BCE loss: 57.75734137621687, Acc: 0.8195, Grad norm: 0.06066142934054633\n",
      "Iteration 8554, BCE loss: 57.75730828818671, Acc: 0.8195, Grad norm: 0.04398689406947126\n",
      "Iteration 8555, BCE loss: 57.75730975335469, Acc: 0.8195, Grad norm: 0.04650643018465502\n",
      "Iteration 8556, BCE loss: 57.75729020093381, Acc: 0.8195, Grad norm: 0.03484990054489036\n",
      "Iteration 8557, BCE loss: 57.75730806502882, Acc: 0.8195, Grad norm: 0.0447577847603728\n",
      "Iteration 8558, BCE loss: 57.75733236935007, Acc: 0.8195, Grad norm: 0.056242223228108935\n",
      "Iteration 8559, BCE loss: 57.75735895235418, Acc: 0.8195, Grad norm: 0.06636910072140256\n",
      "Iteration 8560, BCE loss: 57.75737557444074, Acc: 0.8195, Grad norm: 0.07173008670719092\n",
      "Iteration 8561, BCE loss: 57.75740605043595, Acc: 0.8195, Grad norm: 0.0816938358965321\n",
      "Iteration 8562, BCE loss: 57.757437178926516, Acc: 0.8195, Grad norm: 0.09116017447980791\n",
      "Iteration 8563, BCE loss: 57.75741051015487, Acc: 0.8195, Grad norm: 0.08291840859574941\n",
      "Iteration 8564, BCE loss: 57.757424359980845, Acc: 0.8195, Grad norm: 0.08596798287002537\n",
      "Iteration 8565, BCE loss: 57.757388226526174, Acc: 0.8195, Grad norm: 0.07639599045509726\n",
      "Iteration 8566, BCE loss: 57.75739763041262, Acc: 0.8196, Grad norm: 0.07889847462445757\n",
      "Iteration 8567, BCE loss: 57.7573890793138, Acc: 0.8196, Grad norm: 0.07550974620036338\n",
      "Iteration 8568, BCE loss: 57.75737440308008, Acc: 0.8196, Grad norm: 0.07160002737046309\n",
      "Iteration 8569, BCE loss: 57.75739499055784, Acc: 0.8196, Grad norm: 0.07845594686735145\n",
      "Iteration 8570, BCE loss: 57.757407436188316, Acc: 0.8195, Grad norm: 0.08159771670612719\n",
      "Iteration 8571, BCE loss: 57.757380449994194, Acc: 0.8195, Grad norm: 0.07379106748325892\n",
      "Iteration 8572, BCE loss: 57.75737555595659, Acc: 0.8196, Grad norm: 0.07127408727225326\n",
      "Iteration 8573, BCE loss: 57.75736233937077, Acc: 0.8196, Grad norm: 0.06845687705656694\n",
      "Iteration 8574, BCE loss: 57.75734531117921, Acc: 0.8196, Grad norm: 0.062320680737856064\n",
      "Iteration 8575, BCE loss: 57.757327131590216, Acc: 0.8196, Grad norm: 0.054307199143040344\n",
      "Iteration 8576, BCE loss: 57.75736772009668, Acc: 0.8196, Grad norm: 0.07036878432249928\n",
      "Iteration 8577, BCE loss: 57.75745672954895, Acc: 0.8196, Grad norm: 0.09594833091043867\n",
      "Iteration 8578, BCE loss: 57.757527652464816, Acc: 0.8196, Grad norm: 0.11262103448399467\n",
      "Iteration 8579, BCE loss: 57.75751463138542, Acc: 0.8196, Grad norm: 0.10950158949719224\n",
      "Iteration 8580, BCE loss: 57.75751812876222, Acc: 0.8196, Grad norm: 0.11045842030456972\n",
      "Iteration 8581, BCE loss: 57.75744291239988, Acc: 0.8195, Grad norm: 0.0928859415349263\n",
      "Iteration 8582, BCE loss: 57.75742350444084, Acc: 0.8195, Grad norm: 0.08698177220111136\n",
      "Iteration 8583, BCE loss: 57.75743356306141, Acc: 0.8195, Grad norm: 0.08963515689909353\n",
      "Iteration 8584, BCE loss: 57.75741291730837, Acc: 0.8195, Grad norm: 0.08481451210728383\n",
      "Iteration 8585, BCE loss: 57.75743399962319, Acc: 0.8195, Grad norm: 0.09049682212958061\n",
      "Iteration 8586, BCE loss: 57.75743245574358, Acc: 0.8195, Grad norm: 0.08960408262616185\n",
      "Iteration 8587, BCE loss: 57.757443826888384, Acc: 0.8195, Grad norm: 0.09173748287909177\n",
      "Iteration 8588, BCE loss: 57.75738443911354, Acc: 0.8195, Grad norm: 0.07470230556606366\n",
      "Iteration 8589, BCE loss: 57.7574057159579, Acc: 0.8195, Grad norm: 0.08179381534549333\n",
      "Iteration 8590, BCE loss: 57.75739993016337, Acc: 0.8195, Grad norm: 0.0798284750335085\n",
      "Iteration 8591, BCE loss: 57.757375062638395, Acc: 0.8195, Grad norm: 0.0723987392868293\n",
      "Iteration 8592, BCE loss: 57.7573203912897, Acc: 0.8195, Grad norm: 0.051228113707852956\n",
      "Iteration 8593, BCE loss: 57.75729270695015, Acc: 0.8195, Grad norm: 0.036599805718643026\n",
      "Iteration 8594, BCE loss: 57.75732633553788, Acc: 0.8196, Grad norm: 0.05331373181532293\n",
      "Iteration 8595, BCE loss: 57.757317533948125, Acc: 0.8196, Grad norm: 0.04808245644941473\n",
      "Iteration 8596, BCE loss: 57.75730011818991, Acc: 0.8196, Grad norm: 0.038987176862061663\n",
      "Iteration 8597, BCE loss: 57.75732749577652, Acc: 0.8196, Grad norm: 0.051669534727288136\n",
      "Iteration 8598, BCE loss: 57.757332527769805, Acc: 0.8196, Grad norm: 0.052389328193978325\n",
      "Iteration 8599, BCE loss: 57.757352072170804, Acc: 0.8196, Grad norm: 0.06146792565245617\n",
      "Iteration 8600, BCE loss: 57.75732132341987, Acc: 0.8196, Grad norm: 0.050180178537460166\n",
      "Iteration 8601, BCE loss: 57.757348980761236, Acc: 0.8196, Grad norm: 0.06091561718332769\n",
      "Iteration 8602, BCE loss: 57.757366013245274, Acc: 0.8196, Grad norm: 0.06725827247906875\n",
      "Iteration 8603, BCE loss: 57.75736640692935, Acc: 0.8196, Grad norm: 0.0676934858700649\n",
      "Iteration 8604, BCE loss: 57.757344196658984, Acc: 0.8196, Grad norm: 0.05854047728539685\n",
      "Iteration 8605, BCE loss: 57.7573672852981, Acc: 0.8196, Grad norm: 0.06494831686284601\n",
      "Iteration 8606, BCE loss: 57.757397610701005, Acc: 0.8196, Grad norm: 0.07435823143083897\n",
      "Iteration 8607, BCE loss: 57.757386466504286, Acc: 0.8196, Grad norm: 0.06917558091529459\n",
      "Iteration 8608, BCE loss: 57.75738103466293, Acc: 0.8196, Grad norm: 0.06730342072021493\n",
      "Iteration 8609, BCE loss: 57.7574097635777, Acc: 0.8196, Grad norm: 0.07424757540280862\n",
      "Iteration 8610, BCE loss: 57.75739065161538, Acc: 0.8196, Grad norm: 0.07106633297023195\n",
      "Iteration 8611, BCE loss: 57.75742855856548, Acc: 0.8196, Grad norm: 0.08181720356593271\n",
      "Iteration 8612, BCE loss: 57.75736313230943, Acc: 0.8196, Grad norm: 0.062486979249740185\n",
      "Iteration 8613, BCE loss: 57.75738014962283, Acc: 0.8196, Grad norm: 0.06804148757304687\n",
      "Iteration 8614, BCE loss: 57.757387959312766, Acc: 0.8196, Grad norm: 0.06988120278138929\n",
      "Iteration 8615, BCE loss: 57.75735275135674, Acc: 0.8196, Grad norm: 0.058399261989630365\n",
      "Iteration 8616, BCE loss: 57.757345388563856, Acc: 0.8196, Grad norm: 0.055029186662167544\n",
      "Iteration 8617, BCE loss: 57.75732683735515, Acc: 0.8196, Grad norm: 0.04901665440547711\n",
      "Iteration 8618, BCE loss: 57.75731267019154, Acc: 0.8196, Grad norm: 0.04512638396360429\n",
      "Iteration 8619, BCE loss: 57.75732277146601, Acc: 0.8195, Grad norm: 0.05045271200387344\n",
      "Iteration 8620, BCE loss: 57.7573137084043, Acc: 0.8195, Grad norm: 0.04787070171820211\n",
      "Iteration 8621, BCE loss: 57.75732099636035, Acc: 0.8195, Grad norm: 0.0511923730435689\n",
      "Iteration 8622, BCE loss: 57.757330423855834, Acc: 0.8195, Grad norm: 0.055385616049482604\n",
      "Iteration 8623, BCE loss: 57.757347846718666, Acc: 0.8195, Grad norm: 0.060961233591770275\n",
      "Iteration 8624, BCE loss: 57.757383178235074, Acc: 0.8195, Grad norm: 0.07331208739851917\n",
      "Iteration 8625, BCE loss: 57.757411536238465, Acc: 0.8195, Grad norm: 0.08237663749894154\n",
      "Iteration 8626, BCE loss: 57.757367041582214, Acc: 0.8195, Grad norm: 0.06973847906623754\n",
      "Iteration 8627, BCE loss: 57.757369680882285, Acc: 0.8195, Grad norm: 0.07088034377675316\n",
      "Iteration 8628, BCE loss: 57.75735838804499, Acc: 0.8195, Grad norm: 0.06620779943798387\n",
      "Iteration 8629, BCE loss: 57.75737109656322, Acc: 0.8195, Grad norm: 0.06946246924897602\n",
      "Iteration 8630, BCE loss: 57.75734053884557, Acc: 0.8195, Grad norm: 0.057477049439598464\n",
      "Iteration 8631, BCE loss: 57.757356474609445, Acc: 0.8195, Grad norm: 0.06377745823136444\n",
      "Iteration 8632, BCE loss: 57.75742545437316, Acc: 0.8195, Grad norm: 0.08424816666712884\n",
      "Iteration 8633, BCE loss: 57.757419755998356, Acc: 0.8196, Grad norm: 0.0832257448515441\n",
      "Iteration 8634, BCE loss: 57.75737697108627, Acc: 0.8195, Grad norm: 0.0705721291184025\n",
      "Iteration 8635, BCE loss: 57.75741398341604, Acc: 0.8196, Grad norm: 0.08055874546162314\n",
      "Iteration 8636, BCE loss: 57.7573608264738, Acc: 0.8196, Grad norm: 0.06249926392337941\n",
      "Iteration 8637, BCE loss: 57.75735365177559, Acc: 0.8195, Grad norm: 0.05884826518072444\n",
      "Iteration 8638, BCE loss: 57.757364615909324, Acc: 0.8195, Grad norm: 0.062038946864103474\n",
      "Iteration 8639, BCE loss: 57.75739013101933, Acc: 0.8195, Grad norm: 0.06902847013490886\n",
      "Iteration 8640, BCE loss: 57.75742834623448, Acc: 0.8195, Grad norm: 0.07951680614347828\n",
      "Iteration 8641, BCE loss: 57.75742552620851, Acc: 0.8195, Grad norm: 0.07923566573429228\n",
      "Iteration 8642, BCE loss: 57.75747141865874, Acc: 0.8195, Grad norm: 0.09050418528033545\n",
      "Iteration 8643, BCE loss: 57.75743872802775, Acc: 0.8195, Grad norm: 0.08367648992594406\n",
      "Iteration 8644, BCE loss: 57.75745499547052, Acc: 0.8195, Grad norm: 0.0872739937748407\n",
      "Iteration 8645, BCE loss: 57.75743046472097, Acc: 0.8195, Grad norm: 0.0807901454574915\n",
      "Iteration 8646, BCE loss: 57.75737122721384, Acc: 0.8195, Grad norm: 0.06383456822334853\n",
      "Iteration 8647, BCE loss: 57.75734654247578, Acc: 0.8195, Grad norm: 0.05566066232502402\n",
      "Iteration 8648, BCE loss: 57.75734179134668, Acc: 0.8195, Grad norm: 0.05784260762157579\n",
      "Iteration 8649, BCE loss: 57.75734165791628, Acc: 0.8195, Grad norm: 0.05491422995974772\n",
      "Iteration 8650, BCE loss: 57.75731403844233, Acc: 0.8195, Grad norm: 0.045911120811345484\n",
      "Iteration 8651, BCE loss: 57.75733948461016, Acc: 0.8195, Grad norm: 0.05844578387690654\n",
      "Iteration 8652, BCE loss: 57.75732943061389, Acc: 0.8195, Grad norm: 0.05426975958958357\n",
      "Iteration 8653, BCE loss: 57.757360092738665, Acc: 0.8196, Grad norm: 0.06555988335729679\n",
      "Iteration 8654, BCE loss: 57.757325550585804, Acc: 0.8196, Grad norm: 0.05116616130351142\n",
      "Iteration 8655, BCE loss: 57.75731970124538, Acc: 0.8195, Grad norm: 0.04689054868078057\n",
      "Iteration 8656, BCE loss: 57.75732442681707, Acc: 0.8196, Grad norm: 0.04977895230570282\n",
      "Iteration 8657, BCE loss: 57.7573399808183, Acc: 0.8196, Grad norm: 0.05616210324780295\n",
      "Iteration 8658, BCE loss: 57.75734180815952, Acc: 0.8196, Grad norm: 0.05738978973428259\n",
      "Iteration 8659, BCE loss: 57.75733341611598, Acc: 0.8196, Grad norm: 0.05496890375441256\n",
      "Iteration 8660, BCE loss: 57.75737767272494, Acc: 0.8196, Grad norm: 0.07055101349624046\n",
      "Iteration 8661, BCE loss: 57.75737761375901, Acc: 0.8195, Grad norm: 0.06997930391411307\n",
      "Iteration 8662, BCE loss: 57.757330361176066, Acc: 0.8196, Grad norm: 0.05367558879441486\n",
      "Iteration 8663, BCE loss: 57.75733328816456, Acc: 0.8195, Grad norm: 0.053670411036904606\n",
      "Iteration 8664, BCE loss: 57.7573000929133, Acc: 0.8195, Grad norm: 0.03827873993557145\n",
      "Iteration 8665, BCE loss: 57.75731354921593, Acc: 0.8195, Grad norm: 0.046646843628548464\n",
      "Iteration 8666, BCE loss: 57.75733063665152, Acc: 0.8195, Grad norm: 0.05516597225510003\n",
      "Iteration 8667, BCE loss: 57.75738660092528, Acc: 0.8195, Grad norm: 0.07500803004537271\n",
      "Iteration 8668, BCE loss: 57.75737926071432, Acc: 0.8196, Grad norm: 0.07279453635935108\n",
      "Iteration 8669, BCE loss: 57.75734387813067, Acc: 0.8196, Grad norm: 0.06111625881974467\n",
      "Iteration 8670, BCE loss: 57.757347040915576, Acc: 0.8196, Grad norm: 0.061905695616146256\n",
      "Iteration 8671, BCE loss: 57.757353646947934, Acc: 0.8196, Grad norm: 0.0656944010732722\n",
      "Iteration 8672, BCE loss: 57.757341238830634, Acc: 0.8196, Grad norm: 0.060758070167195775\n",
      "Iteration 8673, BCE loss: 57.757416743465086, Acc: 0.8196, Grad norm: 0.08455674428852268\n",
      "Iteration 8674, BCE loss: 57.757398477829035, Acc: 0.8196, Grad norm: 0.07970388406274367\n",
      "Iteration 8675, BCE loss: 57.75740109114465, Acc: 0.8196, Grad norm: 0.07975433875047323\n",
      "Iteration 8676, BCE loss: 57.75739613871623, Acc: 0.8196, Grad norm: 0.07887339279214246\n",
      "Iteration 8677, BCE loss: 57.75740080271294, Acc: 0.8196, Grad norm: 0.0793501816038946\n",
      "Iteration 8678, BCE loss: 57.757374036882254, Acc: 0.8196, Grad norm: 0.07097495365417233\n",
      "Iteration 8679, BCE loss: 57.75739313666901, Acc: 0.8195, Grad norm: 0.07553103956804602\n",
      "Iteration 8680, BCE loss: 57.75740078195264, Acc: 0.8195, Grad norm: 0.078303320644273\n",
      "Iteration 8681, BCE loss: 57.75739411121906, Acc: 0.8195, Grad norm: 0.07719479169069367\n",
      "Iteration 8682, BCE loss: 57.75742655374123, Acc: 0.8196, Grad norm: 0.08558557908335711\n",
      "Iteration 8683, BCE loss: 57.75738435221325, Acc: 0.8196, Grad norm: 0.07164158298130131\n",
      "Iteration 8684, BCE loss: 57.75738450193829, Acc: 0.8196, Grad norm: 0.07397979544855153\n",
      "Iteration 8685, BCE loss: 57.75735298666021, Acc: 0.8196, Grad norm: 0.06483158568280227\n",
      "Iteration 8686, BCE loss: 57.75733101625583, Acc: 0.8196, Grad norm: 0.0574918394909872\n",
      "Iteration 8687, BCE loss: 57.75736179892667, Acc: 0.8196, Grad norm: 0.069503151813876\n",
      "Iteration 8688, BCE loss: 57.75737561276114, Acc: 0.8196, Grad norm: 0.07253998314556287\n",
      "Iteration 8689, BCE loss: 57.75740923023996, Acc: 0.8195, Grad norm: 0.0812829186761367\n",
      "Iteration 8690, BCE loss: 57.757417944479215, Acc: 0.8195, Grad norm: 0.08449281481277325\n",
      "Iteration 8691, BCE loss: 57.75743270445278, Acc: 0.8195, Grad norm: 0.08847211279235867\n",
      "Iteration 8692, BCE loss: 57.75735879040935, Acc: 0.8195, Grad norm: 0.06772364106052604\n",
      "Iteration 8693, BCE loss: 57.757313880201046, Acc: 0.8195, Grad norm: 0.050245290306914596\n",
      "Iteration 8694, BCE loss: 57.7573214944468, Acc: 0.8195, Grad norm: 0.052437043358450355\n",
      "Iteration 8695, BCE loss: 57.757325012573915, Acc: 0.8195, Grad norm: 0.05336192688851987\n",
      "Iteration 8696, BCE loss: 57.75731373661986, Acc: 0.8195, Grad norm: 0.050160411315255865\n",
      "Iteration 8697, BCE loss: 57.75730263259022, Acc: 0.8196, Grad norm: 0.04550383558799098\n",
      "Iteration 8698, BCE loss: 57.757322495983296, Acc: 0.8196, Grad norm: 0.05414340035342283\n",
      "Iteration 8699, BCE loss: 57.757321808024386, Acc: 0.8195, Grad norm: 0.05363503652008421\n",
      "Iteration 8700, BCE loss: 57.75735217139479, Acc: 0.8195, Grad norm: 0.06569541490869439\n",
      "Iteration 8701, BCE loss: 57.75731652737471, Acc: 0.8195, Grad norm: 0.050665386127493835\n",
      "Iteration 8702, BCE loss: 57.757297658081214, Acc: 0.8195, Grad norm: 0.040536943385170936\n",
      "Iteration 8703, BCE loss: 57.7573304134277, Acc: 0.8195, Grad norm: 0.05660504790070372\n",
      "Iteration 8704, BCE loss: 57.757316352545516, Acc: 0.8195, Grad norm: 0.05131942929779724\n",
      "Iteration 8705, BCE loss: 57.757356399780214, Acc: 0.8195, Grad norm: 0.06605159120697834\n",
      "Iteration 8706, BCE loss: 57.75733200896512, Acc: 0.8195, Grad norm: 0.05735511018126117\n",
      "Iteration 8707, BCE loss: 57.75731537771601, Acc: 0.8195, Grad norm: 0.05040626809959634\n",
      "Iteration 8708, BCE loss: 57.7573019347151, Acc: 0.8195, Grad norm: 0.04388758918996103\n",
      "Iteration 8709, BCE loss: 57.75730010624239, Acc: 0.8195, Grad norm: 0.042059074107010784\n",
      "Iteration 8710, BCE loss: 57.757282661213566, Acc: 0.8195, Grad norm: 0.031061070450969924\n",
      "Iteration 8711, BCE loss: 57.75728750498405, Acc: 0.8195, Grad norm: 0.03419778792529655\n",
      "Iteration 8712, BCE loss: 57.75728283743978, Acc: 0.8195, Grad norm: 0.031219880538830073\n",
      "Iteration 8713, BCE loss: 57.757298303428456, Acc: 0.8195, Grad norm: 0.040804788612432696\n",
      "Iteration 8714, BCE loss: 57.75731980427116, Acc: 0.8195, Grad norm: 0.05094918178827447\n",
      "Iteration 8715, BCE loss: 57.75731325298918, Acc: 0.8195, Grad norm: 0.04863487528085753\n",
      "Iteration 8716, BCE loss: 57.757343567244064, Acc: 0.8195, Grad norm: 0.06133463718393843\n",
      "Iteration 8717, BCE loss: 57.757311172398175, Acc: 0.8195, Grad norm: 0.0490052962684874\n",
      "Iteration 8718, BCE loss: 57.75731517184383, Acc: 0.8195, Grad norm: 0.05132236555550315\n",
      "Iteration 8719, BCE loss: 57.757311574380125, Acc: 0.8195, Grad norm: 0.050107170651475234\n",
      "Iteration 8720, BCE loss: 57.7573156536827, Acc: 0.8195, Grad norm: 0.0512186990338568\n",
      "Iteration 8721, BCE loss: 57.757355075500286, Acc: 0.8195, Grad norm: 0.06703484881113579\n",
      "Iteration 8722, BCE loss: 57.75737147165361, Acc: 0.8195, Grad norm: 0.0718318028360806\n",
      "Iteration 8723, BCE loss: 57.757368087349334, Acc: 0.8195, Grad norm: 0.0710379229606101\n",
      "Iteration 8724, BCE loss: 57.757380614430645, Acc: 0.8195, Grad norm: 0.07573860654543584\n",
      "Iteration 8725, BCE loss: 57.75735944678878, Acc: 0.8195, Grad norm: 0.06930198241003077\n",
      "Iteration 8726, BCE loss: 57.75737094007772, Acc: 0.8195, Grad norm: 0.07304703397324684\n",
      "Iteration 8727, BCE loss: 57.75735412469004, Acc: 0.8195, Grad norm: 0.06692727407587058\n",
      "Iteration 8728, BCE loss: 57.75737218314421, Acc: 0.8195, Grad norm: 0.07302886001295336\n",
      "Iteration 8729, BCE loss: 57.7573530096617, Acc: 0.8195, Grad norm: 0.06665139634251155\n",
      "Iteration 8730, BCE loss: 57.75739441106563, Acc: 0.8195, Grad norm: 0.08018617864487342\n",
      "Iteration 8731, BCE loss: 57.757352493432805, Acc: 0.8195, Grad norm: 0.06736674920685798\n",
      "Iteration 8732, BCE loss: 57.757351122975315, Acc: 0.8195, Grad norm: 0.065911998587511\n",
      "Iteration 8733, BCE loss: 57.757337619562094, Acc: 0.8195, Grad norm: 0.058547648875628334\n",
      "Iteration 8734, BCE loss: 57.75732476978068, Acc: 0.8195, Grad norm: 0.05303102693562319\n",
      "Iteration 8735, BCE loss: 57.75732697327808, Acc: 0.8195, Grad norm: 0.05345577267691752\n",
      "Iteration 8736, BCE loss: 57.75732308295737, Acc: 0.8195, Grad norm: 0.05342951109342078\n",
      "Iteration 8737, BCE loss: 57.75733873092068, Acc: 0.8195, Grad norm: 0.0600036660926469\n",
      "Iteration 8738, BCE loss: 57.75731457946874, Acc: 0.8195, Grad norm: 0.04780030964271753\n",
      "Iteration 8739, BCE loss: 57.75735392154863, Acc: 0.8195, Grad norm: 0.06432095484712737\n",
      "Iteration 8740, BCE loss: 57.75733826125797, Acc: 0.8195, Grad norm: 0.05840164273055936\n",
      "Iteration 8741, BCE loss: 57.75732961528209, Acc: 0.8195, Grad norm: 0.05248156084684343\n",
      "Iteration 8742, BCE loss: 57.75731690777637, Acc: 0.8195, Grad norm: 0.04657222712565296\n",
      "Iteration 8743, BCE loss: 57.757294642539485, Acc: 0.8195, Grad norm: 0.035839453991855466\n",
      "Iteration 8744, BCE loss: 57.75730354702881, Acc: 0.8195, Grad norm: 0.041025864888639886\n",
      "Iteration 8745, BCE loss: 57.75730330194776, Acc: 0.8195, Grad norm: 0.039054421664283225\n",
      "Iteration 8746, BCE loss: 57.75733037952429, Acc: 0.8195, Grad norm: 0.04989461844279116\n",
      "Iteration 8747, BCE loss: 57.757335679940994, Acc: 0.8196, Grad norm: 0.05275777079236383\n",
      "Iteration 8748, BCE loss: 57.75737087588421, Acc: 0.8196, Grad norm: 0.06484440999610148\n",
      "Iteration 8749, BCE loss: 57.75739333141105, Acc: 0.8195, Grad norm: 0.0727021371106576\n",
      "Iteration 8750, BCE loss: 57.757367648098025, Acc: 0.8195, Grad norm: 0.06466147527179193\n",
      "Iteration 8751, BCE loss: 57.757387526811996, Acc: 0.8195, Grad norm: 0.07106094686514136\n",
      "Iteration 8752, BCE loss: 57.757363151614925, Acc: 0.8195, Grad norm: 0.06281465941091717\n",
      "Iteration 8753, BCE loss: 57.75737016952587, Acc: 0.8196, Grad norm: 0.06427178501058767\n",
      "Iteration 8754, BCE loss: 57.75737886350164, Acc: 0.8196, Grad norm: 0.06956274131554369\n",
      "Iteration 8755, BCE loss: 57.75735997722808, Acc: 0.8196, Grad norm: 0.0655892003439449\n",
      "Iteration 8756, BCE loss: 57.75736542598721, Acc: 0.8196, Grad norm: 0.0665345069904533\n",
      "Iteration 8757, BCE loss: 57.75740412615765, Acc: 0.8196, Grad norm: 0.0801075585233018\n",
      "Iteration 8758, BCE loss: 57.75735606741476, Acc: 0.8195, Grad norm: 0.06383846161293666\n",
      "Iteration 8759, BCE loss: 57.75733025009948, Acc: 0.8195, Grad norm: 0.052696894823755176\n",
      "Iteration 8760, BCE loss: 57.757313677505245, Acc: 0.8195, Grad norm: 0.04269732431929128\n",
      "Iteration 8761, BCE loss: 57.75730779260622, Acc: 0.8195, Grad norm: 0.03968206136612759\n",
      "Iteration 8762, BCE loss: 57.757315978597234, Acc: 0.8195, Grad norm: 0.04535278864831357\n",
      "Iteration 8763, BCE loss: 57.75734264104777, Acc: 0.8195, Grad norm: 0.058387480066597705\n",
      "Iteration 8764, BCE loss: 57.75739932394075, Acc: 0.8195, Grad norm: 0.07783405442469064\n",
      "Iteration 8765, BCE loss: 57.75746414064537, Acc: 0.8195, Grad norm: 0.09411091219719131\n",
      "Iteration 8766, BCE loss: 57.75744700195963, Acc: 0.8195, Grad norm: 0.08977136529288737\n",
      "Iteration 8767, BCE loss: 57.75740834884907, Acc: 0.8195, Grad norm: 0.07926978451717487\n",
      "Iteration 8768, BCE loss: 57.757368243732444, Acc: 0.8195, Grad norm: 0.06800173990009228\n",
      "Iteration 8769, BCE loss: 57.757317632055646, Acc: 0.8196, Grad norm: 0.049971430635907595\n",
      "Iteration 8770, BCE loss: 57.7573006377399, Acc: 0.8195, Grad norm: 0.040403897573809563\n",
      "Iteration 8771, BCE loss: 57.75733616760911, Acc: 0.8195, Grad norm: 0.05484302138877257\n",
      "Iteration 8772, BCE loss: 57.75737127447853, Acc: 0.8196, Grad norm: 0.06906429544501312\n",
      "Iteration 8773, BCE loss: 57.75743443374931, Acc: 0.8196, Grad norm: 0.08865094160310974\n",
      "Iteration 8774, BCE loss: 57.75738478817911, Acc: 0.8196, Grad norm: 0.07384411404544239\n",
      "Iteration 8775, BCE loss: 57.75734017429118, Acc: 0.8195, Grad norm: 0.05454121240203951\n",
      "Iteration 8776, BCE loss: 57.75732748997847, Acc: 0.8195, Grad norm: 0.05084953223615346\n",
      "Iteration 8777, BCE loss: 57.75733205648282, Acc: 0.8195, Grad norm: 0.05281919282075864\n",
      "Iteration 8778, BCE loss: 57.75735992601622, Acc: 0.8195, Grad norm: 0.0631270680408292\n",
      "Iteration 8779, BCE loss: 57.75735547587902, Acc: 0.8195, Grad norm: 0.05972046533198694\n",
      "Iteration 8780, BCE loss: 57.75736518718961, Acc: 0.8196, Grad norm: 0.06475449322960979\n",
      "Iteration 8781, BCE loss: 57.75736272908236, Acc: 0.8195, Grad norm: 0.06253284744107568\n",
      "Iteration 8782, BCE loss: 57.75738015289001, Acc: 0.8196, Grad norm: 0.06997316771305614\n",
      "Iteration 8783, BCE loss: 57.75735164979203, Acc: 0.8195, Grad norm: 0.05832072259734293\n",
      "Iteration 8784, BCE loss: 57.75735194493501, Acc: 0.8195, Grad norm: 0.05967088136270189\n",
      "Iteration 8785, BCE loss: 57.75736692641479, Acc: 0.8195, Grad norm: 0.06610047777119694\n",
      "Iteration 8786, BCE loss: 57.75734332679811, Acc: 0.8195, Grad norm: 0.058933618150086034\n",
      "Iteration 8787, BCE loss: 57.757364458436896, Acc: 0.8195, Grad norm: 0.06636372366636743\n",
      "Iteration 8788, BCE loss: 57.75734304427495, Acc: 0.8195, Grad norm: 0.057301904817608856\n",
      "Iteration 8789, BCE loss: 57.75734240484748, Acc: 0.8195, Grad norm: 0.05838678771473968\n",
      "Iteration 8790, BCE loss: 57.75734542470417, Acc: 0.8195, Grad norm: 0.061013891903258795\n",
      "Iteration 8791, BCE loss: 57.75732518384578, Acc: 0.8195, Grad norm: 0.0522209993055151\n",
      "Iteration 8792, BCE loss: 57.75734750389983, Acc: 0.8195, Grad norm: 0.06160799966923013\n",
      "Iteration 8793, BCE loss: 57.757315249464966, Acc: 0.8195, Grad norm: 0.04939934413820206\n",
      "Iteration 8794, BCE loss: 57.75732645984732, Acc: 0.8195, Grad norm: 0.054412658040786877\n",
      "Iteration 8795, BCE loss: 57.757316234415, Acc: 0.8195, Grad norm: 0.04961346198711928\n",
      "Iteration 8796, BCE loss: 57.757305467114165, Acc: 0.8195, Grad norm: 0.04458016237686633\n",
      "Iteration 8797, BCE loss: 57.75730138496755, Acc: 0.8195, Grad norm: 0.04392272702658919\n",
      "Iteration 8798, BCE loss: 57.75729970784447, Acc: 0.8195, Grad norm: 0.04199004753182206\n",
      "Iteration 8799, BCE loss: 57.757336158258, Acc: 0.8195, Grad norm: 0.05762977780329978\n",
      "Iteration 8800, BCE loss: 57.75732793677622, Acc: 0.8195, Grad norm: 0.053429664227271106\n",
      "Iteration 8801, BCE loss: 57.75732894391303, Acc: 0.8195, Grad norm: 0.0544131490765977\n",
      "Iteration 8802, BCE loss: 57.757326356194156, Acc: 0.8195, Grad norm: 0.05188640373257704\n",
      "Iteration 8803, BCE loss: 57.75735268232096, Acc: 0.8195, Grad norm: 0.05923850967388051\n",
      "Iteration 8804, BCE loss: 57.757417762958916, Acc: 0.8195, Grad norm: 0.0819121819277929\n",
      "Iteration 8805, BCE loss: 57.75736800595316, Acc: 0.8195, Grad norm: 0.06744054970187108\n",
      "Iteration 8806, BCE loss: 57.75734400201566, Acc: 0.8195, Grad norm: 0.058432531347026154\n",
      "Iteration 8807, BCE loss: 57.75732888186448, Acc: 0.8196, Grad norm: 0.05414933783489617\n",
      "Iteration 8808, BCE loss: 57.757298077998186, Acc: 0.8196, Grad norm: 0.03755914242948878\n",
      "Iteration 8809, BCE loss: 57.75730356480226, Acc: 0.8196, Grad norm: 0.04202077336997867\n",
      "Iteration 8810, BCE loss: 57.75732183366334, Acc: 0.8196, Grad norm: 0.05191091581709244\n",
      "Iteration 8811, BCE loss: 57.757296551440184, Acc: 0.8196, Grad norm: 0.03812951038340886\n",
      "Iteration 8812, BCE loss: 57.75730944892014, Acc: 0.8196, Grad norm: 0.043633239734643794\n",
      "Iteration 8813, BCE loss: 57.75731439439502, Acc: 0.8196, Grad norm: 0.045775044220634366\n",
      "Iteration 8814, BCE loss: 57.75728727779391, Acc: 0.8196, Grad norm: 0.0319297087543989\n",
      "Iteration 8815, BCE loss: 57.75729629565869, Acc: 0.8196, Grad norm: 0.03992185821875911\n",
      "Iteration 8816, BCE loss: 57.75730579195445, Acc: 0.8196, Grad norm: 0.04671014058483682\n",
      "Iteration 8817, BCE loss: 57.757304956178515, Acc: 0.8196, Grad norm: 0.044918719512924886\n",
      "Iteration 8818, BCE loss: 57.75729309577754, Acc: 0.8196, Grad norm: 0.036666102513166585\n",
      "Iteration 8819, BCE loss: 57.75729743783935, Acc: 0.8196, Grad norm: 0.038063730902991526\n",
      "Iteration 8820, BCE loss: 57.75733655471443, Acc: 0.8196, Grad norm: 0.0550309539732281\n",
      "Iteration 8821, BCE loss: 57.75733302942477, Acc: 0.8196, Grad norm: 0.05327268439366837\n",
      "Iteration 8822, BCE loss: 57.75732538080981, Acc: 0.8196, Grad norm: 0.049369872853623625\n",
      "Iteration 8823, BCE loss: 57.75732074867566, Acc: 0.8195, Grad norm: 0.04883485034221637\n",
      "Iteration 8824, BCE loss: 57.757307505361254, Acc: 0.8196, Grad norm: 0.04326903715754021\n",
      "Iteration 8825, BCE loss: 57.75730230243525, Acc: 0.8196, Grad norm: 0.04106105667214344\n",
      "Iteration 8826, BCE loss: 57.757291844813814, Acc: 0.8196, Grad norm: 0.03592176179653052\n",
      "Iteration 8827, BCE loss: 57.75730248380926, Acc: 0.8196, Grad norm: 0.041940946011403205\n",
      "Iteration 8828, BCE loss: 57.75728870794687, Acc: 0.8196, Grad norm: 0.0335369047706289\n",
      "Iteration 8829, BCE loss: 57.757307629530146, Acc: 0.8196, Grad norm: 0.04284974036186898\n",
      "Iteration 8830, BCE loss: 57.75730290106989, Acc: 0.8196, Grad norm: 0.04156141464727305\n",
      "Iteration 8831, BCE loss: 57.7572967371334, Acc: 0.8196, Grad norm: 0.03699431571736638\n",
      "Iteration 8832, BCE loss: 57.75731054657387, Acc: 0.8196, Grad norm: 0.04488619165985638\n",
      "Iteration 8833, BCE loss: 57.7573355350959, Acc: 0.8196, Grad norm: 0.05551789327039631\n",
      "Iteration 8834, BCE loss: 57.757349296219374, Acc: 0.8196, Grad norm: 0.06125812152540106\n",
      "Iteration 8835, BCE loss: 57.757332066775916, Acc: 0.8196, Grad norm: 0.05525457725806916\n",
      "Iteration 8836, BCE loss: 57.75730900828401, Acc: 0.8196, Grad norm: 0.043201593399206005\n",
      "Iteration 8837, BCE loss: 57.757290965211894, Acc: 0.8196, Grad norm: 0.03520594806315729\n",
      "Iteration 8838, BCE loss: 57.757283748706826, Acc: 0.8196, Grad norm: 0.029763796257695527\n",
      "Iteration 8839, BCE loss: 57.757297762483596, Acc: 0.8196, Grad norm: 0.03969076338029518\n",
      "Iteration 8840, BCE loss: 57.757297374474675, Acc: 0.8196, Grad norm: 0.037709498601411415\n",
      "Iteration 8841, BCE loss: 57.757299442609096, Acc: 0.8196, Grad norm: 0.038467797375971106\n",
      "Iteration 8842, BCE loss: 57.75730353255889, Acc: 0.8196, Grad norm: 0.04105458566762404\n",
      "Iteration 8843, BCE loss: 57.75731849749913, Acc: 0.8196, Grad norm: 0.04820282774020877\n",
      "Iteration 8844, BCE loss: 57.75733261854036, Acc: 0.8196, Grad norm: 0.054026293707694385\n",
      "Iteration 8845, BCE loss: 57.7573264903651, Acc: 0.8196, Grad norm: 0.051325900270759346\n",
      "Iteration 8846, BCE loss: 57.757341734857505, Acc: 0.8196, Grad norm: 0.05949772574281781\n",
      "Iteration 8847, BCE loss: 57.75735412648386, Acc: 0.8196, Grad norm: 0.06497482417523988\n",
      "Iteration 8848, BCE loss: 57.75734018093472, Acc: 0.8196, Grad norm: 0.05954325949376199\n",
      "Iteration 8849, BCE loss: 57.757324278899574, Acc: 0.8196, Grad norm: 0.053558786935132344\n",
      "Iteration 8850, BCE loss: 57.75732669836344, Acc: 0.8196, Grad norm: 0.05314116739724033\n",
      "Iteration 8851, BCE loss: 57.75736640358205, Acc: 0.8195, Grad norm: 0.0676599141945671\n",
      "Iteration 8852, BCE loss: 57.757349549162235, Acc: 0.8196, Grad norm: 0.06175893790374302\n",
      "Iteration 8853, BCE loss: 57.757348349134745, Acc: 0.8196, Grad norm: 0.06181046471677806\n",
      "Iteration 8854, BCE loss: 57.757319969930535, Acc: 0.8196, Grad norm: 0.05054643245602375\n",
      "Iteration 8855, BCE loss: 57.75731286309653, Acc: 0.8196, Grad norm: 0.046315353687711364\n",
      "Iteration 8856, BCE loss: 57.75729672922701, Acc: 0.8196, Grad norm: 0.03592715255478594\n",
      "Iteration 8857, BCE loss: 57.75731072262132, Acc: 0.8196, Grad norm: 0.04143212728154878\n",
      "Iteration 8858, BCE loss: 57.75731107107084, Acc: 0.8196, Grad norm: 0.041405215455523794\n",
      "Iteration 8859, BCE loss: 57.75730563472189, Acc: 0.8196, Grad norm: 0.04022182148681196\n",
      "Iteration 8860, BCE loss: 57.75734563175253, Acc: 0.8196, Grad norm: 0.05958957933908471\n",
      "Iteration 8861, BCE loss: 57.75732267514271, Acc: 0.8196, Grad norm: 0.050046902101380464\n",
      "Iteration 8862, BCE loss: 57.75735034358602, Acc: 0.8196, Grad norm: 0.05977408439376491\n",
      "Iteration 8863, BCE loss: 57.757387125379154, Acc: 0.8196, Grad norm: 0.07247906009366194\n",
      "Iteration 8864, BCE loss: 57.75733129487061, Acc: 0.8196, Grad norm: 0.053732512581408945\n",
      "Iteration 8865, BCE loss: 57.757313238347024, Acc: 0.8196, Grad norm: 0.04649410206186669\n",
      "Iteration 8866, BCE loss: 57.757340014380915, Acc: 0.8196, Grad norm: 0.05827295486900344\n",
      "Iteration 8867, BCE loss: 57.7573749327976, Acc: 0.8196, Grad norm: 0.06909630132954239\n",
      "Iteration 8868, BCE loss: 57.75738716335277, Acc: 0.8195, Grad norm: 0.07392618360165196\n",
      "Iteration 8869, BCE loss: 57.757390487202244, Acc: 0.8195, Grad norm: 0.07576692396131073\n",
      "Iteration 8870, BCE loss: 57.75732349943818, Acc: 0.8196, Grad norm: 0.05184389482936464\n",
      "Iteration 8871, BCE loss: 57.757318532427014, Acc: 0.8196, Grad norm: 0.04880210433808789\n",
      "Iteration 8872, BCE loss: 57.75732124845226, Acc: 0.8196, Grad norm: 0.048420400231987605\n",
      "Iteration 8873, BCE loss: 57.75732078380855, Acc: 0.8196, Grad norm: 0.048990269518882336\n",
      "Iteration 8874, BCE loss: 57.75733534872206, Acc: 0.8195, Grad norm: 0.053877444654194945\n",
      "Iteration 8875, BCE loss: 57.757348696119024, Acc: 0.8195, Grad norm: 0.05691096769426795\n",
      "Iteration 8876, BCE loss: 57.757360947957, Acc: 0.8195, Grad norm: 0.06133236319174119\n",
      "Iteration 8877, BCE loss: 57.75734532795253, Acc: 0.8195, Grad norm: 0.05422548464737634\n",
      "Iteration 8878, BCE loss: 57.757333532653305, Acc: 0.8196, Grad norm: 0.05023873874921451\n",
      "Iteration 8879, BCE loss: 57.757358420509796, Acc: 0.8195, Grad norm: 0.06037431891993668\n",
      "Iteration 8880, BCE loss: 57.75731468757212, Acc: 0.8195, Grad norm: 0.043460927598726315\n",
      "Iteration 8881, BCE loss: 57.757307625351196, Acc: 0.8195, Grad norm: 0.0402976775841623\n",
      "Iteration 8882, BCE loss: 57.75731283091313, Acc: 0.8196, Grad norm: 0.043555717470300556\n",
      "Iteration 8883, BCE loss: 57.75729651857898, Acc: 0.8196, Grad norm: 0.03684577106154602\n",
      "Iteration 8884, BCE loss: 57.75729682070156, Acc: 0.8196, Grad norm: 0.03683106507408824\n",
      "Iteration 8885, BCE loss: 57.7573066548939, Acc: 0.8196, Grad norm: 0.04268996323006487\n",
      "Iteration 8886, BCE loss: 57.75729813978896, Acc: 0.8196, Grad norm: 0.0396713266369247\n",
      "Iteration 8887, BCE loss: 57.75730672900637, Acc: 0.8196, Grad norm: 0.043845823628692784\n",
      "Iteration 8888, BCE loss: 57.75734236927005, Acc: 0.8196, Grad norm: 0.05721822130702856\n",
      "Iteration 8889, BCE loss: 57.7573150279613, Acc: 0.8196, Grad norm: 0.04519848203104542\n",
      "Iteration 8890, BCE loss: 57.75732199888405, Acc: 0.8196, Grad norm: 0.04968300535780017\n",
      "Iteration 8891, BCE loss: 57.75732255785383, Acc: 0.8196, Grad norm: 0.05088678062206153\n",
      "Iteration 8892, BCE loss: 57.75733644813917, Acc: 0.8196, Grad norm: 0.05786394680437958\n",
      "Iteration 8893, BCE loss: 57.75733371067166, Acc: 0.8196, Grad norm: 0.05778187019158665\n",
      "Iteration 8894, BCE loss: 57.75732353171455, Acc: 0.8196, Grad norm: 0.052933789871736414\n",
      "Iteration 8895, BCE loss: 57.757300654958954, Acc: 0.8196, Grad norm: 0.04102395036754506\n",
      "Iteration 8896, BCE loss: 57.75732002449998, Acc: 0.8195, Grad norm: 0.050161105620286126\n",
      "Iteration 8897, BCE loss: 57.75729748604763, Acc: 0.8195, Grad norm: 0.03808106954221745\n",
      "Iteration 8898, BCE loss: 57.757297627469654, Acc: 0.8195, Grad norm: 0.038043625130108526\n",
      "Iteration 8899, BCE loss: 57.75728830160543, Acc: 0.8195, Grad norm: 0.03334545384307833\n",
      "Iteration 8900, BCE loss: 57.75727834140681, Acc: 0.8196, Grad norm: 0.025488052307337618\n",
      "Iteration 8901, BCE loss: 57.75728492333729, Acc: 0.8196, Grad norm: 0.031308639861961154\n",
      "Iteration 8902, BCE loss: 57.757293640903896, Acc: 0.8196, Grad norm: 0.03751899497467902\n",
      "Iteration 8903, BCE loss: 57.7572865680379, Acc: 0.8196, Grad norm: 0.03320712873038325\n",
      "Iteration 8904, BCE loss: 57.75727359337071, Acc: 0.8196, Grad norm: 0.023701053995360134\n",
      "Iteration 8905, BCE loss: 57.75726819390833, Acc: 0.8196, Grad norm: 0.017598927737468576\n",
      "Iteration 8906, BCE loss: 57.75726719787521, Acc: 0.8195, Grad norm: 0.016839130094965158\n",
      "Iteration 8907, BCE loss: 57.75726673825781, Acc: 0.8196, Grad norm: 0.01839540517731433\n",
      "Iteration 8908, BCE loss: 57.75727225013292, Acc: 0.8195, Grad norm: 0.021946602437827555\n",
      "Iteration 8909, BCE loss: 57.75728857940985, Acc: 0.8195, Grad norm: 0.033246556138473236\n",
      "Iteration 8910, BCE loss: 57.75728282911339, Acc: 0.8195, Grad norm: 0.029654552023001598\n",
      "Iteration 8911, BCE loss: 57.75728881621301, Acc: 0.8195, Grad norm: 0.03475074502042785\n",
      "Iteration 8912, BCE loss: 57.75730267989192, Acc: 0.8196, Grad norm: 0.04392017151266273\n",
      "Iteration 8913, BCE loss: 57.75731849794385, Acc: 0.8196, Grad norm: 0.051357383781825944\n",
      "Iteration 8914, BCE loss: 57.757303152116634, Acc: 0.8196, Grad norm: 0.04247094457255047\n",
      "Iteration 8915, BCE loss: 57.757315308839374, Acc: 0.8195, Grad norm: 0.047122936860929436\n",
      "Iteration 8916, BCE loss: 57.757324976691756, Acc: 0.8195, Grad norm: 0.050540420086460314\n",
      "Iteration 8917, BCE loss: 57.75732033115396, Acc: 0.8195, Grad norm: 0.046883043487967076\n",
      "Iteration 8918, BCE loss: 57.757317882589604, Acc: 0.8195, Grad norm: 0.047124853701686224\n",
      "Iteration 8919, BCE loss: 57.75733135900673, Acc: 0.8195, Grad norm: 0.051779263135687044\n",
      "Iteration 8920, BCE loss: 57.75733998334687, Acc: 0.8195, Grad norm: 0.05436236226040908\n",
      "Iteration 8921, BCE loss: 57.757371113540586, Acc: 0.8195, Grad norm: 0.0650755912219922\n",
      "Iteration 8922, BCE loss: 57.75736437860765, Acc: 0.8195, Grad norm: 0.06306015301657446\n",
      "Iteration 8923, BCE loss: 57.75732954070605, Acc: 0.8195, Grad norm: 0.050752788278143376\n",
      "Iteration 8924, BCE loss: 57.75733296514292, Acc: 0.8195, Grad norm: 0.05202563010785706\n",
      "Iteration 8925, BCE loss: 57.75731705812727, Acc: 0.8195, Grad norm: 0.04667266197512941\n",
      "Iteration 8926, BCE loss: 57.75729940092168, Acc: 0.8195, Grad norm: 0.03919382436705499\n",
      "Iteration 8927, BCE loss: 57.75731190644841, Acc: 0.8195, Grad norm: 0.043674207969130886\n",
      "Iteration 8928, BCE loss: 57.757324924447275, Acc: 0.8195, Grad norm: 0.04838451253922817\n",
      "Iteration 8929, BCE loss: 57.757315147312504, Acc: 0.8195, Grad norm: 0.04668109727460843\n",
      "Iteration 8930, BCE loss: 57.75729068311749, Acc: 0.8195, Grad norm: 0.03510259876302819\n",
      "Iteration 8931, BCE loss: 57.75729388132193, Acc: 0.8195, Grad norm: 0.036775574059963796\n",
      "Iteration 8932, BCE loss: 57.75729648472448, Acc: 0.8195, Grad norm: 0.03822600151460237\n",
      "Iteration 8933, BCE loss: 57.75729851526532, Acc: 0.8195, Grad norm: 0.03973425548500553\n",
      "Iteration 8934, BCE loss: 57.75727778951179, Acc: 0.8195, Grad norm: 0.026615399237334712\n",
      "Iteration 8935, BCE loss: 57.75728369329384, Acc: 0.8195, Grad norm: 0.031234921291697988\n",
      "Iteration 8936, BCE loss: 57.75728144703654, Acc: 0.8195, Grad norm: 0.029232298809819972\n",
      "Iteration 8937, BCE loss: 57.75728582477136, Acc: 0.8196, Grad norm: 0.03255024862175008\n",
      "Iteration 8938, BCE loss: 57.75728797674409, Acc: 0.8196, Grad norm: 0.03417207738665372\n",
      "Iteration 8939, BCE loss: 57.757271574797755, Acc: 0.8196, Grad norm: 0.021924828784973358\n",
      "Iteration 8940, BCE loss: 57.7572949940225, Acc: 0.8196, Grad norm: 0.03895114410623671\n",
      "Iteration 8941, BCE loss: 57.75729803262834, Acc: 0.8196, Grad norm: 0.03965764532625516\n",
      "Iteration 8942, BCE loss: 57.75732030587021, Acc: 0.8195, Grad norm: 0.05142338793050173\n",
      "Iteration 8943, BCE loss: 57.75731008137426, Acc: 0.8195, Grad norm: 0.045702560486801797\n",
      "Iteration 8944, BCE loss: 57.757322118548004, Acc: 0.8195, Grad norm: 0.052584365788074676\n",
      "Iteration 8945, BCE loss: 57.75732596098587, Acc: 0.8195, Grad norm: 0.0559630420885603\n",
      "Iteration 8946, BCE loss: 57.75732063428835, Acc: 0.8195, Grad norm: 0.052915662468301115\n",
      "Iteration 8947, BCE loss: 57.757298972156605, Acc: 0.8195, Grad norm: 0.041040716450874266\n",
      "Iteration 8948, BCE loss: 57.75731664413801, Acc: 0.8195, Grad norm: 0.051159784409018985\n",
      "Iteration 8949, BCE loss: 57.75734366931805, Acc: 0.8195, Grad norm: 0.0625042102486565\n",
      "Iteration 8950, BCE loss: 57.7573805286672, Acc: 0.8195, Grad norm: 0.07365237053705175\n",
      "Iteration 8951, BCE loss: 57.75736435922846, Acc: 0.8195, Grad norm: 0.06874934215792218\n",
      "Iteration 8952, BCE loss: 57.75730747270087, Acc: 0.8195, Grad norm: 0.045353138740621524\n",
      "Iteration 8953, BCE loss: 57.75733548420978, Acc: 0.8196, Grad norm: 0.05758043938635476\n",
      "Iteration 8954, BCE loss: 57.75732651318475, Acc: 0.8195, Grad norm: 0.05393464844895627\n",
      "Iteration 8955, BCE loss: 57.75733506213152, Acc: 0.8195, Grad norm: 0.05684982929235649\n",
      "Iteration 8956, BCE loss: 57.75730813362999, Acc: 0.8195, Grad norm: 0.044622662006670824\n",
      "Iteration 8957, BCE loss: 57.75729959564119, Acc: 0.8195, Grad norm: 0.041280983568937744\n",
      "Iteration 8958, BCE loss: 57.75732416288487, Acc: 0.8195, Grad norm: 0.05234134982293961\n",
      "Iteration 8959, BCE loss: 57.757281931310345, Acc: 0.8195, Grad norm: 0.02880723243986883\n",
      "Iteration 8960, BCE loss: 57.757302351816826, Acc: 0.8195, Grad norm: 0.04321442478237106\n",
      "Iteration 8961, BCE loss: 57.75728737895234, Acc: 0.8195, Grad norm: 0.03312671033512633\n",
      "Iteration 8962, BCE loss: 57.75730850756608, Acc: 0.8195, Grad norm: 0.04543976551174721\n",
      "Iteration 8963, BCE loss: 57.757309861012644, Acc: 0.8195, Grad norm: 0.04584004684119701\n",
      "Iteration 8964, BCE loss: 57.75729109999084, Acc: 0.8195, Grad norm: 0.03412638365815787\n",
      "Iteration 8965, BCE loss: 57.757299837814216, Acc: 0.8195, Grad norm: 0.03869181393973884\n",
      "Iteration 8966, BCE loss: 57.75728641038677, Acc: 0.8195, Grad norm: 0.030715810768129636\n",
      "Iteration 8967, BCE loss: 57.757287034468945, Acc: 0.8195, Grad norm: 0.031992475309417755\n",
      "Iteration 8968, BCE loss: 57.75729563867398, Acc: 0.8195, Grad norm: 0.03896290707184721\n",
      "Iteration 8969, BCE loss: 57.75730844873138, Acc: 0.8196, Grad norm: 0.04654399911550685\n",
      "Iteration 8970, BCE loss: 57.757316836981175, Acc: 0.8196, Grad norm: 0.050105484279265707\n",
      "Iteration 8971, BCE loss: 57.757288257034745, Acc: 0.8195, Grad norm: 0.03515185062475593\n",
      "Iteration 8972, BCE loss: 57.757303150290404, Acc: 0.8195, Grad norm: 0.04345646854827958\n",
      "Iteration 8973, BCE loss: 57.75730224622857, Acc: 0.8195, Grad norm: 0.04391732480960537\n",
      "Iteration 8974, BCE loss: 57.75730102012905, Acc: 0.8195, Grad norm: 0.043950077376625436\n",
      "Iteration 8975, BCE loss: 57.75729925110852, Acc: 0.8195, Grad norm: 0.04218594742469471\n",
      "Iteration 8976, BCE loss: 57.757295762309504, Acc: 0.8195, Grad norm: 0.03878191792833683\n",
      "Iteration 8977, BCE loss: 57.757300055414916, Acc: 0.8196, Grad norm: 0.038779191980395485\n",
      "Iteration 8978, BCE loss: 57.75729162812466, Acc: 0.8195, Grad norm: 0.03488070790648406\n",
      "Iteration 8979, BCE loss: 57.75729379134816, Acc: 0.8195, Grad norm: 0.0346009602649775\n",
      "Iteration 8980, BCE loss: 57.75729742339758, Acc: 0.8195, Grad norm: 0.0377206266428292\n",
      "Iteration 8981, BCE loss: 57.75730735036949, Acc: 0.8195, Grad norm: 0.04294134984776034\n",
      "Iteration 8982, BCE loss: 57.75728513391057, Acc: 0.8195, Grad norm: 0.03155501238323684\n",
      "Iteration 8983, BCE loss: 57.75727948168036, Acc: 0.8195, Grad norm: 0.029110962622304327\n",
      "Iteration 8984, BCE loss: 57.75727881959989, Acc: 0.8195, Grad norm: 0.02864157500177844\n",
      "Iteration 8985, BCE loss: 57.757276858637425, Acc: 0.8195, Grad norm: 0.02662516192228102\n",
      "Iteration 8986, BCE loss: 57.75727307090328, Acc: 0.8196, Grad norm: 0.022278294793036856\n",
      "Iteration 8987, BCE loss: 57.757289332047804, Acc: 0.8196, Grad norm: 0.03467709931235146\n",
      "Iteration 8988, BCE loss: 57.75728331712375, Acc: 0.8196, Grad norm: 0.030686741250388685\n",
      "Iteration 8989, BCE loss: 57.75729449617825, Acc: 0.8196, Grad norm: 0.03790098110950744\n",
      "Iteration 8990, BCE loss: 57.75732127548547, Acc: 0.8196, Grad norm: 0.050882206259230206\n",
      "Iteration 8991, BCE loss: 57.75733204568691, Acc: 0.8196, Grad norm: 0.05548482282494553\n",
      "Iteration 8992, BCE loss: 57.757341481994814, Acc: 0.8196, Grad norm: 0.059598381469198\n",
      "Iteration 8993, BCE loss: 57.75730508773279, Acc: 0.8196, Grad norm: 0.043969595218206564\n",
      "Iteration 8994, BCE loss: 57.75729141217036, Acc: 0.8196, Grad norm: 0.03674771113831316\n",
      "Iteration 8995, BCE loss: 57.75730511535758, Acc: 0.8195, Grad norm: 0.04273911713798727\n",
      "Iteration 8996, BCE loss: 57.75730014337619, Acc: 0.8195, Grad norm: 0.04029757955170542\n",
      "Iteration 8997, BCE loss: 57.75729415561335, Acc: 0.8195, Grad norm: 0.03722113257640847\n",
      "Iteration 8998, BCE loss: 57.75730929654693, Acc: 0.8195, Grad norm: 0.04631531033174697\n",
      "Iteration 8999, BCE loss: 57.757297491940115, Acc: 0.8195, Grad norm: 0.04014837412917432\n",
      "Iteration 9000, BCE loss: 57.757305344403505, Acc: 0.8195, Grad norm: 0.04318169965488116\n",
      "Iteration 9001, BCE loss: 57.75735694876591, Acc: 0.8195, Grad norm: 0.0646453888217229\n",
      "Iteration 9002, BCE loss: 57.75739046796095, Acc: 0.8195, Grad norm: 0.07674849335067616\n",
      "Iteration 9003, BCE loss: 57.757387561812436, Acc: 0.8195, Grad norm: 0.07586322509749568\n",
      "Iteration 9004, BCE loss: 57.75736523457047, Acc: 0.8195, Grad norm: 0.06928524333089014\n",
      "Iteration 9005, BCE loss: 57.757407814618574, Acc: 0.8195, Grad norm: 0.08137802521740581\n",
      "Iteration 9006, BCE loss: 57.75743386319327, Acc: 0.8195, Grad norm: 0.0875202161815307\n",
      "Iteration 9007, BCE loss: 57.757417860110905, Acc: 0.8195, Grad norm: 0.08362736923226606\n",
      "Iteration 9008, BCE loss: 57.75738068891154, Acc: 0.8195, Grad norm: 0.07430008303894395\n",
      "Iteration 9009, BCE loss: 57.75740152623413, Acc: 0.8195, Grad norm: 0.0795443313420255\n",
      "Iteration 9010, BCE loss: 57.7573446288301, Acc: 0.8195, Grad norm: 0.060062357740335706\n",
      "Iteration 9011, BCE loss: 57.757335899992185, Acc: 0.8195, Grad norm: 0.05641539477349991\n",
      "Iteration 9012, BCE loss: 57.757342680685944, Acc: 0.8195, Grad norm: 0.057416116711956865\n",
      "Iteration 9013, BCE loss: 57.75737803143532, Acc: 0.8195, Grad norm: 0.06999500651091717\n",
      "Iteration 9014, BCE loss: 57.75738026432863, Acc: 0.8195, Grad norm: 0.06884084477111754\n",
      "Iteration 9015, BCE loss: 57.75737472857678, Acc: 0.8195, Grad norm: 0.0690789300905351\n",
      "Iteration 9016, BCE loss: 57.757400282922305, Acc: 0.8195, Grad norm: 0.07494417102641326\n",
      "Iteration 9017, BCE loss: 57.757370628690886, Acc: 0.8195, Grad norm: 0.06580500147035594\n",
      "Iteration 9018, BCE loss: 57.75739246419529, Acc: 0.8195, Grad norm: 0.07367898620326142\n",
      "Iteration 9019, BCE loss: 57.75739188091674, Acc: 0.8195, Grad norm: 0.0734136572863518\n",
      "Iteration 9020, BCE loss: 57.757356194317325, Acc: 0.8195, Grad norm: 0.06254644905154738\n",
      "Iteration 9021, BCE loss: 57.757390128611654, Acc: 0.8195, Grad norm: 0.07455271453854503\n",
      "Iteration 9022, BCE loss: 57.757401162693036, Acc: 0.8196, Grad norm: 0.07823720709652017\n",
      "Iteration 9023, BCE loss: 57.75740724253817, Acc: 0.8195, Grad norm: 0.0807041633496796\n",
      "Iteration 9024, BCE loss: 57.7573889208159, Acc: 0.8195, Grad norm: 0.07591996915806049\n",
      "Iteration 9025, BCE loss: 57.75734116897812, Acc: 0.8195, Grad norm: 0.05874136048791495\n",
      "Iteration 9026, BCE loss: 57.75730464282263, Acc: 0.8195, Grad norm: 0.042498414702867526\n",
      "Iteration 9027, BCE loss: 57.75732226762036, Acc: 0.8195, Grad norm: 0.05043133225446804\n",
      "Iteration 9028, BCE loss: 57.75731938469809, Acc: 0.8195, Grad norm: 0.04846679799026161\n",
      "Iteration 9029, BCE loss: 57.75733459057872, Acc: 0.8196, Grad norm: 0.05482853905468179\n",
      "Iteration 9030, BCE loss: 57.757317619860686, Acc: 0.8195, Grad norm: 0.04787656874116423\n",
      "Iteration 9031, BCE loss: 57.757303901019384, Acc: 0.8195, Grad norm: 0.040608748128310626\n",
      "Iteration 9032, BCE loss: 57.757299964315635, Acc: 0.8195, Grad norm: 0.03827452814254797\n",
      "Iteration 9033, BCE loss: 57.757286626707355, Acc: 0.8195, Grad norm: 0.030662492669974315\n",
      "Iteration 9034, BCE loss: 57.75729811675834, Acc: 0.8195, Grad norm: 0.039036638747760366\n",
      "Iteration 9035, BCE loss: 57.75730996534856, Acc: 0.8195, Grad norm: 0.045645652904390925\n",
      "Iteration 9036, BCE loss: 57.75730283769607, Acc: 0.8195, Grad norm: 0.04221954914255508\n",
      "Iteration 9037, BCE loss: 57.75732872919181, Acc: 0.8195, Grad norm: 0.055139794990630604\n",
      "Iteration 9038, BCE loss: 57.75728455497713, Acc: 0.8195, Grad norm: 0.03152068142962929\n",
      "Iteration 9039, BCE loss: 57.75729168693708, Acc: 0.8195, Grad norm: 0.03670890893705227\n",
      "Iteration 9040, BCE loss: 57.75729410190692, Acc: 0.8195, Grad norm: 0.038333456605242386\n",
      "Iteration 9041, BCE loss: 57.757279137699456, Acc: 0.8195, Grad norm: 0.030507102470641086\n",
      "Iteration 9042, BCE loss: 57.75726388326336, Acc: 0.8195, Grad norm: 0.015550937918865661\n",
      "Iteration 9043, BCE loss: 57.75727311295481, Acc: 0.8196, Grad norm: 0.0247005939205458\n",
      "Iteration 9044, BCE loss: 57.75727903292723, Acc: 0.8196, Grad norm: 0.03050749636438134\n",
      "Iteration 9045, BCE loss: 57.75727127231053, Acc: 0.8196, Grad norm: 0.02460585015765603\n",
      "Iteration 9046, BCE loss: 57.757275715697716, Acc: 0.8196, Grad norm: 0.02617986108674031\n",
      "Iteration 9047, BCE loss: 57.75729292313956, Acc: 0.8195, Grad norm: 0.03780093492172308\n",
      "Iteration 9048, BCE loss: 57.75729186385218, Acc: 0.8195, Grad norm: 0.03750804649171667\n",
      "Iteration 9049, BCE loss: 57.757308679812624, Acc: 0.8195, Grad norm: 0.04566655854596549\n",
      "Iteration 9050, BCE loss: 57.75731283743504, Acc: 0.8195, Grad norm: 0.04861476743651823\n",
      "Iteration 9051, BCE loss: 57.757327642299316, Acc: 0.8195, Grad norm: 0.05545298034001407\n",
      "Iteration 9052, BCE loss: 57.757296558438874, Acc: 0.8196, Grad norm: 0.03942930896516006\n",
      "Iteration 9053, BCE loss: 57.757296262508845, Acc: 0.8196, Grad norm: 0.041935617755172266\n",
      "Iteration 9054, BCE loss: 57.75732690806066, Acc: 0.8195, Grad norm: 0.05601482531074286\n",
      "Iteration 9055, BCE loss: 57.75732199111869, Acc: 0.8195, Grad norm: 0.05380867425746369\n",
      "Iteration 9056, BCE loss: 57.75734981219652, Acc: 0.8196, Grad norm: 0.06498378721061174\n",
      "Iteration 9057, BCE loss: 57.75731664916146, Acc: 0.8196, Grad norm: 0.050991773311954515\n",
      "Iteration 9058, BCE loss: 57.75729061907994, Acc: 0.8196, Grad norm: 0.03674626969353479\n",
      "Iteration 9059, BCE loss: 57.757295415675145, Acc: 0.8196, Grad norm: 0.03933131551906835\n",
      "Iteration 9060, BCE loss: 57.757300514786834, Acc: 0.8196, Grad norm: 0.0398822946531889\n",
      "Iteration 9061, BCE loss: 57.7573078701028, Acc: 0.8196, Grad norm: 0.042274433530546\n",
      "Iteration 9062, BCE loss: 57.757317548126586, Acc: 0.8196, Grad norm: 0.04759646849823725\n",
      "Iteration 9063, BCE loss: 57.75732820705929, Acc: 0.8196, Grad norm: 0.05272572575708871\n",
      "Iteration 9064, BCE loss: 57.75733635101187, Acc: 0.8196, Grad norm: 0.05812940644847038\n",
      "Iteration 9065, BCE loss: 57.757378057821434, Acc: 0.8195, Grad norm: 0.07310366644653704\n",
      "Iteration 9066, BCE loss: 57.75736567510677, Acc: 0.8195, Grad norm: 0.06883586838358512\n",
      "Iteration 9067, BCE loss: 57.75736118357881, Acc: 0.8195, Grad norm: 0.06759143940535828\n",
      "Iteration 9068, BCE loss: 57.7574186663915, Acc: 0.8195, Grad norm: 0.08429656911764051\n",
      "Iteration 9069, BCE loss: 57.757382271879905, Acc: 0.8195, Grad norm: 0.07165179009632038\n",
      "Iteration 9070, BCE loss: 57.75737570915902, Acc: 0.8195, Grad norm: 0.06927306343808946\n",
      "Iteration 9071, BCE loss: 57.757399674386924, Acc: 0.8195, Grad norm: 0.07596123580987058\n",
      "Iteration 9072, BCE loss: 57.757336620599496, Acc: 0.8195, Grad norm: 0.055553730250676225\n",
      "Iteration 9073, BCE loss: 57.75729681599295, Acc: 0.8195, Grad norm: 0.03743195869872964\n",
      "Iteration 9074, BCE loss: 57.757298712050414, Acc: 0.8195, Grad norm: 0.03683079804043368\n",
      "Iteration 9075, BCE loss: 57.75729616338802, Acc: 0.8195, Grad norm: 0.034843057361755574\n",
      "Iteration 9076, BCE loss: 57.75730087648539, Acc: 0.8195, Grad norm: 0.039240985253334776\n",
      "Iteration 9077, BCE loss: 57.75730151392615, Acc: 0.8195, Grad norm: 0.040769685270932055\n",
      "Iteration 9078, BCE loss: 57.75728538564681, Acc: 0.8195, Grad norm: 0.030626939228140755\n",
      "Iteration 9079, BCE loss: 57.75729505985154, Acc: 0.8195, Grad norm: 0.03500526408655109\n",
      "Iteration 9080, BCE loss: 57.75731126677846, Acc: 0.8195, Grad norm: 0.04277006606711295\n",
      "Iteration 9081, BCE loss: 57.75729776130263, Acc: 0.8195, Grad norm: 0.037611400412561426\n",
      "Iteration 9082, BCE loss: 57.75731751172016, Acc: 0.8195, Grad norm: 0.04718033904791152\n",
      "Iteration 9083, BCE loss: 57.75734116594623, Acc: 0.8195, Grad norm: 0.05651582865229756\n",
      "Iteration 9084, BCE loss: 57.75733265118454, Acc: 0.8195, Grad norm: 0.05332023828793773\n",
      "Iteration 9085, BCE loss: 57.75734009054351, Acc: 0.8195, Grad norm: 0.055820011540811044\n",
      "Iteration 9086, BCE loss: 57.757352637864955, Acc: 0.8195, Grad norm: 0.06162359447403732\n",
      "Iteration 9087, BCE loss: 57.757328899713855, Acc: 0.8195, Grad norm: 0.05402048647944426\n",
      "Iteration 9088, BCE loss: 57.75731324501251, Acc: 0.8195, Grad norm: 0.04768998886560382\n",
      "Iteration 9089, BCE loss: 57.757284084426615, Acc: 0.8196, Grad norm: 0.033177349339417704\n",
      "Iteration 9090, BCE loss: 57.75728686501449, Acc: 0.8195, Grad norm: 0.03331232348406228\n",
      "Iteration 9091, BCE loss: 57.757294586909964, Acc: 0.8195, Grad norm: 0.036618899415624365\n",
      "Iteration 9092, BCE loss: 57.75730163406343, Acc: 0.8196, Grad norm: 0.042060341083807073\n",
      "Iteration 9093, BCE loss: 57.7573113396591, Acc: 0.8196, Grad norm: 0.04770793919276417\n",
      "Iteration 9094, BCE loss: 57.75732160981681, Acc: 0.8196, Grad norm: 0.05095619866732611\n",
      "Iteration 9095, BCE loss: 57.757307109053414, Acc: 0.8195, Grad norm: 0.044112027195275155\n",
      "Iteration 9096, BCE loss: 57.75732269054454, Acc: 0.8195, Grad norm: 0.05245539028516095\n",
      "Iteration 9097, BCE loss: 57.757320502754816, Acc: 0.8195, Grad norm: 0.04957966536394199\n",
      "Iteration 9098, BCE loss: 57.75733057996092, Acc: 0.8195, Grad norm: 0.05349624210395156\n",
      "Iteration 9099, BCE loss: 57.75731160835587, Acc: 0.8195, Grad norm: 0.045967542699796776\n",
      "Iteration 9100, BCE loss: 57.75730343448376, Acc: 0.8195, Grad norm: 0.03950826333645061\n",
      "Iteration 9101, BCE loss: 57.75728818848965, Acc: 0.8195, Grad norm: 0.03110358124029131\n",
      "Iteration 9102, BCE loss: 57.75729762742936, Acc: 0.8195, Grad norm: 0.03603324365086287\n",
      "Iteration 9103, BCE loss: 57.757293715718156, Acc: 0.8195, Grad norm: 0.034321214784721395\n",
      "Iteration 9104, BCE loss: 57.757290075348244, Acc: 0.8195, Grad norm: 0.03278424926294438\n",
      "Iteration 9105, BCE loss: 57.757290085838996, Acc: 0.8195, Grad norm: 0.032757119385380506\n",
      "Iteration 9106, BCE loss: 57.757315010616544, Acc: 0.8195, Grad norm: 0.046440645902693624\n",
      "Iteration 9107, BCE loss: 57.75733380214499, Acc: 0.8195, Grad norm: 0.05395915545047715\n",
      "Iteration 9108, BCE loss: 57.757321173816294, Acc: 0.8195, Grad norm: 0.05050088324972575\n",
      "Iteration 9109, BCE loss: 57.75733993093414, Acc: 0.8195, Grad norm: 0.05683708804482644\n",
      "Iteration 9110, BCE loss: 57.757360627181, Acc: 0.8195, Grad norm: 0.06509070834361337\n",
      "Iteration 9111, BCE loss: 57.757337486590686, Acc: 0.8195, Grad norm: 0.05782306906473116\n",
      "Iteration 9112, BCE loss: 57.757351941328565, Acc: 0.8195, Grad norm: 0.06228622868551192\n",
      "Iteration 9113, BCE loss: 57.757358885617805, Acc: 0.8195, Grad norm: 0.06277711461232803\n",
      "Iteration 9114, BCE loss: 57.757375906852424, Acc: 0.8195, Grad norm: 0.07047937899766069\n",
      "Iteration 9115, BCE loss: 57.75734092591812, Acc: 0.8195, Grad norm: 0.057921998281462155\n",
      "Iteration 9116, BCE loss: 57.757326141402956, Acc: 0.8195, Grad norm: 0.05156819664884547\n",
      "Iteration 9117, BCE loss: 57.757317135584614, Acc: 0.8195, Grad norm: 0.04854616039906012\n",
      "Iteration 9118, BCE loss: 57.75731261950802, Acc: 0.8195, Grad norm: 0.046307231112102006\n",
      "Iteration 9119, BCE loss: 57.757322868971, Acc: 0.8195, Grad norm: 0.053816610580571\n",
      "Iteration 9120, BCE loss: 57.75729554758561, Acc: 0.8196, Grad norm: 0.040230085400412396\n",
      "Iteration 9121, BCE loss: 57.7572842298632, Acc: 0.8195, Grad norm: 0.031923820995180985\n",
      "Iteration 9122, BCE loss: 57.757297323886036, Acc: 0.8195, Grad norm: 0.040579173276694205\n",
      "Iteration 9123, BCE loss: 57.75731105467837, Acc: 0.8196, Grad norm: 0.047457753597396506\n",
      "Iteration 9124, BCE loss: 57.757311041151674, Acc: 0.8196, Grad norm: 0.04684489712109679\n",
      "Iteration 9125, BCE loss: 57.75729827023943, Acc: 0.8195, Grad norm: 0.04061521944550757\n",
      "Iteration 9126, BCE loss: 57.75728911659389, Acc: 0.8195, Grad norm: 0.035317993591763366\n",
      "Iteration 9127, BCE loss: 57.75731319387598, Acc: 0.8195, Grad norm: 0.04883449420796346\n",
      "Iteration 9128, BCE loss: 57.757319642691684, Acc: 0.8195, Grad norm: 0.05350934533121587\n",
      "Iteration 9129, BCE loss: 57.75731303462446, Acc: 0.8196, Grad norm: 0.0488915345693616\n",
      "Iteration 9130, BCE loss: 57.75730417142674, Acc: 0.8195, Grad norm: 0.04563089426633737\n",
      "Iteration 9131, BCE loss: 57.75731879307115, Acc: 0.8195, Grad norm: 0.05213058559119888\n",
      "Iteration 9132, BCE loss: 57.7572934193707, Acc: 0.8195, Grad norm: 0.03957239335927095\n",
      "Iteration 9133, BCE loss: 57.75730400655303, Acc: 0.8195, Grad norm: 0.04479318149758801\n",
      "Iteration 9134, BCE loss: 57.757305565968224, Acc: 0.8195, Grad norm: 0.04663976916127785\n",
      "Iteration 9135, BCE loss: 57.75729334498405, Acc: 0.8195, Grad norm: 0.038722576247398666\n",
      "Iteration 9136, BCE loss: 57.757296612425236, Acc: 0.8195, Grad norm: 0.039348977419517275\n",
      "Iteration 9137, BCE loss: 57.75730252309208, Acc: 0.8195, Grad norm: 0.042407461994345676\n",
      "Iteration 9138, BCE loss: 57.75730908389703, Acc: 0.8195, Grad norm: 0.04547938180154627\n",
      "Iteration 9139, BCE loss: 57.75731852003392, Acc: 0.8195, Grad norm: 0.0519574545374227\n",
      "Iteration 9140, BCE loss: 57.75732756913801, Acc: 0.8195, Grad norm: 0.055435607955381896\n",
      "Iteration 9141, BCE loss: 57.75730043562375, Acc: 0.8195, Grad norm: 0.04247900347001772\n",
      "Iteration 9142, BCE loss: 57.75730283951019, Acc: 0.8196, Grad norm: 0.04340142922749187\n",
      "Iteration 9143, BCE loss: 57.75729641826527, Acc: 0.8196, Grad norm: 0.03998111489983489\n",
      "Iteration 9144, BCE loss: 57.75731682245279, Acc: 0.8196, Grad norm: 0.05011001155759481\n",
      "Iteration 9145, BCE loss: 57.757325873188705, Acc: 0.8196, Grad norm: 0.052406771361345306\n",
      "Iteration 9146, BCE loss: 57.75734095108003, Acc: 0.8196, Grad norm: 0.0569694270651067\n",
      "Iteration 9147, BCE loss: 57.75731131980569, Acc: 0.8196, Grad norm: 0.045492642738517025\n",
      "Iteration 9148, BCE loss: 57.75731282234319, Acc: 0.8196, Grad norm: 0.046640583854056325\n",
      "Iteration 9149, BCE loss: 57.75731094776927, Acc: 0.8196, Grad norm: 0.04666595700118761\n",
      "Iteration 9150, BCE loss: 57.757300345477304, Acc: 0.8196, Grad norm: 0.04050687370362624\n",
      "Iteration 9151, BCE loss: 57.757293028342545, Acc: 0.8196, Grad norm: 0.03634518164932129\n",
      "Iteration 9152, BCE loss: 57.75731520751151, Acc: 0.8196, Grad norm: 0.04740030331573904\n",
      "Iteration 9153, BCE loss: 57.75735244998392, Acc: 0.8196, Grad norm: 0.061851315911979635\n",
      "Iteration 9154, BCE loss: 57.75735193689488, Acc: 0.8196, Grad norm: 0.06253958904992638\n",
      "Iteration 9155, BCE loss: 57.75737582685196, Acc: 0.8196, Grad norm: 0.07124022974089937\n",
      "Iteration 9156, BCE loss: 57.75743134263722, Acc: 0.8196, Grad norm: 0.08727769105719697\n",
      "Iteration 9157, BCE loss: 57.75740350154674, Acc: 0.8196, Grad norm: 0.08010723769782423\n",
      "Iteration 9158, BCE loss: 57.757385777747515, Acc: 0.8196, Grad norm: 0.07385338148245056\n",
      "Iteration 9159, BCE loss: 57.757408771264686, Acc: 0.8196, Grad norm: 0.07995545233416149\n",
      "Iteration 9160, BCE loss: 57.757404418688694, Acc: 0.8196, Grad norm: 0.07925818837241289\n",
      "Iteration 9161, BCE loss: 57.757392641019685, Acc: 0.8196, Grad norm: 0.07643593417988367\n",
      "Iteration 9162, BCE loss: 57.757364301051254, Acc: 0.8196, Grad norm: 0.06655063982600654\n",
      "Iteration 9163, BCE loss: 57.75734979672005, Acc: 0.8196, Grad norm: 0.06309004037874574\n",
      "Iteration 9164, BCE loss: 57.75735825220124, Acc: 0.8196, Grad norm: 0.0664650731906063\n",
      "Iteration 9165, BCE loss: 57.75732705465367, Acc: 0.8196, Grad norm: 0.05327086464594553\n",
      "Iteration 9166, BCE loss: 57.75731770132336, Acc: 0.8196, Grad norm: 0.04892107538815006\n",
      "Iteration 9167, BCE loss: 57.75733290948974, Acc: 0.8196, Grad norm: 0.05446858422152763\n",
      "Iteration 9168, BCE loss: 57.7573133881217, Acc: 0.8195, Grad norm: 0.04626187186957802\n",
      "Iteration 9169, BCE loss: 57.75732686154649, Acc: 0.8195, Grad norm: 0.05237464290689209\n",
      "Iteration 9170, BCE loss: 57.757333005084185, Acc: 0.8195, Grad norm: 0.054788768475666424\n",
      "Iteration 9171, BCE loss: 57.75733526028201, Acc: 0.8195, Grad norm: 0.05560486054005559\n",
      "Iteration 9172, BCE loss: 57.75733053477927, Acc: 0.8195, Grad norm: 0.054248630754864934\n",
      "Iteration 9173, BCE loss: 57.75732890338223, Acc: 0.8195, Grad norm: 0.05336950763846445\n",
      "Iteration 9174, BCE loss: 57.75731964092514, Acc: 0.8195, Grad norm: 0.0513302493605047\n",
      "Iteration 9175, BCE loss: 57.75730862173631, Acc: 0.8195, Grad norm: 0.0453388759409787\n",
      "Iteration 9176, BCE loss: 57.75728648988421, Acc: 0.8195, Grad norm: 0.032544602153793296\n",
      "Iteration 9177, BCE loss: 57.757276108312254, Acc: 0.8195, Grad norm: 0.026226823568079947\n",
      "Iteration 9178, BCE loss: 57.757270436653016, Acc: 0.8195, Grad norm: 0.020885335244437853\n",
      "Iteration 9179, BCE loss: 57.757275618272786, Acc: 0.8195, Grad norm: 0.027392679161988773\n",
      "Iteration 9180, BCE loss: 57.757280411321304, Acc: 0.8195, Grad norm: 0.030358114839063925\n",
      "Iteration 9181, BCE loss: 57.75727744408262, Acc: 0.8195, Grad norm: 0.028365156088714392\n",
      "Iteration 9182, BCE loss: 57.7572926344951, Acc: 0.8196, Grad norm: 0.038708390793406405\n",
      "Iteration 9183, BCE loss: 57.7572977237723, Acc: 0.8196, Grad norm: 0.04001413791903213\n",
      "Iteration 9184, BCE loss: 57.75728288384309, Acc: 0.8196, Grad norm: 0.029933626962127157\n",
      "Iteration 9185, BCE loss: 57.75728221534186, Acc: 0.8196, Grad norm: 0.028130460005166967\n",
      "Iteration 9186, BCE loss: 57.75728946780427, Acc: 0.8195, Grad norm: 0.03271789091998954\n",
      "Iteration 9187, BCE loss: 57.757314365904065, Acc: 0.8196, Grad norm: 0.045447925495906524\n",
      "Iteration 9188, BCE loss: 57.757317230549845, Acc: 0.8196, Grad norm: 0.0445918107296734\n",
      "Iteration 9189, BCE loss: 57.75731382470171, Acc: 0.8196, Grad norm: 0.043993714630105735\n",
      "Iteration 9190, BCE loss: 57.75728742350283, Acc: 0.8196, Grad norm: 0.03219799348818666\n",
      "Iteration 9191, BCE loss: 57.75728787470126, Acc: 0.8195, Grad norm: 0.03370652539476081\n",
      "Iteration 9192, BCE loss: 57.75728965820669, Acc: 0.8195, Grad norm: 0.03350794162107641\n",
      "Iteration 9193, BCE loss: 57.757279586860435, Acc: 0.8195, Grad norm: 0.028845546225902182\n",
      "Iteration 9194, BCE loss: 57.75731010890196, Acc: 0.8195, Grad norm: 0.043599331491187936\n",
      "Iteration 9195, BCE loss: 57.75731636811934, Acc: 0.8195, Grad norm: 0.04450184634715158\n",
      "Iteration 9196, BCE loss: 57.75734115757511, Acc: 0.8195, Grad norm: 0.05333929070302062\n",
      "Iteration 9197, BCE loss: 57.75732619512548, Acc: 0.8195, Grad norm: 0.04822727664859651\n",
      "Iteration 9198, BCE loss: 57.757326237160484, Acc: 0.8195, Grad norm: 0.047932155655646\n",
      "Iteration 9199, BCE loss: 57.75732610843824, Acc: 0.8195, Grad norm: 0.04769611562949131\n",
      "Iteration 9200, BCE loss: 57.75733342189312, Acc: 0.8195, Grad norm: 0.049803289116501476\n",
      "Iteration 9201, BCE loss: 57.757327478089046, Acc: 0.8195, Grad norm: 0.04761247527013392\n",
      "Iteration 9202, BCE loss: 57.75733716952851, Acc: 0.8195, Grad norm: 0.05076615860788898\n",
      "Iteration 9203, BCE loss: 57.75735606886637, Acc: 0.8195, Grad norm: 0.05840562218922661\n",
      "Iteration 9204, BCE loss: 57.75733457319127, Acc: 0.8195, Grad norm: 0.053404365029208506\n",
      "Iteration 9205, BCE loss: 57.757317383020194, Acc: 0.8195, Grad norm: 0.046611432233229916\n",
      "Iteration 9206, BCE loss: 57.75730594774868, Acc: 0.8195, Grad norm: 0.042834908742647976\n",
      "Iteration 9207, BCE loss: 57.75731877146133, Acc: 0.8195, Grad norm: 0.04941439885366062\n",
      "Iteration 9208, BCE loss: 57.757337584526596, Acc: 0.8195, Grad norm: 0.05784640508237269\n",
      "Iteration 9209, BCE loss: 57.75739830505104, Acc: 0.8195, Grad norm: 0.07833460264370008\n",
      "Iteration 9210, BCE loss: 57.75739332561582, Acc: 0.8195, Grad norm: 0.07638189063308023\n",
      "Iteration 9211, BCE loss: 57.75740910172074, Acc: 0.8195, Grad norm: 0.07947267835562952\n",
      "Iteration 9212, BCE loss: 57.75748132388925, Acc: 0.8195, Grad norm: 0.09858568306109408\n",
      "Iteration 9213, BCE loss: 57.757444400990785, Acc: 0.8196, Grad norm: 0.09045752821729774\n",
      "Iteration 9214, BCE loss: 57.757412929982706, Acc: 0.8195, Grad norm: 0.08130988788674225\n",
      "Iteration 9215, BCE loss: 57.757481318143036, Acc: 0.8196, Grad norm: 0.09780655988084427\n",
      "Iteration 9216, BCE loss: 57.757503315614485, Acc: 0.8196, Grad norm: 0.10242584246985166\n",
      "Iteration 9217, BCE loss: 57.757489599519005, Acc: 0.8196, Grad norm: 0.09997739860825745\n",
      "Iteration 9218, BCE loss: 57.75741649290349, Acc: 0.8195, Grad norm: 0.08171605550638125\n",
      "Iteration 9219, BCE loss: 57.75739996080952, Acc: 0.8195, Grad norm: 0.07762213613825306\n",
      "Iteration 9220, BCE loss: 57.757431867951134, Acc: 0.8196, Grad norm: 0.08611509800329728\n",
      "Iteration 9221, BCE loss: 57.757346225614384, Acc: 0.8196, Grad norm: 0.0590684724782187\n",
      "Iteration 9222, BCE loss: 57.757322374435425, Acc: 0.8196, Grad norm: 0.05191080082042007\n",
      "Iteration 9223, BCE loss: 57.75729869926339, Acc: 0.8196, Grad norm: 0.038964083511651836\n",
      "Iteration 9224, BCE loss: 57.75728166479506, Acc: 0.8195, Grad norm: 0.028240396697121792\n",
      "Iteration 9225, BCE loss: 57.757300925770934, Acc: 0.8196, Grad norm: 0.042399003761049846\n",
      "Iteration 9226, BCE loss: 57.75729463323697, Acc: 0.8195, Grad norm: 0.03879489106973678\n",
      "Iteration 9227, BCE loss: 57.75731111339421, Acc: 0.8196, Grad norm: 0.048080936722749226\n",
      "Iteration 9228, BCE loss: 57.757316501950775, Acc: 0.8196, Grad norm: 0.04964464930682975\n",
      "Iteration 9229, BCE loss: 57.75731340014376, Acc: 0.8196, Grad norm: 0.045601533660523455\n",
      "Iteration 9230, BCE loss: 57.75729375141641, Acc: 0.8195, Grad norm: 0.036882509754417085\n",
      "Iteration 9231, BCE loss: 57.757311365650864, Acc: 0.8196, Grad norm: 0.047452813598503285\n",
      "Iteration 9232, BCE loss: 57.757322349852224, Acc: 0.8196, Grad norm: 0.05180012932622507\n",
      "Iteration 9233, BCE loss: 57.757300277023916, Acc: 0.8196, Grad norm: 0.04176833379408972\n",
      "Iteration 9234, BCE loss: 57.75729106131903, Acc: 0.8196, Grad norm: 0.03582114249905003\n",
      "Iteration 9235, BCE loss: 57.75728356151846, Acc: 0.8196, Grad norm: 0.03171067274611125\n",
      "Iteration 9236, BCE loss: 57.75728905969343, Acc: 0.8196, Grad norm: 0.03384148244277372\n",
      "Iteration 9237, BCE loss: 57.75727790972813, Acc: 0.8196, Grad norm: 0.027704275707020713\n",
      "Iteration 9238, BCE loss: 57.757317891506375, Acc: 0.8196, Grad norm: 0.04952409251589729\n",
      "Iteration 9239, BCE loss: 57.75730388479645, Acc: 0.8196, Grad norm: 0.043871619909170365\n",
      "Iteration 9240, BCE loss: 57.75731512990178, Acc: 0.8196, Grad norm: 0.05001142657728389\n",
      "Iteration 9241, BCE loss: 57.75733795883261, Acc: 0.8196, Grad norm: 0.05989764035282379\n",
      "Iteration 9242, BCE loss: 57.75739115112611, Acc: 0.8196, Grad norm: 0.07833322980545623\n",
      "Iteration 9243, BCE loss: 57.7573712737978, Acc: 0.8196, Grad norm: 0.07167157944838329\n",
      "Iteration 9244, BCE loss: 57.75734347802385, Acc: 0.8196, Grad norm: 0.06218023237576166\n",
      "Iteration 9245, BCE loss: 57.75732210574897, Acc: 0.8196, Grad norm: 0.053305851031246194\n",
      "Iteration 9246, BCE loss: 57.75727298421562, Acc: 0.8196, Grad norm: 0.02437663923605072\n",
      "Iteration 9247, BCE loss: 57.75726892387557, Acc: 0.8196, Grad norm: 0.01876373111111282\n",
      "Iteration 9248, BCE loss: 57.75728555311832, Acc: 0.8195, Grad norm: 0.03235970776202722\n",
      "Iteration 9249, BCE loss: 57.75728388230196, Acc: 0.8195, Grad norm: 0.03158624775839224\n",
      "Iteration 9250, BCE loss: 57.757283814494286, Acc: 0.8196, Grad norm: 0.0305260667830186\n",
      "Iteration 9251, BCE loss: 57.75727798812561, Acc: 0.8196, Grad norm: 0.02633412605915784\n",
      "Iteration 9252, BCE loss: 57.75726884515875, Acc: 0.8196, Grad norm: 0.018887484610599758\n",
      "Iteration 9253, BCE loss: 57.757271848088436, Acc: 0.8196, Grad norm: 0.021484617452778836\n",
      "Iteration 9254, BCE loss: 57.757274645725374, Acc: 0.8195, Grad norm: 0.024246913744232282\n",
      "Iteration 9255, BCE loss: 57.75728162235632, Acc: 0.8195, Grad norm: 0.03064866471983712\n",
      "Iteration 9256, BCE loss: 57.75728789534169, Acc: 0.8196, Grad norm: 0.034689273987947765\n",
      "Iteration 9257, BCE loss: 57.757287628381924, Acc: 0.8196, Grad norm: 0.033350812173078094\n",
      "Iteration 9258, BCE loss: 57.75731643077785, Acc: 0.8196, Grad norm: 0.04689298760121419\n",
      "Iteration 9259, BCE loss: 57.75733209335813, Acc: 0.8196, Grad norm: 0.05216369459147745\n",
      "Iteration 9260, BCE loss: 57.75733673882766, Acc: 0.8196, Grad norm: 0.053112290928103635\n",
      "Iteration 9261, BCE loss: 57.75735484127446, Acc: 0.8196, Grad norm: 0.06085447734097748\n",
      "Iteration 9262, BCE loss: 57.75733632491172, Acc: 0.8196, Grad norm: 0.05262400501259359\n",
      "Iteration 9263, BCE loss: 57.75733263222831, Acc: 0.8196, Grad norm: 0.0528377241562683\n",
      "Iteration 9264, BCE loss: 57.757313026640745, Acc: 0.8196, Grad norm: 0.044957271169404366\n",
      "Iteration 9265, BCE loss: 57.757304330512774, Acc: 0.8196, Grad norm: 0.0412473689685931\n",
      "Iteration 9266, BCE loss: 57.75730675742557, Acc: 0.8196, Grad norm: 0.04273229393201521\n",
      "Iteration 9267, BCE loss: 57.75731786282171, Acc: 0.8196, Grad norm: 0.05010792317674849\n",
      "Iteration 9268, BCE loss: 57.757300543582225, Acc: 0.8196, Grad norm: 0.04239881154947721\n",
      "Iteration 9269, BCE loss: 57.75731674118481, Acc: 0.8195, Grad norm: 0.05069232969483763\n",
      "Iteration 9270, BCE loss: 57.75731026363974, Acc: 0.8196, Grad norm: 0.04590748513696156\n",
      "Iteration 9271, BCE loss: 57.75731696036452, Acc: 0.8196, Grad norm: 0.04884456935626633\n",
      "Iteration 9272, BCE loss: 57.75727737890591, Acc: 0.8196, Grad norm: 0.02544703394153956\n",
      "Iteration 9273, BCE loss: 57.757272488150846, Acc: 0.8195, Grad norm: 0.02133045214001014\n",
      "Iteration 9274, BCE loss: 57.757278440641464, Acc: 0.8195, Grad norm: 0.025730351521503658\n",
      "Iteration 9275, BCE loss: 57.75727913461883, Acc: 0.8196, Grad norm: 0.025587786120641685\n",
      "Iteration 9276, BCE loss: 57.75727967512, Acc: 0.8196, Grad norm: 0.027632840140566526\n",
      "Iteration 9277, BCE loss: 57.75727771767694, Acc: 0.8196, Grad norm: 0.027212579696347067\n",
      "Iteration 9278, BCE loss: 57.757285938616164, Acc: 0.8196, Grad norm: 0.03115437143950666\n",
      "Iteration 9279, BCE loss: 57.757282412340395, Acc: 0.8196, Grad norm: 0.026755797340071285\n",
      "Iteration 9280, BCE loss: 57.757284293409505, Acc: 0.8195, Grad norm: 0.02862281685356743\n",
      "Iteration 9281, BCE loss: 57.75729323734946, Acc: 0.8196, Grad norm: 0.03451065232439437\n",
      "Iteration 9282, BCE loss: 57.75730678685596, Acc: 0.8196, Grad norm: 0.043253995413452095\n",
      "Iteration 9283, BCE loss: 57.75730386270575, Acc: 0.8196, Grad norm: 0.04116093062486343\n",
      "Iteration 9284, BCE loss: 57.75729468837828, Acc: 0.8196, Grad norm: 0.038055149998772096\n",
      "Iteration 9285, BCE loss: 57.75728982638761, Acc: 0.8196, Grad norm: 0.033997298681377325\n",
      "Iteration 9286, BCE loss: 57.75729603656853, Acc: 0.8196, Grad norm: 0.03797539614885417\n",
      "Iteration 9287, BCE loss: 57.75731354795255, Acc: 0.8196, Grad norm: 0.04773857937725719\n",
      "Iteration 9288, BCE loss: 57.75731121197752, Acc: 0.8196, Grad norm: 0.046378259372627\n",
      "Iteration 9289, BCE loss: 57.757281882652094, Acc: 0.8196, Grad norm: 0.02917688449794701\n",
      "Iteration 9290, BCE loss: 57.757276601613256, Acc: 0.8195, Grad norm: 0.02449120103091369\n",
      "Iteration 9291, BCE loss: 57.75727667171842, Acc: 0.8195, Grad norm: 0.02580067513536595\n",
      "Iteration 9292, BCE loss: 57.75728155524918, Acc: 0.8195, Grad norm: 0.030747984318491357\n",
      "Iteration 9293, BCE loss: 57.757292627360705, Acc: 0.8195, Grad norm: 0.036409876811159654\n",
      "Iteration 9294, BCE loss: 57.75728181070352, Acc: 0.8195, Grad norm: 0.029466890141693004\n",
      "Iteration 9295, BCE loss: 57.75730649287557, Acc: 0.8195, Grad norm: 0.04249397033695481\n",
      "Iteration 9296, BCE loss: 57.75732038499775, Acc: 0.8195, Grad norm: 0.04834563475477484\n",
      "Iteration 9297, BCE loss: 57.75735927485853, Acc: 0.8195, Grad norm: 0.06377105617529509\n",
      "Iteration 9298, BCE loss: 57.7573913699985, Acc: 0.8195, Grad norm: 0.07433705132440148\n",
      "Iteration 9299, BCE loss: 57.75739957147408, Acc: 0.8195, Grad norm: 0.07842418603149363\n",
      "Iteration 9300, BCE loss: 57.7574112838597, Acc: 0.8195, Grad norm: 0.08261446993053835\n",
      "Iteration 9301, BCE loss: 57.75739240593954, Acc: 0.8195, Grad norm: 0.07812796508164553\n",
      "Iteration 9302, BCE loss: 57.757366966013905, Acc: 0.8195, Grad norm: 0.06964104910174301\n",
      "Iteration 9303, BCE loss: 57.75734855802604, Acc: 0.8195, Grad norm: 0.06283363940324393\n",
      "Iteration 9304, BCE loss: 57.757381664193026, Acc: 0.8195, Grad norm: 0.07444930389297177\n",
      "Iteration 9305, BCE loss: 57.75737663555341, Acc: 0.8195, Grad norm: 0.0712185882441202\n",
      "Iteration 9306, BCE loss: 57.75737310970173, Acc: 0.8195, Grad norm: 0.07020880933256538\n",
      "Iteration 9307, BCE loss: 57.75737801271202, Acc: 0.8195, Grad norm: 0.07142994899965337\n",
      "Iteration 9308, BCE loss: 57.75736696339925, Acc: 0.8195, Grad norm: 0.0672430143759129\n",
      "Iteration 9309, BCE loss: 57.75735334598228, Acc: 0.8195, Grad norm: 0.06241133865009182\n",
      "Iteration 9310, BCE loss: 57.75737549198879, Acc: 0.8195, Grad norm: 0.07101130396869036\n",
      "Iteration 9311, BCE loss: 57.757389225534375, Acc: 0.8195, Grad norm: 0.07531625537217373\n",
      "Iteration 9312, BCE loss: 57.7573445646599, Acc: 0.8195, Grad norm: 0.058687428495842166\n",
      "Iteration 9313, BCE loss: 57.75731526459694, Acc: 0.8195, Grad norm: 0.04482459024513824\n",
      "Iteration 9314, BCE loss: 57.757327596453344, Acc: 0.8195, Grad norm: 0.04978533659662875\n",
      "Iteration 9315, BCE loss: 57.75735427291591, Acc: 0.8195, Grad norm: 0.06179058620107362\n",
      "Iteration 9316, BCE loss: 57.75731177455472, Acc: 0.8195, Grad norm: 0.045855036559143816\n",
      "Iteration 9317, BCE loss: 57.75730060360887, Acc: 0.8195, Grad norm: 0.03997696784894417\n",
      "Iteration 9318, BCE loss: 57.757302159313696, Acc: 0.8195, Grad norm: 0.041253669600840405\n",
      "Iteration 9319, BCE loss: 57.75732149792189, Acc: 0.8195, Grad norm: 0.05042287825089854\n",
      "Iteration 9320, BCE loss: 57.75734494554563, Acc: 0.8195, Grad norm: 0.05962730368848861\n",
      "Iteration 9321, BCE loss: 57.75736412157878, Acc: 0.8195, Grad norm: 0.06731848197903097\n",
      "Iteration 9322, BCE loss: 57.75740299763969, Acc: 0.8195, Grad norm: 0.07973429510989125\n",
      "Iteration 9323, BCE loss: 57.75739341799947, Acc: 0.8195, Grad norm: 0.07647931079248398\n",
      "Iteration 9324, BCE loss: 57.75737400969692, Acc: 0.8195, Grad norm: 0.0704654729417403\n",
      "Iteration 9325, BCE loss: 57.757344170803066, Acc: 0.8195, Grad norm: 0.061970958979266626\n",
      "Iteration 9326, BCE loss: 57.75735966909832, Acc: 0.8195, Grad norm: 0.06636006437407752\n",
      "Iteration 9327, BCE loss: 57.757338137015765, Acc: 0.8195, Grad norm: 0.058586553391890735\n",
      "Iteration 9328, BCE loss: 57.757374092833714, Acc: 0.8195, Grad norm: 0.07136563558482054\n",
      "Iteration 9329, BCE loss: 57.75736567132728, Acc: 0.8195, Grad norm: 0.06702128054719479\n",
      "Iteration 9330, BCE loss: 57.75741290471525, Acc: 0.8196, Grad norm: 0.08214804554586937\n",
      "Iteration 9331, BCE loss: 57.757441310605486, Acc: 0.8196, Grad norm: 0.09111382995407902\n",
      "Iteration 9332, BCE loss: 57.7573923262999, Acc: 0.8196, Grad norm: 0.07737257401467194\n",
      "Iteration 9333, BCE loss: 57.75734459234722, Acc: 0.8196, Grad norm: 0.05949525124825786\n",
      "Iteration 9334, BCE loss: 57.757342924559836, Acc: 0.8196, Grad norm: 0.058189699523222504\n",
      "Iteration 9335, BCE loss: 57.75730439787631, Acc: 0.8196, Grad norm: 0.039473184638646394\n",
      "Iteration 9336, BCE loss: 57.757299303675474, Acc: 0.8195, Grad norm: 0.038641340669270496\n",
      "Iteration 9337, BCE loss: 57.75730442482708, Acc: 0.8195, Grad norm: 0.04117336684739675\n",
      "Iteration 9338, BCE loss: 57.75729505977249, Acc: 0.8195, Grad norm: 0.03612683498805351\n",
      "Iteration 9339, BCE loss: 57.75729833499774, Acc: 0.8195, Grad norm: 0.03894340594130014\n",
      "Iteration 9340, BCE loss: 57.75730465526121, Acc: 0.8195, Grad norm: 0.04362620343677664\n",
      "Iteration 9341, BCE loss: 57.75731181994274, Acc: 0.8195, Grad norm: 0.04669867677491075\n",
      "Iteration 9342, BCE loss: 57.75732382296493, Acc: 0.8195, Grad norm: 0.052521522472424906\n",
      "Iteration 9343, BCE loss: 57.75732040866329, Acc: 0.8195, Grad norm: 0.05072605397683851\n",
      "Iteration 9344, BCE loss: 57.75731259805255, Acc: 0.8196, Grad norm: 0.04715154676757159\n",
      "Iteration 9345, BCE loss: 57.757318343540625, Acc: 0.8196, Grad norm: 0.04981333181266323\n",
      "Iteration 9346, BCE loss: 57.75729352737108, Acc: 0.8196, Grad norm: 0.03611603322186581\n",
      "Iteration 9347, BCE loss: 57.75729011103712, Acc: 0.8196, Grad norm: 0.03491612620767019\n",
      "Iteration 9348, BCE loss: 57.757310607830554, Acc: 0.8196, Grad norm: 0.04804276977079148\n",
      "Iteration 9349, BCE loss: 57.757317096131565, Acc: 0.8196, Grad norm: 0.050805123661586404\n",
      "Iteration 9350, BCE loss: 57.75735277675108, Acc: 0.8196, Grad norm: 0.06549103202451469\n",
      "Iteration 9351, BCE loss: 57.757341691005365, Acc: 0.8196, Grad norm: 0.061018581304290005\n",
      "Iteration 9352, BCE loss: 57.75735043061624, Acc: 0.8195, Grad norm: 0.06519543618747209\n",
      "Iteration 9353, BCE loss: 57.757311311482724, Acc: 0.8195, Grad norm: 0.04850280516091032\n",
      "Iteration 9354, BCE loss: 57.75729143948608, Acc: 0.8196, Grad norm: 0.03797874180254481\n",
      "Iteration 9355, BCE loss: 57.757305750590824, Acc: 0.8196, Grad norm: 0.04592085122287092\n",
      "Iteration 9356, BCE loss: 57.757302364250464, Acc: 0.8196, Grad norm: 0.04471496281236766\n",
      "Iteration 9357, BCE loss: 57.75730072613244, Acc: 0.8196, Grad norm: 0.043416320622442765\n",
      "Iteration 9358, BCE loss: 57.75729444472532, Acc: 0.8195, Grad norm: 0.0394383154938378\n",
      "Iteration 9359, BCE loss: 57.75727094008601, Acc: 0.8196, Grad norm: 0.022294170363697085\n",
      "Iteration 9360, BCE loss: 57.75727788895172, Acc: 0.8196, Grad norm: 0.028950358644250884\n",
      "Iteration 9361, BCE loss: 57.757272474461836, Acc: 0.8196, Grad norm: 0.023810159522206397\n",
      "Iteration 9362, BCE loss: 57.75727188782943, Acc: 0.8196, Grad norm: 0.022236357028017767\n",
      "Iteration 9363, BCE loss: 57.75727570168543, Acc: 0.8196, Grad norm: 0.024074710462156604\n",
      "Iteration 9364, BCE loss: 57.75728716385558, Acc: 0.8196, Grad norm: 0.0312935990484228\n",
      "Iteration 9365, BCE loss: 57.75729059670833, Acc: 0.8196, Grad norm: 0.032989473547634804\n",
      "Iteration 9366, BCE loss: 57.757296137733746, Acc: 0.8196, Grad norm: 0.03748328653827193\n",
      "Iteration 9367, BCE loss: 57.75729746922243, Acc: 0.8196, Grad norm: 0.037353239045339916\n",
      "Iteration 9368, BCE loss: 57.757298619783505, Acc: 0.8196, Grad norm: 0.03922891810420554\n",
      "Iteration 9369, BCE loss: 57.757291182092835, Acc: 0.8196, Grad norm: 0.03525548035783375\n",
      "Iteration 9370, BCE loss: 57.757336347776146, Acc: 0.8196, Grad norm: 0.05687014227004611\n",
      "Iteration 9371, BCE loss: 57.757349436445566, Acc: 0.8196, Grad norm: 0.061777258868062464\n",
      "Iteration 9372, BCE loss: 57.75737062787306, Acc: 0.8196, Grad norm: 0.06867374448422695\n",
      "Iteration 9373, BCE loss: 57.75736809775586, Acc: 0.8196, Grad norm: 0.06672416100443201\n",
      "Iteration 9374, BCE loss: 57.75737718136651, Acc: 0.8196, Grad norm: 0.06890586477600517\n",
      "Iteration 9375, BCE loss: 57.75736163411949, Acc: 0.8196, Grad norm: 0.06429873652859837\n",
      "Iteration 9376, BCE loss: 57.757367329414095, Acc: 0.8196, Grad norm: 0.06596943373665597\n",
      "Iteration 9377, BCE loss: 57.75740969527693, Acc: 0.8196, Grad norm: 0.07889909746936383\n",
      "Iteration 9378, BCE loss: 57.75740038350824, Acc: 0.8196, Grad norm: 0.07707214979064182\n",
      "Iteration 9379, BCE loss: 57.757368740695135, Acc: 0.8196, Grad norm: 0.06761411282716075\n",
      "Iteration 9380, BCE loss: 57.757363313144126, Acc: 0.8196, Grad norm: 0.06513054144802713\n",
      "Iteration 9381, BCE loss: 57.75734762468235, Acc: 0.8196, Grad norm: 0.05867253353833147\n",
      "Iteration 9382, BCE loss: 57.75738470643684, Acc: 0.8196, Grad norm: 0.07084928133805274\n",
      "Iteration 9383, BCE loss: 57.75734319296804, Acc: 0.8196, Grad norm: 0.05464471803126278\n",
      "Iteration 9384, BCE loss: 57.757346707936534, Acc: 0.8196, Grad norm: 0.05510500226400426\n",
      "Iteration 9385, BCE loss: 57.75734989938451, Acc: 0.8196, Grad norm: 0.057916815440914954\n",
      "Iteration 9386, BCE loss: 57.75735957667556, Acc: 0.8196, Grad norm: 0.06074804116577934\n",
      "Iteration 9387, BCE loss: 57.757365061540526, Acc: 0.8196, Grad norm: 0.0637009287204003\n",
      "Iteration 9388, BCE loss: 57.75736123473114, Acc: 0.8196, Grad norm: 0.06211905851240374\n",
      "Iteration 9389, BCE loss: 57.75734802658716, Acc: 0.8196, Grad norm: 0.05586401433368711\n",
      "Iteration 9390, BCE loss: 57.75734314659422, Acc: 0.8196, Grad norm: 0.05696526388586046\n",
      "Iteration 9391, BCE loss: 57.757333968475436, Acc: 0.8196, Grad norm: 0.05166717141180089\n",
      "Iteration 9392, BCE loss: 57.757323089055745, Acc: 0.8196, Grad norm: 0.05017695742601269\n",
      "Iteration 9393, BCE loss: 57.75732944980427, Acc: 0.8196, Grad norm: 0.05130883401666271\n",
      "Iteration 9394, BCE loss: 57.75734175083569, Acc: 0.8196, Grad norm: 0.056498477349931284\n",
      "Iteration 9395, BCE loss: 57.75734340852293, Acc: 0.8196, Grad norm: 0.05753120914943679\n",
      "Iteration 9396, BCE loss: 57.75738499118289, Acc: 0.8196, Grad norm: 0.0706032451714509\n",
      "Iteration 9397, BCE loss: 57.75746255236555, Acc: 0.8196, Grad norm: 0.09386820764958388\n",
      "Iteration 9398, BCE loss: 57.75745979796176, Acc: 0.8196, Grad norm: 0.09276686787394006\n",
      "Iteration 9399, BCE loss: 57.75737098514529, Acc: 0.8196, Grad norm: 0.06807856691152658\n",
      "Iteration 9400, BCE loss: 57.75740015794896, Acc: 0.8196, Grad norm: 0.07570170372796878\n",
      "Iteration 9401, BCE loss: 57.757430078609985, Acc: 0.8196, Grad norm: 0.08493787643713858\n",
      "Iteration 9402, BCE loss: 57.75741986195704, Acc: 0.8196, Grad norm: 0.08257553225488029\n",
      "Iteration 9403, BCE loss: 57.7573474411411, Acc: 0.8196, Grad norm: 0.05850774660535271\n",
      "Iteration 9404, BCE loss: 57.757364799734454, Acc: 0.8196, Grad norm: 0.06473937940631137\n",
      "Iteration 9405, BCE loss: 57.75733060459261, Acc: 0.8196, Grad norm: 0.05205845899185872\n",
      "Iteration 9406, BCE loss: 57.7573108445657, Acc: 0.8196, Grad norm: 0.04263444964834408\n",
      "Iteration 9407, BCE loss: 57.75732529295448, Acc: 0.8196, Grad norm: 0.04989881838905558\n",
      "Iteration 9408, BCE loss: 57.757395685759825, Acc: 0.8196, Grad norm: 0.07537905062540526\n",
      "Iteration 9409, BCE loss: 57.75739360577212, Acc: 0.8196, Grad norm: 0.07546408336075469\n",
      "Iteration 9410, BCE loss: 57.75737622278481, Acc: 0.8196, Grad norm: 0.0693021524561273\n",
      "Iteration 9411, BCE loss: 57.75739048898972, Acc: 0.8196, Grad norm: 0.07582800147596162\n",
      "Iteration 9412, BCE loss: 57.75736724479835, Acc: 0.8196, Grad norm: 0.0671011186959953\n",
      "Iteration 9413, BCE loss: 57.757409041925115, Acc: 0.8196, Grad norm: 0.07984305224928584\n",
      "Iteration 9414, BCE loss: 57.757467479269465, Acc: 0.8196, Grad norm: 0.0950056321573925\n",
      "Iteration 9415, BCE loss: 57.75745851004001, Acc: 0.8196, Grad norm: 0.09303304502685994\n",
      "Iteration 9416, BCE loss: 57.75757674271929, Acc: 0.8196, Grad norm: 0.11881959049432686\n",
      "Iteration 9417, BCE loss: 57.75758335276379, Acc: 0.8196, Grad norm: 0.11985582997127979\n",
      "Iteration 9418, BCE loss: 57.757586181980514, Acc: 0.8196, Grad norm: 0.12017765559242122\n",
      "Iteration 9419, BCE loss: 57.75751367583791, Acc: 0.8196, Grad norm: 0.10400370081397667\n",
      "Iteration 9420, BCE loss: 57.75747729441194, Acc: 0.8196, Grad norm: 0.09392712859085384\n",
      "Iteration 9421, BCE loss: 57.75746577493985, Acc: 0.8196, Grad norm: 0.0904442509656258\n",
      "Iteration 9422, BCE loss: 57.75743844012002, Acc: 0.8196, Grad norm: 0.0841824206701009\n",
      "Iteration 9423, BCE loss: 57.75748778358023, Acc: 0.8196, Grad norm: 0.09748240619389328\n",
      "Iteration 9424, BCE loss: 57.75750035446079, Acc: 0.8196, Grad norm: 0.09970039536905594\n",
      "Iteration 9425, BCE loss: 57.75745526783885, Acc: 0.8196, Grad norm: 0.08890048252519288\n",
      "Iteration 9426, BCE loss: 57.75744686343814, Acc: 0.8196, Grad norm: 0.0874915937801916\n",
      "Iteration 9427, BCE loss: 57.75748125892777, Acc: 0.8196, Grad norm: 0.09453829305555454\n",
      "Iteration 9428, BCE loss: 57.75749165254442, Acc: 0.8196, Grad norm: 0.09805433639520374\n",
      "Iteration 9429, BCE loss: 57.757421362437185, Acc: 0.8196, Grad norm: 0.08078197167731016\n",
      "Iteration 9430, BCE loss: 57.75738546938358, Acc: 0.8196, Grad norm: 0.06993428064110789\n",
      "Iteration 9431, BCE loss: 57.75736533072908, Acc: 0.8196, Grad norm: 0.0633094404104808\n",
      "Iteration 9432, BCE loss: 57.75736366210042, Acc: 0.8196, Grad norm: 0.06304249594491743\n",
      "Iteration 9433, BCE loss: 57.75732822762161, Acc: 0.8196, Grad norm: 0.04758074871367779\n",
      "Iteration 9434, BCE loss: 57.757321495353025, Acc: 0.8195, Grad norm: 0.045149271731399765\n",
      "Iteration 9435, BCE loss: 57.75730696642834, Acc: 0.8195, Grad norm: 0.03994861094498259\n",
      "Iteration 9436, BCE loss: 57.75730336774819, Acc: 0.8196, Grad norm: 0.039979011808164745\n",
      "Iteration 9437, BCE loss: 57.7573306400061, Acc: 0.8195, Grad norm: 0.051751203168418275\n",
      "Iteration 9438, BCE loss: 57.757323981513615, Acc: 0.8195, Grad norm: 0.050509479681496446\n",
      "Iteration 9439, BCE loss: 57.75735182926469, Acc: 0.8195, Grad norm: 0.06187383988056615\n",
      "Iteration 9440, BCE loss: 57.75732787606428, Acc: 0.8195, Grad norm: 0.051500121586246596\n",
      "Iteration 9441, BCE loss: 57.7573257000507, Acc: 0.8196, Grad norm: 0.04912831032233264\n",
      "Iteration 9442, BCE loss: 57.75735104445217, Acc: 0.8196, Grad norm: 0.06039138803286459\n",
      "Iteration 9443, BCE loss: 57.75731771296378, Acc: 0.8196, Grad norm: 0.04802397621586708\n",
      "Iteration 9444, BCE loss: 57.757305296802784, Acc: 0.8195, Grad norm: 0.040371411886455905\n",
      "Iteration 9445, BCE loss: 57.75731217901527, Acc: 0.8195, Grad norm: 0.04496026984188205\n",
      "Iteration 9446, BCE loss: 57.757301331334276, Acc: 0.8195, Grad norm: 0.03838718522620719\n",
      "Iteration 9447, BCE loss: 57.75730151797956, Acc: 0.8195, Grad norm: 0.03846732735452844\n",
      "Iteration 9448, BCE loss: 57.7572981622892, Acc: 0.8196, Grad norm: 0.03529962063945042\n",
      "Iteration 9449, BCE loss: 57.757296540426175, Acc: 0.8195, Grad norm: 0.03586038118281548\n",
      "Iteration 9450, BCE loss: 57.75730373036305, Acc: 0.8195, Grad norm: 0.03921719770113541\n",
      "Iteration 9451, BCE loss: 57.75734356893548, Acc: 0.8196, Grad norm: 0.056987924712803555\n",
      "Iteration 9452, BCE loss: 57.75738530875338, Acc: 0.8196, Grad norm: 0.0722909951231096\n",
      "Iteration 9453, BCE loss: 57.757342782856234, Acc: 0.8196, Grad norm: 0.05788970266749756\n",
      "Iteration 9454, BCE loss: 57.75734279047125, Acc: 0.8195, Grad norm: 0.056669279574306015\n",
      "Iteration 9455, BCE loss: 57.75739654675127, Acc: 0.8196, Grad norm: 0.07519411025984606\n",
      "Iteration 9456, BCE loss: 57.75735383715693, Acc: 0.8196, Grad norm: 0.062010003276424544\n",
      "Iteration 9457, BCE loss: 57.757343825229796, Acc: 0.8196, Grad norm: 0.05860047829509801\n",
      "Iteration 9458, BCE loss: 57.75735143850531, Acc: 0.8195, Grad norm: 0.06122979172568073\n",
      "Iteration 9459, BCE loss: 57.757354395744045, Acc: 0.8195, Grad norm: 0.06248452109045548\n",
      "Iteration 9460, BCE loss: 57.757345658657556, Acc: 0.8195, Grad norm: 0.05820115821944054\n",
      "Iteration 9461, BCE loss: 57.757321357397565, Acc: 0.8195, Grad norm: 0.04906667401436053\n",
      "Iteration 9462, BCE loss: 57.757325641938095, Acc: 0.8195, Grad norm: 0.04933074512805881\n",
      "Iteration 9463, BCE loss: 57.75731498959246, Acc: 0.8195, Grad norm: 0.046045225529090274\n",
      "Iteration 9464, BCE loss: 57.75734014545747, Acc: 0.8195, Grad norm: 0.05623414307634087\n",
      "Iteration 9465, BCE loss: 57.75731364654561, Acc: 0.8195, Grad norm: 0.04625779503269646\n",
      "Iteration 9466, BCE loss: 57.75729704182924, Acc: 0.8195, Grad norm: 0.037459785574113016\n",
      "Iteration 9467, BCE loss: 57.7573196094228, Acc: 0.8195, Grad norm: 0.04847988556078321\n",
      "Iteration 9468, BCE loss: 57.757312730178924, Acc: 0.8195, Grad norm: 0.04514497632632184\n",
      "Iteration 9469, BCE loss: 57.75729986083323, Acc: 0.8195, Grad norm: 0.03894860716785133\n",
      "Iteration 9470, BCE loss: 57.757294769043185, Acc: 0.8195, Grad norm: 0.03727422501858\n",
      "Iteration 9471, BCE loss: 57.75730735114014, Acc: 0.8195, Grad norm: 0.04380223618181395\n",
      "Iteration 9472, BCE loss: 57.75729078110397, Acc: 0.8195, Grad norm: 0.03459362480451819\n",
      "Iteration 9473, BCE loss: 57.757277896526645, Acc: 0.8195, Grad norm: 0.02699339745294224\n",
      "Iteration 9474, BCE loss: 57.757300136598886, Acc: 0.8195, Grad norm: 0.041098885272608\n",
      "Iteration 9475, BCE loss: 57.75729054225454, Acc: 0.8195, Grad norm: 0.0370329374778451\n",
      "Iteration 9476, BCE loss: 57.75728441797118, Acc: 0.8195, Grad norm: 0.03266460180424698\n",
      "Iteration 9477, BCE loss: 57.75729387185598, Acc: 0.8195, Grad norm: 0.03658985444044769\n",
      "Iteration 9478, BCE loss: 57.757281598523996, Acc: 0.8195, Grad norm: 0.029302722993494786\n",
      "Iteration 9479, BCE loss: 57.75728448504388, Acc: 0.8195, Grad norm: 0.031129515845766755\n",
      "Iteration 9480, BCE loss: 57.757316530101974, Acc: 0.8195, Grad norm: 0.05012287590467082\n",
      "Iteration 9481, BCE loss: 57.75733846888444, Acc: 0.8196, Grad norm: 0.05992496109063234\n",
      "Iteration 9482, BCE loss: 57.75731252244018, Acc: 0.8196, Grad norm: 0.05041831013318426\n",
      "Iteration 9483, BCE loss: 57.757310332614985, Acc: 0.8196, Grad norm: 0.04938066315172439\n",
      "Iteration 9484, BCE loss: 57.75729271381595, Acc: 0.8196, Grad norm: 0.040328316370673224\n",
      "Iteration 9485, BCE loss: 57.75727945424384, Acc: 0.8195, Grad norm: 0.030752182559476364\n",
      "Iteration 9486, BCE loss: 57.75728838087879, Acc: 0.8195, Grad norm: 0.03709990666856122\n",
      "Iteration 9487, BCE loss: 57.7572887637391, Acc: 0.8195, Grad norm: 0.036427317324271916\n",
      "Iteration 9488, BCE loss: 57.75729463437542, Acc: 0.8195, Grad norm: 0.04008889589428011\n",
      "Iteration 9489, BCE loss: 57.75732772624818, Acc: 0.8195, Grad norm: 0.05664224866010155\n",
      "Iteration 9490, BCE loss: 57.75730182979832, Acc: 0.8195, Grad norm: 0.043753322510306676\n",
      "Iteration 9491, BCE loss: 57.75729148908842, Acc: 0.8195, Grad norm: 0.03692209684074994\n",
      "Iteration 9492, BCE loss: 57.75727737559411, Acc: 0.8195, Grad norm: 0.02888498538307757\n",
      "Iteration 9493, BCE loss: 57.75730963199467, Acc: 0.8195, Grad norm: 0.04842654714749201\n",
      "Iteration 9494, BCE loss: 57.75731483515447, Acc: 0.8195, Grad norm: 0.05089907659299993\n",
      "Iteration 9495, BCE loss: 57.75728867079727, Acc: 0.8195, Grad norm: 0.0368053560021363\n",
      "Iteration 9496, BCE loss: 57.75729174725751, Acc: 0.8195, Grad norm: 0.038209045830909366\n",
      "Iteration 9497, BCE loss: 57.757291371533434, Acc: 0.8195, Grad norm: 0.037965917420678966\n",
      "Iteration 9498, BCE loss: 57.75728826500061, Acc: 0.8195, Grad norm: 0.03608486191371798\n",
      "Iteration 9499, BCE loss: 57.75729199345499, Acc: 0.8195, Grad norm: 0.03875758834104106\n",
      "Iteration 9500, BCE loss: 57.757282955362015, Acc: 0.8195, Grad norm: 0.03335489753636985\n",
      "Iteration 9501, BCE loss: 57.75728800442954, Acc: 0.8195, Grad norm: 0.03671147077353595\n",
      "Iteration 9502, BCE loss: 57.75730196046062, Acc: 0.8195, Grad norm: 0.04524782183098694\n",
      "Iteration 9503, BCE loss: 57.75728339987971, Acc: 0.8195, Grad norm: 0.03348567135462356\n",
      "Iteration 9504, BCE loss: 57.75729435301254, Acc: 0.8195, Grad norm: 0.039984957972826474\n",
      "Iteration 9505, BCE loss: 57.75728951389628, Acc: 0.8195, Grad norm: 0.036876492058691375\n",
      "Iteration 9506, BCE loss: 57.75728614320306, Acc: 0.8195, Grad norm: 0.03294684602542767\n",
      "Iteration 9507, BCE loss: 57.757299796048514, Acc: 0.8196, Grad norm: 0.04219444325599378\n",
      "Iteration 9508, BCE loss: 57.757313285287694, Acc: 0.8196, Grad norm: 0.0499877659851587\n",
      "Iteration 9509, BCE loss: 57.75733986202508, Acc: 0.8196, Grad norm: 0.06023264104012051\n",
      "Iteration 9510, BCE loss: 57.75734663497249, Acc: 0.8196, Grad norm: 0.06261654476048865\n",
      "Iteration 9511, BCE loss: 57.757370795985025, Acc: 0.8196, Grad norm: 0.07164816091685701\n",
      "Iteration 9512, BCE loss: 57.75740102813016, Acc: 0.8196, Grad norm: 0.07965766910420863\n",
      "Iteration 9513, BCE loss: 57.75736581227679, Acc: 0.8196, Grad norm: 0.06955992226005105\n",
      "Iteration 9514, BCE loss: 57.757380881559456, Acc: 0.8196, Grad norm: 0.07412063578593142\n",
      "Iteration 9515, BCE loss: 57.75737206498256, Acc: 0.8196, Grad norm: 0.07161951702380545\n",
      "Iteration 9516, BCE loss: 57.757351691509356, Acc: 0.8196, Grad norm: 0.06465460933259697\n",
      "Iteration 9517, BCE loss: 57.75734093742031, Acc: 0.8196, Grad norm: 0.06059702026092259\n",
      "Iteration 9518, BCE loss: 57.75736604326556, Acc: 0.8196, Grad norm: 0.06895034464521566\n",
      "Iteration 9519, BCE loss: 57.75737650291367, Acc: 0.8196, Grad norm: 0.0729697835136816\n",
      "Iteration 9520, BCE loss: 57.757392852310346, Acc: 0.8196, Grad norm: 0.07713497913989138\n",
      "Iteration 9521, BCE loss: 57.757346919022304, Acc: 0.8196, Grad norm: 0.06195173769406275\n",
      "Iteration 9522, BCE loss: 57.75739186994899, Acc: 0.8196, Grad norm: 0.07678015943068002\n",
      "Iteration 9523, BCE loss: 57.75738437101293, Acc: 0.8196, Grad norm: 0.07407179161904709\n",
      "Iteration 9524, BCE loss: 57.7573848016371, Acc: 0.8196, Grad norm: 0.07438602752111256\n",
      "Iteration 9525, BCE loss: 57.757406495851434, Acc: 0.8196, Grad norm: 0.08003693446335255\n",
      "Iteration 9526, BCE loss: 57.75743043285448, Acc: 0.8196, Grad norm: 0.08678542272633637\n",
      "Iteration 9527, BCE loss: 57.75741336007527, Acc: 0.8196, Grad norm: 0.08086028009177834\n",
      "Iteration 9528, BCE loss: 57.75742325236868, Acc: 0.8196, Grad norm: 0.08303297020463218\n",
      "Iteration 9529, BCE loss: 57.75742885627062, Acc: 0.8196, Grad norm: 0.08326962328934595\n",
      "Iteration 9530, BCE loss: 57.75739391035552, Acc: 0.8196, Grad norm: 0.0741859240700575\n",
      "Iteration 9531, BCE loss: 57.75739424553478, Acc: 0.8196, Grad norm: 0.07344149533714757\n",
      "Iteration 9532, BCE loss: 57.757426848859865, Acc: 0.8196, Grad norm: 0.0824709256296207\n",
      "Iteration 9533, BCE loss: 57.75749534851924, Acc: 0.8196, Grad norm: 0.0987322760361065\n",
      "Iteration 9534, BCE loss: 57.757453582263594, Acc: 0.8196, Grad norm: 0.08882176116110287\n",
      "Iteration 9535, BCE loss: 57.75739947824995, Acc: 0.8196, Grad norm: 0.0756461822687997\n",
      "Iteration 9536, BCE loss: 57.75737101312978, Acc: 0.8196, Grad norm: 0.06703304301500755\n",
      "Iteration 9537, BCE loss: 57.75735533158617, Acc: 0.8196, Grad norm: 0.061393041578372484\n",
      "Iteration 9538, BCE loss: 57.7573759533337, Acc: 0.8196, Grad norm: 0.06778246368921355\n",
      "Iteration 9539, BCE loss: 57.757359592439556, Acc: 0.8195, Grad norm: 0.06218496075518207\n",
      "Iteration 9540, BCE loss: 57.75733246353889, Acc: 0.8195, Grad norm: 0.05086093432874345\n",
      "Iteration 9541, BCE loss: 57.75732132245086, Acc: 0.8195, Grad norm: 0.04736627599950059\n",
      "Iteration 9542, BCE loss: 57.75733916523059, Acc: 0.8195, Grad norm: 0.056488504220431175\n",
      "Iteration 9543, BCE loss: 57.757346465183694, Acc: 0.8195, Grad norm: 0.058229471151341965\n",
      "Iteration 9544, BCE loss: 57.75733580871463, Acc: 0.8195, Grad norm: 0.05553077470406392\n",
      "Iteration 9545, BCE loss: 57.757325226455954, Acc: 0.8195, Grad norm: 0.052118912656140194\n",
      "Iteration 9546, BCE loss: 57.75733470492497, Acc: 0.8195, Grad norm: 0.055289427416943364\n",
      "Iteration 9547, BCE loss: 57.75732437197237, Acc: 0.8195, Grad norm: 0.05052829981419592\n",
      "Iteration 9548, BCE loss: 57.75733951787832, Acc: 0.8195, Grad norm: 0.05709626153527781\n",
      "Iteration 9549, BCE loss: 57.75736166188072, Acc: 0.8195, Grad norm: 0.06623993161380645\n",
      "Iteration 9550, BCE loss: 57.757374225180214, Acc: 0.8195, Grad norm: 0.06940176446411557\n",
      "Iteration 9551, BCE loss: 57.75737183461492, Acc: 0.8195, Grad norm: 0.06716872968995483\n",
      "Iteration 9552, BCE loss: 57.757329172245676, Acc: 0.8196, Grad norm: 0.05234342437087397\n",
      "Iteration 9553, BCE loss: 57.75733459623926, Acc: 0.8196, Grad norm: 0.05494947012752179\n",
      "Iteration 9554, BCE loss: 57.757329209342544, Acc: 0.8196, Grad norm: 0.0535808162637851\n",
      "Iteration 9555, BCE loss: 57.75730871130595, Acc: 0.8196, Grad norm: 0.045787612947761513\n",
      "Iteration 9556, BCE loss: 57.757302130121154, Acc: 0.8196, Grad norm: 0.04319183535889201\n",
      "Iteration 9557, BCE loss: 57.75730764073862, Acc: 0.8196, Grad norm: 0.04642936896227627\n",
      "Iteration 9558, BCE loss: 57.75733627580079, Acc: 0.8196, Grad norm: 0.05795809642570752\n",
      "Iteration 9559, BCE loss: 57.75732188959911, Acc: 0.8196, Grad norm: 0.0520623178531429\n",
      "Iteration 9560, BCE loss: 57.75733323321579, Acc: 0.8196, Grad norm: 0.05634733809828214\n",
      "Iteration 9561, BCE loss: 57.75733770090783, Acc: 0.8196, Grad norm: 0.05920587670159952\n",
      "Iteration 9562, BCE loss: 57.75732044869994, Acc: 0.8196, Grad norm: 0.051973194273896066\n",
      "Iteration 9563, BCE loss: 57.75730172049356, Acc: 0.8196, Grad norm: 0.04257053981403544\n",
      "Iteration 9564, BCE loss: 57.75728904545369, Acc: 0.8196, Grad norm: 0.03465856241553326\n",
      "Iteration 9565, BCE loss: 57.75729646411472, Acc: 0.8196, Grad norm: 0.038101895661928085\n",
      "Iteration 9566, BCE loss: 57.757285535357525, Acc: 0.8196, Grad norm: 0.032192356511805964\n",
      "Iteration 9567, BCE loss: 57.75730964306091, Acc: 0.8195, Grad norm: 0.044565005134554865\n",
      "Iteration 9568, BCE loss: 57.75730409939916, Acc: 0.8196, Grad norm: 0.04200334854491312\n",
      "Iteration 9569, BCE loss: 57.757286708957665, Acc: 0.8196, Grad norm: 0.032258001481212606\n",
      "Iteration 9570, BCE loss: 57.75730801682602, Acc: 0.8196, Grad norm: 0.044841259720961806\n",
      "Iteration 9571, BCE loss: 57.75730326269655, Acc: 0.8195, Grad norm: 0.041747244705186384\n",
      "Iteration 9572, BCE loss: 57.75732029420253, Acc: 0.8195, Grad norm: 0.04978808966048662\n",
      "Iteration 9573, BCE loss: 57.75732473646045, Acc: 0.8195, Grad norm: 0.05316256521964393\n",
      "Iteration 9574, BCE loss: 57.757335678077546, Acc: 0.8195, Grad norm: 0.05885251178227282\n",
      "Iteration 9575, BCE loss: 57.75731404815421, Acc: 0.8196, Grad norm: 0.05056762213842163\n",
      "Iteration 9576, BCE loss: 57.757286978667985, Acc: 0.8196, Grad norm: 0.03647221257520946\n",
      "Iteration 9577, BCE loss: 57.75729099527398, Acc: 0.8196, Grad norm: 0.03732490091811884\n",
      "Iteration 9578, BCE loss: 57.757287959460406, Acc: 0.8196, Grad norm: 0.034885287222254716\n",
      "Iteration 9579, BCE loss: 57.75731114492793, Acc: 0.8196, Grad norm: 0.047076643489854963\n",
      "Iteration 9580, BCE loss: 57.757306004882224, Acc: 0.8196, Grad norm: 0.04259833966286824\n",
      "Iteration 9581, BCE loss: 57.75732562441818, Acc: 0.8196, Grad norm: 0.05359819732864985\n",
      "Iteration 9582, BCE loss: 57.75730873531316, Acc: 0.8196, Grad norm: 0.045744759224368464\n",
      "Iteration 9583, BCE loss: 57.757312311354696, Acc: 0.8196, Grad norm: 0.04644519045092433\n",
      "Iteration 9584, BCE loss: 57.757304336250144, Acc: 0.8196, Grad norm: 0.042264413684349224\n",
      "Iteration 9585, BCE loss: 57.75730908951083, Acc: 0.8196, Grad norm: 0.04490121933632956\n",
      "Iteration 9586, BCE loss: 57.7573213575735, Acc: 0.8196, Grad norm: 0.049288359366974344\n",
      "Iteration 9587, BCE loss: 57.75731156586022, Acc: 0.8195, Grad norm: 0.045060585524682104\n",
      "Iteration 9588, BCE loss: 57.75731884938464, Acc: 0.8195, Grad norm: 0.0494199741973793\n",
      "Iteration 9589, BCE loss: 57.75730235736506, Acc: 0.8196, Grad norm: 0.04122443282455364\n",
      "Iteration 9590, BCE loss: 57.75733025846404, Acc: 0.8196, Grad norm: 0.05611246057167364\n",
      "Iteration 9591, BCE loss: 57.75734298005068, Acc: 0.8195, Grad norm: 0.06050803363969426\n",
      "Iteration 9592, BCE loss: 57.757330144833574, Acc: 0.8195, Grad norm: 0.053941046569361106\n",
      "Iteration 9593, BCE loss: 57.75734639864806, Acc: 0.8195, Grad norm: 0.060807600953565494\n",
      "Iteration 9594, BCE loss: 57.757326231109495, Acc: 0.8195, Grad norm: 0.052225019287569625\n",
      "Iteration 9595, BCE loss: 57.757328434004336, Acc: 0.8195, Grad norm: 0.053225526099677754\n",
      "Iteration 9596, BCE loss: 57.75730237195699, Acc: 0.8195, Grad norm: 0.04148166442502768\n",
      "Iteration 9597, BCE loss: 57.75732192028367, Acc: 0.8196, Grad norm: 0.05029643118831493\n",
      "Iteration 9598, BCE loss: 57.75729583113609, Acc: 0.8195, Grad norm: 0.03836985785161098\n",
      "Iteration 9599, BCE loss: 57.757296361941584, Acc: 0.8195, Grad norm: 0.037879940129913525\n",
      "Iteration 9600, BCE loss: 57.757290005382075, Acc: 0.8195, Grad norm: 0.032629012807669375\n",
      "Iteration 9601, BCE loss: 57.75729500431943, Acc: 0.8196, Grad norm: 0.03483377283629112\n",
      "Iteration 9602, BCE loss: 57.75728599402525, Acc: 0.8195, Grad norm: 0.02998338206349445\n",
      "Iteration 9603, BCE loss: 57.75728737578781, Acc: 0.8196, Grad norm: 0.031571019340853525\n",
      "Iteration 9604, BCE loss: 57.7572819259085, Acc: 0.8195, Grad norm: 0.027145468021079883\n",
      "Iteration 9605, BCE loss: 57.757287521892906, Acc: 0.8195, Grad norm: 0.031238021297709036\n",
      "Iteration 9606, BCE loss: 57.757303041111996, Acc: 0.8195, Grad norm: 0.040392574470707626\n",
      "Iteration 9607, BCE loss: 57.75729822451474, Acc: 0.8195, Grad norm: 0.039315885320895035\n",
      "Iteration 9608, BCE loss: 57.75727801509838, Acc: 0.8195, Grad norm: 0.027798603423301826\n",
      "Iteration 9609, BCE loss: 57.7572846052574, Acc: 0.8195, Grad norm: 0.033110716618131676\n",
      "Iteration 9610, BCE loss: 57.75728710040055, Acc: 0.8195, Grad norm: 0.03467941792473434\n",
      "Iteration 9611, BCE loss: 57.757288148596665, Acc: 0.8195, Grad norm: 0.0357306155622005\n",
      "Iteration 9612, BCE loss: 57.75727992252099, Acc: 0.8195, Grad norm: 0.029467116779203755\n",
      "Iteration 9613, BCE loss: 57.757285514418065, Acc: 0.8195, Grad norm: 0.03489686210408116\n",
      "Iteration 9614, BCE loss: 57.757279491860075, Acc: 0.8195, Grad norm: 0.030874864516399293\n",
      "Iteration 9615, BCE loss: 57.75728710987672, Acc: 0.8195, Grad norm: 0.03560046741453329\n",
      "Iteration 9616, BCE loss: 57.75729918988573, Acc: 0.8195, Grad norm: 0.041163252880387635\n",
      "Iteration 9617, BCE loss: 57.75731829926439, Acc: 0.8195, Grad norm: 0.049763653010234034\n",
      "Iteration 9618, BCE loss: 57.75729473115024, Acc: 0.8195, Grad norm: 0.04023888653687614\n",
      "Iteration 9619, BCE loss: 57.75730018787715, Acc: 0.8195, Grad norm: 0.04360229644484088\n",
      "Iteration 9620, BCE loss: 57.75731625834128, Acc: 0.8195, Grad norm: 0.051019091341116256\n",
      "Iteration 9621, BCE loss: 57.757318263159064, Acc: 0.8195, Grad norm: 0.051393977802187914\n",
      "Iteration 9622, BCE loss: 57.75732152804995, Acc: 0.8195, Grad norm: 0.05119815948573989\n",
      "Iteration 9623, BCE loss: 57.757325924881215, Acc: 0.8195, Grad norm: 0.05273831459385183\n",
      "Iteration 9624, BCE loss: 57.75730463727126, Acc: 0.8195, Grad norm: 0.04100390170375418\n",
      "Iteration 9625, BCE loss: 57.75731017984428, Acc: 0.8195, Grad norm: 0.04654244079898395\n",
      "Iteration 9626, BCE loss: 57.757305806416895, Acc: 0.8195, Grad norm: 0.043914737278571175\n",
      "Iteration 9627, BCE loss: 57.75729790554946, Acc: 0.8195, Grad norm: 0.04030426759211334\n",
      "Iteration 9628, BCE loss: 57.75730149339359, Acc: 0.8195, Grad norm: 0.04194096188747513\n",
      "Iteration 9629, BCE loss: 57.75732596745388, Acc: 0.8195, Grad norm: 0.05255489406606352\n",
      "Iteration 9630, BCE loss: 57.757368415385386, Acc: 0.8195, Grad norm: 0.06762871756686696\n",
      "Iteration 9631, BCE loss: 57.757381893943034, Acc: 0.8196, Grad norm: 0.07288622723978948\n",
      "Iteration 9632, BCE loss: 57.757385663105296, Acc: 0.8196, Grad norm: 0.07385230599465852\n",
      "Iteration 9633, BCE loss: 57.75739560643631, Acc: 0.8196, Grad norm: 0.07793526209459585\n",
      "Iteration 9634, BCE loss: 57.75735052413563, Acc: 0.8196, Grad norm: 0.06219535544462804\n",
      "Iteration 9635, BCE loss: 57.757376590637804, Acc: 0.8196, Grad norm: 0.0712065651411821\n",
      "Iteration 9636, BCE loss: 57.75739592196362, Acc: 0.8196, Grad norm: 0.07706258669930736\n",
      "Iteration 9637, BCE loss: 57.75736785730297, Acc: 0.8196, Grad norm: 0.0680719657203955\n",
      "Iteration 9638, BCE loss: 57.7573615733073, Acc: 0.8196, Grad norm: 0.06428908851707686\n",
      "Iteration 9639, BCE loss: 57.757394709745995, Acc: 0.8196, Grad norm: 0.07525530631218363\n",
      "Iteration 9640, BCE loss: 57.75740634835405, Acc: 0.8196, Grad norm: 0.07880401106127272\n",
      "Iteration 9641, BCE loss: 57.75735949472996, Acc: 0.8196, Grad norm: 0.06360251493550449\n",
      "Iteration 9642, BCE loss: 57.757362261186145, Acc: 0.8196, Grad norm: 0.06401455293383407\n",
      "Iteration 9643, BCE loss: 57.75734934596156, Acc: 0.8196, Grad norm: 0.06067604416870767\n",
      "Iteration 9644, BCE loss: 57.757347516333496, Acc: 0.8196, Grad norm: 0.06054235602656412\n",
      "Iteration 9645, BCE loss: 57.757335548845774, Acc: 0.8196, Grad norm: 0.057366125582171856\n",
      "Iteration 9646, BCE loss: 57.7573439767842, Acc: 0.8196, Grad norm: 0.059033883241959434\n",
      "Iteration 9647, BCE loss: 57.757313644653124, Acc: 0.8196, Grad norm: 0.048236725766094515\n",
      "Iteration 9648, BCE loss: 57.75731378523575, Acc: 0.8196, Grad norm: 0.0479856871495503\n",
      "Iteration 9649, BCE loss: 57.75730773888233, Acc: 0.8196, Grad norm: 0.04495304286017042\n",
      "Iteration 9650, BCE loss: 57.75731192072088, Acc: 0.8196, Grad norm: 0.045687915962225926\n",
      "Iteration 9651, BCE loss: 57.75732925911487, Acc: 0.8195, Grad norm: 0.05497137012133307\n",
      "Iteration 9652, BCE loss: 57.75733441297943, Acc: 0.8195, Grad norm: 0.05844470991170276\n",
      "Iteration 9653, BCE loss: 57.75734792740758, Acc: 0.8195, Grad norm: 0.06356510840438745\n",
      "Iteration 9654, BCE loss: 57.7573228768767, Acc: 0.8195, Grad norm: 0.05459228565916034\n",
      "Iteration 9655, BCE loss: 57.75731136909984, Acc: 0.8195, Grad norm: 0.04897706746037601\n",
      "Iteration 9656, BCE loss: 57.75735076376951, Acc: 0.8195, Grad norm: 0.06515921000159809\n",
      "Iteration 9657, BCE loss: 57.75733430384752, Acc: 0.8195, Grad norm: 0.05947418923021076\n",
      "Iteration 9658, BCE loss: 57.757329868497514, Acc: 0.8195, Grad norm: 0.057606088322588116\n",
      "Iteration 9659, BCE loss: 57.757282735724814, Acc: 0.8195, Grad norm: 0.03429759067748364\n",
      "Iteration 9660, BCE loss: 57.757267692697, Acc: 0.8195, Grad norm: 0.02108407861903174\n",
      "Iteration 9661, BCE loss: 57.75726533349365, Acc: 0.8195, Grad norm: 0.01888972832255209\n",
      "Iteration 9662, BCE loss: 57.75726951378206, Acc: 0.8195, Grad norm: 0.022349003202914483\n",
      "Iteration 9663, BCE loss: 57.757280893107534, Acc: 0.8195, Grad norm: 0.029870998020274913\n",
      "Iteration 9664, BCE loss: 57.757294132524486, Acc: 0.8196, Grad norm: 0.03909980646222487\n",
      "Iteration 9665, BCE loss: 57.757289983922334, Acc: 0.8195, Grad norm: 0.036461317828248796\n",
      "Iteration 9666, BCE loss: 57.75731039558049, Acc: 0.8196, Grad norm: 0.0477894209299257\n",
      "Iteration 9667, BCE loss: 57.75729550547595, Acc: 0.8196, Grad norm: 0.040330023050248125\n",
      "Iteration 9668, BCE loss: 57.75730417064455, Acc: 0.8195, Grad norm: 0.04376562248888552\n",
      "Iteration 9669, BCE loss: 57.7572999139831, Acc: 0.8195, Grad norm: 0.04142051664184125\n",
      "Iteration 9670, BCE loss: 57.75728880126464, Acc: 0.8195, Grad norm: 0.03382095845957163\n",
      "Iteration 9671, BCE loss: 57.75727611386384, Acc: 0.8195, Grad norm: 0.025833261808427294\n",
      "Iteration 9672, BCE loss: 57.75728079961736, Acc: 0.8195, Grad norm: 0.028734188229816035\n",
      "Iteration 9673, BCE loss: 57.7572780080837, Acc: 0.8196, Grad norm: 0.028102652004885993\n",
      "Iteration 9674, BCE loss: 57.75727928857626, Acc: 0.8195, Grad norm: 0.03054369537295296\n",
      "Iteration 9675, BCE loss: 57.75726485430472, Acc: 0.8195, Grad norm: 0.01801194547652012\n",
      "Iteration 9676, BCE loss: 57.75728544058833, Acc: 0.8195, Grad norm: 0.03629403991604299\n",
      "Iteration 9677, BCE loss: 57.75727714933785, Acc: 0.8196, Grad norm: 0.029978908144574497\n",
      "Iteration 9678, BCE loss: 57.75728245958295, Acc: 0.8195, Grad norm: 0.03402574180897712\n",
      "Iteration 9679, BCE loss: 57.757304710598476, Acc: 0.8195, Grad norm: 0.04658302677054577\n",
      "Iteration 9680, BCE loss: 57.75730307904756, Acc: 0.8195, Grad norm: 0.04450074641265567\n",
      "Iteration 9681, BCE loss: 57.75729397904964, Acc: 0.8195, Grad norm: 0.038651270135028434\n",
      "Iteration 9682, BCE loss: 57.75729074608649, Acc: 0.8195, Grad norm: 0.03725295123144456\n",
      "Iteration 9683, BCE loss: 57.757298154591794, Acc: 0.8195, Grad norm: 0.04256116443673031\n",
      "Iteration 9684, BCE loss: 57.75730831049722, Acc: 0.8195, Grad norm: 0.04830511052041745\n",
      "Iteration 9685, BCE loss: 57.75732828477858, Acc: 0.8195, Grad norm: 0.05686769468385808\n",
      "Iteration 9686, BCE loss: 57.75733159796947, Acc: 0.8195, Grad norm: 0.0589652910180142\n",
      "Iteration 9687, BCE loss: 57.757384995262925, Acc: 0.8195, Grad norm: 0.07664537020216597\n",
      "Iteration 9688, BCE loss: 57.75738138564964, Acc: 0.8195, Grad norm: 0.07576906911963079\n",
      "Iteration 9689, BCE loss: 57.757384388412575, Acc: 0.8195, Grad norm: 0.0765014279310443\n",
      "Iteration 9690, BCE loss: 57.75743689317579, Acc: 0.8195, Grad norm: 0.09145017351867415\n",
      "Iteration 9691, BCE loss: 57.757406176838785, Acc: 0.8195, Grad norm: 0.08335193132379529\n",
      "Iteration 9692, BCE loss: 57.75738904094516, Acc: 0.8195, Grad norm: 0.07839783224444336\n",
      "Iteration 9693, BCE loss: 57.75739643490104, Acc: 0.8195, Grad norm: 0.08069823706738878\n",
      "Iteration 9694, BCE loss: 57.75740057104312, Acc: 0.8195, Grad norm: 0.08187892273269139\n",
      "Iteration 9695, BCE loss: 57.757378792489334, Acc: 0.8195, Grad norm: 0.07514942742498686\n",
      "Iteration 9696, BCE loss: 57.75741278514765, Acc: 0.8195, Grad norm: 0.0848311042535105\n",
      "Iteration 9697, BCE loss: 57.75743612838025, Acc: 0.8195, Grad norm: 0.09060447052131575\n",
      "Iteration 9698, BCE loss: 57.757465794357216, Acc: 0.8195, Grad norm: 0.09770402347801128\n",
      "Iteration 9699, BCE loss: 57.757458786329295, Acc: 0.8195, Grad norm: 0.09626569233253292\n",
      "Iteration 9700, BCE loss: 57.75742792680745, Acc: 0.8195, Grad norm: 0.08758381690514089\n",
      "Iteration 9701, BCE loss: 57.75735467265798, Acc: 0.8195, Grad norm: 0.06449696813829112\n",
      "Iteration 9702, BCE loss: 57.75735334792559, Acc: 0.8196, Grad norm: 0.06312283810966667\n",
      "Iteration 9703, BCE loss: 57.75734543209903, Acc: 0.8196, Grad norm: 0.06037004797352776\n",
      "Iteration 9704, BCE loss: 57.757351117841935, Acc: 0.8196, Grad norm: 0.060600629651132415\n",
      "Iteration 9705, BCE loss: 57.7573340126021, Acc: 0.8196, Grad norm: 0.054352387487829286\n",
      "Iteration 9706, BCE loss: 57.757355543290075, Acc: 0.8196, Grad norm: 0.0644606614218585\n",
      "Iteration 9707, BCE loss: 57.75734757769002, Acc: 0.8196, Grad norm: 0.06168767351545422\n",
      "Iteration 9708, BCE loss: 57.757340289258174, Acc: 0.8196, Grad norm: 0.059185210811174095\n",
      "Iteration 9709, BCE loss: 57.757305331014024, Acc: 0.8196, Grad norm: 0.04385038044027991\n",
      "Iteration 9710, BCE loss: 57.757281109540244, Acc: 0.8196, Grad norm: 0.02792624188027627\n",
      "Iteration 9711, BCE loss: 57.75727838963715, Acc: 0.8196, Grad norm: 0.028577037812145056\n",
      "Iteration 9712, BCE loss: 57.75728358100203, Acc: 0.8196, Grad norm: 0.03268710704845519\n",
      "Iteration 9713, BCE loss: 57.757297596744806, Acc: 0.8196, Grad norm: 0.04017839835074859\n",
      "Iteration 9714, BCE loss: 57.75730907686206, Acc: 0.8196, Grad norm: 0.04514136973358654\n",
      "Iteration 9715, BCE loss: 57.75731496767882, Acc: 0.8196, Grad norm: 0.049722640127843394\n",
      "Iteration 9716, BCE loss: 57.75730915452068, Acc: 0.8195, Grad norm: 0.04420949910802801\n",
      "Iteration 9717, BCE loss: 57.757309817150045, Acc: 0.8196, Grad norm: 0.0465637472819876\n",
      "Iteration 9718, BCE loss: 57.75731215409712, Acc: 0.8196, Grad norm: 0.04732862092249804\n",
      "Iteration 9719, BCE loss: 57.75732518561877, Acc: 0.8195, Grad norm: 0.05308525993235484\n",
      "Iteration 9720, BCE loss: 57.7573342632346, Acc: 0.8196, Grad norm: 0.05613818598761447\n",
      "Iteration 9721, BCE loss: 57.75734641203334, Acc: 0.8195, Grad norm: 0.059258080757962614\n",
      "Iteration 9722, BCE loss: 57.757311714263594, Acc: 0.8195, Grad norm: 0.044711727736425344\n",
      "Iteration 9723, BCE loss: 57.75731544124817, Acc: 0.8195, Grad norm: 0.04866419880529294\n",
      "Iteration 9724, BCE loss: 57.75732697997933, Acc: 0.8195, Grad norm: 0.05356487322760764\n",
      "Iteration 9725, BCE loss: 57.75730147026027, Acc: 0.8195, Grad norm: 0.04070393103806127\n",
      "Iteration 9726, BCE loss: 57.75730506052196, Acc: 0.8195, Grad norm: 0.041042000892178486\n",
      "Iteration 9727, BCE loss: 57.75731150250466, Acc: 0.8195, Grad norm: 0.042381317797846874\n",
      "Iteration 9728, BCE loss: 57.75733049005616, Acc: 0.8196, Grad norm: 0.05392491809317666\n",
      "Iteration 9729, BCE loss: 57.75730732788402, Acc: 0.8196, Grad norm: 0.04573584747394731\n",
      "Iteration 9730, BCE loss: 57.75729284024398, Acc: 0.8196, Grad norm: 0.03646615308135321\n",
      "Iteration 9731, BCE loss: 57.75730071954512, Acc: 0.8196, Grad norm: 0.042167590429795614\n",
      "Iteration 9732, BCE loss: 57.75728708837302, Acc: 0.8196, Grad norm: 0.034804886848500256\n",
      "Iteration 9733, BCE loss: 57.757300053794765, Acc: 0.8195, Grad norm: 0.041074316194338545\n",
      "Iteration 9734, BCE loss: 57.757282741836796, Acc: 0.8195, Grad norm: 0.03009727029430986\n",
      "Iteration 9735, BCE loss: 57.757282925271454, Acc: 0.8195, Grad norm: 0.031516165018066325\n",
      "Iteration 9736, BCE loss: 57.75729213157753, Acc: 0.8195, Grad norm: 0.036574623429552075\n",
      "Iteration 9737, BCE loss: 57.75727958599087, Acc: 0.8195, Grad norm: 0.02779967176539726\n",
      "Iteration 9738, BCE loss: 57.757279588508645, Acc: 0.8195, Grad norm: 0.027870451057132147\n",
      "Iteration 9739, BCE loss: 57.757280444355615, Acc: 0.8195, Grad norm: 0.02809629422940781\n",
      "Iteration 9740, BCE loss: 57.757282482058656, Acc: 0.8195, Grad norm: 0.030655889432070926\n",
      "Iteration 9741, BCE loss: 57.757293681755925, Acc: 0.8195, Grad norm: 0.037200147242305207\n",
      "Iteration 9742, BCE loss: 57.75730248669683, Acc: 0.8195, Grad norm: 0.040687946862927905\n",
      "Iteration 9743, BCE loss: 57.75731289011014, Acc: 0.8195, Grad norm: 0.044487222397846526\n",
      "Iteration 9744, BCE loss: 57.75730811973054, Acc: 0.8195, Grad norm: 0.042965536138338124\n",
      "Iteration 9745, BCE loss: 57.75733107462672, Acc: 0.8195, Grad norm: 0.053889965832377706\n",
      "Iteration 9746, BCE loss: 57.75734657411199, Acc: 0.8195, Grad norm: 0.05997806011528448\n",
      "Iteration 9747, BCE loss: 57.75740514207626, Acc: 0.8195, Grad norm: 0.0788518169245115\n",
      "Iteration 9748, BCE loss: 57.75737902476435, Acc: 0.8195, Grad norm: 0.0706073052195244\n",
      "Iteration 9749, BCE loss: 57.75742214621136, Acc: 0.8195, Grad norm: 0.08229286570855134\n",
      "Iteration 9750, BCE loss: 57.75738702475497, Acc: 0.8195, Grad norm: 0.07166585636711376\n",
      "Iteration 9751, BCE loss: 57.75738143008488, Acc: 0.8195, Grad norm: 0.06936046222425542\n",
      "Iteration 9752, BCE loss: 57.757388531591744, Acc: 0.8195, Grad norm: 0.07242346795286254\n",
      "Iteration 9753, BCE loss: 57.757346051363825, Acc: 0.8195, Grad norm: 0.056612794963408916\n",
      "Iteration 9754, BCE loss: 57.757368862464446, Acc: 0.8195, Grad norm: 0.06542904676936705\n",
      "Iteration 9755, BCE loss: 57.75736286843728, Acc: 0.8195, Grad norm: 0.0630258484579795\n",
      "Iteration 9756, BCE loss: 57.757338880514865, Acc: 0.8195, Grad norm: 0.05671426639464568\n",
      "Iteration 9757, BCE loss: 57.75734909541673, Acc: 0.8195, Grad norm: 0.05877123150994094\n",
      "Iteration 9758, BCE loss: 57.75735100432057, Acc: 0.8195, Grad norm: 0.058835578469117134\n",
      "Iteration 9759, BCE loss: 57.757345395179925, Acc: 0.8195, Grad norm: 0.05653869041426452\n",
      "Iteration 9760, BCE loss: 57.757381643180786, Acc: 0.8195, Grad norm: 0.06751720106346322\n",
      "Iteration 9761, BCE loss: 57.75738952091878, Acc: 0.8195, Grad norm: 0.07048250512652733\n",
      "Iteration 9762, BCE loss: 57.75735277581525, Acc: 0.8195, Grad norm: 0.05853049319800603\n",
      "Iteration 9763, BCE loss: 57.75736190138854, Acc: 0.8195, Grad norm: 0.06287376048357224\n",
      "Iteration 9764, BCE loss: 57.75741171862973, Acc: 0.8195, Grad norm: 0.07907196578849565\n",
      "Iteration 9765, BCE loss: 57.757416329141925, Acc: 0.8195, Grad norm: 0.0799155719030208\n",
      "Iteration 9766, BCE loss: 57.7574071895085, Acc: 0.8195, Grad norm: 0.07683156047698664\n",
      "Iteration 9767, BCE loss: 57.75743667512042, Acc: 0.8195, Grad norm: 0.08535754912871962\n",
      "Iteration 9768, BCE loss: 57.7573924596698, Acc: 0.8195, Grad norm: 0.07329164441811374\n",
      "Iteration 9769, BCE loss: 57.75735709741251, Acc: 0.8195, Grad norm: 0.06335482550504212\n",
      "Iteration 9770, BCE loss: 57.75737184043038, Acc: 0.8195, Grad norm: 0.06875840434479169\n",
      "Iteration 9771, BCE loss: 57.75739315771621, Acc: 0.8195, Grad norm: 0.07580034512298725\n",
      "Iteration 9772, BCE loss: 57.75736508938999, Acc: 0.8195, Grad norm: 0.06744570178707637\n",
      "Iteration 9773, BCE loss: 57.757324444365445, Acc: 0.8195, Grad norm: 0.05212734569081105\n",
      "Iteration 9774, BCE loss: 57.75729891325868, Acc: 0.8195, Grad norm: 0.03951507267980316\n",
      "Iteration 9775, BCE loss: 57.75732000992613, Acc: 0.8195, Grad norm: 0.05026858979338435\n",
      "Iteration 9776, BCE loss: 57.75734384367139, Acc: 0.8195, Grad norm: 0.06044251615397929\n",
      "Iteration 9777, BCE loss: 57.75733747786704, Acc: 0.8195, Grad norm: 0.058284191448260796\n",
      "Iteration 9778, BCE loss: 57.75732977028958, Acc: 0.8195, Grad norm: 0.055106285733041445\n",
      "Iteration 9779, BCE loss: 57.757336678316946, Acc: 0.8195, Grad norm: 0.05890742700539156\n",
      "Iteration 9780, BCE loss: 57.75730590921138, Acc: 0.8195, Grad norm: 0.04570192844894163\n",
      "Iteration 9781, BCE loss: 57.757304696255645, Acc: 0.8195, Grad norm: 0.04383740141773718\n",
      "Iteration 9782, BCE loss: 57.75729392683185, Acc: 0.8195, Grad norm: 0.0373936119051098\n",
      "Iteration 9783, BCE loss: 57.75728711373954, Acc: 0.8195, Grad norm: 0.034464084133102126\n",
      "Iteration 9784, BCE loss: 57.757303436298145, Acc: 0.8196, Grad norm: 0.04308713319192251\n",
      "Iteration 9785, BCE loss: 57.75731481832613, Acc: 0.8195, Grad norm: 0.04818358002219427\n",
      "Iteration 9786, BCE loss: 57.7573050274875, Acc: 0.8195, Grad norm: 0.043503472256815244\n",
      "Iteration 9787, BCE loss: 57.757287891078136, Acc: 0.8195, Grad norm: 0.03407043054481292\n",
      "Iteration 9788, BCE loss: 57.75729466536612, Acc: 0.8195, Grad norm: 0.038835596245846773\n",
      "Iteration 9789, BCE loss: 57.757282760656, Acc: 0.8195, Grad norm: 0.03229171435468707\n",
      "Iteration 9790, BCE loss: 57.75728586399494, Acc: 0.8195, Grad norm: 0.034725750493088126\n",
      "Iteration 9791, BCE loss: 57.757273489126135, Acc: 0.8195, Grad norm: 0.02546173268334317\n",
      "Iteration 9792, BCE loss: 57.75727008399941, Acc: 0.8195, Grad norm: 0.02206011468219717\n",
      "Iteration 9793, BCE loss: 57.75728780358179, Acc: 0.8195, Grad norm: 0.03561753663967967\n",
      "Iteration 9794, BCE loss: 57.75730580995479, Acc: 0.8195, Grad norm: 0.04626105656738808\n",
      "Iteration 9795, BCE loss: 57.75729098112902, Acc: 0.8195, Grad norm: 0.0382566406471203\n",
      "Iteration 9796, BCE loss: 57.757296049315364, Acc: 0.8196, Grad norm: 0.04167093896074814\n",
      "Iteration 9797, BCE loss: 57.75730302329562, Acc: 0.8196, Grad norm: 0.044960740945285196\n",
      "Iteration 9798, BCE loss: 57.75728996559772, Acc: 0.8196, Grad norm: 0.03679483024081028\n",
      "Iteration 9799, BCE loss: 57.757284933396534, Acc: 0.8196, Grad norm: 0.033315794136719507\n",
      "Iteration 9800, BCE loss: 57.75728422537736, Acc: 0.8195, Grad norm: 0.03172173970454079\n",
      "Iteration 9801, BCE loss: 57.75727232740718, Acc: 0.8195, Grad norm: 0.023967661118601626\n",
      "Iteration 9802, BCE loss: 57.75728203841939, Acc: 0.8195, Grad norm: 0.03149755799866216\n",
      "Iteration 9803, BCE loss: 57.75731241818501, Acc: 0.8195, Grad norm: 0.0493084649776442\n",
      "Iteration 9804, BCE loss: 57.75728285013237, Acc: 0.8195, Grad norm: 0.03380168677380731\n",
      "Iteration 9805, BCE loss: 57.75728644936852, Acc: 0.8196, Grad norm: 0.03358248330775397\n",
      "Iteration 9806, BCE loss: 57.75730051440118, Acc: 0.8196, Grad norm: 0.04098068168426646\n",
      "Iteration 9807, BCE loss: 57.7573410378988, Acc: 0.8196, Grad norm: 0.05855051827419503\n",
      "Iteration 9808, BCE loss: 57.757307116506325, Acc: 0.8196, Grad norm: 0.044756936750612285\n",
      "Iteration 9809, BCE loss: 57.757316067779094, Acc: 0.8196, Grad norm: 0.050235371431121466\n",
      "Iteration 9810, BCE loss: 57.757320025356584, Acc: 0.8196, Grad norm: 0.05131649560035538\n",
      "Iteration 9811, BCE loss: 57.75731416118549, Acc: 0.8196, Grad norm: 0.0492853790368154\n",
      "Iteration 9812, BCE loss: 57.757306092307125, Acc: 0.8196, Grad norm: 0.045283997868945394\n",
      "Iteration 9813, BCE loss: 57.757305392607705, Acc: 0.8196, Grad norm: 0.04374043173244234\n",
      "Iteration 9814, BCE loss: 57.757294824613794, Acc: 0.8196, Grad norm: 0.03831319592238544\n",
      "Iteration 9815, BCE loss: 57.75730738945701, Acc: 0.8196, Grad norm: 0.04360971335971413\n",
      "Iteration 9816, BCE loss: 57.757330369981325, Acc: 0.8196, Grad norm: 0.053126187761908875\n",
      "Iteration 9817, BCE loss: 57.75735710274469, Acc: 0.8196, Grad norm: 0.06239416176616953\n",
      "Iteration 9818, BCE loss: 57.75733913886509, Acc: 0.8196, Grad norm: 0.05480786488444973\n",
      "Iteration 9819, BCE loss: 57.75733652234014, Acc: 0.8196, Grad norm: 0.0564491561831433\n",
      "Iteration 9820, BCE loss: 57.75732770232801, Acc: 0.8196, Grad norm: 0.05307822599522161\n",
      "Iteration 9821, BCE loss: 57.757373704908936, Acc: 0.8196, Grad norm: 0.06979558396884758\n",
      "Iteration 9822, BCE loss: 57.75747679375589, Acc: 0.8196, Grad norm: 0.09840457774870144\n",
      "Iteration 9823, BCE loss: 57.7574157132371, Acc: 0.8196, Grad norm: 0.08078414019218293\n",
      "Iteration 9824, BCE loss: 57.75745288768947, Acc: 0.8196, Grad norm: 0.09211340526483927\n",
      "Iteration 9825, BCE loss: 57.75739497648196, Acc: 0.8196, Grad norm: 0.07618480793481007\n",
      "Iteration 9826, BCE loss: 57.757405212514726, Acc: 0.8196, Grad norm: 0.07941353504777209\n",
      "Iteration 9827, BCE loss: 57.75737084231307, Acc: 0.8196, Grad norm: 0.06863054205792078\n",
      "Iteration 9828, BCE loss: 57.75736047271406, Acc: 0.8196, Grad norm: 0.06566804319230762\n",
      "Iteration 9829, BCE loss: 57.75733531527034, Acc: 0.8196, Grad norm: 0.05501973322620779\n",
      "Iteration 9830, BCE loss: 57.75733180462048, Acc: 0.8196, Grad norm: 0.05298264027461503\n",
      "Iteration 9831, BCE loss: 57.75732868299441, Acc: 0.8195, Grad norm: 0.05166809052924638\n",
      "Iteration 9832, BCE loss: 57.75735956716517, Acc: 0.8195, Grad norm: 0.06486959778861495\n",
      "Iteration 9833, BCE loss: 57.75738554075535, Acc: 0.8195, Grad norm: 0.07332846620782137\n",
      "Iteration 9834, BCE loss: 57.75735535431117, Acc: 0.8196, Grad norm: 0.06357867772824391\n",
      "Iteration 9835, BCE loss: 57.757368629468196, Acc: 0.8196, Grad norm: 0.067878540340396\n",
      "Iteration 9836, BCE loss: 57.757380911389205, Acc: 0.8196, Grad norm: 0.07350356525431917\n",
      "Iteration 9837, BCE loss: 57.75740555868943, Acc: 0.8196, Grad norm: 0.08042433533778404\n",
      "Iteration 9838, BCE loss: 57.75734335201571, Acc: 0.8196, Grad norm: 0.059283474155305105\n",
      "Iteration 9839, BCE loss: 57.757314177854575, Acc: 0.8196, Grad norm: 0.045026690569309386\n",
      "Iteration 9840, BCE loss: 57.75730305361974, Acc: 0.8196, Grad norm: 0.03945937734335307\n",
      "Iteration 9841, BCE loss: 57.75730300956878, Acc: 0.8196, Grad norm: 0.03932274481946261\n",
      "Iteration 9842, BCE loss: 57.75729883236991, Acc: 0.8195, Grad norm: 0.037193117386427256\n",
      "Iteration 9843, BCE loss: 57.7573132696952, Acc: 0.8195, Grad norm: 0.043476245523558645\n",
      "Iteration 9844, BCE loss: 57.75731985364767, Acc: 0.8195, Grad norm: 0.04702649126271059\n",
      "Iteration 9845, BCE loss: 57.75733215827823, Acc: 0.8195, Grad norm: 0.05241488818496591\n",
      "Iteration 9846, BCE loss: 57.757314970590954, Acc: 0.8195, Grad norm: 0.04588874612570527\n",
      "Iteration 9847, BCE loss: 57.75730663262304, Acc: 0.8195, Grad norm: 0.04031207649355198\n",
      "Iteration 9848, BCE loss: 57.75731438976236, Acc: 0.8195, Grad norm: 0.044257565742736815\n",
      "Iteration 9849, BCE loss: 57.75733200439831, Acc: 0.8195, Grad norm: 0.05174699577386545\n",
      "Iteration 9850, BCE loss: 57.75730980849928, Acc: 0.8195, Grad norm: 0.04196813163917552\n",
      "Iteration 9851, BCE loss: 57.75728963194693, Acc: 0.8195, Grad norm: 0.031718856406745426\n",
      "Iteration 9852, BCE loss: 57.75729953394086, Acc: 0.8195, Grad norm: 0.03837758714667013\n",
      "Iteration 9853, BCE loss: 57.757301962871935, Acc: 0.8195, Grad norm: 0.03875280566429379\n",
      "Iteration 9854, BCE loss: 57.75730078450067, Acc: 0.8195, Grad norm: 0.038092803473879805\n",
      "Iteration 9855, BCE loss: 57.757303172957805, Acc: 0.8196, Grad norm: 0.03853066508984252\n",
      "Iteration 9856, BCE loss: 57.757299278827276, Acc: 0.8196, Grad norm: 0.03731676078188436\n",
      "Iteration 9857, BCE loss: 57.757307726276665, Acc: 0.8195, Grad norm: 0.040758079576493765\n",
      "Iteration 9858, BCE loss: 57.75730254870417, Acc: 0.8195, Grad norm: 0.037609275871451985\n",
      "Iteration 9859, BCE loss: 57.7573005240973, Acc: 0.8196, Grad norm: 0.037617586590960005\n",
      "Iteration 9860, BCE loss: 57.75730290336929, Acc: 0.8195, Grad norm: 0.039091844180985776\n",
      "Iteration 9861, BCE loss: 57.75729227307816, Acc: 0.8195, Grad norm: 0.03349448536023356\n",
      "Iteration 9862, BCE loss: 57.757291889519884, Acc: 0.8195, Grad norm: 0.03372021385961409\n",
      "Iteration 9863, BCE loss: 57.75731088522099, Acc: 0.8195, Grad norm: 0.04442133230157686\n",
      "Iteration 9864, BCE loss: 57.75733648213708, Acc: 0.8195, Grad norm: 0.05531330636371503\n",
      "Iteration 9865, BCE loss: 57.75734529425278, Acc: 0.8196, Grad norm: 0.06081142384966259\n",
      "Iteration 9866, BCE loss: 57.75740282987029, Acc: 0.8196, Grad norm: 0.08013334698087267\n",
      "Iteration 9867, BCE loss: 57.75738530126835, Acc: 0.8195, Grad norm: 0.07521949709649095\n",
      "Iteration 9868, BCE loss: 57.757391225329044, Acc: 0.8195, Grad norm: 0.07773210056942254\n",
      "Iteration 9869, BCE loss: 57.757387886097156, Acc: 0.8195, Grad norm: 0.0753040577551213\n",
      "Iteration 9870, BCE loss: 57.75736823819152, Acc: 0.8196, Grad norm: 0.06913038818748277\n",
      "Iteration 9871, BCE loss: 57.75735813342739, Acc: 0.8196, Grad norm: 0.0661319248106808\n",
      "Iteration 9872, BCE loss: 57.75735562693013, Acc: 0.8196, Grad norm: 0.0654071591984489\n",
      "Iteration 9873, BCE loss: 57.7573557128969, Acc: 0.8196, Grad norm: 0.06541717126484131\n",
      "Iteration 9874, BCE loss: 57.75732804555406, Acc: 0.8196, Grad norm: 0.054424070898587425\n",
      "Iteration 9875, BCE loss: 57.757310386963766, Acc: 0.8196, Grad norm: 0.046491319987545465\n",
      "Iteration 9876, BCE loss: 57.7573163806007, Acc: 0.8196, Grad norm: 0.05080787225492099\n",
      "Iteration 9877, BCE loss: 57.757299821171415, Acc: 0.8196, Grad norm: 0.042991860248296945\n",
      "Iteration 9878, BCE loss: 57.757273222803164, Acc: 0.8196, Grad norm: 0.02539469816553917\n",
      "Iteration 9879, BCE loss: 57.75727307667175, Acc: 0.8195, Grad norm: 0.02450275744794236\n",
      "Iteration 9880, BCE loss: 57.75727751110816, Acc: 0.8195, Grad norm: 0.028356347153147627\n",
      "Iteration 9881, BCE loss: 57.7572772871816, Acc: 0.8195, Grad norm: 0.02660798549208494\n",
      "Iteration 9882, BCE loss: 57.75728471441939, Acc: 0.8195, Grad norm: 0.03227899951594836\n",
      "Iteration 9883, BCE loss: 57.75728110274122, Acc: 0.8196, Grad norm: 0.02900338139025299\n",
      "Iteration 9884, BCE loss: 57.75728384857109, Acc: 0.8195, Grad norm: 0.03026458121975425\n",
      "Iteration 9885, BCE loss: 57.75729597946566, Acc: 0.8196, Grad norm: 0.03808479390239792\n",
      "Iteration 9886, BCE loss: 57.7573413326613, Acc: 0.8196, Grad norm: 0.06001297191879169\n",
      "Iteration 9887, BCE loss: 57.75733906940982, Acc: 0.8196, Grad norm: 0.059836465906200065\n",
      "Iteration 9888, BCE loss: 57.75733389133087, Acc: 0.8196, Grad norm: 0.05707436511302475\n",
      "Iteration 9889, BCE loss: 57.75734357588679, Acc: 0.8196, Grad norm: 0.059854642506974184\n",
      "Iteration 9890, BCE loss: 57.757331591220506, Acc: 0.8196, Grad norm: 0.054538214307668274\n",
      "Iteration 9891, BCE loss: 57.75733335632854, Acc: 0.8196, Grad norm: 0.053771210728187004\n",
      "Iteration 9892, BCE loss: 57.757324289621764, Acc: 0.8196, Grad norm: 0.04902948817736612\n",
      "Iteration 9893, BCE loss: 57.75734853701094, Acc: 0.8196, Grad norm: 0.059699036935169404\n",
      "Iteration 9894, BCE loss: 57.75732310999513, Acc: 0.8196, Grad norm: 0.05012491794777657\n",
      "Iteration 9895, BCE loss: 57.75733760228554, Acc: 0.8195, Grad norm: 0.05589126284520792\n",
      "Iteration 9896, BCE loss: 57.757329874192806, Acc: 0.8195, Grad norm: 0.05272699574402154\n",
      "Iteration 9897, BCE loss: 57.757329460182135, Acc: 0.8195, Grad norm: 0.05273869240168257\n",
      "Iteration 9898, BCE loss: 57.75732803995713, Acc: 0.8195, Grad norm: 0.05293396391281542\n",
      "Iteration 9899, BCE loss: 57.7572999950259, Acc: 0.8195, Grad norm: 0.03916486897865\n",
      "Iteration 9900, BCE loss: 57.75731833382576, Acc: 0.8195, Grad norm: 0.04753044918400713\n",
      "Iteration 9901, BCE loss: 57.75733263433523, Acc: 0.8195, Grad norm: 0.054818719836637574\n",
      "Iteration 9902, BCE loss: 57.757332441593945, Acc: 0.8195, Grad norm: 0.05320972939124028\n",
      "Iteration 9903, BCE loss: 57.757318741342985, Acc: 0.8195, Grad norm: 0.04743286552473178\n",
      "Iteration 9904, BCE loss: 57.75734506552353, Acc: 0.8195, Grad norm: 0.0560248268275547\n",
      "Iteration 9905, BCE loss: 57.75735792254694, Acc: 0.8195, Grad norm: 0.05984733674066809\n",
      "Iteration 9906, BCE loss: 57.757356792564096, Acc: 0.8195, Grad norm: 0.05825192103105907\n",
      "Iteration 9907, BCE loss: 57.75736124851206, Acc: 0.8195, Grad norm: 0.05952128902639242\n",
      "Iteration 9908, BCE loss: 57.757342297128986, Acc: 0.8195, Grad norm: 0.054838688388624726\n",
      "Iteration 9909, BCE loss: 57.75734353280528, Acc: 0.8195, Grad norm: 0.05613621259785509\n",
      "Iteration 9910, BCE loss: 57.75734642890555, Acc: 0.8195, Grad norm: 0.059041364326867365\n",
      "Iteration 9911, BCE loss: 57.757332467133566, Acc: 0.8195, Grad norm: 0.050987440242501875\n",
      "Iteration 9912, BCE loss: 57.75731111650721, Acc: 0.8195, Grad norm: 0.043134192819228194\n",
      "Iteration 9913, BCE loss: 57.75732419179066, Acc: 0.8195, Grad norm: 0.049148678918862806\n",
      "Iteration 9914, BCE loss: 57.75734952141005, Acc: 0.8195, Grad norm: 0.06103598597427516\n",
      "Iteration 9915, BCE loss: 57.757341030974, Acc: 0.8195, Grad norm: 0.05815522240256141\n",
      "Iteration 9916, BCE loss: 57.75731926035211, Acc: 0.8195, Grad norm: 0.04838756720792791\n",
      "Iteration 9917, BCE loss: 57.75730000057635, Acc: 0.8195, Grad norm: 0.04054484354834278\n",
      "Iteration 9918, BCE loss: 57.75729653342387, Acc: 0.8195, Grad norm: 0.039447922928162016\n",
      "Iteration 9919, BCE loss: 57.75731495799678, Acc: 0.8196, Grad norm: 0.04967056119366571\n",
      "Iteration 9920, BCE loss: 57.75733888201038, Acc: 0.8196, Grad norm: 0.06071308114252545\n",
      "Iteration 9921, BCE loss: 57.757360581700546, Acc: 0.8196, Grad norm: 0.06927015257245359\n",
      "Iteration 9922, BCE loss: 57.75733785155285, Acc: 0.8196, Grad norm: 0.06039624114355404\n",
      "Iteration 9923, BCE loss: 57.757321814801934, Acc: 0.8196, Grad norm: 0.05273996904662684\n",
      "Iteration 9924, BCE loss: 57.757302185031726, Acc: 0.8196, Grad norm: 0.0431575116813954\n",
      "Iteration 9925, BCE loss: 57.75733465829208, Acc: 0.8196, Grad norm: 0.05665097500143255\n",
      "Iteration 9926, BCE loss: 57.75731129386755, Acc: 0.8196, Grad norm: 0.04593327461100389\n",
      "Iteration 9927, BCE loss: 57.757307339391645, Acc: 0.8196, Grad norm: 0.044695749499055484\n",
      "Iteration 9928, BCE loss: 57.75730432949226, Acc: 0.8196, Grad norm: 0.0430003088640768\n",
      "Iteration 9929, BCE loss: 57.75729163678686, Acc: 0.8196, Grad norm: 0.036848886484529754\n",
      "Iteration 9930, BCE loss: 57.75728623545423, Acc: 0.8196, Grad norm: 0.03323373350024662\n",
      "Iteration 9931, BCE loss: 57.757291164159646, Acc: 0.8196, Grad norm: 0.035801194763566364\n",
      "Iteration 9932, BCE loss: 57.75728840156802, Acc: 0.8196, Grad norm: 0.03324942424407939\n",
      "Iteration 9933, BCE loss: 57.75729402941867, Acc: 0.8196, Grad norm: 0.03749308598949798\n",
      "Iteration 9934, BCE loss: 57.757298953068, Acc: 0.8196, Grad norm: 0.03941569476962494\n",
      "Iteration 9935, BCE loss: 57.75727801163725, Acc: 0.8195, Grad norm: 0.025826086051514974\n",
      "Iteration 9936, BCE loss: 57.7572792723058, Acc: 0.8195, Grad norm: 0.026595559912063103\n",
      "Iteration 9937, BCE loss: 57.757300034106976, Acc: 0.8195, Grad norm: 0.03917260981813722\n",
      "Iteration 9938, BCE loss: 57.75730225128012, Acc: 0.8195, Grad norm: 0.04147356924813055\n",
      "Iteration 9939, BCE loss: 57.75730689746314, Acc: 0.8195, Grad norm: 0.04350109825176639\n",
      "Iteration 9940, BCE loss: 57.757304428537864, Acc: 0.8195, Grad norm: 0.043327282018813806\n",
      "Iteration 9941, BCE loss: 57.75730964023649, Acc: 0.8195, Grad norm: 0.04755932222507924\n",
      "Iteration 9942, BCE loss: 57.75729050600938, Acc: 0.8195, Grad norm: 0.03816346300853959\n",
      "Iteration 9943, BCE loss: 57.75731059436025, Acc: 0.8195, Grad norm: 0.048688918313598326\n",
      "Iteration 9944, BCE loss: 57.75732401100824, Acc: 0.8195, Grad norm: 0.05462963453832092\n",
      "Iteration 9945, BCE loss: 57.75731789591808, Acc: 0.8195, Grad norm: 0.05198193404278578\n",
      "Iteration 9946, BCE loss: 57.757311031986134, Acc: 0.8195, Grad norm: 0.049530273037957984\n",
      "Iteration 9947, BCE loss: 57.75729925322508, Acc: 0.8195, Grad norm: 0.04335720625414555\n",
      "Iteration 9948, BCE loss: 57.75729966841324, Acc: 0.8195, Grad norm: 0.043094457016349674\n",
      "Iteration 9949, BCE loss: 57.75730410628418, Acc: 0.8195, Grad norm: 0.044487742091592584\n",
      "Iteration 9950, BCE loss: 57.75730892052535, Acc: 0.8195, Grad norm: 0.04671494775487253\n",
      "Iteration 9951, BCE loss: 57.757339799867594, Acc: 0.8195, Grad norm: 0.06013460569356712\n",
      "Iteration 9952, BCE loss: 57.75732573589428, Acc: 0.8196, Grad norm: 0.05434677854782361\n",
      "Iteration 9953, BCE loss: 57.757326728250234, Acc: 0.8195, Grad norm: 0.055379081265197416\n",
      "Iteration 9954, BCE loss: 57.75734751335422, Acc: 0.8195, Grad norm: 0.06344212680281505\n",
      "Iteration 9955, BCE loss: 57.757330207886696, Acc: 0.8195, Grad norm: 0.05632222152422611\n",
      "Iteration 9956, BCE loss: 57.75732568348076, Acc: 0.8195, Grad norm: 0.05519546823922966\n",
      "Iteration 9957, BCE loss: 57.75733113160176, Acc: 0.8195, Grad norm: 0.05806688496986734\n",
      "Iteration 9958, BCE loss: 57.75734071585505, Acc: 0.8195, Grad norm: 0.06131467029303339\n",
      "Iteration 9959, BCE loss: 57.75730637114941, Acc: 0.8195, Grad norm: 0.047085006268557064\n",
      "Iteration 9960, BCE loss: 57.75729179552788, Acc: 0.8195, Grad norm: 0.03899453552592216\n",
      "Iteration 9961, BCE loss: 57.757274184997826, Acc: 0.8195, Grad norm: 0.023731200729561537\n",
      "Iteration 9962, BCE loss: 57.757270283041, Acc: 0.8196, Grad norm: 0.020222724831143384\n",
      "Iteration 9963, BCE loss: 57.75728325969271, Acc: 0.8196, Grad norm: 0.030446605222951287\n",
      "Iteration 9964, BCE loss: 57.75727318838857, Acc: 0.8195, Grad norm: 0.024319363623691494\n",
      "Iteration 9965, BCE loss: 57.75726798574347, Acc: 0.8195, Grad norm: 0.019427442105957565\n",
      "Iteration 9966, BCE loss: 57.757282485745456, Acc: 0.8195, Grad norm: 0.031264170005609146\n",
      "Iteration 9967, BCE loss: 57.7572758917127, Acc: 0.8195, Grad norm: 0.027125673016672174\n",
      "Iteration 9968, BCE loss: 57.75728461580624, Acc: 0.8195, Grad norm: 0.03494051610015029\n",
      "Iteration 9969, BCE loss: 57.7572935016566, Acc: 0.8195, Grad norm: 0.04080298643092088\n",
      "Iteration 9970, BCE loss: 57.75730030789954, Acc: 0.8195, Grad norm: 0.04397766589998634\n",
      "Iteration 9971, BCE loss: 57.75728945662734, Acc: 0.8195, Grad norm: 0.037105154287658364\n",
      "Iteration 9972, BCE loss: 57.75727757795552, Acc: 0.8195, Grad norm: 0.028256179394280045\n",
      "Iteration 9973, BCE loss: 57.75728175550712, Acc: 0.8195, Grad norm: 0.030874065008726248\n",
      "Iteration 9974, BCE loss: 57.75728485319625, Acc: 0.8195, Grad norm: 0.032734749558432544\n",
      "Iteration 9975, BCE loss: 57.757274903611524, Acc: 0.8195, Grad norm: 0.02583266930853821\n",
      "Iteration 9976, BCE loss: 57.75728993866771, Acc: 0.8196, Grad norm: 0.03636591595942634\n",
      "Iteration 9977, BCE loss: 57.75730266505509, Acc: 0.8196, Grad norm: 0.04314742569098656\n",
      "Iteration 9978, BCE loss: 57.757271019305826, Acc: 0.8195, Grad norm: 0.022213859854901638\n",
      "Iteration 9979, BCE loss: 57.75728197032096, Acc: 0.8196, Grad norm: 0.03193272741210478\n",
      "Iteration 9980, BCE loss: 57.757284085364326, Acc: 0.8196, Grad norm: 0.033036431872846904\n",
      "Iteration 9981, BCE loss: 57.75731266122574, Acc: 0.8196, Grad norm: 0.04861036891027035\n",
      "Iteration 9982, BCE loss: 57.757309227963326, Acc: 0.8196, Grad norm: 0.045786730631815646\n",
      "Iteration 9983, BCE loss: 57.75731706047796, Acc: 0.8196, Grad norm: 0.05095732426520378\n",
      "Iteration 9984, BCE loss: 57.75730324013614, Acc: 0.8196, Grad norm: 0.04376990010340385\n",
      "Iteration 9985, BCE loss: 57.75730118880432, Acc: 0.8196, Grad norm: 0.042243025581944295\n",
      "Iteration 9986, BCE loss: 57.75728706101921, Acc: 0.8196, Grad norm: 0.0331843746410129\n",
      "Iteration 9987, BCE loss: 57.75729420051509, Acc: 0.8196, Grad norm: 0.035232113592042785\n",
      "Iteration 9988, BCE loss: 57.757294197342944, Acc: 0.8196, Grad norm: 0.034539724956672985\n",
      "Iteration 9989, BCE loss: 57.75729436506545, Acc: 0.8196, Grad norm: 0.035341230289809594\n",
      "Iteration 9990, BCE loss: 57.75729996798077, Acc: 0.8195, Grad norm: 0.036231176025205855\n",
      "Iteration 9991, BCE loss: 57.75730972585968, Acc: 0.8196, Grad norm: 0.0403602698707664\n",
      "Iteration 9992, BCE loss: 57.75733446652307, Acc: 0.8196, Grad norm: 0.0527840812662255\n",
      "Iteration 9993, BCE loss: 57.75735378104006, Acc: 0.8196, Grad norm: 0.06194288446422245\n",
      "Iteration 9994, BCE loss: 57.75731363483641, Acc: 0.8196, Grad norm: 0.04702967554129333\n",
      "Iteration 9995, BCE loss: 57.75729250009648, Acc: 0.8195, Grad norm: 0.03395634614024049\n",
      "Iteration 9996, BCE loss: 57.757293050392974, Acc: 0.8195, Grad norm: 0.03440517386218872\n",
      "Iteration 9997, BCE loss: 57.75731005546406, Acc: 0.8195, Grad norm: 0.04341897154591865\n",
      "Iteration 9998, BCE loss: 57.75732018503152, Acc: 0.8195, Grad norm: 0.050727297147346743\n",
      "Iteration 9999, BCE loss: 57.75730589798673, Acc: 0.8196, Grad norm: 0.044547094485035375\n",
      "Iteration 10000, BCE loss: 57.757344099511265, Acc: 0.8196, Grad norm: 0.060981437209692575\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data()\n",
    "max_iter = 10000\n",
    "tol = 5 * (10 ** -6)\n",
    "gram_maitrx = np.dot(X, X.T) / X.shape[0]\n",
    "eig_values, eig_vectors = np.linalg.eig(gram_maitrx)\n",
    "step_size = max(eig_values).real\n",
    "#step_size = 0.05\n",
    "LR = HandCraftLogisticRegression(X, y)\n",
    "LR.solve(max_iter, step_size, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEGCAYAAACD7ClEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA/9ElEQVR4nO3dd5hU5fXA8e/ZhaX3JlJcQARRKbog2EVUFAPG3o0l2FuM/rDFRBN7i9Ek9pJoYo8kiiiKDZUqXTqo9KUtve2e3x/3znJ35s7Mnb7lfJ5nnp25c+fed3Z27ztvO0dUFWOMMTVTXq4LYIwxJnesEjDGmBrMKgFjjKnBrBIwxpgazCoBY4ypwWrlugCJaNmypRYWFua6GMYYU6VMnjx5jaq28nuuSlUChYWFTJo0KdfFMMaYKkVEfoz2nHUHGWNMDZazSkBE6orIBBGZJiKzROQPuSqLMcbUVLnsDtoBDFTVzSJSG/haREap6nc5LJMxxtQoOasE1IlXsdl9WNu9WQwLY4zJopjdQSIyQESeFpHpIlIsIj+JyIcico2INEn15CKSLyJTgdXAJ6o63mef4SIySUQmFRcXp3pKY4wxHlErAREZBVwOjAYGA22BHsCdQF3gfREZmsrJVbVUVXsD7YF+InKgzz7PqmqRqha1auU7w8kYY0ySYnUHXaiqa8K2bQamuLdHRaRlOgqhqhtEZCxOZTMzHcc0xhgTX9SWgLcCEJF9RGSQe7+eiDQK3ydRItJKRJqGjgkcD8xJ9nixfPrDKv76+YJMHNoYY6q0uFNEReTXwNvAM+6m9sB/0nDutsBYEZkOTMQZE/hfGo4b4fO5xTz/1eJMHNoYY6q0ILODrgH6AeMBVHW+iLRO9cSqOh3ok+pxgsgTKLPkOcYYEyHIYrEdqroz9EBEalHFpnKKCGVlVarIxhiTFUEqgS9E5HagnogcD7wF/DezxUqvPBGsIWCMMZGCVAIjgGJgBnAF8CHONNEqQ6w7yBhjfMUdE1DVMuA591Yl5UkV678yxpgsiVoJiMgMYlw7VbVnRkqUAXki1hIwxhgfsVoCp2StFBkmIti4sDHGRIpaCahq1CQEVU2egFpLwBhjIgRZLNZfRCaKyGYR2SkipSKyMRuFSxdnYDjXpTDGmMonyOygp4BzgflAPZygck9nslDp5kwRtVrAGGPCBcospqoLgHw36udLOIHeqgwbEzDGGH9BwkZsFZECYKqIPASsoIrlJs4T56eqIiK5LYwxxlQiQS7mF7r7XQtsAToAp2eyUOkmOBd+aw0YY0xFQVoCa4Cdqrod+IOI5AN1Mlus9PK2BMBaAsYYExKkJfApUN/zuB4wJjPFyYy8PGsJGGOMnyCVQF1VDSWEx71fP8b+lU5oGMBWDRtjTEVBKoEtInJw6IGIHAJsy1yR0i/PrQWsDjDGmIqCjAncCLwlIstxOtT3As7OZKHSbfuuUsBaAsYYEy5IFNGJItId6OZumququzJbrPR6Ysx8AL7/aQNHdG2Z49IYY0zlESRsxJk44wIzgVOBN7zdQ1XJxu1Vqu4yxpiMCzImcJeqbhKRI4DjgBeAv6V6YhHpICJjRWS2iMwSkRtSPWY81h1kjDEVBakESt2fQ4DnVPUDoCAN594N3KyqPYD+wDUi0iMNx43qlW+WZPLwxhhT5QSpBJaJyDM4g8EfikidgK+LSVVXqOoU9/4m4AegXarHjWXikvWZPLwxxlQ5QS7mZwGjgRNVdQPQHLglnYUQkUKgDzDe57nhIjJJRCYVFxen87TGGFPjBZkdtBV41/N4BU4QubQQkYbAO8CNqhqRp0BVnwWeBSgqKrJOfWOMSaOcRgMVkdo4FcBrqvpuvP2T1bZJ3Uwd2hhjqrScVQLixHR+AfhBVR/L5LlOP7g9sCeQnDHGGEcuWwKH44SpHigiU93byZk4USh2kPUlGWNMRXHHBERkE5HXzxJgEs4Uz0XJnFhVvyZLcZ2tAWCMMf6CxA56AlgKvI5zPT0H6AJMAV4EjslQ2dLHAsgZY4yvIN1BQ1X1GVXdpKob3dk6J6rqG0CzDJcvLawlYIwx/oJUAltF5CwRyXNvZwHb3efsu7UxxlRhQSqB83EGcFe7twuBC0SkHk7e4UrPcssbY4y/IIvFFgG/iPL01+ktTmaIdQgZY4yvIKGk24vIeyKy2r29IyLts1G4dLGWgDHG+AvSHfQSMBLY2739191mjDGmigtSCbRS1ZdUdbd7exloleFypZU1BIwxxl+QSmCtiFwgIvnu7QJgbaYLlk7WHWSMMf6CVAKX4oSTXokTPfQM4JJMFirdxGoBY4zxFWR20I/A0CyUJWOsDjDGGH9RKwER+QsxFoOp6vUZKVEGeKeIfj53Ncd0a53D0hhjTOURqyUwKWulyLAWDfekRJ61fKNVAsYY44paCajqK9ksSCYN6703t749PdfFMMaYSifqwLCIPCciB0Z5roGIXCoi52euaOmT5xkU+HZhlZrYZIwxGRWrO+hp4HcichAwEygG6gJdgcY4YaRfy3gJ08A7Lvz1gjU5K4cxxlQ2sbqDpgJnuYngi4C2wDacdJBzs1O89LAposYY4y/IFNHNwOeZL4oxxphsy2WO4ayxBPPGGOMvp5WAiLzoRiadmeHzZPLwxhhTZQUJJX1mkG1JehkYnKZjGWOMSVCQlsBtAbclTFW/BNal41jGGGMSFytsxEnAyUA7EXnS81RjYHemC5ZJk5aso6iwea6LYYwxORdrdtBynNARQ4HJnu2bgJsyWSgvERkODAfo2LFjWo65vGR7Wo5jjDFVXax1AtOAaSLyuqruymKZwsvxLPAsQFFRUdSAdsYYYxIXZEygn4h8IiLzRGSRiCwWkUUZL1kGTft5Q66LYIwxlULcxWLACzjdP5OB0nSeXET+BRwDtBSRpcDdqvpCOs/hx+IHGWOMI0glUKKqozJxclU9NxPHjWfN5h25OK0xxlQ6QSqBsSLyMPAuUH71VNUpGStVhq3eZJWAMcZAsErgUPdnkWebAgPTXxxjjDHZFCSA3LHZKEi2LV6zhU4tG+S6GMYYk1NBwka0EZEXRGSU+7iHiFyW+aJl1pK1W3JdBGOMybkgU0RfBkYDe7uP5wE3Zqg8xhhjsihIJdBSVd8EygBUdTdpniqaCx9MX4GqrT0zxtRsQSqBLSLSAmcwGBHpD5RktFRZ8PbkpXw4Y2Wui2GMMTkVZHbQb4CRQBcRGQe0As7IaKmyZMO2nbkugjHG5FSQ2UFTRORooBtOzva5uYwllCwRCO/9ESzZjDGmZovaHSQiA92fp+FEEu0G7Af8wt1WpZx+cPtcF8EYYyqdWC2Bo4HPgF/4PKc4K4irDL88w1/OK+a8Q9MTntoYY6qiWKGk73Z/XpK94mTONcfuy5uTllbY9tEsGxg2xtRssTKL/SbWC1X1sfQXJ3P2aeG/OrhwxAc8dEZPzirqkOUSGWNM7sWaItrIvRUBVwHt3NuVwMGZL1r2PPnp/FwXwRhjciJWd9AfAETkS+BgVd3kPv498EFWSpcltmbMGFNTBVks1gbwTqjf6W6rNsqsFjDG1FBBFou9CkwQkffcx6cCr2SsRDmwomQ781dtomubRrkuijHGZFXcloCq/gm4FFjv3i5R1fsyXbBsO/7xL3NdBGOMybogLQFUdbKI/AzUBRCRjqr6U0ZLZowxJuOC5BMYKiLzgcXAF+7PjOQczrWT/vwVqzdtz3UxjDEma4IMDN8L9AfmqWonYBDwXTpOLiKDRWSuiCwQkRHpOGYqflixkf98vyzXxTDGmKwJUgnsUtW1QJ6I5KnqWCrmG06KiOQDTwMnAT2Ac0WkR6rHTVWeWFA5Y0zNEWRMYIOINAS+BF4TkdVAOnIz9gMWqOoiABH5NzAMmJ2GYydt6fptuTy9McZkVZCWwDBgK3AT8BGwEP+gcolqB/zsebzU3VaBiAwXkUkiMqm4uDilEx7UrkncfYo370jpHMYYU5XErATcLpv/qWqZqu5W1VdU9Um3eygrVPVZVS1S1aJWrVqldKwRJ3WPu88H01fwoyWhN8bUEDErAVUtBcpEJP5X6MQtA7xR29q72zKmX6fmgfY7+uHPM1kMY4ypNIKMCWwGZojIJ3jGAlT1+hTPPRHoKiKdcC7+5wDnpXjMmGrnB+n9MsaYmiNIJfAuGUggo6q7ReRaYDSQD7yoqrPSfZ5krd+yk2YNCnJdDGOMyaggOYYzFidIVT8EPszU8VNx98hZPHlun1wXwxhjMirIiuEZIjI97PaViDwuIi2yUch0OrJry0D7jZy2PMMlMcaY3AvSST4KJ3/A+e7tv8AkYCXwcsZKliG/OyX4erQla7ZQVmZhpo0x1VeQSmCQqt6mqjPc2x3A0ar6IFCY2eKlX+N6tQPve8wjn/PYJ/Pi7vfzuq2MX5S1WbPGGJM2QSqBfBHpF3ogIn1xBnIBdmekVJXIc18tirvPkQ+N5exn0xJOyRhjsipIJXA58IKILBaRxcALwOUi0gC4P6OlqwR27C5jUfFmLnt5onUNGWOqnSCzgyYCB4UWjKlqiefpNzNVsExJJpPkwEe/AODRT+Yy9ecN/LJPe844pD2lZcrjAbqLjDGmsgqUVAYiLv410tNjFwIwbsFazjikPV/OL+apsQtyXCpjjElejVtC26hu4Hovrt2lkc2Kkq27WFliiWmMMVVDjasEGtRJXyXg58iHPqP//Z9m9BzGGJMuQRaL1ReRu0TkOfdxVxE5JfNFq/zUZ4Bh4/ZqP2HKGFONBGkJvATsAAa4j5cBf8xYiYwxxmRNkEqgi6o+BOwCUNWtQI3PwTh96QaWb7AsZMaYqi1IB/lOEakHKICIdMFpGdRoQ58aF7FtwerN5fcf+2Qevzl+v2wWyRhjEhakJXA3TlrJDiLyGvApcGtGS5VhSx4YwpIHhqT9uIMe+6L8/pOfzq/w3JOfzueZLxb6vm728o3MXbkp7eUxxph4giwW+0REpgD9cbqBblDVNRkvWTUTikG0omQ7vzqskMKWDcqfO/nJrwAyUjEZY0wsQaeI1gXWAxuBHiJyVOaKVP14ZxG9/M0Srnl9Sg5LY4wxe8RtCYjIg8DZwCygzN2swJcZLFe1sah4M3/47+wK21Rh0/ZdLCreQq8OTXNTMGOMIdjA8KlAN1Wt8YPByQjFHfJSYPirk/l20Vrm3Ds4+4UyxhhXkO6gRUDwIPxVyBvD++fs3N//vB6AsgQi2m3fVco1r03hp7VbM1UsY0wNE7USEJG/iMiTwFZgqog8IyJPhm6pnFREzhSRWSJSJiJFqRwrFYd2zmx2zOv/9X1C+z8dJxjdNwvX8MGMFdw9cmYqxTLGmHKxuoMmuT8nAyPDnks1sP5M4DTgmRSPU6nFylO8fZczvLJ+667ybW9M/Jlrjt036muSCYNtjDGxRG0JqOorqvoK0DR037OtWSonVdUfVHVuKsdItxEndc/aubzLrRcXbwn8unVbdgKws7Qszp7GGBNMkDGBi322/SrN5YhKRIaLyCQRmVRcXJyx8yTSN5+qbbtKy++/Oenn8vsSJxjHQ6OdenPcgsh8xrtLyyqsWDbGmCBijQmcKyL/BTqJyEjPbSywLt6BRWSMiMz0uQ1LpICq+qyqFqlqUatWrRJ5aUIK8rMXVXvxmj3f/r1dRjt2lXH/hz+wbWep38vYHaUF8NX8Yi56cQKDHvuCJWtityze+34pW3dapFNjjCPWmMA3wAqgJfCoZ/smYHq8A6vqoNSKll1d2zTKdRFYuXE7z3y5iPoFtbhhUNeI56O1VS58YUL5/YXFm/n+5/X8sk/7iP1mL9/ITW9MY1jvYv58Tp90FdsYU4VFrQRU9UfgR/aEkK7W/HID5MrjY+Zxw6Cu7NxdRp5AnggiUOpJdF+8aQetGtWJeO2Id2dQvGkHXVs34sB2TSo8N3+1E5/Iuo2MMSE5ySwmIr8UkaU4FcwHIjI6F+XwqjxVgGPxmi3sd+co+t//GZ1v/5CznvmWBgV76uzLX5no+7riTc6avm27Snlr0s8UjviAnbudbqTH3fhFs5ZvdH+WsGO3f9eTMaZmyEkloKrvqWp7Va2jqm1U9cRclKMyu+O9GQCs2exc1CcuWc/A/VuXPz9taUncYzwwag4AJducaaiehgTLNmxjyJNf8/uRs9JVZGNMFRRrYPhT9+eD2SuOCdnkk6by9fE/BX69d6JRaNaReto7G7Y6002//2lDMsUzxlQTsQaG24rIYcBQEfk3YdnEVLVahMKsX5DP4fu2LO8POrRTc8Yvjjv5KeNmLIv/TT8WkcguLrGEcMaYMLEqgd8BdwHtgcfCnlNgYKYKlU2z73ECuH02ZxUA9Qry+fWRnXjuq8W5LFYaRF7wz+3XkQc/msOpvfdm+YbtAMxZuYnFa7bQyZPfwBhTc8RaMfy2qp4EPKSqx4bdqkUF4FW3dj4AzeoXcMeQHjkuTTCbd+zm/g9/8H3uy3mRC+vq1HI+7q8XrOXXr04q337co5+zdL0FpTOmJoo7MKyq94rIUBF5xL2dko2CZduAzi34w9ADuGfYAbkuSmCPjJ7LM18u8n3uz5/OL18FLcDS9VtZUOxMDQ0NNoeUKRzx4Fi+8FQcazbv4JrXp/DI6Lls32UziIyproIklbkf6Ae85m66QUQOU9XbM1qyLBMRLj6sMNfFSMiO3bFjCG3wBKc74sGxcY938YsTylNcPvrxPD6YvgKAglp5XH9c5OI1Y0zVFySpzBCgt6qWAYjIK8D3QLWqBKLp2Lw+P62rnF0l/5oQbLaQxAtK5GNlybby+9usJWBMtRV0nUBTz/0m0XYy1Yd3imqeTSoyptoKUgncD3wvIi+7rYDJwJ8yW6zc6+bGEtq/be5jCqUqmWv4dM8U1USmln4xr5i5KzeVP17rjj+8M3kphSM+YM7KjUmUxhiTKUEGhv8F9AfeBd4BBqjqG5kuWK6NvukoljwwhGb1C8q3fXLTUTksUXaEIpXu9Iw3ROtN+mjmSgpHfMDyDXu6ji5+cQInPvElAKNmrOCQP45h4pJ13PzWNAAm/7g+QyU3xiQjUHeQqq5Q1ZHubWWmC1WZeOPKVYZIo8l47/tlgff986fzI7ZFG1N4y82FEIpFFO7bRU7eg1kpLnwzxmROTmIHVSVJjKlWOu/HSHMZbtTMyDo+2phAeTiKKBFYQ1NU//r5wvJtj348j1EzVgQujzEms6wSqAGm/bwh8L5+YaYFYfGaLfS55+OwYzm1QLQIrKG6YfWmPesS1m3ZyVWvVYuII8ZUC4EqARFpJiI9ReTg0C3TBats7jh5/1wXIWsKR3xQ4fHkn9Zz7COfs37rrvK+ffC2BGBlyXam/LSnvz+RBWZlZcr9o35gRck2VLXCeERVtH1XKWVllS04uTH+giwWuxcnp/BC9nzpqzaxg+Lp3MqJqbNvm4Y5LknueENQhFoKK0q28clsJ97Slf+cHPGa7nd9RL9OzQMdf+6qTTzzxSLGL1rHjGUllJYpM35/Ao3q1i7fp7RM+cN/Z3H5EZ3p2KJ+Uu9j7JzV7NOiPp1bJf9ZfrtwLStKtnHawZGZ28CpALrf9RFXHN2Z206qOV8cTNUVZLHYWUAXVd2Z6cJURpcf0Zme7ZvSv3OLCtv/fE5vBu3fhgPuznk+nKwLbylEMzvKgLHXF/OK+cgdh9i2s7Q8e9riNVvo2Lw+Td3ZWTOWlfDqtz8ybWkJ719zeKDzvz91GXVq5TP4wL0AuORlJxFPaFV0yO3vzWDuyk28c9VhcY957nPfAUStBLbscNZXvDnxZ6sETJUQpDtoJhUXi9UoeXkSUQEADOvdjgZ14tehTerVjrtPdVUap0tkwuJ1XPzihPKVz95B+KFPjaP3PZ9EvGbrjsg8C6VlSuGID3j047kVtt/w76lc+c/JqCorPCugwRnMvvb1KXy7cC2vj/8p5tTV4k07WLVxe8z3Un5c92dedZhRYGqERBaLjRaRkaFbpgtW1QzavzVf3Xqs73Of3nx0lktTOZTGyNtcsnUXZz3zbeBjhS6p8z0D16s2bqdk6y52uWsb/vLZAv7x3Y8Rr73l7ekMuP+zCtu27yrjf9NXlH+zj6Xvn8Zw6H2fBipnaEbU2i01suFsqqAg3UGvAA8CM4CqPWKXQc9f3Nd3uwh0SaEPuiqLNcDb656PI7bFinHkV52ELsw/uDkhAO76z0wu7L9Phf3enrw04rVfzY8Mte173hgVmZ85KzbF38mYSiRIJbBVVZ9M50lF5GHgF8BOnAHnS1R1QzrPkSl1auVViN751pUDKoRJMMmL1YHy/tSKC96meqaqhnfl7C4to1Z+9Ebud4vW8n2cabMjpy3n+n99z12nJJZb4qIXJwTed8LidXTbq1Gl6DL8duFaurZpSMuGdXJdFJNlQSqBr9xw0iOB8gnfKaaX/AS4TVV3uzmMbwP+L4XjZc13tx3Hlp17+qX7Fjanb2GwWTAmtvB+e4BXvlnCV/PXMOaHVRW2n/r0uPL767dW7HrZGacSOOfZ+F1Af3cXuE2Mkmp09vKN9Ni7cYVtwz2JeuLZvquUs575lqJ9mvF2gAHpTDv3ue/o1LIBNw7qyjHdWleKislkR5BKoI/7s79nW0pTRFXV2xfwHXBGssfKtmYNCmjWoCD+ji4bHgxuvSf/QcjdI2fFfd3X89dUeLx1Zyn1C4L8aVe0etN2WjeqC/h3P23xDEqv3LgtohL4eHbFikpVo3Zx7XYHzWeviJxBNX3pBvZr06g82122LF6zhRv+PZXje7ThuYuKsnrubNmyY3egCR01ScyBYRHJB0ZmOL3kpcCoGGUYLiKTRGRScXGwflxTvb367ZIKj99wYxiFFP1xTNyZSX76/Sly8LfYk4XNWwkEiazqNxYRz/IN2xj61Dju+s9MSsuUe/47m5UlwWYmpUu2z5ct3/+0ngPuHs3Hs2pU+LO4YlYCqloKnJvMgUVkjIjM9LkN8+xzB7CbPVnL/MrwrKoWqWpRq1atkilKTjUPazW8ecWAlI9Z0+P7/+79+K2DD5KMTxTK2Rxa8ewdb/DOdgp9wS8tU7bs2M2V/4hcMLdk7RaGvzqJf3z3I6VlGmgVdck2pzU0fWkJExav48Vxi7nl7WlxXpWa8NXNlWF269fz15TP+kqXUMiTcQvWxN6xhgkyRXSciDwlIkcmEjZCVQep6oE+t/cBRORXwCnA+ZroFIwq4IL+HQHKFzuF9OvUnMfP7sUBYV0Jidi3dc2cbZSI96Yk/i0cKM/ZvHjNlojn3pq055giws7dZXS5/UMOuHs0H0X5dvnx7FXc9Z+ZDHv6a7rf9VHE896//O27SvlszmoAtuzcTb5b22/bmb7Mbn7/ap1v/7DCY2FP/od1KUx1/fWrkxj4yOcJv27iknVc8MJ4HvtkXtLn9hPqbpzy04a0HjeW6Us3ULxpR/wdcyhIJdAbOAC4B3jUvT2SyklFZDBwKzBUVStn7sYkfXbz0cz6w4kM692uwvYPrj+Cly9xppH+sk97bhy0n+/rD2yXfOVg9hg7N/muwyMe/Mx3uzc20gOj5rB9d/CL88xlTt9/6NttaD2B91t397s+4uHRzoK3peu3sdFtFewO2LW1eM2W8pbEM18sZNhTX5c/t3nHbl4at5hOt33Iz/HSpYqUr7dYsjayMozmvOe+48IXxpc//mT2Khb5VKbxrHEvmouKI4MZxlNapoyetdK3smvZyJn51DWLIWCGPjWOk/78VdbOl4y4IySq6r8CKjVPAXWAT9yBs+9U9coMnCfrwuPShP7HD9g7WFbOWnmx6+V+nZpzcMdmzFuV+D+ICWbp+shZSlDxG/kPKzZS4jOQ7fXtwrUR25yxijLu/3AOULElEG6lu0o53vjGouLNjJ61igc/co75+Nm9uH/UnAr7HP3Q2PIFbG9NXspvjvf/EgIVuxtvfXs6H15/JAW1nL/LHbtLqVPLf8D6G5/3m4pov5sv5xVTvyCfIp9Zea98s4R7/jebJ87uzal92vm8GurVzuentVsZv3gtZxZ1SGeRfa3ZXMVbAiLSREQeCw3OisijIpJSnmFV3VdVO6hqb/dWLSqARIR3u4689nAePqMnB3dsFvN1jevW4pYTu2WuYCaq8WHTReN9w/Prdpi5rISud4wqD5URy3/cZEAz4iTlOf7xL8srAICb3tgzhnCrO57gXcH8v+mx80uEWiDgBAwct3AN93/4A0+MmUe3Oz/ig+mJj7dM+3kDhSM+YNIS/ym3XqHW0eoo3SgXvTiBM/7uv9o8VIF7W20hodZBnggXvzSBW96entautqoqSHfQi8AmnEByZwEbgZcyWajqINFRjjaN63JmUQduHdyNvoWRFcHVx3QpP25+2Mhw47o25S0XNvvEMYpn7NzVEdtGTltenqXNa2vAC1SslsKbkyLHRhYVO100m3fs9g0GuLB4C997KjDBGSt5YoyTde6a1+MvEfKuFv/ndz+Wr9D2e/9eP6/bWt6imJpAHoyQ5g2c9Q0FPutEQgPgIpT30+9IoEuvugpy9eiiqqd7Hv9BRKZmqDzVTrSZFuHbQ3+0dWvnc2qfdkxcUvGbTO0Yi5+uG9iV5SXbeGncklSKarJgy46KF51tu0q5/l/f++7rXUPw0EdzGNi9Ndt3lVGybRdDerZNuSze3NCxeDPDBbFh605uf29G+eM7/zOT357gdD+NmrmSW07s7vu6Yx/53HdAPhGhbivv/5eqcvt7M9m43WnhvPrtnvhSQcdbYtm+q5SBj3zOfacdxDHdWqd8vGwL0hLYJiJHhB6IyOFAsL+eGqybm4/4yqO7BNq/cQorNPPzhNOjhDY2lcvL3yxJ6nV//XwhZ/z9Wy54YTzXvD6FiUvWcd+HPwQK671ha+QMnxlLSwJHOp0QZdW0U64FzF+1qcKAcO97PuHDGRVnS4VaxqFWiJ94FcDazTtYHRbN9cTHv6ywUtuvBf7zum38a8JPvt1YyawnCTdzWQnLS7Yz/NWK04TDp95+Nb+YwU98WaGVVLxpB69+u6S8q2rUjBU8MSa9s6LiCdISuBJ41R0HEGAdTpIZE0OT+rUj4tZ7ef//hhzUtkIXT9N6kSuS6xU4g3GNfLp+lOTndg/o3KI8IbypOoa/Osl3hbUfv5Dcv3jqa8b+9pikz799Vyl9/ziGTTt289BHc+O/IA0O+eOYiG1zV21i7qo9sbtCl90la/fMgJoQYxwivCUwftFaGtatFXgiB8B/3RzeO8PWNSwLa2nd9u4Mlq7fxsqS7eWJkX771jS+mFdMv07N6b5X4/LUq9FmD2ZCkNlB04BeItLYfRw/U4hJSb2CyAba8CM7I8A5/Tr6vibIClaAly/py+qNO7j1nekAvHRJX6b8uJ7znh8f55WmMglaAcSSyqLDpeu3simJMZFMCy3u+2T2KnbsLmXLjtKYY2YzlpawsmQ7h+zjjMOd7caVivUFLlx4aJDZyzcyd9VGRrwzI2y/yNd+4Wbty2VK1SDpJesApwOFQK3QG1bVezJasmrOe9Fu36xe1OdC8vKEK6J0LTkxaoKd96B2TSjYJ6+8EqhbO5/D9m0ZsNSmOjk3QCC9aBLtRfk+LOrr6X/7hj4dm/Le1bGzxE1csi5wgMZvFqwpT3nasmEdut3pLM778Pojo74mlBr1l33acdkRnWIef+3mHUz9eQPH7d+mfNvMZSUVQosAnPyk/6yxre54kPpEpgr6JS4TgowJvA8MwwnvsMVzM2ly8wmxp3w+dEbPmM/37tA0UCWwX5uGtLBQwca1PIUYQX5rIGIJrYQGOP1v3wBUmIEUTSLTUa/whO444YA9F2q/i264975fxgUvxG4NX/zSBC57ZRJL1mwpD0Fxyl++DlzGWImGVm/azlU+ubqzIciYQHtVHRx/N5OITi2dBPb3DjugfEZDSHjLoEfb6KuIp919Ak3q1Y7IaTDkoLYR8XNCi3yqXYwOk3XpiuuzbWcp9QryWbDaPydHIhFlvN1T3oV8oUonng2e1/x32nJ+0WvvCs8vWeOMMxzjhsLw6zIKjQ/E4veWLnsleBjydAvSEvhGRA7KeElqmMKWDZj++xO4ICwLFkDXNo344pZj6L6XM8Mo1rf8UNz3ZvX3zC769/D+PHVenwr71amVx5PnOttCf4R+c6mjSaSP1M9R+1W94H8muvlpWrF+2SsT2bB1Jxe94J+MJ1YV8Pr46AvuvF+Atu9KvMK6Lsq03WRfF15plpUpf/l0ftTjFI74IOXpskEFuQocAUwWkbkiMl1EZojI9EwXrCZoXLd21Hjz+7RowK2Du1G/IL+81RBL68Z1y+/379wi4rhz/3hS+XFq5zvP9enYNKHy7h+jRRJP8/qWpKQ6CQ/fnaxvFq7liTHzo87X37B1F78fOYvP5qyKeM67FiETlm3Yxqzle1ZrJ7M4MGTeqootnWFPj+PROAHyxsyOfM+ZEKQ76KSMl8L4Gti9DbPv8e+J+/fw/klnf6pfUIs3rxhAN7elEcug/Vsz5genP3fUDUcGmpfux7qgTDJGut0r8dZXxIvjlIzDH3ACCS6872Se+mxBxPPh6wBiGfLk1xUexwsFApFTTjMlyBTRH+PtY7Kvf+cWKb2+X6dgMy6evbCoPOIlwPMXFXF5AmkUjQki1S8Jve75OP5OSfpo5koe91nAlez6mqDvNd35FKIJ3ilsqpzuAb7px5OXJxXy9Q7q0SbG3sYkpzLH3N9d5n8xPj/JtTVBVwQ/9dkCLnnJf6wknawSqMbevDL5LGZnHNKes4rSF4qi+qUNMulSVsn/ONK9kOv9qfFnEIGzmjmVvBhBWSVQjTWum/xg7CNn9uKhM3ol/Lo6tfz/pDLxbz6s997xdzKVnjegW2V0y9u5nQczM8D4QSqC5BM4TUTmi0iJiGwUkU0iYqEjjK9EYhgd2TW1lcp/PqdPxLZB+1t3laleTvnL1xz+wGes3pT84r5YgrQEHsJJA9lEVRuraiNVtRyIlVD9gnyuOLpz1s73+NmRLYVoy9/9Fv00qx8ZKC+Wob3if/NPdNqrMVXBsg3b+Gimfx7rVAWZIrpKVX/IyNlNWvlNJ71zyP4cvE/sbGXJauNZmwBw+RGdombMUpyV0N7UjX6thsIW9StEgPQKT6ZjTE2Sqb//IC2BSSLyhoic63YNnSYip2WkNCbtLj+yc9yUlely7qEd6dWhadTnX7+8f4X8Cs0bRLYEzjvUP0oqRKbk9JPI3O1MuXPI/km/9q0rBzD5zkFpLI2pLoLmf0j4uAH2aQxsBU4AfuHeTknlpCJyr7v6eKqIfCwiNsJXxf17eH+6tGrIn34ZJcKIQscW9Rlx0p6sUu2a1uOeYQdU3C3GNTza6mqviw8vDFLccuf0TX+i8fDYT4noW9jcgvwZX/m5qgRU9RKf26UpnvdhVe2pqr2B/wG/S/F4JkWH75v44rPuezXmwHbO8FCDAqdnsW7taLODfMLnipRnYAsiyP9A47q1ueuUHhW2XXVMF9o2qeu7//8N9k91mBrrtgoJhSgxqcvLVXeQiNQVkWtE5K8i8mLolspJwxLTNMCiCuTci7/qy7TfnZDQaxrWqZXS/H8BurdtTJ4EW9jm9z9wqM/K5/BB6P8b3J1vbzsu2WImzDvv/dLDY8eor+4SHfw30WVqSCxId9A/gL2AE4EvgPaAf9zXBIjIn0TkZ+B8YrQERGS4iEwSkUnFxZlfOFFT1amVT5MEg7x5v5mH7rd0uzJ+dVhhhX39KouCWnk0qVebRfcPCTRd1G/m0VPnHRyxLUjFdO+wA/jVYYVJx1+KxZu39ne/6BFjz+TcG9aFVpmtrsQrgauaXA4M76uqdwFbVPUVYAhwaLwXicgYEZnpcxsGoKp3qGoH4DXg2mjHUdVnVbVIVYtatbJwxJWJ359k7fw8ljwwhN8Pjd/Xf1ZRYv3x3krn9pOdbpyWDSO/aQZZgXrhgEJ+P/QA8vKEI+JkVit088EGlWzjKDx+fTQ/rEz5O1jSBh+wV0L724Su9MnlwHAoPN8GETkQaAK0jvciVR2kqgf63N4P2/U1nPSVpooJMlAbizeZTpBj3XLingxsw4/qEvG6UHrAdE8QGtAlsUVtiSRC8Xri7N6B9hs1I3i2rUT8e3j/QGHLE1G/IMgsdBNELlsCz4pIM+AuYCQwG2cBWdJEpKvn4TBgTirHM7khxO566dB8zyyZICn+nP2ia9GwDu9cNYDnLyryfT40IBztXKcf7B8LKV79EyQ8hXfgOdlxkqD/5MkkmT+tTzsm3TmIRfedHHWf/p1b8M5Vh8U8Tu8EF+M1ipHk3SQmZy0BVX1eVder6heq2llVW6vq31M87wNu19B0nKmnN6R4PJNGQbs/4v1NvnvV4RFjAwCN6kReGKId6sZBXSs8PmSf5nEjmUa7CD96VuKxkJY8MIT+nVuUZ2Xzc8uJ3Xj/2j0J07u3TT16q1eLBgUJd0mFU5zxmngzTOLVQ6f0bJvQeVtW8umu0WaNVUaZagnEraZFpA1wH7C3qp4kIj2AAar6QrInVVXr/qmkvrr12MADxCJCp1YNmL1iIw18LuytGtWhf+fmvPzNkgoX5i9uPZaSbf7fZr37hVJa9itszqIEUu2dfFBbHh49N/D+Xmcc0p6B3Vtz9WtTKmzfp7lzEe6+VyPmhPXJ9+/cnNaN9lxMuu/lTJs9tlvqY1j3/fKg8gV0oYQ+rRvViRhwPb5HGz6JkYkqaBdVqHtNJD2RX4cf1TnpVI3ZUMkDmFaQQDbYhAQ57MvAaCDUJp4H3JiZ4phc69C8fkLRRx86vScv/apvjL5k56Li/V9r3qAgcv8YX3IO27elby7maDq1bJBQTmTvuEK3No04+aDIb7s92zfh1sHdeNy33z6y8EseGMJLl/QLXIZwoa60egWR/6LXDdw3YttzPl1k3plPQa91oV9Fgyh9+YleNGsnceXqW9iM967275b6ZsTAhI8XS9BuymQNPyp9sbxSHYOLJsgn1FJV3wTKAFR1N1CakdKYKsE7I6dBnVoc2z36PIFE/26T/adMV0s52vlFhKuP2ZcWPqEuMvG/+eqlh9KnY1OO84mKmp8X7MJ64gFtON9tRcQbLN/b7RYJXeRFnIV/nVulNlB8bPfEW0PtmtajT5RQJ3s3rZdQruubBu0X8/lMtwSOT2MSppytGAa2iEgL3C8TItIfyGyAa1OpvXf14TH7yL2yMUPwyXP7MOY3Rwfa95SebXn2wkOSPtcun6tpvPd48/GxL0R+OrVswHtXH+7bKgtaUR7csVl5GtF43UEnua2f0H55Isy59yQ+u/mYwKlIw/36yE7UqZWf1GtjSeRvyq/V5NWyYR3GjRiYsZXNBWnsw8lQHRCoEvgNzqygLiIyDngVuC4zxTFVQYfm9QOFdQY4pltrzu3XgT+eemDg4x/csWlCMX2G9tqbzq0aBtr3qfMO5oSwue7e/6143wyTCVB33XFdeWN4/wrbXr00+a4iv9ZIuJuP34+zPb/DeF0JoQVuDerUol7t/IjQGyGtGlUc6L3evcgW1Mrj7xdUrFzvGJL+hXJe/QpjV071C/LjDoS/dElf2jWtRyNPZTskwcHvWOoVpK8SjBamPVVBEs1PEZGjgW44/y9zVTXxOWqmRiqolcf9p/WMu5/3D/zdqw+PsWdmxQu77RccLnSBPauoPSf08F9MVRqndvGugYjnxAP2oiA/j52lZZx3aEeO9Fns1qlVgwoX/niXj1AlUDs/jx/ujQxJDs46grq1/S9qB+zdmMEH7nnvV6TQFx5eYfXu0JSpP2/wPO/8HHFydwry8zjlL1/7Hyfs8fmHduS18XtCnb9z1WER4dC/uvVYHv04uUkFfrq2DvblJIisrxMICxs9FKcS2A/4hYWSNpmSy9kaT53Xh75xvl2KCH8+p3eFbQ3rOBfGh87oFXX6aodmFad4ptK0FxFOOsi54PYrbF7eleNVyx03CPr7vO646N0mt57YjXZN63FguyYRzzV2B5/Dz3PbycmH0w6p51Y44de+UPa4UJm6eMYtTvJURN7KpF+n5vzplwdVWLdwiE+FX68gP61DxSJCUZryefjFyUqHWN1BobDRlwEv4MT4OR94Hkg1iqgxFYT+kfdJcT68l1++Aj+hf/qGPtNc/YQuLsd2a8WzFx7Cvq3jrwvo0Lw+U393fFLRWkOuG7gvj4WtdYg2PtDbzesQej5WpXP/aQdVmOIarqiwOeNGDCz//XjXfoRaTt5S9IgzcBtvjCFU1FDZwxdJ3XBcVybdOaj8W/wlniB93u6fM4ucxYHTf38C/7gsdveb9wyFLdK7atpbJm8+jVSOk05RK4FQ2GigNtBDVU935/cf4G4zJm3OOKQ971w1gCE+32qT9dENR/JulKmGXqF+/qArMkN71S+oFTG+EEvTFCNq3nxCN05zVz3HK+leYYug/PY/O8HYTSE3HLdnAV/4cT++6Sj+fUV/YqkfsJ881LoI/1zy8qTCIrQKgQw9+93ljkk0rls77gD1RQMKy8sWbzC54uviT132zuq5NUC33wlpnFEURJCB4Q6q6g1WsgqInv7JmCSICIfs0zytc6FbN64bKKta6BtjtwDhrCE9szQE4a0rB6R+oDhC0yn9pvGGuq56tW+a0DGb+bWw3Cv2fm0axV1nck7fOJcP9/cbal3E+317x5O8fz+JfHO+duC+TP3d8dQvqEWt/Dyaxlkwefi+LZh9z4kJV+xBynRYl+Rbi8kI0v79VERGA/9yH58NjMlckYzJrmG92zGsd7uEX5fqQqO+hc25+pgu/PXzhSkdJ5buezVm5h9O9O3qOr5HGxb86SRqpTCNMXTRDfqbCLKIL/ybfyKVbrxrbLSn8/OkwgV9zG+OpnjTDk7681e++195dBfqF9QKNF/ngL0b8+2iteWPO7dskNAK+EwLEjvoWuAZoJd7e1ZVbYqoqbFSmaoXPoAab9ZQIseLFl8o1lhHKhUAQJvGTrfMMfvFXxQWPqAeTei3e7R7zIvdrpqo+3s+jnQFWWvZsE6gRWkN6sTv2gpFyw2tHg6fkRQu23MjAv0FqOq7qnqTe3sv04UyJtcG7d86bqKbZK7fe7kXgPruxaN+becCHWTuv5dft9knARfMpcNJB+7FDcd1pW2Teky4/ThujLMyF6jQ2vroxiOj7hd6a385tw9f3HKM7+ynqK+N83y6LrChLwKhSidUGfo53J3CG5rVdMeQijOnuu/ViP9ee8SeMma5Foj6FUFEvlbVI0RkExV/d04EYdXga7eNqWKev7hv1OdS+bJ576kHctR+rcrHKq44ujON6tbizAQHaUN91t5B1mTi9CTrb56FYa3jfLMdflRn9g1bzBcKsucndIGtWzuffQLM1BnSsy23vTsj9OLY0nyBDV2wG9WtzaqN/lnUDt+3JXPuHVy+xuLAdk1Y8sCQ8oCAjevW5qD2kdNvsyVqJaCqR7g/0xsX15hqIplvbA3q1OLUPnu+Edetnc+lRySeh/j/Bnenc8sGnJhgpq9cuD3BNQN+lexLl/RlUbF/P3rjurXp1b4J05aW0LZJXX59ZKeoA7BpawmUD16HZpbF3j/aIjs/3jLu16Yh81ZtTrB0iYnVEog5mVdV16W/OMZUfgfs7XyLTWd4gUTVrZ3PhXH6yqua64/rypOfzme/NpHfO4/t1ppjY8yuPHq/VkxbWkJBfj63nNg96n5BUo8mwm8aq1+48ZjCKpBm9WtzSs+2DOjSgnkrN+WuEgAm41RKfnWcAumLkWpMFbJPiwYsvv/kjIX2TcXT5x3M1p27c12MhP3jsn4c2bUVx3ZrVb7QLRFBL+3p7m8PhZLy/i3ccFxXrgrLR5GIU3u3K18Pcvf7M1MqXxCxuoMSb6MaU0NUxgoActs6SYdoIaTjCY2xxEt/mfaWQGhFtmdbwn8bYUXK1MrgaOKOJInIL0WkiedxUxE5NaOlMsbUKKkOah/bvTWT7hxUPq00Gr/kO0E9fEZPursLCstDW3jyL4TkCYy89vCIqKrRhCqSv19wSNQ82Kf1SXwdS1BBFovd7Z0WqqobRORu4D8ZK5UxpkZJR3C0IPmMjwqwnsFPaHbPO1OWOhvci34HN+1oYcsGzFq+EYCju7WiTq18evpfzyOEKpLBB+5VIRKrV88Mzh4KUgn4VdHBIm3FISI3A48ArVR1TTqOaYyJbfhRnXn2y0W5LkYF2exe++OpB8aNFhuufDZQWNfNL3q2pW2TumzesZsPpjvRdRJNpJPrNMdB2mCTROQxEeni3h7DGTROiYh0AE4Afoq3rzEmfW4/ef+EcjBXNxf03ydwnKiQiFAWblNARBKuUMLFyvrW0V0nER4QMJ2CVALXATuBN9zbDuCaNJz7ceBWcl8RGmNy6P8GR5/SWVnsCW/tr1WArqhwQVaJX3JYIa9e2i+j60GCZBbbAoxI50lFZBiwTFWnxWsGishwYDhAx44WvNSY6uYIn8xolU28oHYHtmvCCxcXlYeICOKlS/oy9KlxMXM55OVJ0uMYQcWtBERkLD4VoKoOjPO6MYBf9XUHcDtOV1Bcqvos8CxAUVGRtRqMqSbyxJln36V1epO4ZEL5tM0YV6Dj9k8sD0DP9k155MxeHJ/l/AHhggzw/tZzvy5wOhB3NYqqDvLbLiIHAZ2AUCugPTBFRPqp6soA5THGVAOL7q/84xL7tm7IgtV7Vuz6rQtIxRmHBJxClEFBuoPCB4HHiciEZE+oqjOA8gwXIrIEKLLZQcaYyubNKwawqDgybENlXSyYjCDdQd6h7zzgECB3Ie+MMSZLmjcooHmDPZfAUCC4LC/qzagg3UHeGEK7gcU4yefTQlUL03UsY4zJpEfP7MU/vvsxUNrSqiJId5DFEDLGGJzcCTefED9ZfFUSdZ2AiNzquX9m2HP3ZbJQxhhjsiPWYrFzPPdvC3tucAbKYowxJstiVQIS5b7fY2OMMVVQrEpAo9z3e2yMMaYKijUw3EtENuJ866/n3sd9nLloRsYYY7ImVmaxxOKhGmOMqXJSS+djjDGmSrNKwBhjajCJldCgshGRYuDHJF/eEqhp8YnsPdcM9p5rhlTe8z6q6huTukpVAqkQkUmqmnyW6SrI3nPNYO+5ZsjUe7buIGOMqcGsEjDGmBqsJlUCz+a6ADlg77lmsPdcM2TkPdeYMQFjjDGRalJLwBhjTBirBIwxpgarEZWAiAwWkbkiskBERuS6PMkSkQ4iMlZEZovILBG5wd3eXEQ+EZH57s9m7nYRkSfd9z1dRA72HOtid//5InJxrt5TUCKSLyLfi8j/3MedRGS8+97eEJECd3sd9/EC9/lCzzFuc7fPFZETc/RWAhGRpiLytojMEZEfRGRAdf+cReQm9+96poj8S0TqVrfPWUReFJHVIjLTsy1tn6uIHCIiM9zXPClBkiGrarW+AfnAQqAzUABMA3rkulxJvpe2wMHu/UbAPKAH8BAwwt0+AnjQvX8yMAon6F9/YLy7vTmwyP3ZzL3fLNfvL857/w3wOvA/9/GbwDnu/b8DV7n3rwb+7t4/B3jDvd/D/ezrAJ3cv4n8XL+vGO/3FeBy934B0LQ6f85AO5zUtfU8n++vqtvnDBwFHAzM9GxL2+cKTHD3Ffe1J8UtU65/KVn4pQ8ARnse3wbclutypem9vQ8cD8wF2rrb2gJz3fvPAOd69p/rPn8u8Ixne4X9KtsNaA98CgwE/uf+ga8BaoV/xsBoYIB7v5a7n4R/7t79KtsNaOJeECVse7X9nN1K4Gf3wlbL/ZxPrI6fM1AYVgmk5XN1n5vj2V5hv2i3mtAdFPrjClnqbqvS3OZvH2A80EZVV7hPrQTauPejvfeq9jt5ArgVKHMftwA2qOpu97G3/OXvzX2+xN2/Kr3nTkAx8JLbBfa8iDSgGn/OqroMeAT4CViB87lNpnp/ziHp+lzbuffDt8dUEyqBakdEGgLvADeq6kbvc+p8Bag2835F5BRgtapOznVZsqgWTpfB31S1D7AFp5ugXDX8nJsBw3AqwL2BBtTANLa5+FxrQiWwDOjgedze3VYliUhtnArgNVV91928SkTaus+3BVa726O996r0OzkcGCoiS4B/43QJ/RloKiKhfBje8pe/N/f5JsBaqtZ7XgosVdXx7uO3cSqF6vw5DwIWq2qxqu4C3sX57Kvz5xySrs91mXs/fHtMNaESmAh0dWcZFOAMIo3McZmS4o70vwD8oKqPeZ4aCYRmCFyMM1YQ2n6RO8ugP1DiNjtHAyeISDP3G9gJ7rZKR1VvU9X2qlqI89l9pqrnA2OBM9zdwt9z6Hdxhru/utvPcWeVdAK64gyiVTqquhL4WUS6uZuOA2ZTjT9nnG6g/iJS3/07D73navs5e6Tlc3Wf2ygi/d3f4UWeY0WX60GSLA3EnIwzk2YhcEeuy5PC+zgCp6k4HZjq3k7G6Qv9FJgPjAGau/sL8LT7vmcARZ5jXQoscG+X5Pq9BXz/x7BndlBnnH/uBcBbQB13e1338QL3+c6e19/h/i7mEmDWRI7fa29gkvtZ/wdnFki1/pyBPwBzgJnAP3Bm+FSrzxn4F86Yxy6cFt9l6fxcgSL397cQeIqwyQV+NwsbYYwxNVhN6A4yxhgThVUCxhhTg1klYIwxNZhVAsYYU4NZJWCMMTWYVQImp0REReRRz+Pfisjv03Tsl0XkjPh7pnyeM8WJ9Dk2bPveIvK2e7+3iJycxnM2FZGr/c5lTCKsEjC5tgM4TURa5rogXp5VqkFcBvxaVY/1blTV5aoaqoR646zpSFcZmuJE0vQ7lzGBWSVgcm03Tu7Um8KfCP8mLyKb3Z/HiMgXIvK+iCwSkQdE5HwRmeDGUu/iOcwgEZkkIvPcOESh3AQPi8hEN077FZ7jfiUiI3FWq4aX51z3+DNF5EF32+9wFvG9ICIPh+1f6O5bANwDnC0iU0XkbBFpIE5s+QlukLhh7mt+JSIjReQz4FMRaSgin4rIFPfcw9zDPwB0cY/3cOhc7jHqishL7v7fi8ixnmO/KyIfiROH/iHP7+Nlt6wzRCTiszDVVyLfdozJlKeB6aGLUkC9gP2BdTjx1J9X1X7iJNq5DrjR3a8Q6Ad0AcaKyL44y+lLVLWviNQBxonIx+7+BwMHqupi78lEZG/gQeAQYD3wsYicqqr3iMhA4LeqOsmvoKq6060silT1Wvd49+GEOrhURJoCE0RkjKcMPVV1ndsa+KWqbnRbS9+5ldQIt5y93eMVek55jXNaPUhEurtl3c99rjdO9NkdwFwR+QvQGminqge6x2oa4/duqhlrCZicUycS6qvA9Qm8bKKqrlDVHThL5EMX8Rk4F/6QN1W1TFXn41QW3XFirVwkIlNxQnG3wIkxAzAhvAJw9QU+VyfA2W7gNZwEIck6ARjhluFznDAIHd3nPlHVde59Ae4Tkek4IQXasSfUcDRHAP8EUNU5wI9AqBL4VFVLVHU7TmtnH5zfS2cR+YuIDAY2+hzTVFPWEjCVxRPAFOAlz7bduF9URCQPJ8NWyA7P/TLP4zIq/l2Hx0VRnAvrdapaIZiaiByDE7Y5GwQ4XVXnhpXh0LAynA+0Ag5R1V3iRFOtm8J5vb+3UpyELetFpBdOEpcrgbNwYtOYGsBaAqZScL/5vokzyBqyBKf7BWAoUDuJQ58pInnuOEFnnKBio4GrxAnLjYjsJ07SllgmAEeLSEsRycfJ2vRFAuXYhJMSNGQ0cJ0b7RER6RPldU1w8inscvv294lyPK+vcCoP3G6gjjjv25fbzZSnqu8Ad+J0R5kawioBU5k8CnhnCT2Hc+GdhpNaMJlv6T/hXMBHAVe63SDP43SFTHEHU58hTqtYnTC9I3BCG08DJqtq/DC9e4wFeoQGhoF7cSq16SIyy33s5zWgSERm4IxlzHHLsxZnLGNm+IA08Fcgz33NG8Cv3G6zaNoBn7tdU//ESdFoagiLImqMMTWYtQSMMaYGs0rAGGNqMKsEjDGmBrNKwBhjajCrBIwxpgazSsAYY2owqwSMMaYG+3/o/o4F5YnbrAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_handcraft = LR.loss_record\n",
    "grad_norm = LR.grad_record\n",
    "_xaxis = np.arange(max_iter)\n",
    "plt.ylabel(\"Euclidean norm of the gradient (log scale)\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.plot(_xaxis, np.log(grad_norm))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "def compute_cross_entropy_loss(y_pred, y_true):\n",
    "    eps = 1e-12\n",
    "    y_pred = np.clip(y_pred, eps, 1-eps)\n",
    "    cross_entropy = -np.dot(y_true, np.log(y_pred)) - np.dot((1-y_true), np.log(1 - y_pred))\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='none',\n",
       "                   random_state=None, solver='lbfgs',\n",
       "                   tol=4.9999999999999996e-06, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_sklearn = LogisticRegression(penalty=\"none\", max_iter = max_iter, tol = tol)\n",
    "LR_sklearn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sklearn = LR_sklearn.predict_proba(X)\n",
    "theoretical_loss = compute_cross_entropy_loss(y_pred_sklearn[:, 1], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "theoretical_loss_max_iter = np.repeat(theoretical_loss, max_iter)\n",
    "loss_diff = np.array(loss_handcraft) - theoretical_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEGCAYAAACO8lkDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA7+0lEQVR4nO3dd3wUdf748dc7BULvIE1DVQERKSp2FBH7nb2feupZzrOePyxnOeXOXs+7r73ceXY9C1YsCBaqoFTpAlICKFUISd6/P2Z2M7s7uztJtiV5Px+PfWR3Znbms9lk3jOf8v6IqmKMMcZ45WW7AMYYY3KPBQdjjDExLDgYY4yJYcHBGGNMDAsOxhhjYhRkuwCp0LZtWy0uLs52MYwxplaZOnXqWlVt57euTgSH4uJipkyZku1iGGNMrSIiS+Ots2olY4wxMSw4GGOMiWHBwRhjTAwLDsYYY2JYcDDGGBPDgoMxxpgYFhyMMcbEqNfBYeWGX7nvo3ksKtmc7aIYY0xOqdfBYc3G7Tzy6QKWrNuS7aIYY0xOycngICJdReQzEZktIrNE5Ir0HMf5WVGRjr0bY0ztlavpM8qAa1R1mog0A6aKyMeqOjuVBxGc6GBz4RljTKScvHNQ1ZWqOs19vgmYA3RO9XFCdw42VaoxxkTKyeDgJSLFwF7AxNTv2/lZYbHBGGMi5HRwEJGmwOvAlaq6MWrdRSIyRUSmlJSUVG//brWSVSwZY0yknA0OIlKIExheUNU3oter6uOqOlhVB7dr55uOPKm8vNC+alBQY4ypg3IyOIiIAE8Bc1T1/rQdx71zsGolY4yJlJPBAdgfOBs4VESmu4+jUn2QvFCDtFUrGWNMhJzsyqqqEyDcIJA21iBtjDH+cvXOIUPccQ7W6GCMMRHqdXDIS/u9iTHG1E71OjiIhBqk7c7BGGO86ndwcH9abDDGmEj1OjjkSajNIcsFMcaYHFOvg0NlbyWLDsYY42XBAUueYYwx0ep5cLCurMYY46d+Bwf3p8UGY4yJVK+DQ7hBOsvlMMaYXFOvg0OozWHcvOql/DbGmLqqfgcH9+cHs1ZltRzGGJNr6nVwSH9qP2OMqZ3qdXAQiw7GGOOrXgcHY4wx/up1cBC7cTDGGF/1OjgYY4zxZ8HBGGNMjHodHBoU1OuPb4wxcdXrs2PzosJsF8EYY3JSzgYHERkpIvNEZIGIjMp2eYwxpj7JyeAgIvnAo8CRQB/gdBHpk91SGWNM/ZGTwQHYG1igqotUtRR4CTg+y2Uyxph6I1eDQ2dgmef1cndZmIhcJCJTRGRKSYklzjPGmFTK1eCQlKo+rqqDVXVwu3btsl0cY4ypU3I1OKwAunped3GXGWOMyYCEwUFEhorIoyLynYiUiMiPIvKeiFwmIi3SWK7JQC8R6SYiDYDTgLfTeDxjjDEecYODiLwPXAB8CIwEOuL0HLoJKALeEpHj0lEoVS0D/ugeew7wiqrOSsexjDHGxCpIsO5sVV0btWwzMM193CcibdNVMFV9D3gvXfuPVlZeQUF+rtayGWNMZsU9G3oDg4jsIiLD3eeNRKRZ9Da13fayimwXwRhjckbSS2URuRB4DXjMXdQF+F8ay5QVmu0CGGNMDglSj3IZsD+wEUBV5wPt01koY4wx2RUkOGx3RykDICIF1MEL7Z+3lCbfyBhj6okgwWGciNwANBKRw4FXgXfSW6zMu/g/U7NdBGOMyRlBgsMooAT4HvgDTg+im9JZqGxYum5rtotgjDE5I1FXVgBUtQJ4wn3UWZu3l2W7CMYYkzPiBgcR+Z4EbQuq2j8tJTLGGJN1ie4cjslYKYwxxuSUuMFBVZdmsiDGGGNyR5BBcPuKyGQR2SwipSJSLiIbM1E4Y4wx2RGkt9I/gNOB+UAjnGR8j6azUMYYY7IrUKY5VV0A5Ktquao+g5Ol1RhjTB2VtCsrsNWdU2G6iNwNrCR3JwmqkWXrt9K1deNsF8MYY7IuyEn+bHe7PwJbcGZoOzGdhcqWz+atyXYRjDEmJwS5c1gLlKrqNuA2EckHGqa3WMYYY7IpyJ3DJ4C3rqURMDY9xckuGyVtjDGOIMGhSFU3h164z+tkxfzdH8zLdhGMMSYnBAkOW0RkYOiFiAwCfk1fkYwxxmRbkDaHK4FXReQnQICdgFPTVSARuQc4FigFFgLnqeov6TqeMcaYWEnvHFR1MrAbcAlwMbC7qqZz8oOPgX5uYr8fgOvTeCy6t22Szt0bY0ytFCR9xsk47Q4zgd8AL3urmVJNVT9S1VDL8Dc4c1anzaNnpu2jGGNMrRWkzeEvqrpJRA4ADgOeAv6V3mKFnQ+877dCRC4SkSkiMqWkpKTaByjMl2q/1xhj6qogwaHc/Xk08ISqjgEa1OSgIjJWRGb6PI73bHMjUAa84LcPVX1cVQer6uB27dpVuyx5YsHBGGOiBWmQXiEijwGHA3eJSENqmD5DVYcnWi8i5+LMJ3GYqsadcCgVurdrms7dG2NMrRTkJH8K8CFwhNtrqDXw53QVSERGAtcBx6lqxid2Lq9IaywyxphaIUhvpa2q+oaqzndfr1TVj9JYpn8AzYCPRWS6iPxfGo8V45pXpmfycMYYk5NyLruqqvZU1a6qOsB9XJzJ4/9v+k+ZPJwxxuSknAsOxhhjss+CgzHGmBhBBsFtEpGNUY9lIvKmiHTPRCEz7fvlG7JdBGOMyaogdw4P4vRO6owzWvla4L/AS8DTaStZFh37jwnZLoIxxmRVkOBwnKo+pqqbVHWjqj6O0631ZaBVmstnjDEmC4IEh60icoqI5LmPU4Bt7jobFGCMMXVQkOBwJs480mvcx9nAWSLSCGdeaWOMMXVM0vQZqroIZ34FP3W2cn7+6k306tAs28UwxpisCNJbqYvbM2mN+3hdRNKaRjsXHP7AF9zx7uxsF8MYY7IiSLXSM8DbQCf38Y67rM57csLibBfBGGOyIkhwaKeqz6hqmft4Fqh+jmxjjDE5L0hwWCciZ4lIvvs4C1iX7oIZY4zJniDB4XyctN2rgJXAScB56SxULtlRXpHtIhhjTMYFSdm9VFWPU9V2qtpeVX+jqj9monCZMnz39nHX9brRd5ZSY4yp0+J2ZRWRR0gwyE1V/5SWEmXBP84YyG5/+SDu+g9nreKIvjtlsETGGJNdicY5TMlYKbKsqDA/4foHPv6BIcWtad2kRlNnG2NMrRE3OKjqc5ksSC6bu2oTA2//mCV3Hp3tohhjTEbEbXMQkSdEpF+cdU1E5HwROTN9RTPGGJMtiaqVHgVuFpE9gJlACVAE9AKa46TrfiHtJTTGGJNxiaqVpgOniEhTYDDQEfgVmKOq89JdMBG5BrgXZxDe2nQfzxhjTKUgXVk3q+rnqvqiqv4vQ4GhKzACyFiX2ad+NzjpNs9+aek0jDH1Q67OIf0AcB0ZnC/isN07JN3m1ndm88vW0gyUxhhjsivngoOIHA+sUNUZSba7SESmiMiUkpKSDJUO1KY3MsbUA4GDg4g0TtVBRWSsiMz0eRwP3ADcnGwfqvq4qg5W1cHt2mUuD+Dr05Zn7FjGGJMtQeZz2E9EZgNz3dd7isg/a3JQVR2uqv2iH8AioBswQ0SWAF2AaSKSM8OT7xgzhzLLt2SMqeOC3Dk8AByBm4nVre45KB2FUdXv3fxNxapaDCwHBqrqqnQcr7qe/WpJtotgjDFpFahaSVWXRS0qT0NZao07xsxhUcnmbBfDGGPSJkhwWCYi+wEqIoUici0wJ83lAsC9g8jJMQ6H3jcu20Uwxpi0CRIcLgYuAzoDK4AB7ut6b+m6LZSVVzB31Ubf9Ru27uDspyayeuO2DJfMGGNqJlH6DADcK/d6kUOpU4siftoQ/ER+/KNf0qRBASt++ZWxVx9Mz/ZNI9a/OnUZ4+ev5bFxi7j52D6pLq4xxqRNkN5Kz4lIS8/rViLydFpLVUv8snUHK375FYBVG7axZXtZlktkjDGpkfTOAeivqr+EXqjqzyKyV/qKlD01Gd927jOTKKvQcFrvZ75czEuTl7n7tZFzxpjaJUhwyBORVqr6M4CItA74vlqneVEhK6tQreRVVhEZAG57Z3YqimSMMVkRpEH6PuBrEbldRO4AvgLuTm+xsuOpc5Mn36suVeWbRetQy79hjKkFgmRlfR44EVgNrAJOUNV/p7tg2dClVWO+GnVoyverCi9OWsZpj3/De9/n1Hg+Y4zxFbR6aC7wc2h7EdlZVTOWTjuTOrVsVKP3b9y2g+ZFhTHLl6zbAsDyn7fWaP/GGJMJQXorXY5z1/Ax8C4wxv1pfPS/9SN+XBcbAMT9aZVKxpjaIEibwxXArqraV1X7q+oeqto/3QWrzWavjBwU9+xXS8LR4c7352a+QMYYU0WB0mcAG9JdkLpkwZpNMcseG7fId9vyCuUnd6yEMcbkiiDBYRHwuYhcLyJXhx7pLlhtdu9HPwTe9v6P57HfnZ+ybP1WyqO6w5aWVTBzhcVlY0zmBQkOP+K0NzQAmnkeJgUe/WwhAAfe/Rn9bvkwYt3f3pvDMY9MYKnbmG2MMZkSJLfSbeDMBKeq9aKrzZH9duL9menvcrr7Xz6IeP3rDicT+g+rN9GtbRO+/fFnANZvKWWXNk3SXh5jjAkJ0ltpaKpngst1/zxzYNqP8c6Mn8LBwGv5z1sZ8cAXjB4zJ9yzKU8kZjtjjEmnINVKD5KhmeByhYgQOh/3aJf6K/YJ89dy+Yvf+q77ZesOACYvWU9oMLXFBmNMptlMcHEUFeQD8Nz5e6d832c9NTHuulAgKK/QcMK+v7w1K+k+P569ms/nrUlJ+YwxJsgI6YiZ4HDGPWRkJrhc0LpJg4weL1SFpAoVFc6yGct+Sfq+C5+fAhDOCmuMMTVhM8HFka0026HgUKEa07XVGGMyJeGdg4jkAw+pakZngnNTdlyGU301RlWvy+TxI8pCZiv8123eDsD8NZtpUBCo1s8YY1Iu4dlHVcuBXUQkY3UrIjIMOB7YU1X7Avdm6the2cqsfcaTle0RpWUV4efFo8Zw/rOTq7Qvde8+nvtqCYvX2lgJY0xwQdocFgFfisjbQPgMo6r3p6lMlwB3qup29zhZaWXdo3MLpiz9mbwcunj/dK7zqyivUDZvL6NFo9jsr17/+HQB933sjNZu1biQb28eEXfb0x7/GoCXLhqaotIaY2qzIMFhofvIIzMjo3sDB4rIaGAbcK2qxlwyi8hFwEUAO++8c8oL8dS5Q5i/ehMN3V5LueKGN7+nQX4ez361hD8c1J3HvljEgtFHhtdf++oM7j15TwD+O6kyq/rPW3ewcdsOGhfmU5AfG/G+WbQ+/YU3xtQaWRkhLSJjgZ18Vt3olqk1sC8wBHhFRLpr1BRqqvo48DjA4MGDU14J1KJRIYOLW6d6tzX234mVJ/zHvnCS+ZWWV1Y/vTZ1eTg4RLeW9L/1I04a1CW83hhj4snKCGlVHa6q/XwebwHLgTfUMQmoANrW5HipctXw3tkugq94WV3FZ/TcG9OWo6rc/9E8lq33j/UrN/wad50xpn7IxRHS/wOGAYhIb5yEf2vTeLzArhjeK9tF8DX8/i8CbysiLFq7hYc/XRAeGxFt6N8/5cC7P0tV8YwxtVCgaUJVdVnUVWg6R0g/DTwtIjOBUuB30VVKmXb3if1rZbdSv7QbQmVPLG91lDHGeOXcCGlVLQXOStf+q+OUIV2zXYQqKS2roEFBnm9w0LgvYq3bvJ02TRumsmjGmFrCRkjXQcPvH8fm7WUsWx/bFuE36jrejdmgO8bywsSlcdcbY+quuMFBRO5ynw5T1TNVtYOqtlfVs1R1XYbKl3M6tSjKdhGS+nH9Vp77aknc9d4eT+u3lHLDm9+HXx9yT2Rbw41vzuSaV2fE7OP1qcv5ZlG9/TMwps5LdOdwlDgNDddnqjC1wduXH5DtIgTy4az4kxU9/eXi8PM735/Di5Mqk+4uWRfbS+mNaSsiXg/9+ydc8+oMTnv8mxSU1BiTixIFhw+An4H+IrJRRDZ5f2aofDmnrVsHv3PrxuFlj589KFvFieu75cnnnlagOm3SKzdsq/qbjDG1SqLgcJOqtsRJfNdcVZt5f2aofDnpyXMG8+rFlWkmRvT1G8+X+1Q1a9lnjTG5LVFw+Nr9WW/vEuIZ3qcDHZrnfttDbfHx7NUsKtmc7WIYYzwSBYcGInIGsJ+InBD9yFQBa4sz94nM77R3t9xLvRGttKyCj2atDrTtNne+66A9l16bujzihP/GtOUcfM9n4fff9s4s3v9+JSt++ZULn5/CofeNq2LpjTHplGicw8XAmUBL4NiodQq8kaYy1UrNihJnSM1FP1Wh7WDIHWP5/rYjAg+cu/bVGTQsyGPeHU5SwKtfiezx9MyXS3jmyyWcOrh2jSExpr6IGxxUdQIwQUSmqOpTGSxTrRRdd9+3U3MmLa47mU43bS8DYue5mL7sF9Zv2c6hu3WIec/2sthAoho5cvvlKdHTkxtjckHc4CAih6rqp8DPftVIqlrv7xxGHbkbA3du5byIOmnm+Q1PjnLsnp14Z8ZPaShZepz7zCTaNIkcMf2bR78EIueuTlT1lKhS6sVJP3L63qlPv26MqbpEbQ4Huz+P9Xkck+Zy1QoXH9wj3LYQfdLLCzC76CG926W+UGn0+bwSXp+23Hfdnz0D5WZ4utGG2ipCEgWO69/4noqoEdw3vzWTq1+ZTmlZBfd9NI8t7h2MMSa9ElUr3eL+PC9zxam9Qie9c/crpke7JmwtdU6KRYV5/PW4flz3+nex78loCdPr1anL+XnrDi48sBunegbH7faXDyK2e/e7lezRpUXc/UT/Tp7/eikAe+3cikc+XcDkJet59IyBETmfTn3sa7aUlvHu5QdWq+xl5RVsK6ugacNAeSh9bS0t4//GLeKPw3rGTdK4euM2Nm8vo0e7ptU+jjGZkqha6epEb0zjNKG1Uuhk1adTc04Z3JWvFjhZxh88dS9G9tvJNzjsUwt6NFXF2DmrGTsnce+nK1+ennC9qvLRrNU8OHY+73pGo4d6Pn2zaD2D7hjLlJuGhwckTkzQtvPzllI2by+jq2fQ4o7yChaVbGHXnZyJDS9/8Vven7kqXDX28ezVNG6Qz/49g08j8vSExTz8yXyaFxVwwYHdfbfZ52+fAJFVcMbkqkSXSqEpQXfFmZHtbff1scCkdBaqNrrggG60bdqQE/bqDMB+Pdsy6cbDaN8s/niIrq0b0yA/z1Jne1So09Np47Yyut/wXnh5dMLANRu3h4NDIvvd+Sm/7iiPOCHf/u5snv96KRP+3zAOuCt23orQPBdTbhrOX9+Zzd9P2IMmSe4qdpQ75du4zaq9TN0Qt81BVW9zpwjtAgxU1WtU9RpgEGCthlEK8vM4aVAX8jyNDYkCQ1iAton6ZNZPG3xPsOu3lEa83pEgoG7ctoMrXvqWJWu38Kvb5rHHLR8yeYlzhzFlyc8ArNm0PWFZHhz7A2/P+Ik34rSz3PPhXI59ZAIVFVrZA8sy2Jo6IkjK7g44k+6ElLrLTApYbIj0239+5bv83e9WRrzeuG1HzDahxuzfPzuZt6b/xIgHKmfI27S9jIc/mQ/AnFXOoP8T4hwrRNxvxyfLOQCPfraQ71dsYPyCteFtLTSYuiJIcHgemCQit4rIrcBE4Nl0Fqouaug2Ul42rEfE8jP2sZuw6thRXkHJpu0848kwW65KyabtTHbvDKKr60rdcRfxLu6/WriWs56cGH4duhuI7mH11vQVrN5YOYCwvKIivO2vpf6TJJZ5yrJuc+I7lsVrtzB16c8Jt8mULdvLKElyh2XqpqTBQVVHA+fhZGj9GThPVf+e7oLVNeOvG8aYPx3AkOLIRuhrRuzKbjs1C7/u2rpRpotWKy1dt5Uho8dy2zuzw8sqVBkyemzc9ySqigL404vTmbCgcrryUE+pKZ4T9ebtZVzx0vSIILJle3n4DvDJCZXBKmR7WTk9b3w//PrNb1fEbOM17N7POfFfie9qMuXYRyYk/J2auivQxMiqOk1VH3If36a7UHVR++ZF9O3Ugj4dIxPaNm1YwAdXHhTO8vqf3++TjeLVOi9Pjh1ZfaBP47LX7JUbI67go62Nc0X/7ndODiiobBhfvHZLeP01r8zwnZI1JHripR/Xx86ZEcTt787mmldiJ15Kp0Xu55y0eD1fegKnqfsCBYdMEpEBIvKNiEwXkSkisne2y5RK7eNkcx1S3Joldx7NLm2a+K5vkJ/8qzp3v+KaFK1WiR5cB8kbmLftqOD2d2cn3Cae/e/8NOJ1machorS8AomKDj+s3sTXC52Z8jZHNbCH7kiq6qkJi8ODEM96ciLnPJ25ToOnPPY1Z3ruluqauas2svzn6gXtuirnggNwN3Cbqg4AbnZf11uF+c5Jp03TBkm3veno3dNdnJzhN2NdEM9V88QcFqe94tmou4MRD3zB6U84gwHL4rVoV1NpWQUTFqzlix9KUrrfaFtL09Mtt6JCw+OAqmreqk1pSe8+8sHxvt2a67OkwUFEmohInvu8t4gcJyLpTEGqQKjupQVQe5IPpUFRYT4AHQPMXV2Qn0fbAEHEpF68RtuD7v6Mf36+MGb5E18sYsBfP+KXraUcfv84FiY44akqn81bE3496o3YAZXp8PpU/y68P6zeVKPA8a9xCznjyYnVCm5HPPhFnUjvXlZeEbfzQq4IcufwBVAkIp2Bj4CzSW9vpSuBe0RkGXAvceawFpGL3GqnKSUl6b2CSrVrDu/NlcN7Bdq2Y4si/nHGXjxxzmDf9QVRSZysm336DL7jYxauTX7V+sHMyvm747UvjH5vDr9s3cF/J/3I/DWbOSzBCe/j2as575nJ4defz6vZ37tfw/zW0rKYgYZ+WXVLyyoY8cAXXPyfaYGPV1pWEdH7au6qTQD8vLU03lvqvIv/M43db/4g+YZZFCQ4iKpuBU4A/qmqJwN9a3JQERkrIjN9HscDlwBXqWpX4CrAN124qj6uqoNVdXC7drUrgd3lh/XiyuG9A237t9/uwTH9O0XkEvLKC5Lhz6TE2s2lScdGAFz8n6mB9/nIJwsiXp/9VGy9vrfbLMQOCEzmL/+bSfGoMTw49gdmrthArxvf54qXnH4lkxavp3jUGPrc/CFXRaU28Ru5v+FXZ3zJFz+U8OT4RYGOP3rMbE7811csWOMEhVB7UcM4OajS5aGx8/kkSXqXTEmWZiYXBAoOIjIUZ+KfMe6y/JocVFWHq2o/n8dbwO+onEjoVaBONUhX1WBP11e/nDz5AVKDm9z1q6dhfcGazYyfH1kXP35+SUx7hld0FluvO96dTfGoMfz7G6ed5cGx88NX8G9N/4ml67bw7neVtbZvR6WP31EWu29vF9s7xsyhrLzCd0Ci15yVTlAo2eQEtVkrnKy9oQSFazZtY1OSfQRRWlYRDkB+Hhj7A79/bkrS/Xw+bw3by3K7yicTggSHK3Gqdt5U1Vki0h1IZ8vNT1SmCz8UmJ/GY9V6eQK7tGlM307NY9bVZJa16MF6Jr0O6t2O4fdHVi3NXbWRs5+axMKSLXHeBZtLy8JdcEs2bY+ox/Ybc1HhqXfcXlYRt7H8iS8W8cDYH2KWR1eTXfD8FPrf+hH7/G0sxaPG+KZk916/LFizOTwD4aTFTqDae/QnMZ/dz/zV8U/8ALe8PYvh93/Bmk3BZziMtmDNZs59ZjI3vDGz2vvwUtXAU+vmmiCD4Map6nGqepfbML1WVf+UxjJdCNwnIjOAvwEXpfFYtd7LfxjKuD8PY8yfnHTVoT/DKTcN566T+ldpX8+cNyT8vGd7SyudSX6NsyMfHJ/0fWc/NYnBd4xlw687GDJ6LCc/5lzZxxuz4W1vqFBlWYI2kSBC7R+rNzrH+3rRurjbKhpx4v6/cZUN9aH3+ykrr+C971dyuCcdip+vFzp3XZu3lfHfiT9SPGpM0obzzVEjwENVXnNWbkz4vqCOfGg8/W/7KO76UDXdY+MW5lxX2iC9lf4rIs1FpAkwE5gtIn9OV4FUdYKqDlLVPVV1H1UNXoFbR3x/64jA2/br7D83QuhirSrpoQ/wpKjOE7HU0rXAjGW/ALCnewKaucLprz/4Dv9RzY9/UdlO8O+vl8ZUYyW7Ok9mw9YdzF21keluuTZs3RGRUj3IDInRLnx+Cpe+kLwBvNy9Qs/PEx79zGnLWbc5cfvM8PvGRYwAn+9WS1WosjkFE0vNXbWJTQky9Y4eM5uVG37l7+/P5fxnJ8fdrqy8glcmL4vpNJBOQaqV+qjqRuA3wPtAN5weSyZNmhUVcmCvtuH03169OzSle1v/gXIAzYuc1NLRg7KCKPQMtEv2/m4JymCyyzt6O9paz8nyhYk/xqxPdnWezI4KZeSD4/nNo19y9MPj2fOvlVfNZzwxkUUJqsii2zxCPkvSO2vq0vX0uOE9lq13RrELEq4+S9ZhY1VUY/9VLzsj0Oeu2kS/Wz7kf26qkx/XbeXBsT+kpIrIe4JfuWFb+LV3sKSq8udXZ/Dtj07V27+/Wcp1r3/HfyfWcJxOFQQJDoXuuIbfAG+r6g4s+WTa/fv3+3D/qQNiln901cG8een+ADQrip1j4N+/34ebj+lD6yb+4x327NrSd7BcdCxI9k8woGvLhOtN9pz9VPamW/GOXJ/1U2zVTOiK3s+8VRvZsHUHPwYY4LjEEwCf+GJxxAn3hUlLw8Eh9Gf90NjqNV2GuiWf/9xkHhw7P5xGpSa8XZ3LyjV8Ieb9j9vw6w5enbqcc90uzKHAER3M0ilIcHgMWAI0Ab4QkV2A1FTImWopcEdN+7ULdG3dmPMP6Ob7vgWjj+SNS/ajeaPIMYxDilvxrzMHAdChudNl9oi+OyUsQ6gh8+RBXRi4c8sqld/ktgnzq59D6brXEg/QS3RyffSzhez514846J7k/V0Ouffz8HONulZdt7k0fDJdsMYZl+JtXH9lyjKKR42JyXnlZ+XGbSxeuyUc9PzStoAzFsU7v/m2HeWs2hB5Il+wZhPFo8bw1cLK3++ecS6yQlVav5aW88yXi8OBNtWj7RNJOmmuqj4MPOxZtFREhqWvSCaZJg0LeO78vekfp70hngK32sg7cG7cnw+JyOf01ajDqFCNqGLyGtC1JdOX/cKgnVvyzoyfOLp/R+av3sy0H3+p+gcxOeksn7EW6bIlTr3+jvKKuH+D0aJvcvNF2OL22vrbe3PYI+r/5Oa3nJ5It7w9K+m+Zyz7hWGeQHTn+3N58ndDIrZZWLKZC5+fwtH9O3LrsX1p16wh5z0zOaZx/ssFzutP51aOdt+7W6tw1dFKTzAJ3emUlldEZh7OpTYHEWkhIveHRiOLyH04dxEmiw7u3Y5WcaqOktltp8pur9GJ/vLzJOKf8pTBXSLWh+JKv84tWDD6SA7ZtT3n7LdLtcoRcsLA2LYVUz+E8k9Fu/kt58Qd70rd66PZkQPKvFWk5RUa0X3X2Wf1p+UdO2cN30Sd9EPdh8d8t5Iho8fyzoyfYgKDqvL810uAyCBQUQETF62P2dZvdDrEn3gqHYKE5qeBTcAp7mMj8Ew6C2XSq4/PmIh4Bu3SKuJ1nqd+NHQn0rAgP9B8zvEc2a9jtd9rarfvlm/wXf7ipB9ZWLI54URDRz003ndSpJc86dzTUUcfyrYbj1+13JcL1vmOV9lSWsakxZHBYfebP4jbOP/UhMVpT7gYEiQ49FDVW1R1kfu4Deie7oKZ3LRPd2fEdruoYFBagxGllgHE+DnsvnHhUdR+Zq/cmHRSpE3bysJdXFOltLyCf32+MNwIHr17v9HVoaqjaFe8NJ15nu7D5RWa9M4mU6nagwSHX0XkgNALEdkfqHmTvakVJGqW66sP35XPrj2E4qiurHeftGfC/Uy64bC466rT992YoCqqX4vk61+fL+SuD+by7FdLmLliAw9/GtkTSkTCqfZD7vs4drR5tB7tmoSnss0FQYLDxcCjIrJERJYA/wD+kNZSmZyVnye+YxxG9tuJ/17gzGK3T7fWMetJcP5PdWw4bUj104aY3JJoAFlQ6Zj/AZwcUcc8MoGPo9o83vx2BTvKq363srBkS0zPq2wKkj5jhqruCfQH+qvqXjg5j0wtUZP2gEQn9XjyRGLyOkXfgURvn0p3nhibNqRNNRvvTXYFybmUzHcr/Ns1auqNJHOBV0cupWEKnDNXVTe6I6UBrk5TeUwavHfFAbx+yX4Ryy6IMxYiXRKd/1MRG/5wUOJmsCP3SDxuw9Rdr/jMN56rgraPFI8aw2yfQYapVN2E6lZJXIu0b1YU0+vopmP6BMqdVN0vev9elXmaju7f0Xc0d4jf/8PDp+9VpeOlel6Lkwd1Sb6RqRUWJUgnkmvKqlAd9drU5RSPGsNTPtl3U6G6wSGHbn5MrvD+UQzb1ZmASQQeOnUADQuqNgXIcXt2ilnWpVWjuNtHhwa/aq1HzxgY+Ph+KdCNSbeBt38ceNunv3SCwu3vzk6yZfXEDQ4isklENvo8NgGx/7mmTtpr51bJN4oiUpm4r3Fhfng8RDxDilszcOeWHLJr4hn9Et1xR69q1CAyGIk4dzBBVSjcU8WU5+n0+wO68ek1Byff0JgUiftfq6rNVLW5z6OZqiZNu2Hqhp7tm1YrdXeixH1XHx45RWqjBvm8cen+MVfrX19/KF9fH6zvw/aovuEXJmmDANjXHbNx6G7tY9Yd1LsdR+6RO4PzThjYme7tbI4NkzmZncTV1Dt+qb/7dPSvsglNebprh2YAdGzRiI4tKquSEgWcpg0j7xRaN07eO6m4TROW3Hl0TIbZ8/Yvpmf7pjk1OK+o0Pl8fzqsV5ZLYuoLuwMwKVPTbnihQHJEP/+eRYl2f+mwnrRr1pAz93HyPEVXKyU8ruf5obu15/+N3M1dnjw6NGtYwKYUTAqTTPMiJ5Nul5bx211qav+ebcLJ4YyxOwcTSMcWRYG3FfE/kX816lDGXh2/3jyU82m3nZr5ro8XfM7ad2eKCvM5e2ixb6+l/Xu24dJDeiYsb8jT5w4JX6UH6WJblCQIdW2dmpN5u2bJx6p0b1ezfJiDqtG+5NWsoV1r1iUWHEwgb//xAF69eGiV3+c9v3Zq2Yie7Zuyb4827O5TtXRE350Ye/VBHBWnrj86u+Zlw3oA0LJR4iqkFy7Yl518gltod/FmvUsWHPp0bE5Zefx0B00a5DP+Ov82k3l3jEy883gSlKlJg9iTc5ABkFNvGs6Um4bza4AMqImkujuxya6sBAcROVlEZolIhYgMjlp3vYgsEJF5InJENspnYrVr1pAhxT5pMZLwu9hv2rCA96840Hf7nu397xr89tUg37lqr+k5KV4QSFStdMkhPXj7j/szbNfYxuyQRNVghXnB//VeSxCUk6UKueDAbuzXo03Cbdo0bUjbpg15Yrx/f/nvbh3hO2VttFRMoWlyR7buHGYCJwARE9aKSB/gNKAvMBL4p4hUrYO8yZp054WJPvdUVF76B97HpBsOC0+TGirvOUOLfbdNtNt8EQry83xTdcTjbUyuyqhw78f2JnTr3LJRxPH99pmKa/nmRYURU9Y2jVN9VJXYcEDPtsk3qsNqwziarAQHVZ2jqvN8Vh0PvKSq21V1MbAA2DuzpTM1JQhNGhTQsnEhfz2+b9ztThrUhXP3Kw6835H9OgDw4oX7cs9J/enQ3Kkq2ql58PaQ9s2LYkZrxzvZRZ9Y548+Mvw8lOagQUEefzioe6DPcfXhvcl3b3NEJFwtlox39q9j+lcOMWrTNLI6rSqB4L6TE2fRTeREn8mZurZuVKWG+QsOTG36loN6Jx4jk2uO9RnkmWtyrQWpM+CdGmq5u8zUAl1aNQbggF5tyc8Tpt88IuH291bxBHXrsX25cnhvtx69DRUVSusmhYzo49+7KTSlaRCvX7Ifc1dF5qqJbovwzpDnndD++qOcO5Fno+Yk9ruSfuePB/DpXCeLZ6sA3W0hcvYvbxmePGewz9aRnAGJsctPHNSFa16dEej40W4+ti+n7b0zRz40PrysqjVK+dWsCzxtSNeIyXxC9unWOuWT4DQqzK9xO0w8O3IoNXc8abtzEJGxIjLT53F8ivZ/UWjq0pKSzMyMZBLr1rYJk244LGkSvOoqyM+LaGDNyxNG9usYtyH0pYv2ZdpfDo+7P+8JbdAurcLdYEMSnb6SjeaOp0+n5vzxUKd66bS9dw50BRldl7/kzqNZcufRtHfvmEJ3QtcesWvMewvy8mJO3Hf8pl91ih6WnyfstlMzfl/N5I0nDepS7WqleNV4u3ZoxowkFyNeQdpQ3rn8gKTbVNe6LaVp23eqpC04qOpwVe3n83grwdtWAN4Wti7uMr/9P66qg1V1cLt2teuWsi5r37wobu+fTCsqzKe1T6ruIOMXIPKKu5HbvTU0QC/oVX8iTRsW8IhPgsGbj+kT8TrZRXmoWsxvBHV+nsQEh7P2rdmc3+DcVY06crfw60R3Dm9dtn/E6+uP3C09fyPuLps1LOCZc4fE7RINMPq3e4Sf+81PAk52gHc9AWJk39Rl9m1QkMdDpw1I2f7SIde6sr4NnCYiDUWkG9ALyMyceCZn3XJsn/BEQqmU7KTrPYGNu+4QoLK7ZnS3WoBWjQsjXv+uCu0pXh2i2lCi2xaihSaWKcwT2kZt26ZpA9+yBuWty28Z9fmCzsOxR+cW4ec/3HEkbRJ0r03WsyqRUHEUGLZbez648qCI9Xt7JqFq1CA/HBQ+veZgPr4qctsQ7zSl955S/XaaaEUFeRw/ILdrzLPVlfW3IrIcGAqMEZEPAVR1FvAKMBv4ALhMVdNT6WdqjfP278Z+qezdUsWL1qYNC2jfzDlhh2qw/M63X42qnAp10d+O4v+NdKp59q5GF2Cv3XZK3LOl3J0HMz9PmHJTZTXag6cO4Og9OtKsqPKkfmEVG4KfP7+yP8i4a4cx0TPdq7c278g4o9ohcvxDojmhAfYP8D2/dNG+McuUAF9r1Hf25qX78d6fDkREYi4UZt4W2Yu+V/umcTsuVEeyZJRVccY+O6dsX15ZaZBW1TeBN+OsGw2MzmyJTH1S4J6sCgP8g9514h7s3a3yajZ0textkA7xpuzwnhD/fcHeSSeNr4lQWaJPOL9x69W9dzR+gw+DatG4kBZU7ktEmHv7SLaWltOiUSG/HdiZox+eAMCfj9iVez6s7JC4S5vGLF23tdrH9vKOYA8FaW+7TNDxFi0bN6ClWz3ofcsFB3RLaSDwOrBXW8bPX1vtBnk/iVLZ10Su9VYyJu2O3bMT81Zt4tJh8VNqhJw6JPKqLFG1UjwNC/KrNJ9FVavjX7xoX96ctoLmCSZUCuLwPh0i5kO+7bj43ZBDigrzwyfrvp0qq4/22rllxHbvXH4AG3/dkXBfjRvkBzqxh7bp37kFM5ZXTgEauis5vE8H//e59wcv+955VB7Xr00mtPboPToy5vuVScsYT+MGoYGbqQsO0R0pUiXX2hyMSbvC/DyuP2p3WjQqTL5xlNAFX03q8aN9cs3BNZqroW+nFtx0TJ+4jbzekkaflLy/g6PcqVSPH9CJyTcOr3abCUDfji0iXjcvKgx3dfYz6sjdmPaXwwN1iQ1t4v28ihOEv7n+MO4+yb9tIFG6FO9xiz0N1KEr/CbuncQfDg7WE88vDbz3OH43DkHT00drkMIqKi+7czCmCvLD1Uqp22ePqF5GQZLsVYU3kHnPizNvOyL8ebwkDWVIppHnDiSZ0B1Sj3ZNY8ax+OXQiuYXQ+MF++I2TTh//26cPdS5Ou/fpSXzRx/J0L9/ytrN233fE5r/pHjUmJh1oaNEB+k/HdYrIj19VXhHzaeSBQdjqqA61UpVcXDvdgwpbs0LF+zD9GW/MLCGmVIBLj64B29Mc3qEe6+a46bBqMGxmhcVsHGbM1J6xi0jAu2sX+fmHD+gU+Bj92zfjOfP35shxa15fdrywGVLtO94X2d+nnDzsZFdiwvz86pc9RcSelv02JyapGJPV9dxq1YypgrC1Uo+DdI1Nf66YTx29iDA6bVz2bCeDK1B186Q3h2acbSb6TZRO2ho/EdN4t6NR++OCDRumE+LRoW0aJy86u7li4aGG4Z/G2BwGjhdbBs1yA+PPYh39ew3ijwVp9Lhu8dPuJhI6I4huriJ8pIlq8pK16giCw7GVEGyqo/CfKFzNa8Cu7ZuHLhqpapCdzqJGkK94wSq69QhO7P470cH6gnmp2vrxpwzNHgD610n9efaEb05pLf/yXqf7pXdiO86sT9H79GR/l1axmxX1YD42726VO0NrlCPtlBbxhPnDGZA15bhkfLjrxvG7Z58ZAtGH0m3NrGD9LxtVOkac2rVSsZUwd0n9eep8YvZp7v/Ff2cv1ZznoYUix5PEAoOuTF23ZGfJ5RXaLgHT4hfGf9+wh608xk816JRYTgdiZ/QGI8De7WlZ/umPHrmQN/t0lVNGK2yG67zKQ/v0yGid1XX1o0j2h4K4lRheUfDp6tayYKDMVXQvllRONGen1QObqquZ84bQs+oRu7dOzbnw1mrAzXYZsqMW0awdtP2QCe30/eu/kCvUANxIpmaiSJUHZlonEN0WbKVjib7f8nGmJQatmt7uraO7DZ6+aG9+N9l+7NXgAbuTE3a07RhQUS30Xj8RkSnWk27gx4cIGV4j3ZNwncofr3EQqJ//96qwL+fsAe9O8Tm0EoHCw7G1AP5ecKAri2zXYxq2TdOFV4q9ani5Dvec/vNx/ThH2fEJlCM9trF+7HNTdWdaO7xmDsH92f/Li04fe+d+eiq6o+JqQqrVjLGREjl6N3a5Kajd2fsnNXJN/To3rYJ5+1fzNZS/xRwOzUvYtXGbQC0atKA647YlXyBEXFGcUNs43ioEdubwDATLDgYYwBnFDNAh+aZHQCXSHT68nS64MDuXHBg1eYiadWkASLi22g85abhFBXm0++WD8PLurZuzIOnJb/L8Dqi705cN3LXuNPZposFB2MM4Exg9MCpe3KUOyYiW7wXzr07xJ+TIZuiY4HfHCGhiam8dw/BRN465OcJlx6SPA9YqllwMMYATq+Y6vbfTyVvtcoBvVKYqj2NEtXEfeNJcx5E0P4Ax+7ZiQPT+Pux4GCMySnVHUCXSaER3aHZ5lLZTBO0r5jfLIKpZMHBGJNTCgucM+01h/fOckni69m+Ka9ePJT+XZxG4qBTzwaRoZ7ESVlwMMbklNCYg0YJunvmgiGeGf5SeefQqWVuDFS04GCMySmXHNKD0rIK30l3clUoNqQiSIQGKnYPMEAwnSw4GGNySuMGBQlTlOSiUIqLJg1Sc0oNkvIj3Sw4GGNMDeXnCTcctRvDdq1eKu9clJVuASJysojMEpEKERnsWX64iEwVke/dn9WbN88YYzLsooN60CtHx2VUR7buHGYCJwCPRS1fCxyrqj+JSD/gQyDY7B/GGGNSJivBQVXnQGwqWlX91vNyFtBIRBqqqv9krcYYY9Iil0ebnAhMixcYROQiEZkiIlNKSkoyXDRjjKnb0nbnICJjgZ18Vt2oqm8leW9f4C5gRLxtVPVx4HGAwYMH58iwEWOMqRvSFhxUdXh13iciXYA3gXNUdWFqS2WMMSaInKpWEpGWwBhglKp+meXiGGNMvZWtrqy/FZHlwFBgjIiEEp7/EegJ3Cwi091H3ek4bIwxtUS2eiu9iVN1FL38DuCOzJfIGGOMl2RqMvF0EpESYGkNdtEWZ4xFfVHfPi/YZ64v7DNXzS6q2s5vRZ0IDjUlIlNUdXDyLeuG+vZ5wT5zfWGfOXVyqkHaGGNMbrDgYIwxJoYFB8fj2S5AhtW3zwv2mesL+8wpYm0OxhhjYtidgzHGmBgWHIwxxsSo18FBREaKyDwRWSAio7JdnpoQka4i8pmIzHYnUrrCXd5aRD4Wkfnuz1buchGRh93P/p2IDPTs63fu9vNF5HfZ+kxBiEi+iHwrIu+6r7uJyET3c70sIg3c5Q3d1wvc9cWefVzvLp8nIkdk6aMEIiItReQ1EZkrInNEZGg9+I6vcv+mZ4rIiyJSVNe+ZxF5WkTWiMhMz7KUfa8iMkicSdQWuO9NPtu1qtbLB5APLAS6Aw2AGUCfbJerBp+nIzDQfd4M+AHoA9yNk6sKYBRwl/v8KOB9nLnR9wUmustbA4vcn63c562y/fkSfO6rgf8C77qvXwFOc5//H3CJ+/xS4P/c56cBL7vP+7jffUOgm/s3kZ/tz5Xg8z4HXOA+bwC0rMvfMc5kX4uBRp7v99y69j0DBwEDgZmeZSn7XoFJ7rbivvfIpGXK9i8li1/GUOBDz+vrgeuzXa4Ufr63gMOBeUBHd1lHYJ77/DHgdM/289z1pwOPeZZHbJdLD6AL8AlwKPCu+4e/FiiI/o5xZhUc6j4vcLeT6O/du12uPYAW7olSopbX5e+4M7DMPeEVuN/zEXXxewaKo4JDSr5Xd91cz/KI7eI96nO1UuiPLmQ5dWRKUvdWei9gItBBVVe6q1YBHdzn8T5/bfq9PAhcB1S4r9sAv6hqmfvaW/bw53LXb3C3r02ftxtQAjzjVqU9KSJNqMPfsaquAO4FfgRW4nxvU6nb33NIqr7Xzu7z6OUJ1efgUCeJSFPgdeBKVd3oXafOZUOd6LssIscAa1R1arbLkkEFOFUP/1LVvYAtONUNYXXpOwZw69mPxwmMnYAmwMisFioLsvG91ufgsALo6nndxV1Wa4lIIU5geEFV33AXrxaRju76jsAad3m8z19bfi/7A8eJyBLgJZyqpYeAliISyjbsLXv4c7nrWwDrqD2fF5wrvuWqOtF9/RpOsKir3zHAcGCxqpao6g7gDZzvvi5/zyGp+l5XuM+jlydUn4PDZKCX2+uhAU7j1dtZLlO1ub0PngLmqOr9nlVvA6FeC7/DaYsILT/H7fmwL7DBvYX9EBghIq3cq7YR7rKcoqrXq2oXVS3G+e4+VdUzgc+Ak9zNoj9v6Pdwkru9ustPc3u5dAN64TTe5RxVXQUsE5Fd3UWHAbOpo9+x60dgXxFp7P6Nhz5znf2ePVLyvbrrNorIvu7v8BzPvuLLdiNMlhuAjsLp1bMQZ27rrJepBp/lAJzbzu+A6e7jKJz61k+A+cBYoLW7vQCPup/9e2CwZ1/nAwvcx3nZ/mwBPvshVPZW6o7zT78AeBVo6C4vcl8vcNd397z/Rvf3MI8AvTiy/FkHAFPc7/l/OL1S6vR3DNwGzAVmAv/G6XFUp75n4EWcNpUdOHeIv0/l9woMdn9/C4F/ENWpwe9h6TOMMcbEqM/VSsYYY+Kw4GCMMSaGBQdjjDExLDgYY4yJYcHBGGNMDAsOJieJiIrIfZ7X14rIrSna97MiclLyLWt8nJPFyZz6WdTyTiLymvt8gIgclcJjthSRS/2OZUxVWHAwuWo7cIKItM12Qbw8o3KD+D1woaoO8y5U1Z9UNRScBuCMR0lVGVriZCb1O5YxgVlwMLmqDGdu3KuiV0Rf+YvIZvfnISIyTkTeEpFFInKniJwpIpPcXPY9PLsZLiJTROQHN09TaG6Ie0Rkspsn/w+e/Y4XkbdxRudGl+d0d/8zReQud9nNOAMTnxKRe6K2L3a3bQD8FThVRKaLyKki0kSc3P6T3OR6x7vvOVdE3haRT4FPRKSpiHwiItPcYx/v7v5OoIe7v3tCx3L3USQiz7jbfysiwzz7fkNEPhBnHoC7Pb+PZ92yfi8iMd+FqbuqchVkTKY9CnwXOlkFtCewO7AeJ5/9k6q6tziTH10OXOluVwzsDfQAPhORnjhpBTao6hARaQh8KSIfudsPBPqp6mLvwUSkE3AXMAj4GfhIRH6jqn8VkUOBa1V1il9BVbXUDSKDVfWP7v7+hpPy4XwRaQlMEpGxnjL0V9X17t3Db1V1o3t39Y0bvEa55Rzg7q/Yc8jLnMPqHiKym1vW3u66ATiZfLcD80TkEaA90FlV+7n7apng927qGLtzMDlLnayyzwN/qsLbJqvqSlXdjpMqIHRy/x4nIIS8oqoVqjofJ4jshpOL5hwRmY6T7rwNTg4egEnRgcE1BPhcncRwZcALOBO3VNcIYJRbhs9x0kHs7K77WFXXu88F+JuIfIeTWqEzlSmd4zkA+A+Aqs4FlgKh4PCJqm5Q1W04d0e74PxeuovIIyIyEtjos09TR9mdg8l1DwLTgGc8y8pwL2xEJA9nRrSQ7Z7nFZ7XFUT+vUfnjVGcE+7lqhqRhE5EDsFJj50JApyoqvOiyrBPVBnOBNoBg1R1hzjZaYtqcFzv760cZyKdn0VkT5zJdS4GTsHJ3WPqAbtzMDnNvVJ+BadxN2QJTjUOwHFAYTV2fbKI5LntEN1xkrF9CFwiTupzRKS3OJPpJDIJOFhE2opIPs4sW+OqUI5NONO6hnwIXO5mz0RE9orzvhY481nscNsOdomzP6/xOEEFtzppZ5zP7cutrspT1deBm3CqtUw9YcHB1Ab3Ad5eS0/gnJBn4EwRWZ2r+h9xTuzvAxe71SlP4lSpTHMbcR8jyd21OumQR+GkkJ4BTFXV5OmQK30G9Ak1SAO34wS770RklvvazwvAYBH5HqetZK5bnnU4bSUzoxvCgX8Cee57XgbOdavf4ukMfO5Wcf0HZ6pNU09YVlZjjDEx7M7BGGNMDAsOxhhjYlhwMMYYE8OCgzHGmBgWHIwxxsSw4GCMMSaGBQdjjDEx/j8Vq1SMs4p5TQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.ylabel(\"Loss difference (log scale)\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.plot(_xaxis, np.log(loss_diff))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "853a6c1de715781c4e36e93313002732d37040606184d41a7b9ff831089b66e5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('EEML': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
